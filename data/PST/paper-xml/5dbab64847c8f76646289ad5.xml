<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A survey and critique of multiagent deep reinforcement learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pablo</forename><surname>Hernandez-Leal</surname></persName>
							<email>pablo.hernandez@borealisai.com</email>
							<idno type="ORCID">0000-0002-8530-6775</idno>
							<affiliation key="aff0">
								<orgName type="institution">Borealis AI</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bilal</forename><surname>Kartal</surname></persName>
							<email>bilal.kartal@borealisai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Borealis AI</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
							<email>matthew.taylor@borealisai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Borealis AI</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Borealis AI</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A survey and critique of multiagent deep reinforcement learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">13977B196F5E1CCCDB38007B653C7254</idno>
					<idno type="DOI">10.1007/s10458-019-09421-1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multiagent learning</term>
					<term>Multiagent systems</term>
					<term>Multiagent reinforcement learning</term>
					<term>Deep reinforcement learning</term>
					<term>Survey</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Almost 20 years ago Stone and Veloso's seminal survey <ref type="bibr" target="#b304">[305]</ref> laid the groundwork for defining the area of multiagent systems (MAS) and its open problems in the context of AI. About 10 years ago, Shoham et al. <ref type="bibr" target="#b288">[289]</ref> noted that the literature on multiagent learning (MAL) was growing and it was not possible to enumerate all relevant articles. Since then, the number of published MAL works continues to steadily rise, which led to different surveys on the area, ranging from analyzing the basics of MAL and their challenges <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b332">333]</ref>, to addressing specific subareas: game theory and MAL <ref type="bibr" target="#b232">[233,</ref><ref type="bibr" target="#b288">289]</ref>, cooperative scenarios <ref type="bibr" target="#b212">[213,</ref><ref type="bibr" target="#b247">248]</ref>, and evolutionary dynamics of MAL <ref type="bibr" target="#b37">[38]</ref>. In just the last couple of years, three surveys related to MAL have been published: learning in non-stationary environments <ref type="bibr" target="#b140">[141]</ref>, agents modeling agents <ref type="bibr" target="#b5">[6]</ref>, and transfer learning in multiagent RL <ref type="bibr" target="#b289">[290]</ref>.</p><p>The research interest in MAL has been accompanied by successes in artificial intelligence, first, in single-agent video games <ref type="bibr" target="#b220">[221]</ref>; more recently, in two-player games, for example, playing Go <ref type="bibr" target="#b290">[291,</ref><ref type="bibr" target="#b292">293]</ref>, poker <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b223">224]</ref>, and games of two competing teams, e.g., DOTA 2 <ref type="bibr" target="#b234">[235]</ref> and StarCraft II <ref type="bibr" target="#b338">[339]</ref>.</p><p>While different techniques and algorithms were used in the above scenarios, in general, they are all a combination of techniques from two main areas: reinforcement learning (RL) <ref type="bibr" target="#b314">[315]</ref> and deep learning <ref type="bibr" target="#b183">[184,</ref><ref type="bibr" target="#b280">281]</ref>.</p><p>RL is an area of machine learning where an agent learns by interacting (i.e., taking actions) within a dynamic environment. However, one of the main challenges to RL, and traditional machine learning in general, is the need for manually designing quality features on which to learn. Deep learning enables efficient representation learning, thus allowing the automatic discovery of features <ref type="bibr" target="#b183">[184,</ref><ref type="bibr" target="#b280">281]</ref>. In recent years, deep learning has had successes in different areas such as computer vision and natural language processing <ref type="bibr" target="#b183">[184,</ref><ref type="bibr" target="#b280">281]</ref>. One of the key aspects of deep learning is the use of neural networks (NNs) that can find compact representations in high-dimensional data <ref type="bibr" target="#b12">[13]</ref>.</p><p>In deep reinforcement learning (DRL) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b100">101]</ref> deep neural networks are trained to approximate the optimal policy and/or the value function. In this way the deep NN, serving as function approximator, enables powerful generalization. One of the key advantages of DRL is that it enables RL to scale to problems with high-dimensional state and action spaces. However, most existing successful DRL applications so far have been on visual domains (e.g., Atari games), and there is still a lot of work to be done for more realistic applications <ref type="bibr" target="#b358">[359,</ref><ref type="bibr" target="#b363">364]</ref> with complex dynamics, which are not necessarily vision-based.</p><p>DRL has been regarded as an important component in constructing general AI systems <ref type="bibr" target="#b178">[179]</ref> and has been successfully integrated with other techniques, e.g., search <ref type="bibr" target="#b290">[291]</ref>, planning <ref type="bibr" target="#b319">[320]</ref>, and more recently with multiagent systems, with an emerging area of multiagent deep reinforcement learning (MDRL) <ref type="bibr" target="#b231">[232,</ref><ref type="bibr" target="#b250">251]</ref>. <ref type="foot" target="#foot_0">1</ref>Learning in multiagent settings is fundamentally more difficult than the single-agent case due to the presence of multiagent pathologies, e.g., the moving target problem (non-stationarity) <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b288">289]</ref>, curse of dimensionality <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b288">289]</ref>, multiagent credit assignment <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b354">355]</ref>, global exploration <ref type="bibr" target="#b212">[213]</ref>, and relative overgeneralization <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b246">247,</ref><ref type="bibr" target="#b346">347]</ref>. Despite this complexity, top AI conferences like AAAI, ICML, ICLR, IJCAI and NeurIPS, and specialized conferences such as AAMAS, have published works reporting successes in MDRL. In light of these works, we believe it is pertinent to first, have an overview of the recent MDRL works, and second, understand how these recent works relate to the existing literature.</p><p>This article contributes to the state of the art with a brief survey of the current works in MDRL in an effort to complement existing surveys on multiagent learning <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b140">141]</ref>, cooperative learning <ref type="bibr" target="#b212">[213,</ref><ref type="bibr" target="#b247">248]</ref>, agents modeling agents <ref type="bibr" target="#b5">[6]</ref>, knowledge reuse in multiagent RL <ref type="bibr" target="#b289">[290]</ref>, and (single-agent) deep reinforcement learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b190">191]</ref>.</p><p>First, we provide a short review of key algorithms in RL such as Q-learning and REIN-FORCE (see Sect. 2.1). Second, we review DRL highlighting the challenges in this setting and reviewing recent works (see Sect. 2.2). Third, we present the multiagent setting and give an overview of key challenges and results (see <ref type="bibr">Sect. 3.1)</ref>. Then, we present the identified four categories to group recent MDRL works (see Fig. <ref type="figure" target="#fig_1">1</ref>):</p><p>-Analysis of emergent behaviors: evaluate single-agent DRL algorithms in multiagent scenarios (e.g., Atari games, social dilemmas, 3D competitive games). -Learning communication: agents learn communication protocols to solve cooperative tasks. -Learning cooperation: agents learn to cooperate using only actions and (local) observations. -Agents modeling agents: agents reason about others to fulfill a task (e.g., best response learners).</p><p>For each category we provide a description as well as outline the recent works (see Sect. 3.2 and Tables 1, 2, 3, 4). Then, we take a step back and reflect on how these new works relate to the existing literature. In that context, first, we present examples on how methods and algorithms originally introduced in RL and MAL were successfully been scaled to MDRL (see <ref type="bibr">Sect. 4.1)</ref>. Second, we provide some pointers for new practitioners in the area by describing general lessons learned from the existing MDRL works (see Sect. <ref type="bibr" target="#b3">4</ref>.2) and point to recent multiagent benchmarks (see Sect. <ref type="bibr">4.3)</ref>. Third, we take a more critical view and describe practical challenges in MDRL, such as reproducibility, hyperparameter tunning, and computational demands (see Sect. <ref type="bibr">4.4)</ref>. Then, we outline some open research questions (see Sect. <ref type="bibr">4.5)</ref>. Lastly, we present our conclusions from this work (see Sect. 5).</p><p>Our goal is to outline a recent and active area (i.e., MDRL), as well as to motivate future research to take advantage of the ample and existing literature in multiagent learning. We aim to enable researchers with experience in either DRL or MAL to gain a common understanding about recent works, and open problems in MDRL, and to avoid having scattered sub-communities with little interaction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b288">289]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Single-agent learning</head><p>This section presents the formalism of reinforcement learning and its main components before outlining deep reinforcement learning along with its particular challenges and recent algorithms. For a more detailed description we refer the reader to excellent books and surveys on the area <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b314">315,</ref><ref type="bibr" target="#b352">353]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reinforcement learning</head><p>RL formalizes the interaction of an agent with an environment using a Markov decision process (MDP) <ref type="bibr" target="#b260">[261]</ref>. An MDP is defined by the tuple S, A, R, T , γ where S represents a finite set of states. A represents a finite set of actions. The transition function T : S ×A×S → [0, 1] determines the probability of a transition from any state s ∈ S to any state s ∈ S given any possible action a ∈ A. The reward function R : S × A × S → R defines the immediate and possibly stochastic reward that an agent would receive given that the agent executes action a while in state s and it is transitioned to state s , γ ∈ [0, 1] represents the discount factor that balances the trade-off between immediate rewards and future rewards.  MDPs are adequate models to obtain optimal decisions in single agent fully observable environments. <ref type="foot" target="#foot_1">2</ref> Solving an MDP will yield a policy π : S → A, which is a mapping from states to actions. An optimal policy π * is the one that maximizes the expected discounted sum of rewards. There are different techniques for solving MDPs assuming a complete description of all its elements. One of the most common techniques is the value iteration algorithm <ref type="bibr" target="#b32">[33]</ref>, which requires a complete and accurate representation of states, actions, rewards, and transitions. However, this may be difficult to obtain in many domains. For this reason, RL algorithms often learn from experience interacting with the environment in discrete time steps.</p><p>Q-learning One of the most well known algorithms for RL is Q-learning <ref type="bibr" target="#b345">[346]</ref>. It has been devised for stationary, single-agent, fully observable environments with discrete actions. A Q-learning agent keeps the estimate of its expected payoff starting in state s, taking action a as Q(s, a). Each tabular entry Q(s, a) is an estimate of the corresponding optimal Q * function that maps state-action pairs to the discounted sum of future rewards starting with action a at state s and following the optimal policy thereafter. Each time the agent transitions from a state s to a state s via action a receiving payoff r , the Q table is updated as follows:</p><formula xml:id="formula_0">Q(s, a) ← Q(s, a) + α[(r + γ max a Q(s , a )) -Q(s, a)] (1)</formula><p>with the learning rate α ∈ [0, 1]. Q-learning is proven to converge to Q * if state and action spaces are discrete and finite, the sum of the learning rates goes to infinity (so that each state-action pair is visited infinitely often) and that the sum of the squares of the learning rates is finite (which is required to show that the convergence is with probability one) <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b153">154,</ref><ref type="bibr" target="#b167">168,</ref><ref type="bibr" target="#b317">318,</ref><ref type="bibr" target="#b318">319,</ref><ref type="bibr" target="#b328">329,</ref><ref type="bibr" target="#b345">346]</ref>. The convergence of single-step on-policy RL algorithms, i.e, SARSA (λ = 0), for both decaying exploration (greedy in the limit with infinite exploration) and persistent exploration (selecting actions probabilistically according to the ranks of the Q values) was demonstrated by Singh et al. <ref type="bibr" target="#b293">[294]</ref>. Furthermore, Van Seijen <ref type="bibr" target="#b336">[337]</ref> has proven convergence for Expected SARSA (see Sect. 3.1 for convergence results in multiagent domains).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REINFORCE (Monte Carlo policy gradient)</head><p>In contrast to value-based methods, which do not try to optimize directly over a policy space <ref type="bibr" target="#b174">[175]</ref>, policy gradient methods can learn parameterized policies without using intermediate value estimates. Policy parameters are learned by following the gradient of some performance measure with gradient descent <ref type="bibr" target="#b315">[316]</ref>. For example, REINFORCE <ref type="bibr" target="#b353">[354]</ref> uses estimated return by Monte Carlo (MC) methods with full episode trajectories to learn policy parameters θ , with π(a; s, θ) ≈ π(a; s), as follows</p><formula xml:id="formula_1">θ t+1 = θ t + αG t ∇π(A t ; S t , θ t ) π(A t ; S t , θ t )<label>(2)</label></formula><p>where G t represents the return, α is the learning rate, and A t ∼ π. A main limitation is that policy gradient methods can have high variance <ref type="bibr" target="#b174">[175]</ref>.</p><p>The policy gradient update can be generalized to include a comparison to an arbitrary baseline of the state <ref type="bibr" target="#b353">[354]</ref>. The baseline, b(s), can be any function, as long as it does not vary with the action; the baseline leaves the expected value of the update unchanged, but it can have an effect on its variance <ref type="bibr" target="#b314">[315]</ref>. A natural choice for the baseline is a learned state-value function, this reduces the variance, and it is bias-free if learned by MC. <ref type="foot" target="#foot_2">3</ref> Moreover, when using the state-value function for bootstrapping (updating the value estimate for a state from the estimated values of subsequent states) it assigns credit (reducing the variance but introducing bias), i.e., criticizes the policy's action selections. Thus, in actor-critic methods <ref type="bibr" target="#b174">[175]</ref>, the actor represents the policy, i.e., action-selection mechanism, whereas a critic is used for the value function learning. In the case when the critic learns a state-action function (Q function) and a state value function (V function), an advantage function can be computed by subtracting state values from the state-action values <ref type="bibr" target="#b282">[283,</ref><ref type="bibr" target="#b314">315]</ref>. The advantage function indicates the relative quality of an action compared to other available actions computed from the baseline, i.e., state value function. An example of an actor-critic algorithm is Deterministic Policy Gradient (DPG) <ref type="bibr" target="#b291">[292]</ref>. In DPG <ref type="bibr" target="#b291">[292]</ref> the critic follows the standard Q-learning and the actor is updated following the gradient of the policy's performance <ref type="bibr" target="#b127">[128]</ref>, DPG was later extended to DRL (see Sect. 2.2) and MDRL (see Sect. 3.5). For multiagent learning settings the variance is further increased as all the agents' rewards depend on the rest of the agents, and it is formally shown that as the number of agents increase, the probability of taking a correct gradient direction decreases exponentially <ref type="bibr" target="#b205">[206]</ref>. Recent MDRL works addressed this high variance issue, e.g., COMA <ref type="bibr" target="#b96">[97]</ref> and MADDPG <ref type="bibr" target="#b205">[206]</ref> (see Sect. 3.5).</p><p>Policy gradient methods have a clear connection with deep reinforcement learning since the policy might be represented by a neural network whose input is a representation of the state, whose output are action selection probabilities or values for continuous control <ref type="bibr" target="#b191">[192]</ref>, and whose weights are the policy parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep reinforcement learning</head><p>While tabular RL methods such as Q-learning are successful in domains that do not suffer from the curse of dimensionality, there are many limitations: learning in large state spaces can be prohibitively slow, methods do not generalize (across the state space), and state representations need to be hand-specified <ref type="bibr" target="#b314">[315]</ref>. Function approximators tried to address those limitations, using for example, decision trees <ref type="bibr" target="#b261">[262]</ref>, tile coding <ref type="bibr" target="#b313">[314]</ref>, radial basis functions <ref type="bibr" target="#b176">[177]</ref>, and locally weighted regression <ref type="bibr" target="#b45">[46]</ref> to approximate the value function.</p><p>Similarly, these challenges can be addressed by using deep learning, i.e., neural networks <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b261">262]</ref> as function approximators. For example, Q(s, a; θ) can be used to approximate the state-action values with θ representing the neural network weights. This has two advantages, first, deep learning helps to generalize across states improving the sample efficiency for large state-space RL problems. Second, deep learning can be used to reduce (or eliminate) the need for manually designing features to represent state information <ref type="bibr" target="#b183">[184,</ref><ref type="bibr" target="#b280">281]</ref>.</p><p>However, extending deep learning to RL problems comes with additional challenges including non-i.i.d. (not independently and identically distributed) data. Many supervised learning methods assume that training data is from an i.i.d. stationary distribution <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b268">269,</ref><ref type="bibr" target="#b280">281]</ref>. However, in RL, training data consists of highly correlated sequential agentenvironment interactions, which violates the independence condition. Moreover, RL training data distribution is non-stationary as the agent actively learns while exploring different parts of the state space, violating the condition of sampled data being identically distributed <ref type="bibr" target="#b219">[220]</ref>.</p><p>In practice, using function approximators in RL requires making crucial representational decisions and poor design choices can result in estimates that diverge from the optimal value function <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b333">334,</ref><ref type="bibr" target="#b350">351]</ref>. In particular, function approximation, bootstrapping, and off-policy learning are considered the three main properties that when combined, can make the learning to diverge and are known as the deadly triad <ref type="bibr" target="#b314">[315,</ref><ref type="bibr" target="#b333">334]</ref>. Recently, some works have shown that non-linear (i.e., deep) function approximators poorly estimate the value function <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b330">331]</ref> and another work found problems with Q-learning using function approximation (over/under-estimation, instability and even divergence) due to the delusional bias: "delusional bias occurs whenever a backed-up value estimate is derived from action choices that are not realizable in the underlying policy class" <ref type="bibr" target="#b206">[207]</ref>. Additionally, convergence results for reinforcement learning using function approximation are still scarce <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b206">207,</ref><ref type="bibr" target="#b216">217,</ref><ref type="bibr" target="#b329">330]</ref>; in general, stronger convergence guarantees are available for policy-gradient methods <ref type="bibr" target="#b315">[316]</ref> than for value-based methods <ref type="bibr" target="#b314">[315]</ref>.</p><p>Below we mention how the existing DRL methods aim to address these challenges when briefly reviewing value-based methods, such as DQN <ref type="bibr" target="#b220">[221]</ref>; policy gradient methods, like Proximal Policy Optimization (PPO) <ref type="bibr" target="#b282">[283]</ref>; and actor-critic methods like Asynchronous Advantage Actor-Critic (A3C) <ref type="bibr" target="#b157">[158]</ref>. We refer the reader to recent surveys on single-agent DRL <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b190">191]</ref> for a more detailed discussion of the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Value-based methods</head><p>The major breakthrough work combining deep learning with Q-learning was the Deep Q-Network (DQN) <ref type="bibr" target="#b220">[221]</ref>. DQN uses a deep neural network for function approx-Fig. <ref type="figure">2</ref> Deep Q-Network (DQN) <ref type="bibr" target="#b220">[221]</ref>: Inputs are four stacked frames; the network is composed of several layers: Convolutional layers employ filters to learn features from high-dimensional data with a much smaller number of neurons and Dense layers are fully-connected layers. The last layer represents the actions the agent can take (in this case, 10 possible actions). Deep Recurrent Q-Network (DRQN) <ref type="bibr" target="#b130">[131]</ref>, which extends DQN to partially observable domains <ref type="bibr" target="#b62">[63]</ref>, is identical to this setup except the penultimate layer (1 × 256 Dense layer) is replaced with a recurrent LSTM layer <ref type="bibr" target="#b146">[147]</ref> imation <ref type="bibr" target="#b267">[268]</ref> <ref type="foot" target="#foot_3">4</ref> (see Fig. <ref type="figure">2</ref>) and maintains an experience replay (ER) buffer <ref type="bibr" target="#b192">[193,</ref><ref type="bibr" target="#b193">194]</ref> to store interactions s, a, r , s . DQN keeps an additional copy of neural network parameters, θ -, for the target network in addition to the θ parameters to stabilize the learning, i.e., to alleviate the non-stationary data distribution. <ref type="foot" target="#foot_4">5</ref> For each training iteration i, DQN minimizes the mean-squared error (MSE) between the Q-network and its target network using the loss function:</p><formula xml:id="formula_2">L i (θ i ) = E s,a,r ,s [(r + γ max a Q(s , a ; θ - i ) -Q(s, a; θ i )) 2 ]<label>(3)</label></formula><p>where target network parameters θ -are set to Q-network parameters θ periodically and mini-batches of s, a, r , s tuples are sampled from the ER buffer, as depicted in Fig. <ref type="figure">3</ref>.</p><p>The ER buffer provides stability for learning as random batches sampled from the buffer helps alleviating the problems caused by the non-i.i.d. data. However, it comes with disadvantages, such as higher memory requirements and computation per real interaction <ref type="bibr" target="#b218">[219]</ref>. The ER buffer is mainly used for off-policy RL methods as it can cause a mismatch between buffer content from earlier policy and from the current policy for on-policy methods <ref type="bibr" target="#b218">[219]</ref>. Extending the ER buffer for the multiagent case is not trivial, see Sects. 3.5, 4.1 and 4.2. Recent works were designed to reduce the problem of catastrophic forgetting (this occurs when the trained neural network performs poorly on previously learned tasks due to a non-stationary training distribution <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b213">214]</ref>) and the ER buffer, in DRL <ref type="bibr" target="#b152">[153]</ref> and MDRL <ref type="bibr" target="#b245">[246]</ref>.</p><p>DQN has been extended in many ways, for example, by using double estimators <ref type="bibr" target="#b129">[130]</ref> to reduce the overestimation bias with Double DQN <ref type="bibr" target="#b335">[336]</ref> (see Sect. 4.1) and by decomposing the Q-function with a dueling-DQN architecture <ref type="bibr" target="#b344">[345]</ref>, where two streams are learned, one estimates state values and another one advantages, those are combined in the final layer to form Q values (this method improved over Double DQN).</p><p>In practice, DQN is trained using an input of four stacked frames (last four frames the agent has encountered). If a game requires a memory of more than four frames it will appear non-Markovian to DQN because the future game states (and rewards) do not depend only on the input (four frames) but rather on the history <ref type="bibr" target="#b131">[132]</ref>. Thus, DQN's performance declines Fig. <ref type="figure">3</ref> Representation of a DQN agent that uses an experience replay buffer <ref type="bibr" target="#b192">[193,</ref><ref type="bibr" target="#b193">194]</ref> to keep s, a, r , s tuples for minibatch updates. The Q-values are parameterized with a NN and a policy is obtained by selecting (greedily) over those at every timestep when given incomplete state observations (e.g., one input frame) since DQN assumes full state observability.</p><p>Real-world tasks often feature incomplete and noisy state information resulting from partial observability (see Sect. 2.1). Deep Recurrent Q-Networks (DRQN) <ref type="bibr" target="#b130">[131]</ref> proposed using recurrent neural networks, in particular, Long Short-Term Memory (LSTMs) cells <ref type="bibr" target="#b146">[147]</ref> in DQN, for this setting. Consider the architecture in Fig. <ref type="figure">2</ref> with the first dense layer after convolution replaced by a layer of LSTM cells. With this addition, DRQN has memory capacity so that it can even work with only one input frame rather than a stacked input of consecutive frames. This idea has been extended to MDRL, see Fig. <ref type="figure">6</ref> and Sect. 4.2. There are also other approaches to deal with partial observability such as finite state controllers <ref type="bibr" target="#b217">[218]</ref> (where action selection is performed according to the complete observation history) and using an initiation set of options conditioned on the previously employed option <ref type="bibr" target="#b301">[302]</ref>.</p><p>Policy gradient methods For many tasks, particularly for physical control, the action space is continuous and high dimensional where DQN is not suitable. Deep Deterministic Policy Gradient (DDPG) <ref type="bibr" target="#b191">[192]</ref> is a model-free off-policy actor-critic algorithm for such domains, based on the DPG algorithm <ref type="bibr" target="#b291">[292]</ref> (see Sect. 2.1). Additionally, it proposes a new method for updating the networks, i.e., the target network parameters slowly change (this could also be applicable to DQN), in contrast to the hard reset (direct weight copy) used in DQN. Given the off-policy nature, DDPG generates exploratory behavior by adding sampled noise from some noise processes to its actor policy. The authors also used batch normalization <ref type="bibr" target="#b151">[152]</ref> to ensure generalization across many different tasks without performing manual normalizations. However, note that other works have shown batch normalization can cause divergence in DRL <ref type="bibr" target="#b273">[274,</ref><ref type="bibr" target="#b334">335]</ref>.</p><p>Asynchronous Advantage Actor-Critic (A3C) <ref type="bibr" target="#b218">[219]</ref> is an algorithm that employs a parallelized asynchronous training scheme (using multiple CPU threads) for efficiency. It is an on-policy RL method that does not use an experience replay buffer. A3C allows multiple workers to simultaneously interact with the environment and compute gradients locally. All the workers pass their computed local gradients to a global NN which performs the optimization and synchronizes with the workers asynchronously (see Fig. <ref type="figure">4</ref>). There is also the Advantage Actor-Critic (A2C) method <ref type="bibr" target="#b233">[234]</ref> that combines all the gradients from all the workers to update the global NN synchronously. The loss function for A3C is composed of two terms: policy loss (actor), L π , and value loss (critic), L v . A3C parameters are updated using the advantage function A(s t , a t ; θ v ) = Q(s, a) -V (s), commonly used to reduce variance (see Sect. 2.1). An entropy loss for the policy, H (π), is also commonly added, which helps to improve exploration by discouraging premature con-Fig. <ref type="figure">4</ref> Asynchronous Advantage Actor-Critic (A3C) employs multiple (CPUs) workers without needing an ER buffer. Each worker has its own NN and independently interacts with the environment to compute the loss and gradients. Workers then pass computed gradients to the global NN that optimizes the parameters and synchronizes with the worker asynchronously. This distributed system is designed for single-agent deep RL. Compared to different DQN variants, A3C obtains better performance on a variety of Atari games using substantially less training time with multiple CPU cores of standard laptops without a GPU <ref type="bibr" target="#b218">[219]</ref>. However, we note that more recent approaches use both multiple CPU cores for more efficient training data generation and GPUs for more efficient learning vergence to suboptimal deterministic policies <ref type="bibr" target="#b218">[219]</ref>. Thus, the loss function is given by:</p><formula xml:id="formula_3">L A3C = λ v L v + λ π L π -λ H E s∼π [H (π(s, •, θ)] with λ v ,</formula><p>λ π , and λ H , being weighting terms on the individual loss components. Wang et al. <ref type="bibr" target="#b343">[344]</ref> took A3C's framework but used off-policy learning to create the Actor-critic with experience replay (ACER) algorithm. Gu et al. <ref type="bibr" target="#b117">[118]</ref> introduced the Interpolated Policy Gradient (IPG) algorithm and showed a connection between ACER and DDPG: they are a pair of reparametrization terms (they are special cases of IPG) when they are put under the same stochastic policy setting, and when the policy is deterministic they collapse into DDPG.</p><p>Jaderberg et al. <ref type="bibr" target="#b157">[158]</ref> built the Unsupervised Reinforcement and Auxiliary Learning (UNREAL) framework on top of A3C and introduced unsupervised auxiliary tasks (e.g., reward prediction) to speed up the learning process. Auxiliary tasks in general are not used for anything other than shaping the features of the agent, i.e., facilitating and regularizing the representation learning process <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b287">288]</ref>; their formalization in RL is related to the concept of general value functions <ref type="bibr" target="#b314">[315,</ref><ref type="bibr" target="#b316">317]</ref>. The UNREAL framework optimizes a combined loss function L UNREAL ≈ L A3C + i λ AT i L AT i , that combines the A3C loss, L A3C , together with auxiliary task losses L AT i , where λ AT i are weight terms (see Sect. 4.1 for use of auxiliary tasks in MDRL). In contrast to A3C, UNREAL uses a prioritized ER buffer, in which transitions with positive reward are given higher probability of being sampled. This approach can be viewed as a simple form of prioritized replay <ref type="bibr" target="#b277">[278]</ref>, which was in turn inspired by model-based RL algorithms like prioritized sweeping <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b222">223]</ref>.</p><p>Another distributed architecture is the Importance Weighted Actor-Learner Architecture (IMPALA) <ref type="bibr" target="#b92">[93]</ref>. Unlike A3C or UNREAL, IMPALA actors communicate trajectories of experience (sequences of states, actions, and rewards) to a centralized learner, thus IMPALA decouples acting from learning.</p><p>Trust Region Policy Optimization (TRPO) <ref type="bibr" target="#b282">[283]</ref> and Proximal Policy Optimization (PPO) <ref type="bibr" target="#b283">[284]</ref> are recently proposed policy gradient algorithms where the latter represents the state-of-the art with advantages such as being simpler to implement and having better empirical sample complexity. Interestingly, a recent work <ref type="bibr" target="#b150">[151]</ref> studying PPO and TRPO arrived at the surprising conclusion that these methods often deviate from what the theoretical framework would predict: gradient estimates are poorly correlated with the true gradient and value networks tend to produce inaccurate predictions for the true value function. Compared to vanilla policy gradient algorithms, PPO prevents abrupt changes in policies during training through the loss function, similar to early work by Kakade <ref type="bibr" target="#b165">[166]</ref>. Another advantage of PPO is that it can be used in a distributed fashion, i.e, Distributed PPO (DPPO) <ref type="bibr" target="#b133">[134]</ref>. Note that distributed approaches like DPPO or A3C use parallelization only to improve the learning by more efficient training data generation through multiple CPU cores for single agent DRL and they should not be considered multiagent approaches (except for recent work which tries to exploit this parallelization in a multiagent environment <ref type="bibr" target="#b18">[19]</ref>).</p><p>Lastly, there's a connection between policy gradient algorithms and Q-learning <ref type="bibr" target="#b281">[282]</ref> within the framework of entropy-regularized reinforcement learning <ref type="bibr" target="#b125">[126]</ref> where the value and Q functions are slightly altered to consider the entropy of the policy. In this vein, Soft Actor-Critic (SAC) <ref type="bibr" target="#b126">[127]</ref> is a recent algorithm that concurrently learns a stochastic policy, two Q-functions (taking inspiration from Double Q-learning) and a value function. SAC alternates between collecting experience with the current policy and updating from batches sampled from the ER buffer.</p><p>We have reviewed recent algorithms in DRL, while the list is not exhaustive, it provides an overview of the different state-of-art techniques and algorithms which will become useful while describing the MDRL techniques in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multiagent deep reinforcement learning (MDRL)</head><p>First, we briefly introduce the general framework on multiagent learning and then we dive into the categories and the research on MDRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multiagent learning</head><p>Learning in a multiagent environment is inherently more complex than in the single-agent case, as agents interact at the same time with environment and potentially with each other <ref type="bibr" target="#b54">[55]</ref>. The independent learners, a.k.a. decentralized learners approach <ref type="bibr" target="#b322">[323]</ref> directly uses singleagent algorithms in the multi-agent setting despite the underlying assumptions of these algorithms being violated (each agent independently learns its own policy, treating other agents as part of the environment). In particular the Markov property (the future dynamics, transitions, and rewards depend only on the current state) becomes invalid since the environment is no longer stationary <ref type="bibr" target="#b181">[182,</ref><ref type="bibr" target="#b232">233,</ref><ref type="bibr" target="#b332">333]</ref>. This approach ignores the multiagent nature of the setting entirely and it can fail when an opponent adapts or learns, for example, based on the past history of interactions <ref type="bibr" target="#b288">[289]</ref>. Despite the lack of guarantees, independent learners have been used in practice, providing advantages with regards to scalability while often achieving good results <ref type="bibr" target="#b212">[213]</ref>.</p><p>To understand why multiagent domains are non-stationary from agents' local perspectives, consider a simple stochastic (also known as Markov) game (S, N , A, T , R), which can be seen as an extension of an MDP to multiple agents <ref type="bibr" target="#b197">[198,</ref><ref type="bibr" target="#b199">200]</ref>. One key distinction is that the transition, T , and reward function, R, depend on the actions</p><formula xml:id="formula_4">A = A 1 × • • • × A N of all, N , agents, this means, R = R 1 × • • • × R N and T = S × A 1 × • • • × A N .</formula><p>Given a learning agent i and using the common shorthand notation -i = N \ {i} for the set of opponents, the value function now depends on the joint action a = (a i , a -i ), and the joint policy π(s, a) = j π j (s, a j ) <ref type="foot" target="#foot_5">6</ref> :</p><formula xml:id="formula_5">V π i (s) = a∈A π(s, a) s ∈S T (s, a i , a -i , s )[R i (s, a i , a -i , s ) + γ V i (s )].<label>(4)</label></formula><p>Consequently, the optimal policy is dependent on the other agents' policies,</p><formula xml:id="formula_6">π * i (s, a i , π -i ) = arg max π i V (π i ,π -i ) i (s) = arg max π i a∈A π i (s, a i )π -i (s, a -i ) s ∈S T (s, a i , a -i , s )[R i (s, a i , a -i , s ) + γ V (π i ,π -i ) i (s )].</formula><p>(5) Specifically, the opponents' joint policy π -i (s, a -i ) can be non-stationary, i.e., changes as the opponents' policies change over time, for example with learning opponents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence results</head><p>Littman <ref type="bibr" target="#b199">[200]</ref> studied convergence properties of reinforcement learning joint action agents <ref type="bibr" target="#b69">[70]</ref> in Markov games with the following conclusions: in adversarial environments (zero-sum games) an optimal play can be guaranteed against an arbitrary opponent, i.e., Minimax Q-learning <ref type="bibr" target="#b197">[198]</ref>. In coordination environments (e.g., in cooperative games all agents share the same reward function), strong assumptions need be made about other agents to guarantee convergence to optimal behavior <ref type="bibr" target="#b199">[200]</ref>, e.g., Nash Q-learning <ref type="bibr" target="#b148">[149]</ref> and Friendor-Foe Q-learning <ref type="bibr" target="#b198">[199]</ref>. In other types of environments no value-based RL algorithms with guaranteed convergence properties are known <ref type="bibr" target="#b199">[200]</ref>.</p><p>Recent work on MDRL have addressed scalability and have focused significantly less on convergence guarantees, with few exceptions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b254">255,</ref><ref type="bibr" target="#b296">297]</ref>. One notable work has shown a connection between update rules for actor-critic algorithms for multiagent partially observable settings and (counterfactual) regret minimization <ref type="foot" target="#foot_6">7</ref> : the advantage values are scaled counterfactual regrets. This lead to new convergence properties of independent RL algorithms in zero-sum games with imperfect information <ref type="bibr" target="#b299">[300]</ref>. The result is also used to support policy gradient optimization against worst-case opponents, in a new algorithm called Exploitability Descent <ref type="bibr" target="#b203">[204]</ref>. <ref type="foot" target="#foot_7">8</ref>We refer the interested reader to seminal works about convergence in multiagent domains <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b164">165,</ref><ref type="bibr" target="#b166">167,</ref><ref type="bibr" target="#b276">277,</ref><ref type="bibr" target="#b294">295,</ref><ref type="bibr" target="#b356">357,</ref><ref type="bibr" target="#b366">367]</ref>. Note that instead of convergence, some MAL algorithms have proved learning a best response against classes of opponents <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b325">326,</ref><ref type="bibr" target="#b348">349]</ref>.</p><p>There are other common problems in MAL, including action shadowing <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b346">347]</ref>, the curse of dimensionality <ref type="bibr" target="#b54">[55]</ref>, and multiagent credit assignment <ref type="bibr" target="#b1">[2]</ref>. Describing each problem is out of the scope of this survey. However, we refer the interested reader to excellent resources on general MAL <ref type="bibr" target="#b208">[209,</ref><ref type="bibr" target="#b332">333,</ref><ref type="bibr" target="#b349">350]</ref>, as well as surveys in specific areas: game theory and multiagent reinforcement learning <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b232">233]</ref>, cooperative scenarios <ref type="bibr" target="#b212">[213,</ref><ref type="bibr" target="#b247">248]</ref>, evolutionary dynamics of multiagent learning <ref type="bibr" target="#b37">[38]</ref>, learning in non-stationary environments <ref type="bibr" target="#b140">[141]</ref>, agents modeling agents <ref type="bibr" target="#b5">[6]</ref>, and transfer learning in multiagent RL <ref type="bibr" target="#b289">[290]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MDRL categorization</head><p>In Sect. 2.2 we outlined some recent works in single-agent DRL since an exhaustive list is out of the scope of this article. This explosion of works has led DRL to be extended and combined with other techniques <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b190">191,</ref><ref type="bibr" target="#b250">251]</ref>. One natural extension to DRL is to test whether these approaches could be applied in a multiagent environment.</p><p>We analyzed the most recent works (that are not covered by previous MAL surveys [6,141] and we do not consider genetic algorithms or swarm intelligence in this survey) that have a clear connection with MDRL. We propose 4 categories which take inspiration from previous surveys <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b247">248,</ref><ref type="bibr" target="#b304">305]</ref> and that conveniently describe and represent current works. Note that some of these works fit into more than one category (they are not mutually exclusive), therefore their summaries are presented in all applicable Tables 1, 2, 3 and 4, however, for the ease of exposition when describing them in the text we only do so in one category. Additionally, for each work we present its learning type, either a value-based method (e.g., DQN) or a policy gradient method (e.g., actor-critic); also, we mention if the setting is evaluated in a fully cooperative, fully competitive or mixed environment (both cooperative and competitive).</p><p>-Analysis of emergent behaviors These works, in general, do not propose learning algorithms-their main focus is to analyze and evaluate DRL algorithms, e.g., DQN <ref type="bibr" target="#b187">[188,</ref><ref type="bibr" target="#b263">264,</ref><ref type="bibr" target="#b321">322]</ref>, PPO <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b263">264]</ref> and others <ref type="bibr" target="#b186">[187,</ref><ref type="bibr" target="#b224">225,</ref><ref type="bibr" target="#b263">264]</ref>, in a multiagent environment. In this category we found works which analyze behaviors in the three major settings: cooperative, competitive and mixed scenarios; see Sect. 3.3 and Table <ref type="table" target="#tab_0">1</ref>. -Learning communication <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b182">183,</ref><ref type="bibr" target="#b224">225,</ref><ref type="bibr" target="#b252">253,</ref><ref type="bibr" target="#b255">256,</ref><ref type="bibr" target="#b311">312]</ref>. These works explore a sub-area in which agents can share information with communication protocols, for example through direct messages <ref type="bibr" target="#b95">[96]</ref> or via a shared memory <ref type="bibr" target="#b255">[256]</ref>. This area is attracting attention and it had not been explored much in the MAL literature. See Sect. 3.4 and Table <ref type="table" target="#tab_1">2</ref>. -Learning cooperation While learning to communicate is an emerging area, fostering cooperation in learning agents has a long history of research in MAL <ref type="bibr" target="#b212">[213,</ref><ref type="bibr" target="#b247">248]</ref>. In this category the analyzed works are evaluated in either cooperative or mixed settings. Some works in this category take inspiration from MAL (e.g., leniency, hysteresis, and difference rewards concepts) and extend them to the MDRL setting <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b243">244,</ref><ref type="bibr" target="#b246">247]</ref>. A notable exception <ref type="bibr" target="#b98">[99]</ref> takes a key component from RL (i.e., experience replay buffer) and adapts it for MDRL. See Sect. 3.5 and Table <ref type="table" target="#tab_2">3</ref>. -Agents modeling agents Albrecht and Stone <ref type="bibr" target="#b5">[6]</ref> presented a thorough survey in this topic and we have found many works that fit into this category in the MDRL setting, some taking inspiration from DRL <ref type="bibr" target="#b132">[133,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b264">265]</ref>, and others from MAL <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b179">180,</ref><ref type="bibr" target="#b262">263,</ref><ref type="bibr" target="#b357">358]</ref>. Modeling agents is helpful not only to cooperate, but also for modeling opponents <ref type="bibr" target="#b132">[133,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b179">180]</ref>, inferring goals <ref type="bibr" target="#b264">[265]</ref>, and accounting for the learning behavior of other agents <ref type="bibr" target="#b96">[97]</ref>. In this category the analyzed algorithms present their results in either a competitive setting or a mixed one (cooperative and competitive). See Sect. 3.6 and Table <ref type="table" target="#tab_3">4</ref>.</p><p>In the rest of this section we describe each category along with the summaries of related works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Emergent behaviors</head><p>Some recent works have analyzed the previously mentioned independent DRL agents (see Sect. 3.1) from the perspective of types of emerging behaviors (e.g., cooperative or competitive).</p><p>One of the earliest MDRL works is by Tampuu et al. <ref type="bibr" target="#b321">[322]</ref>, which had two independent DQN learning agents to play the Atari Pong game. Their focus was to adapt the reward function for the learning agents, which resulted in either cooperative or competitive emergent behaviors.</p><p>Leibo et al. <ref type="bibr" target="#b187">[188]</ref> meanwhile studied independent DQNs in the context of sequential social dilemmas: a Markov game that satisfies certain inequalities <ref type="bibr" target="#b187">[188]</ref>. The focus of this work was to highlight that cooperative or competitive behaviors exist not only as discrete (atomic) actions, but they are temporally extended (over policies). In the related setting of one shot Markov social dilemmas, Lerer and Peysakhovich <ref type="bibr" target="#b188">[189]</ref> extended the famous Tit-for-Tat (TFT) <ref type="foot" target="#foot_8">9</ref> strategy <ref type="bibr" target="#b14">[15]</ref> for DRL (using function approximators) and showed (theoretically and experimentally) that such agents can maintain cooperation. To construct the agents they used self-play and two reward schemes: selfish and cooperative. Previously, different MAL algorithms were designed to foster cooperation in social dilemmas with Q-learning agents <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b302">303]</ref>. Self-play is a useful concept for learning algorithms (e.g., fictitious play <ref type="bibr" target="#b48">[49]</ref>) since under certain classes of games it can guarantee convergence <ref type="foot" target="#foot_9">10</ref> and it has been used as a standard technique in previous RL and MAL works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b290">291,</ref><ref type="bibr" target="#b324">325]</ref>. Despite its common usage self-play can be brittle to forgetting past knowledge <ref type="bibr" target="#b179">[180,</ref><ref type="bibr" target="#b185">186,</ref><ref type="bibr" target="#b274">275]</ref> (see Sect. 4.5 for a note on the role of self-play as an open question in MDRL). To overcome this issue, Leibo et al. <ref type="bibr" target="#b186">[187]</ref> proposed Malthusian reinforcement learning as an extension of self-play to population dynamics. The approach can be thought of as community coevolution and has been shown to produce better results (avoiding local optima) than independent agents with intrinsic motivation <ref type="bibr" target="#b29">[30]</ref>. A limitation of this work is that it does not place itself within the state of the art in evolutionary and genetic algorithms. Evolutionary strategies have been employed for solving reinforcement learning problems <ref type="bibr" target="#b225">[226]</ref> and for evolving function approximators <ref type="bibr" target="#b350">[351]</ref>. Similarly, they have been used multiagent scenarios to compute approximate Nash equilibria <ref type="bibr" target="#b237">[238]</ref> and as metaheuristic optimization algorithms <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b247">248]</ref>.</p><p>Bansal et al. <ref type="bibr" target="#b23">[24]</ref> explored the emergent behaviors in competitive scenarios using the MuJoCo simulator <ref type="bibr" target="#b326">[327]</ref>. They trained independent learning agents with PPO and incorporated two main modifications to deal with the MAL nature of the problem. First, they used MADDPG <ref type="bibr" target="#b205">[206]</ref> Use an actor-critic approach where the critic is augmented with information from other agents, the actions of all agents.</p><p>PG Mixed DRON <ref type="bibr" target="#b132">[133]</ref> Have a network to infer the opponent behavior together with the standard DQN architecture.</p><p>VB Mixed DPIQN, DPIRQN <ref type="bibr" target="#b147">[148]</ref> Learn policy features from raw observations that represent high-level opponent behaviors via auxiliary tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VB Mixed</head><p>SOM <ref type="bibr" target="#b264">[265]</ref> Assume the reward function depends on a hidden goal of both agents and then use an agent's own policy to infer the goal of the other agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PG Mixed</head><p>NFSP <ref type="bibr" target="#b135">[136]</ref> Compute approximate Nash equilibria via self-play and two neural networks.</p><p>VB CMP PSRO/DCH <ref type="bibr" target="#b179">[180]</ref> Policies can overfit to opponents: better compute approximate best responses to a mixture of policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PG CO and CMP</head><p>M3DDPG <ref type="bibr" target="#b189">[190]</ref> Extend MADDPG with minimax objective to robustify the learned policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PG Mixed</head><p>LOLA <ref type="bibr" target="#b96">[97]</ref> Use a learning rule where the agent accounts for the parameter update of other agents to maximize its own reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PG Mixed</head><p>ToMnet <ref type="bibr" target="#b262">[263]</ref> Use an architecture for end-to-end learning and inference of diverse opponent types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PG Mixed</head><p>Deep Bayes-ToMoP <ref type="bibr" target="#b357">[358]</ref> Best respond to opponents using Bayesian policy reuse, theory of mind, and deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VB CMP</head><p>Deep BPR+ <ref type="bibr" target="#b365">[366]</ref> Bayesian policy reuse and policy distillation to quickly best respond to opponents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VB CO and CMP</head><p>Learning type is either value-based (VB) or policy gradient (PG). Setting where experiments were performed: cooperative (CO), competitive (CMP) or mixed. A more detailed description is given in Sect. 3.6</p><p>exploration rewards <ref type="bibr" target="#b121">[122]</ref> which are dense rewards that allow agents to learn basic (noncompetitive) behaviors-this type of reward is annealed through time giving more weight to the environmental (competitive) reward. Exploration rewards come from early work in robotics <ref type="bibr" target="#b211">[212]</ref> and single-agent RL <ref type="bibr" target="#b175">[176]</ref>, and their goal is to provide dense feedback for the learning algorithm to improve sample efficiency (Ng et al. <ref type="bibr" target="#b230">[231]</ref> studied the theoretical conditions under which modifications of the reward function of an MDP preserve the optimal policy). For multiagent scenarios, these dense rewards help agents in the beginning phase of the training to learn basic non-competitive skills, increasing the probability of random actions from the agent yielding a positive reward. The second contribution was opponent sampling which maintains a pool of older versions of the opponent to sample from, in contrast to using the most recent version. Raghu et al. <ref type="bibr" target="#b263">[264]</ref> investigated how DRL algorithms (DQN, A2C, and PPO) performed in a family of two-player zero-sum games with tunable complexity, called Erdos-Selfridge-Spencer games <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b298">299]</ref>. Their reasoning is threefold: (i) these games provide a parameterized family of environments where (ii) optimal behavior can be completely characterized, and (iii) support multiagent play. Their work showed that algorithms can exhibit wide variation in performance as the algorithms are tuned to the game's difficulty.</p><p>Lazaridou et al. <ref type="bibr" target="#b182">[183]</ref> proposed a framework for language learning that relies on multiagent communication. The agents, represented by (feed-forward) neural networks, need to develop an emergent language to solve a task. The task is formalized as a signaling game <ref type="bibr" target="#b102">[103]</ref> in which two agents, a sender and a receiver, obtain a pair of images. The sender is told one of them is the target and is allowed to send a message (from a fixed vocabulary) to the receiver. Only when the receiver identifies the target image do both agents receive a positive reward. The results show that agents can coordinate for the experimented visual-based domain. To analyze the semantic properties <ref type="foot" target="#foot_10">11</ref> of the learned communication protocol they looked whether symbol usage reflects the semantics of the visual space, and that despite some variation, many high level objects groups correspond to the same learned symbols using a t-SNE <ref type="bibr" target="#b209">[210]</ref> based analysis (t-SNE is a visualization technique for high-dimensional data and it has also been used to better understand the behavior of trained DRL agents <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b361">362]</ref>). A key objective of this work was to determine if the agent's language could be human-interpretable. To achieve this, learned symbols were grounded with natural language by extending the signaling game with a supervised image labelling task (the sender will be encouraged to use conventional names, making communication more transparent to humans). To measure the interpretability of the extended game, a crowdsourced survey was performed, and in essence, the trained agent receiver was replaced with a human. The results showed that 68% of the cases, human participants picked the correct image.</p><p>Similarly, Mordatch and Abbeel <ref type="bibr" target="#b224">[225]</ref> investigated the emergence of language with the difference that in their setting there were no explicit roles for the agents (i.e., sender or receiver). To learn, they proposed an end-to-end differentiable model of all agent and environment state dynamics over time to calculate the gradient of the return with backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning communication</head><p>As we discussed in the previous section, one of the desired emergent behaviors of multiagent interaction is the emergence of communication <ref type="bibr" target="#b182">[183,</ref><ref type="bibr" target="#b224">225]</ref>. This setting usually considers a set of cooperative agents in a partially observable environment (see Sect. 2.2) where agents need to maximize their shared utility by means of communicating information.</p><p>Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL) are two methods using deep networks to learn to communicate <ref type="bibr" target="#b95">[96]</ref>. Both methods use a neural net that outputs the agent's Q values (as done in standard DRL algorithms) and a message to communicate to other agents in the next timestep. RIAL is based on DRQN and also uses the concept of parameter sharing, i.e., using a single network whose parameters are shared among all agents. In contrast, DIAL directly passes gradients via the communication channel during learning, and messages are discretized and mapped to the set of communication actions during execution.</p><p>Memory-driven (MD) communication was proposed on top of the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) <ref type="bibr" target="#b205">[206]</ref> method. In MD-MADDPG <ref type="bibr" target="#b255">[256]</ref>, the agents use a shared memory as a communication channel: before taking an action, the agent first reads the memory, then writes a response. In this case the agent's policy becomes dependent on its private observation and its interpretation of the collective memory. Experiments were performed with two agents in cooperative scenarios. The results highlighted the fact that the communication channel was used differently in each environment, e.g., in simpler tasks agents significantly decrease their memory activity near the end of the task as there are no more changes in the environment; in more complex environments, the changes in memory usage appear at a much higher frequency due to the presence of many sub-tasks.</p><p>Dropout <ref type="bibr" target="#b300">[301]</ref> is a technique to prevent overfitting (in supervised learning this happens when the learning algorithm achieves good performance only on a specific data set and fails to generalize) in neural networks which is based on randomly dropping units and their connections during training time. Inspired by dropout, Kim et al. <ref type="bibr" target="#b172">[173]</ref> proposed a similar approach in multiagent environments where direct communication through messages is allowed. In this case, the messages of other agents are dropped out at training time, thus the authors proposed the Message-Dropout MADDPG algorithm <ref type="bibr" target="#b172">[173]</ref>. This method is expected to work in fully or limited communication environments. The empirical results show that with properly chosen message dropout rate, the proposed method both significantly improves the training speed and the robustness of learned policies (by introducing communication errors) during execution time. This capability is important as MDRL agents trained in simulated or controlled environments will be less fragile when transferred to more realistic environments.</p><p>While RIAL and DIAL used a discrete communication channel, CommNet <ref type="bibr" target="#b311">[312]</ref> used a continuous vector channel. Through this channel agents receive the summed transmissions of other agents. The authors assume full cooperation and train a single network for all the agents. There are two distinctive characteristics of CommNet from previous works: it allows multiple communication cycles at each timestep and a dynamic variation of agents at run time, i.e., agents come and go in the environment.</p><p>In contrast to previous approaches, in Multiagent Bidirectionally Coordinated Network (BiCNet) <ref type="bibr" target="#b252">[253]</ref>, communication takes place in the latent space (i.e., in the hidden layers). It also uses parameter sharing, however, it proposes bidirectional recurrent neural networks <ref type="bibr" target="#b284">[285]</ref> to model the actor and critic networks of their model. Note that in BiCNet agents do not explicitly share a message and thus it can be considered a method for learning cooperation.</p><p>Learning communication is an active area in MDRL with many open questions, in this context, we refer the interested reader to a recent work by Lowe et al. <ref type="bibr" target="#b204">[205]</ref> where it discusses common pitfalls (and recommendations to avoid those) while measuring communication in multiagent environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning cooperation</head><p>Although explicit communication is a new emerging trend in MDRL, there has already been a large amount of work in MAL for cooperative settings <ref type="foot" target="#foot_11">12</ref> that do not involve communication <ref type="bibr" target="#b212">[213,</ref><ref type="bibr" target="#b247">248]</ref>. Therefore, it was a natural starting point for many recent MDRL works.</p><p>Foerster et al. <ref type="bibr" target="#b98">[99]</ref> studied the simple scenario of cooperation with independent Q-learning agents (see Sect. 3.1), where the agents use the standard DQN architecture of neural networks and an experience replay buffer (see Fig. <ref type="figure">3</ref>). However, for the ER to work, the data distribution needs to follow certain assumptions (see Sect. 2.2) which are no loger valid due to the multiagent nature of the world: the dynamics that generated the data in the ER no longer reflect the current dynamics, making the experience obsolete <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b193">194]</ref>. Their solution is to add information to the experience tuple that can help to disambiguate the age of the sampled data from the replay memory. Two approaches were proposed. The first is Multiagent Importance Sampling which adds the probability of the joint action so an importance sampling correction <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b259">260]</ref> can computed when the tuple is later sampled for training. This was similar to previous works in adaptive importance sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b101">102]</ref> and off-environment RL <ref type="bibr" target="#b67">[68]</ref>. The second approach is Multiagent Fingerprints which adds the estimate (i.e., fingerprint) of other agents' policies (loosely inspired by Hyper-Q <ref type="bibr" target="#b325">[326]</ref>, see Sect. 4.1). For the practical implementation, good results were obtained by using the training iteration number and exploration rate as the fingerprint.</p><p>Gupta et al. <ref type="bibr" target="#b122">[123]</ref> tackled cooperative environments in partially observable domains without explicit communication. They proposed parameter sharing (PS) as a way to improve learning in homogeneous multiagent environments (where agents have the same set of actions). The idea is to have one globally shared learning network that can still behave differently in execution time, i.e., because its inputs (individual agent observation and agent index) will be different. They tested three variations of this approach with parameter sharing: PS-DQN, PS-DDPG and PS-TRPO, which extended single-agent DQN, DDPG and TRPO algorithms, respectively. The results showed that PS-TRPO outperformed the other two. Note that Foerster et al. <ref type="bibr" target="#b95">[96]</ref> concurrently proposed a similar concept, see Sect. 3.4.</p><p>Lenient-DQN (LDQN) <ref type="bibr" target="#b246">[247]</ref> took the leniency concept <ref type="bibr" target="#b36">[37]</ref> (originally presented in MAL) and extended their use to MDRL. The purpose of leniency is to overcome a pathology called relative overgeneralization <ref type="bibr" target="#b248">[249,</ref><ref type="bibr" target="#b249">250,</ref><ref type="bibr" target="#b346">347]</ref>. Similar to other approaches designed to overcome relative overgeneralization (e.g., distributed Q-learning <ref type="bibr" target="#b180">[181]</ref> and hysteretic Qlearning <ref type="bibr" target="#b212">[213]</ref>) lenient learners initially maintain an optimistic disposition to mitigate the noise from transitions resulting in miscoordination, preventing agents from being drawn towards sub-optimal but wide peaks in the reward search space <ref type="bibr" target="#b245">[246]</ref>. However, similar to other MDRL works <ref type="bibr" target="#b98">[99]</ref>, the LDQN authors experienced problems with the ER buffer and arrived at a similar solution: adding information to the experience tuple, in their case, the leniency value. When sampling from the ER buffer, this value is used to determine a leniency condition; if the condition is not met then the sample is ignored.</p><p>In a similar vein, Decentralized-Hysteretic Deep Recurrent Q-Networks (DEC-HDRQNs) <ref type="bibr" target="#b243">[244]</ref> were proposed for fostering cooperation among independent learners. The motivation is similar to LDQN, making an optimistic value update, however, their solution is different. Here, the authors took inspiration from Hysteretic Q-learning <ref type="bibr" target="#b212">[213]</ref>, originally presented in MAL, where two learning rates were used. A difference between lenient agents and hysteretic Q-learning is that lenient agents are only initially forgiving towards teammates. Lenient learners over time apply less leniency towards updates that would lower utility values, taking into account how frequently observation-action pairs have been encountered. The idea being that the transition from optimistic to average reward learner will help make lenient learners more robust towards misleading stochastic rewards <ref type="bibr" target="#b36">[37]</ref>. Additionally, in DEC-HDRQNs the ER buffer is also extended into concurrent experience replay trajectories, which are composed of three dimensions: agent index, the episode, and the timestep; when training, the sampled traces have the same starting timesteps. Moreover, to improve on generalization over different tasks, i.e., multi-task learning <ref type="bibr" target="#b61">[62]</ref>, DEC-HDRQNs make use of policy distillation <ref type="bibr" target="#b145">[146,</ref><ref type="bibr" target="#b272">273]</ref> (see Sect. 4.1). In contrast to other approaches, DEC-HDRQNS are fully decentralized during learning and execution.</p><p>Weighted Double Deep Q-Network (WDDQN) <ref type="bibr" target="#b364">[365]</ref> is based on having double estimators. This idea was originally introduced in Double Q-learning <ref type="bibr" target="#b129">[130]</ref> and aims to remove the existing overestimation bias caused by using the maximum action value as an approximation for the maximum expected action value (see Sect. 4.1). It also uses a lenient reward <ref type="bibr" target="#b36">[37]</ref> to be optimistic during initial phase of coordination and proposes a scheduled replay strategy in Fig. <ref type="figure">5</ref> A schematic view of the architecture used in FTW (For the Win) <ref type="bibr" target="#b155">[156]</ref>: two unrolled recurrent neural networks (RNNs) operate at different time-scales, the idea is that the Slow RNN helps with long term temporal correlations. Observations are latent space output of some convolutional neural network to learn non-linear features. Feudal Networks <ref type="bibr" target="#b337">[338]</ref> is another work in single-agent DRL that also maintains a multi-time scale hierarchy where the slower network sets the goal, and the faster network tries to achieve them. Fedual Networks were in turn, inspired by early work in RL which proposed a hierarchy of Q-learners <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b295">296]</ref> which samples closer to the terminal states are heuristically given higher priority; this strategy might not be applicable for any domain. For other works extending the ER to multiagent settings see MADDPG <ref type="bibr" target="#b205">[206]</ref>, Sects. 4.1 and 4.2.</p><p>While previous approaches were mostly inspired by how MAL algorithms could be extended to MDRL, other works take as base the results by single-agent DRL. One example is the For The Win (FTW) <ref type="bibr" target="#b155">[156]</ref> agent which is based on the actor-learner structure of IMPALA <ref type="bibr" target="#b92">[93]</ref> (see Sect. 2.2). The authors test FTW in a game where two opposing teams compete to capture each other's flags <ref type="bibr" target="#b56">[57]</ref>. To deal with the MAL problem they propose two main additions: a hierarchical two-level representation with recurrent neural networks operating at different timescales, as depicted in Fig. <ref type="figure">5</ref>, and a population based training <ref type="bibr" target="#b156">[157,</ref><ref type="bibr" target="#b184">185,</ref><ref type="bibr" target="#b270">271]</ref> where 30 agents were trained in parallel together with a stochastic matchmaking scheme that biases agents to be of similar skills. The Elo rating system <ref type="bibr" target="#b89">[90]</ref> was originally devised to rate chess player skills, <ref type="foot" target="#foot_12">13</ref> TrueSkill <ref type="bibr" target="#b137">[138]</ref> extended Elo by tracking uncertainty in skill rating, supporting draws, and matches beyond 1 vs 1; α-Rank is a more recent alternative to ELO <ref type="bibr" target="#b242">[243]</ref>. FTW did not use TrueSkill but a simpler extension of Elo for n vs n games (by adding individual agent ratings to compute the team skill). Hierarchical approaches were previously proposed in RL, e.g., Feudal RL <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b295">296]</ref>, and were later extended to DRL in Feudal networks <ref type="bibr" target="#b337">[338]</ref>; population based training can be considered analogous to evolutionary strategies that employ self-adaptive hyperparameter tuning to modify how the genetic algorithm itself operates <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b184">185</ref>]. An interesting result from FTW is that the populationbased training obtained better results than training via self-play <ref type="bibr" target="#b324">[325]</ref>, which was a standard concept in previous works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b290">291]</ref>. FTW used heavy compute resources, it used 30 agents (processes) in parallel where every training game lasted 4500 agent steps (≈ 5 min) and agents were trained for two billion steps (≈ 450K games).</p><p>Lowe et al. <ref type="bibr" target="#b205">[206]</ref> noted that using standard policy gradient methods (see Sect. 2.1) on multiagent environments yields high variance and performs poorly. This occurs because the variance is further increased as all the agents' rewards depend on the rest of the agents, and it is formally shown that as the number of agents increase, the probability of taking a correct gradient direction decreases exponentially <ref type="bibr" target="#b205">[206]</ref>. Therefore, to overcome this issue Lowe et al. proposed the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) <ref type="bibr" target="#b205">[206]</ref>, building on DDPG <ref type="bibr" target="#b191">[192]</ref> (see Sect. 2.2), to train a centralized critic per agent that is given all agents' policies during training to reduce the variance by removing the non-stationarity caused by the concurrently learning agents. Here, the actor only has local information (turning the method into a centralized training with decentralized execution) and the ER buffer records experiences of all agents. MADDPG was tested in both cooperative and competitive scenarios, experimental results show that it performs better than several decentralized methods (such as DQN, DDPG, and TRPO). The authors mention that traditional RL methods do not produce consistent gradient signals. This is exemplified in a challenging competitive scenarios where agents continuously adapt to each other causing the learned best-response policies oscillate-for such a domain, MADDPG is shown to learn more robustly than DDPG.</p><p>Another approach based on policy gradients is the Counterfactual Multi-Agent Policy Gradients (COMA) <ref type="bibr" target="#b97">[98]</ref>. COMA was designed for the fully centralized setting and the multiagent credit assignment problem <ref type="bibr" target="#b331">[332]</ref>, i.e., how the agents should deduce their contributions when learning in a cooperative setting in the presence of only global rewards. Their proposal is to compute a counterfactual baseline, that is, marginalize out the action of the agent while keeping the rest of the other agents' actions fixed. Then, an advantage function can be computed comparing the current Q value to the counterfactual. This counterfactual baseline has its roots in difference rewards, which is a method for obtaining the individual contribution of an agent in a cooperative multiagent team <ref type="bibr" target="#b331">[332]</ref>. In particular, the aristocrat utility aims to measure the difference between an agent's actual action and the average action <ref type="bibr" target="#b354">[355]</ref>. The intention would be equivalent to sideline the agent by having the agent perform an action where the reward does not depend on the agent's actions, i.e., to consider the reward that would have arisen assuming a world without that agent having ever existed (see Sect. 4.2).</p><p>On the one hand, fully centralized approaches (e.g., COMA) do not suffer from nonstationarity but have constrained scalability. On the other hand, independent learning agents are better suited to scale but suffer from non-stationarity issues. There are some hybrid approaches that learn a centralized but factored Q value function <ref type="bibr" target="#b118">[119,</ref><ref type="bibr" target="#b173">174]</ref>. Value Decomposition Networks (VDNs) <ref type="bibr" target="#b312">[313]</ref> decompose a team value function into an additive decomposition of the individual value functions. Similarly, QMIX <ref type="bibr" target="#b265">[266]</ref> relies on the idea of factorizing, however, instead of sum, QMIX assumes a mixing network that combines the local values in a non-linear way, which can represent monotonic action-value functions. While the mentioned approaches have obtained good empirical results, the factorization of value-functions in multiagent scenarios using function approximators (MDRL) is an ongoing research topic, with open questions such as how well factorizations capture complex coordination problems and how to learn those factorizations <ref type="bibr" target="#b63">[64]</ref> (see Sect. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Agents modeling agents</head><p>An important ability for agents to have is to reason about the behaviors of other agents by constructing models that make predictions about the modeled agents <ref type="bibr" target="#b5">[6]</ref>. An early work for Fig. <ref type="figure">6</ref> a Deep policy inference Q-network: receives four stacked frames as input (similar to DQN, see Fig. <ref type="figure">2</ref>). b Deep Policy Inference Recurrent Q-Network: receives one frame as input and has an LSTM layer instead of a fully connected layer (FC). Both approaches <ref type="bibr" target="#b147">[148]</ref> condition the Q M value outputs on the policy features, h P I , which are also used to learn the opponent policy π o modeling agents while using deep neural networks was the Deep Reinforcement Opponent Network (DRON) <ref type="bibr" target="#b132">[133]</ref>. The idea is to have two networks: one which evaluates Q-values and a second one that learns a representation of the opponent's policy. Moreover, the authors proposed to have several expert networks to combine their predictions to get the estimated Q value, the idea being that each expert network captures one type of opponent strategy <ref type="bibr" target="#b108">[109]</ref>. This is related to previous works in type-based reasoning from game theory <ref type="bibr" target="#b128">[129,</ref><ref type="bibr" target="#b166">167]</ref> later applied in AI <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b108">109]</ref>. The mixture of experts idea was presented in supervised learning where each expert handled a subset of the data (a subtask), and then a gating network decided which of the experts should be used <ref type="bibr" target="#b154">[155]</ref>.</p><p>DRON uses hand-crafted features to define the opponent network. In contrast, Deep Policy Inference Q-Network (DPIQN) and its recurrent version, DPIRQN <ref type="bibr" target="#b147">[148]</ref> learn policy features directly from raw observations of the other agents. The way to learn these policy features is by means of auxiliary tasks <ref type="bibr" target="#b157">[158,</ref><ref type="bibr" target="#b316">317]</ref> (see Sects. 2.2 and 4.1) that provide additional learning goals, in this case, the auxiliary task is to learn the opponents' policies. This auxiliary task modifies the loss function by computing an auxiliary loss: the cross entropy loss between the inferred opponent policy and the ground truth (one-hot action vector) of the opponent. Then, the Q value function of the learning agent is conditioned on the opponent's policy features (see Fig. <ref type="figure">6</ref>), which aims to reduce the non-stationarity of the environment. The authors used an adaptive training procedure to adjust the attention (a weight on the loss function) to either emphasize learning the policy features (of the opponent) or the respective Q values of the agent. An advantage of these approaches is that modeling the agents can work for both opponents and teammates <ref type="bibr" target="#b147">[148]</ref>.</p><p>In many previous works an opponent model is learned from observations. Self Other Modeling (SOM) <ref type="bibr" target="#b264">[265]</ref> proposed a different approach, this is, using the agent's own policy as a means to predict the opponent's actions. SOM can be used in cooperative and competitive settings (with an arbitrary number of agents) and infers other agents' goals. This is important because in the evaluated domains, the reward function depends on the goal of the agents. SOM uses two networks, one used for computing the agents' own policy, and a second one used to infer the opponent's goal. The idea is that these networks have the same input parameters but with different values (the agent's or the opponent's). In contrast to previous approaches, SOM is not focused on learning the opponent policy, i.e., a probability distribution over next actions, but rather on estimating the opponent's goal. SOM is expected to work best when agents share a set of goals from which each agent gets assigned one at the beginning of the episode and the reward structure depends on both of their assigned goals. Despite its simplicity, training takes longer as an additional optimization step is performed given the other agent's observed actions.</p><p>There is a long-standing history of combining game theory and MAL <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b232">233,</ref><ref type="bibr" target="#b288">289]</ref>. From that context, some approaches were inspired by influential game theory approaches. Neural Fictitious Self-Play (NFSP) <ref type="bibr" target="#b135">[136]</ref> builds on fictitious (self-) play <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b134">135]</ref>, together with two deep networks to find approximate Nash equilibria <ref type="foot" target="#foot_13">14</ref> in two-player imperfect information games <ref type="bibr" target="#b340">[341]</ref> (for example, consider Poker: when it is an agent's turn to move it does not have access to all information about the world). One network learns an approximate best response ( -greedy over Q values) to the historical behavior of other agents and the second one (called the average network) learns to imitate its own past best response behaviour using supervised classification. The agent behaves using a mixture of the average and the best response networks depending on the probability of an anticipatory parameter <ref type="bibr" target="#b286">[287]</ref>. Comparisons with DQN in Leduc Hold'em Poker revealed that DQN's deterministic strategy is highly exploitable. Such strategies are sufficient to behave optimally in single-agent domains, i.e., MDPs for which DQN was designed. However, imperfect-information games generally require stochastic strategies to achieve optimal behaviour <ref type="bibr" target="#b135">[136]</ref>. DQN learning experiences are both highly correlated over time, and highly focused on a narrow state distribution. In contrast to NFSP agents whose experience varies more smoothly, resulting in a more stable data distribution, more stable neural networks and better performance.</p><p>The (N)FSP concept was further generalized in Policy-Space Response Oracles (PSRO) <ref type="bibr" target="#b179">[180]</ref>, where it was shown that fictitious play is one specific meta-strategy distribution over a set of previous (approximate) best responses (summarized by a meta-game obtained by empirical game theoretic analysis <ref type="bibr" target="#b341">[342]</ref>), but there are a wide variety to choose from. One reason to use mixed meta-strategies is that it prevents overfitting <ref type="foot" target="#foot_14">15</ref> the responses to one specific policy, and hence provides a form of opponent/teammate regularization. An approximate scalable version of the algorithm leads to a graph of agents best-responding independently called Deep Cognitive Hierarchies (DCHs) <ref type="bibr" target="#b179">[180]</ref> due to its similarity to behavioral game-theoretic models <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b71">72]</ref>.</p><p>Minimax is a paramount concept in game theory that is roughly described as minimizing the worst case scenario (maximum loss) <ref type="bibr" target="#b340">[341]</ref>. Li et al. <ref type="bibr" target="#b189">[190]</ref> took the minimax idea as an approach to robustify learning in multiagent environments so that the learned robust policy should be able to behave well even with strategies not seen during training. They extended the MADDPG algorithm <ref type="bibr" target="#b205">[206]</ref> to Minimax Multiagent Deep Deterministic Policy Gradients (M3DDPG), which updates policies considering a worst-case scenario: assuming that all other agents act adversarially. This yields a minimax learning objective which is computationally intractable to directly optimize. They address this issue by taking ideas from robust reinforcement learning <ref type="bibr" target="#b226">[227]</ref> which implicitly adopts the minimax idea by using the worst noise concept <ref type="bibr" target="#b256">[257]</ref>. In MAL different approaches were proposed to assess the robustness of an algorithm, e.g., guarantees of safety <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b258">259]</ref>, security <ref type="bibr" target="#b72">[73]</ref> or exploitability <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b160">161,</ref><ref type="bibr" target="#b214">215]</ref>.</p><p>Previous approaches usually learned a model of the other agents as a way to predict their behavior. However, they do not explicitly account for anticipated learning of the other agents, which is the objective of Learning with Opponent-Learning Awareness (LOLA) <ref type="bibr" target="#b96">[97]</ref>. LOLA optimizes the expected return after the opponent updates its policy one step. Therefore, a LOLA agent directly shapes the policy updates of other agents to maximize its own reward. One of LOLA's assumptions is having access to opponents' policy parameters. LOLA builds on previous ideas by Zhang and Lesser <ref type="bibr" target="#b362">[363]</ref> where the learning agent predicts the opponent's policy parameter update but only uses it to learn a best response (to the anticipated updated parameters).</p><p>Theory of mind is part of a group of recursive reasoning approaches <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b109">110]</ref> in which agents have explicit beliefs about the mental states of other agents. The mental states of other agents may, in turn, also contain beliefs and mental states of other agents, leading to a nesting of beliefs <ref type="bibr" target="#b5">[6]</ref>. Theory of Mind Network (ToMnet) <ref type="bibr" target="#b262">[263]</ref> starts with a simple premise: when encountering a novel opponent, the agent should already have a strong and rich prior about how the opponent should behave. ToMnet has an architecture composed of three networks: (i) a character network that learns from historical information, (ii) a mental state network that takes the character output and the recent trajectory, and (iii) the prediction network that takes the current state as well as the outputs of the other networks as its input. The output of the architecture is open for different problems but in general its goal is to predict the opponent's next action. A main advantage of ToMnet is that it can predict general behavior, for all agents; or specific, for a particular agent.</p><p>Deep Bayesian Theory of Mind Policy (Bayes-ToMoP) <ref type="bibr" target="#b357">[358]</ref> is another algorithm that takes inspiration from theory of mind <ref type="bibr" target="#b75">[76]</ref>. The algorithm assumes the opponent has different stationary strategies to act and changes among them over time <ref type="bibr" target="#b139">[140]</ref>. Earlier work in MAL dealt with this setting, e.g., BPR+ <ref type="bibr" target="#b142">[143]</ref> extends the Bayesian policy reuse <ref type="foot" target="#foot_15">16</ref> framework <ref type="bibr" target="#b271">[272]</ref> to multiagent settings (BPR assumes a single-agent environment; BPR+ aims to best respond to the opponent in a multiagent game). A limitation of BPR+ is that it behaves poorly against itself (self-play), thus, Deep Bayes-ToMoP uses theory of mind to provide a higher-level reasoning strategy which provides an optimal behavior against BPR+ agents.</p><p>Deep BPR+ <ref type="bibr" target="#b365">[366]</ref> is another work inspired by BPR+ which uses neural networks as value-function approximators. It not only uses the environment reward but also uses the online learned opponent model <ref type="bibr" target="#b138">[139,</ref><ref type="bibr" target="#b143">144]</ref> to construct a belief over the opponent strategy. Additionally, it leverages ideas from policy distillation <ref type="bibr" target="#b145">[146,</ref><ref type="bibr" target="#b272">273]</ref> and extends them to the multiagent case to create a distilled policy network. In this case, whenever a new acting policy is learned, distillation is applied to consolidate the new updated library which improves in terms of storage and generalization (over opponents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bridging RL, MAL and MDRL</head><p>This section aims to provide directions to promote fruitful cooperations between subcommunities. First, we address the pitfall of deep learning amnesia, roughly described as missing citations to the original works and not exploiting the advancements that have been made in the past. We present examples on how ideas originated earlier, for example in RL and MAL, were successfully extended to MDRL (see Sect. 4.1). Second, we outline lessons learned from the works analyzed in this survey (see Sect. 4.2). Then we point the readers to recent benchmarks for MDRL (see Sect. 4.3) and we discuss the practical challenges that arise in MDRL like high computational demands and reproducibility (see Sect. 4.4). Lastly, we pose some open research challenges and reflect on their relation with previous open questions in MAL <ref type="bibr" target="#b5">[6]</ref> (see Sect. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Avoiding deep learning amnesia: examples in MDRL</head><p>This survey focuses on recent deep works, however, in previous sections, when describing recent algorithms, we also point to original works that inspired them. Schmidhuber said "Machine learning is the science of credit assignment. The machine learning community itself profits from proper credit assignment to its members" <ref type="bibr" target="#b279">[280]</ref>. In this context, we want to avoid committing the pitfall of not giving credit to original ideas that were proposed earlier, a.k.a. deep learning amnesia. Here, we provide some specific examples of research milestones that were studied earlier, e.g., RL or MAL, and that now became highly relevant for MDRL. Our purpose is to highlight that existent literature contains pertinent ideas and algorithms that should not be ignored. On the contrary, they should be examined and cited <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b78">79]</ref> to understand recent developments <ref type="bibr" target="#b342">[343]</ref>.</p><p>Dealing with non-stationarity in independent learners It is well known that using independent learners makes the environment non-stationary from the agent's point of view <ref type="bibr" target="#b181">[182,</ref><ref type="bibr" target="#b332">333]</ref>. Many MAL algorithms tried to solve this problem in different ways <ref type="bibr" target="#b140">[141]</ref>. One example is Hyper-Q <ref type="bibr" target="#b325">[326]</ref> which accounts for the (values of mixed) strategies of other agents and includes that information in the state representation, which effectively turns the learning problem into a stationary one. Note that in this way it is possible to even consider adaptive agents. Foerster et al. <ref type="bibr" target="#b95">[96]</ref> make use of this insight to propose their fingerprint algorithm in an MDRL problem (see Sect. <ref type="bibr">3.5)</ref>. Other examples include the leniency concept <ref type="bibr" target="#b36">[37]</ref> and Hysteretic Q-learning <ref type="bibr" target="#b212">[213]</ref> originally presented in MAL, which now have their "deep" counterparts, LDQNs <ref type="bibr" target="#b246">[247]</ref> and DEC-HDRQNs <ref type="bibr" target="#b243">[244]</ref>, see Sect. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiagent credit assignment</head><p>In cooperative multiagent scenarios, it is common to use either local rewards, unique for each agent, or global rewards, which represent the entire group's performance <ref type="bibr" target="#b2">[3]</ref>. However, local rewards are usually harder to obtain, therefore, it is common to rely only on the global ones. This raises the problem of credit assignment: how does a single agent's actions contribute a system that involves the actions of many agents <ref type="bibr" target="#b1">[2]</ref>. A solution that came from MAL research that has proven successful in many scenarios is difference rewards <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b331">332]</ref>, which aims to capture an agent's contribution to the system's global performance. In particular the aristocrat utility aims to measure the difference between an agent's actual action and the average action <ref type="bibr" target="#b354">[355]</ref>, however, it has a self-consistency problem and in practice it is more common to compute the wonderful life utility <ref type="bibr" target="#b354">[355,</ref><ref type="bibr" target="#b355">356]</ref>, which proposes to use a clamping operation that would be equivalent to removing that player from the team. COMA <ref type="bibr" target="#b97">[98]</ref> builds on these concepts to propose an advantage function based on the contribution of the agent, which can be efficiently computed with deep neural networks (see Sect. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multitask learning</head><p>In the context of RL, multitask learning <ref type="bibr" target="#b61">[62]</ref> is an area that develops agents that can act in several related tasks rather than just in a single one <ref type="bibr" target="#b323">[324]</ref>. Distillation, roughly defined as transferring the knowledge from a large model to a small model, was a concept originally introduced for supervised learning and model compression <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b145">146]</ref>. Inspired by those works, Policy distillation <ref type="bibr" target="#b272">[273]</ref> was extended to the DRL realm. Policy distillation was used to train a much smaller network and to merge several task-specific policies into a single policy, i.e., for multitask learning. In the MDRL setting, Omidshafiei et al. <ref type="bibr" target="#b243">[244]</ref> successfully adapted policy distillation within Dec-HDRQNs to obtain a more general multitask multiagent network (see Sect. 3.5). Another example is Deep BPR+ <ref type="bibr" target="#b365">[366]</ref> which uses distillation to generalize over multiple opponents (see Sect. 3.6).</p><p>Auxiliary tasks Jaderberg et al. <ref type="bibr" target="#b157">[158]</ref> introduced the term auxiliary task with the insight that (single-agent) environments contain a variety of possible training signals (e.g., pixel changes). These tasks are naturally implemented in DRL in which the last layer is split into multiple parts (heads), each working on a different task. All heads propagate errors into the same shared preceding part of the network, which would then try to form representations, in its next-to-last layer, to support all the heads <ref type="bibr" target="#b314">[315]</ref>. However, the idea of multiple predictions about arbitrary signals was originally suggested for RL, in the context of general value functions <ref type="bibr" target="#b314">[315,</ref><ref type="bibr" target="#b316">317]</ref> and there still open problems, for example, better theoretical understanding <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b87">88]</ref>. In the context of neural networks, early work proposed hints that improved the network performance and learning time. Suddarth and Kergosien <ref type="bibr" target="#b310">[311]</ref> presented a minimal example of a small neural network where it was shown that adding an auxiliary task effectively removed local minima. One could think of extending these auxiliary tasks to modeling other agents' behaviors <ref type="bibr" target="#b141">[142,</ref><ref type="bibr" target="#b224">225]</ref>, which is one of the key ideas that DPIQN and DRPIQN <ref type="bibr" target="#b147">[148]</ref> proposed in MDRL settings (see Sect. 3.6).</p><p>Experience replay Lin <ref type="bibr" target="#b192">[193,</ref><ref type="bibr" target="#b193">194]</ref> proposed the concept of experience replay to speed up the credit assignment propagation process in single agent RL. This concept became central to many DRL works <ref type="bibr" target="#b219">[220]</ref> (see Sect. 2.2). However, Lin stated that a condition for the ER to be useful is that "the environment should not change over time because this makes past experiences irrelevant or even harmful" <ref type="bibr" target="#b193">[194]</ref>. This is a problem in domains where many agents are learning since the environment becomes non-stationary from the point of view of each agent. Since DRL relies heavily on experience replay, this is an issue in MDRL: the non-stationarity introduced means that the dynamics that generated the data in the agent's replay memory no longer reflect the current dynamics in which it is learning <ref type="bibr" target="#b95">[96]</ref>. To overcome this problem different methods have been proposed <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b243">244,</ref><ref type="bibr" target="#b246">247,</ref><ref type="bibr" target="#b364">365]</ref>, see Sect. 4.2.</p><p>Double estimators Double Q-learning <ref type="bibr" target="#b129">[130]</ref> proposed to the overestimation of action values in Q-learning, this is caused by using the maximum action value as an approximation for the maximum expected action value. Double Q-learning works by keeping two Q functions and was proven to convergence to the optimal policy <ref type="bibr" target="#b129">[130]</ref>. Later this idea was applied to arbitrary function approximators, including deep neural networks, i.e., Double DQN <ref type="bibr" target="#b335">[336]</ref>, which were naturally applied since two networks were already used in DQN (see Sect. 2.2). These ideas have also been recently applied to MDRL <ref type="bibr" target="#b364">[365]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lessons learned</head><p>We have exemplified how RL and MAL can be extended for MDRL settings. Now, we outline general best practices learned from the works analyzed throughout this paper.</p><p>-Experience replay buffer in MDRL While some works removed the ER buffer in MDRL <ref type="bibr" target="#b95">[96]</ref> it is an important component in many DRL and MDRL algorithms. However, using the standard buffer (i.e., keeping s, a, r , s ) will probably fail due to a lack of theoretical guarantees under this setting, see Sects. 2.2 and 4.1. Adding information in the experience tuple that can help disambiguate the sample is the solution adopted in many works, whether a value based method <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b243">244,</ref><ref type="bibr" target="#b246">247,</ref><ref type="bibr" target="#b364">365]</ref> or a policy gradient method <ref type="bibr" target="#b205">[206]</ref>. In this regard, it is an open question to consider how new DRL ideas could be best integrated into the ER <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b195">196,</ref><ref type="bibr" target="#b277">278]</ref> and how those ideas would fare in a MDRL setting. -Centralized learning with decentralized execution Many MAL works were either fully centralized or fully decentralized approaches. However, inspired by decentralized partially observable Markov decison processes (DEC-POMDPs) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b236">237]</ref>, 17 in MDRL this new mixed paradigm has been commonly used <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b179">180,</ref><ref type="bibr" target="#b205">206,</ref><ref type="bibr" target="#b246">247,</ref><ref type="bibr" target="#b265">266]</ref> (a notable exception are DEC-HDRQNs <ref type="bibr" target="#b243">[244]</ref> which perform learning and execution in a decentralized manner, see Sect. 3.5). Note that not all real-world problems fit into this paradigm and it is more common for robotics or games where a simulator is generally available <ref type="bibr" target="#b95">[96]</ref>.</p><p>The main benefit is that during learning additional information can be used (e.g., global state, action, or rewards) and during execution this information is removed. -Parameter sharing Another frequent component in many MDRL works is the idea of sharing parameters, i.e., training a single network in which agents share their weights. Note that, since agents could receive different observations (e.g., in partially observable scenarios), they can still behave differently. This method was proposed concurrently in different works <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b123">124]</ref> and later it has been successfully applied in many others <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b252">253,</ref><ref type="bibr" target="#b265">266,</ref><ref type="bibr" target="#b311">312,</ref><ref type="bibr" target="#b312">313]</ref>. -Recurrent networks Recurrent neural networks (RNNs) enhanced neural networks with a memory capability, however, they suffer from the vanishing gradient problem, which renders them inefficient for long-term dependencies <ref type="bibr" target="#b251">[252]</ref>. However, RNN variants such as LSTMs <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b146">147]</ref> and GRUs (Gated Recurrent Unit) <ref type="bibr" target="#b66">[67]</ref> addressed this challenge. In single-agent DRL, DRQN <ref type="bibr" target="#b130">[131]</ref> initially proposed idea of using recurrent networks in single-agent partially observable environments. Then, Feudal Networks <ref type="bibr" target="#b337">[338]</ref> proposed a hierarchical approach <ref type="bibr" target="#b81">[82]</ref>, multiple LSTM networks with different time-scales, i.e., the observation input schedule is different for each LSTM network, to create a temporal hierarchy so that it can better address the long-term credit assignment challenge for RL problems. Recently, the use of recurrent networks has been extended to MDRL to 17 Centralized planning and decentralized execution is also a standard paradigm for multiagent planning <ref type="bibr" target="#b238">[239]</ref>.</p><p>address the challenge of partially <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b243">244,</ref><ref type="bibr" target="#b252">253,</ref><ref type="bibr" target="#b262">263,</ref><ref type="bibr" target="#b264">265,</ref><ref type="bibr" target="#b265">266,</ref><ref type="bibr" target="#b312">313]</ref> for example, in FTW <ref type="bibr" target="#b155">[156]</ref>, depicted in Fig. <ref type="figure">5</ref> and DRPIRQN <ref type="bibr" target="#b147">[148]</ref> depicted in Fig. <ref type="figure">6</ref>. See Sect. 4.4 for practical challenges (e.g., training issues) of recurrent networks in MDRL. -Overfitting in MAL In single-agent RL, agents can overfit to the environment <ref type="bibr" target="#b351">[352]</ref>.</p><p>A similar problem can occur in multiagent settings <ref type="bibr" target="#b159">[160]</ref>, agents can overfit, i.e., an agent's policy can easily get stuck in a local optima and the learned policy may be only locally optimal to other agents' current policies <ref type="bibr" target="#b189">[190]</ref>. This has the effect of limiting the generalization of the learned policies <ref type="bibr" target="#b179">[180]</ref>. To reduce this problem, a solution is to have a set of policies (an ensemble) and learn from them or best respond to the mixture of them <ref type="bibr" target="#b132">[133,</ref><ref type="bibr" target="#b179">180,</ref><ref type="bibr" target="#b205">206]</ref>. Another solution has been to robustify algorithms-a robust policy should be able to behave well even with strategies different from its training (better generalization) <ref type="bibr" target="#b189">[190]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmarks for MDRL</head><p>Standardized environments such as the Arcade Learning Environment (ALE) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b210">211]</ref> and OpenAI Gym <ref type="bibr" target="#b47">[48]</ref> have allowed single-agent RL to move beyond toy domains. For DRL there are open-source frameworks that provide compact and reliable implementations of some state-of-the-art DRL algorithms <ref type="bibr" target="#b64">[65]</ref>. Even though MDRL is a recent area, there are now a number of open sourced simulators and benchmarks to use with different characteristics, which we describe below.</p><p>-Fully Cooperative Multiagent Object Transporation Problems (CMOTPs) <ref type="foot" target="#foot_16">18</ref> were originally presented by Busoniu et al. <ref type="bibr" target="#b55">[56]</ref> as a simple two-agent coordination problem in MAL. Palmer et al. <ref type="bibr" target="#b246">[247]</ref> proposed two pixel-based extensions to the original setting which include narrow passages that test the agents' ability to master fully-cooperative sub-tasks, stochastic rewards and noisy observations, see Fig. <ref type="figure" target="#fig_3">7a</ref>. -The Apprentice Firemen Game <ref type="foot" target="#foot_17">19</ref> (inspired by the classic climb game <ref type="bibr" target="#b69">[70]</ref>) is another two-agent pixel-based environment that simultaneously confronts learners with four pathologies in MAL: relative overgeneralization, stochasticity, the moving target problem, and alter exploration problem <ref type="bibr" target="#b245">[246]</ref>. -Pommerman <ref type="bibr" target="#b266">[267]</ref> is a multiagent benchmark useful for testing cooperative, competitive and mixed (cooperative and competitive) scenarios. It supports partial observability and communication among agents, see Fig. <ref type="figure">7b</ref>. Pommerman is a very challenging domain from the exploration perspective as the rewards are very sparse and delayed <ref type="bibr" target="#b106">[107]</ref>. A recent competition was held during NeurIPS-2018 <ref type="foot" target="#foot_18">20</ref> and the top agents from that competition are available for training purposes. -Starcraft Multiagent Challenge <ref type="bibr" target="#b275">[276]</ref> is based on the real-time strategy game StarCraft II and focuses on micromanagement challenges, <ref type="foot" target="#foot_19">21</ref> that is, fine-grained control of individual units, where each unit is controlled by an independent agent that must act based on local observations. It is accompanied by a MDRL framework including state-of-the-art algorithms (e.g., QMIX and COMA).  -The Multi-Agent Reinforcement Learning in Malmö (MARLÖ) competition <ref type="bibr" target="#b253">[254]</ref> is another multiagent challenge with multiple cooperative 3D games <ref type="foot" target="#foot_21">23</ref> within Minecraft.</p><p>The scenarios were created with the open source Malmö platform <ref type="bibr" target="#b161">[162]</ref>, providing examples of how a wider range of multiagent cooperative, competitive and mixed scenarios can be experimented on within Minecraft. -Hanabi is a cooperative multiplayer card game (two to five players). The main characteristic of the game is that players do not observe their own cards but other players can reveal information about them. This makes an interesting challenge for learning algorithms in particular in the context of self-play learning and ad-hoc teams <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b303">304]</ref>. The Hanabi Learning Environment <ref type="bibr" target="#b24">[25]</ref> was recently released <ref type="foot" target="#foot_22">24</ref> and it is accompanied with a baseline (deep RL) agent <ref type="bibr" target="#b144">[145]</ref>. -Arena <ref type="bibr" target="#b297">[298]</ref> is platform for multiagent research <ref type="foot" target="#foot_23">25</ref> based on the Unity engine <ref type="bibr" target="#b162">[163]</ref>. It has 35 multiagent games (e.g., social dilemmas) and supports communication among agents. It has basseline implementations of recent DRL algorithms such as independent PPO learners. -MuJoCo Multiagent Soccer <ref type="bibr" target="#b202">[203]</ref> uses the MuJoCo physics engine <ref type="bibr" target="#b326">[327]</ref>. The environment simulates a 2 vs. 2 soccer game with agents having a 3-dimensional action space.<ref type="foot" target="#foot_24">26</ref> -Neural MMO <ref type="bibr" target="#b307">[308]</ref> is a research platform <ref type="foot" target="#foot_25">27</ref> inspired by the human game genre of Massively Multiplayer Online (MMO) Role-Playing Games. These games involve a large, variable number of players competing to survive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Practical challenges in MDRL</head><p>this section we take a more critical view with respect to MDRL and highlight different practical challenges that already happen in DRL and that are likely to occur in MDRL such as reproducibility, hyperparameter tuning, the need of computational resources and conflation of results. We provide pointers on how we think those challenges could be (partially) addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility, troubling trends and negative results</head><p>Reproducibility is a challenge in RL which is only aggravated in DRL due to different sources of stochasticity: baselines, hyperparameters, architectures <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b227">228]</ref> and random seeds <ref type="bibr" target="#b68">[69]</ref>. Moreover, DRL does not have common practices for statistical testing <ref type="bibr" target="#b99">[100]</ref> which has led to bad practices such as only reporting the results when algorithms perform well, sometimes referred as cherry picking <ref type="bibr" target="#b15">[16]</ref> (Azizzadenesheli also describes cherry planting as adapting an environment to a specific algorithm <ref type="bibr" target="#b15">[16]</ref>). We believe that together with following the advice on how to design experiments and report results <ref type="bibr" target="#b196">[197]</ref>, the community would also benefit from reporting negative results <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b269">270,</ref><ref type="bibr" target="#b285">286]</ref> for carefully designed hypothesis and experiments. <ref type="foot" target="#foot_26">28</ref> However, we found very few papers with this characteristic <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b169">170,</ref><ref type="bibr" target="#b207">208]</ref> -we note that this is not encouraged in the ML community; moreover, negative results reduce the chance of paper acceptance <ref type="bibr" target="#b196">[197]</ref>. In this regard, we ask the community to reflect on these practices and find ways to remove these obstacles.</p><p>Implementation challenges and hyperparameter tuning One problem is that canonical implementations of DRL algorithms often contain additional non-trivial optimizations-these are sometimes necessary for the algorithms to achieve good performance <ref type="bibr" target="#b150">[151]</ref>. A recent study by Tucker et al. <ref type="bibr" target="#b330">[331]</ref> found that several published works on action-dependant baselines contained bugs and errors-those were the real reason of the high performance in the experimental results, not the proposed method. Melis et al. <ref type="bibr" target="#b215">[216]</ref> compared a series of works with increasing innovations in network architectures and the vanilla LSTMs <ref type="bibr" target="#b146">[147]</ref> (originally proposed in 1997). The results showed that, when properly tuned, LSTMs outperformed the more recent models. In this context, Lipton and Steinhardt noted that the community may have benefited more by learning the details of the hyperparameter tuning <ref type="bibr" target="#b196">[197]</ref>. A partial reason for this surprising result might be that this type of networks are known for being difficult to train <ref type="bibr" target="#b251">[252]</ref> and there are recent works in DRL that report problems when using recurrent networks <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b122">123]</ref>. Another known complication is catastrophic forgetting (see Sect. 2.2) with recent examples in DRL <ref type="bibr" target="#b263">[264,</ref><ref type="bibr" target="#b335">336]</ref>-we expect that these issues would likely occur in MDRL. The effects of hyperparameter tuning were analyzed in more detail in DRL by Henderson et al. <ref type="bibr" target="#b136">[137]</ref>, who arrived at the conclusion that hyperparameters can have significantly different effects across algorithms (they tested TRPO, DDPG, PPO and ACKTR) and environments since there is an intricate interplay among them <ref type="bibr" target="#b136">[137]</ref>. The authors urge the community to report all parameters used in the experimental evaluations for accurate comparison-we encourage a similar behavior for MDRL. Note that hyperparameter tuning is related to the troubling trend of cherry picking in that it can show a carefully picked set of parameters that make an algorithm work (see challenge). Lastly, note that hyperparameter tuning is computationally very expensive, which relates to the connection with the following challenge of computational demands.</p><p>Computational resources Deep RL usually requires millions of interactions for an agent to learn <ref type="bibr" target="#b8">[9]</ref>, i.e., low sample efficiency <ref type="bibr" target="#b360">[361]</ref>, which highlights the need for large computational infrastructure in general. The original A3C implementation <ref type="bibr" target="#b218">[219]</ref> uses 16 CPU workers for 4 days to learn to play an Atari game with a total of 200M training frames <ref type="foot" target="#foot_27">29</ref> (results are reported for 57 Atari games). Distributed PPO used 64 workers (presumably one CPU per worker, although this is not clearly stated in the paper) for 100 hours (more than 4 days) to learn locomotion tasks <ref type="bibr" target="#b133">[134]</ref>. In MDRL, for example, the Atari Pong game, agents were trained for 50 epochs, 250k time steps each, for a total of 1.25M training frames <ref type="bibr" target="#b321">[322]</ref>. The FTW agent <ref type="bibr" target="#b155">[156]</ref> uses 30 agents (processes) in parallel and every training game lasts for 5 min; FTW agents were trained for approximately 450K games ≈ 4.2 years. These examples highlight the computational demands sometimes needed within DRL and MDRL.</p><p>Recent works have reduced the learning of an Atari game to minutes (Stooke and Abbeel <ref type="bibr" target="#b305">[306]</ref> trained DRL agents in less than one hour with hardware consisting of 8 GPUs and 40 cores). However, this is (for now) the exception and computational infrastructure is a major bottleneck for doing DRL and MDRL, especially for those who do not have such large compute power (e.g., most companies and most academic research groups) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b285">286]</ref>. <ref type="foot" target="#foot_28">30</ref> Within this context we propose two ways to address this problem. (1) Raising awareness: For DRL we found few works that study the computational demands of recent algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. For MDRL most published works do not provide information regarding computational resources used such as CPU/GPU usage, memory demands, and wall-clock computation. Therefore, the first way to tackle this issue is by raising awareness and encouraging authors to report metrics about computational demands for accurately comparison and evaluation. (2) Delve into algorithmic contributions. Another way to address these issues is to prioritize the algorithmic contribution for the new MDRL algorithms rather than the computational resources spent. Indeed, for this to work, it needs to be accompanied with high-quality reviewers.</p><p>We have argued to raise awareness on the computational demands and report results, however, there is still the open question on how and what to measure/report. There are several dimensions to measure efficiency: sample efficiency is commonly measured by counting state-action pairs used for training; computational efficiency could be measured by number of CPUs/GPUs and days used for training. How do we measure the impact of other resources, such as external data sources or annotations? <ref type="foot" target="#foot_29">31</ref> Similarly, do we need to differentiate the computational needs of the algorithm itself versus the environment it is run in? We do not have the answers, however, we point out that current standard metrics might not be entirely comprehensive.</p><p>In the end, we believe that high compute based methods act as a frontier to showcase benchmarks <ref type="bibr" target="#b234">[235,</ref><ref type="bibr" target="#b338">339]</ref>, i.e., they show what results are possible as data and compute is scaled up (e.g., OpenAI Five generates 180 years of gameplay data each day using 128,000 CPU cores and GPUs <ref type="bibr" target="#b234">[235]</ref>; AlphaStar uses 200 years of Starcraft II gameplay <ref type="bibr" target="#b338">[339]</ref>); however, lighter compute based algorithmic methods can also yield significant contributions to better tackle real-world problems.</p><p>Occam's razor and ablative analysis Finding the simplest context that exposes the innovative research idea remains challenging, and if ignored it leads to a conflation of fundamental research (working principles in the most abstract setting) and applied research (working systems as complete as possible). In particular, some deep learning papers are presented as learning from pixels without further explanation, while object-level representations would have already exposed the algorithmic contribution. This still makes sense to remain comparable with established benchmarks (e.g., OpenAI Gym <ref type="bibr" target="#b47">[48]</ref>), but less so if custom simulations are written without open source access, as it introduces unnecessary variance in pixel-level representations and artificially inflates computational resources (see previous point about computational resources). <ref type="foot" target="#foot_30">32</ref> In this context there are some notable exceptions where the algorithmic contribution is presented in a minimal setting and then results are scaled into complex settings: LOLA <ref type="bibr" target="#b96">[97]</ref> first presented a minimalist setting with a two-player twoaction game and then with a more complex variant; similarly, QMIX <ref type="bibr" target="#b265">[266]</ref> presented its results in a two-step (matrix) game and then in the more involved Starcraft II micromanagement domain <ref type="bibr" target="#b275">[276]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Open questions</head><p>Finally, here we present some open questions for MDRL and point to suggestions on how to approach them. We believe that there are solid ideas in earlier literature and we refer the reader to Sect. 4.1 to avoid deep learning amnesia.</p><p>-On the challenge of sparse and delayed rewards.</p><p>Recent MDRL competitions and environments have complex scenarios where many actions are taken before a reward signal is available (see Sect. 4.3). This sparseness is already a challenge for RL <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b314">315]</ref> where approaches such as count-based exploration/intrinsic motivation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b278">279,</ref><ref type="bibr" target="#b306">307]</ref> and hierarchical learning <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b177">178,</ref><ref type="bibr" target="#b277">278]</ref> have been proposed to address it-in MDRL this is even more problematic since the agents not only need to learn basic behaviors (like in DRL), but also to learn the strategic element (e.g., competitive/collaborative) embedded in the multiagent setting. To address this issue, recent MDRL approaches applied dense rewards <ref type="bibr" target="#b175">[176,</ref><ref type="bibr" target="#b211">212,</ref><ref type="bibr" target="#b230">231]</ref> (a concept originated in RL) at each step to allow the agents to learn basic motor skills and then decrease these dense rewards over time in favor of the environmental reward <ref type="bibr" target="#b23">[24]</ref>, see Sect. 3.3. Recent works like OpenAI Five <ref type="bibr" target="#b234">[235]</ref> uses hand-crafted intermediate rewards to accelerate the learning and FTW <ref type="bibr" target="#b155">[156]</ref> lets the agents learn their internal rewards by a hierarchical two-tier optimization. In single agent domains, RUDDER <ref type="bibr" target="#b11">[12]</ref> has been recently proposed for such delayed sparse reward problems. RUDDER generates a new MDP with more intermediate rewards whose optimal solution is still an optimal solution to the original MDP. This is achieved by using LSTM networks to redistribute the original sparse reward to earlier state-action pairs and automatically provide reward shaping. How to best extend RUDDER to multiagent domains is an open avenue of research. -On the role of self-play.</p><p>Self-play is a cornerstone in MAL with impressive results <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b148">149]</ref>. While notable results had also been shown in MDRL recent works have also shown that plain self-play does not yield the best results. However, adding diversity, i.e., evolutionary methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b184">185,</ref><ref type="bibr" target="#b270">271]</ref> or sampling-based methods, have shown good results <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b155">156,</ref><ref type="bibr" target="#b186">187]</ref>. A drawback of these solutions is the additional computational requirements since they need either parallel training (more CPU computation) or memory requirements. Then, it is still an open problem to improve the computational efficiency of these previously proposed successful methods, i.e., achieving similar training stability with smaller population sizes that uses fewer CPU workers in MAL and MDRL (see Sect. Monte Carlo tree search (MCTS) <ref type="bibr" target="#b50">[51]</ref> has been the backbone of the major breakthroughs behind AlphaGo <ref type="bibr" target="#b290">[291]</ref> and AlphaGo Zero <ref type="bibr" target="#b292">[293]</ref> that combined search and DRL. A recent work <ref type="bibr" target="#b339">[340]</ref> has outlined how search and RL can be better combined for potentially new methods. However, for multiagent scenarios, there is an additional challenge of the exponential growth of all the agents' action spaces for centralized methods <ref type="bibr" target="#b168">[169]</ref>. One way to tackle this challenge within multiagent scenarios is the use of search parallelization <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b170">171]</ref>. Given more scalable planners, there is room for research in combining these techniques in MDRL settings.</p><p>To learn complex multiagent interactions some type of abstraction <ref type="bibr" target="#b83">[84]</ref> is often needed, for example, factored value functions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b118">[119]</ref><ref type="bibr" target="#b119">[120]</ref><ref type="bibr" target="#b120">[121]</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b235">236]</ref> (see QMIX and VDN in Sect. 3.5 for recent work in MDRL) try to exploit independence among agents through (factored) structure; however, in MDRL there are still open questions such as understanding their representational power <ref type="bibr" target="#b63">[64]</ref> (e.g., the accuracy of the learned Q-function approximations) and how to learn those factorizations, where ideas from transfer planning techniques could be useful <ref type="bibr" target="#b239">[240,</ref><ref type="bibr" target="#b334">335]</ref>. In transfer planning the idea is to define a simpler "source problem" (e.g., with fewer agents), in which the agent(s) can plan <ref type="bibr" target="#b239">[240]</ref> or learn <ref type="bibr" target="#b334">[335]</ref>; since it is less complex than the real multiagent problem, issues such as the non-stationarity of the environment can be reduced/removed. Lastly, another related idea are influence abstractions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b240">241]</ref>, where instead of learning a complex multiagent model, these methods try to build smaller models based on the influence agents can exert on one another. While this has not been sufficiently explored in actual multiagent settings, there is some evidence that these ideas can lead to effective inductive biases, improving effectiveness of DRL in such local abstractions <ref type="bibr" target="#b308">[309]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Deep reinforcement learning has shown recent success on many fronts <ref type="bibr" target="#b220">[221,</ref><ref type="bibr" target="#b223">224,</ref><ref type="bibr" target="#b290">291]</ref> and a natural next step is to test multiagent scenarios. However, learning in multiagent environments is fundamentally more difficult due to non-stationarity, the increase of dimensionality, and the credit-assignment problem, among other factors <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b245">246,</ref><ref type="bibr" target="#b304">305,</ref><ref type="bibr" target="#b331">332,</ref><ref type="bibr" target="#b347">348]</ref>. This survey provides a broad overview of recent works in the emerging area of Multiagent Deep Reinforcement Learning (MDRL). First, we categorized recent works into four different topics: emergent behaviors, learning communication, learning cooperation, and agents modeling agents. Then, we exemplified how key components (e.g., experience replay and difference rewards) originated in RL and MAL need to be adapted to work in MDRL. We provided general lessons learned applicable to MDRL, pointed to recent multiagent benchmarks and highlighted some open research problems. Finally, we also reflected the practical challenges such as computational demands and reproducibility in MDRL.</p><p>Our conclusions of this work are that while the number of works in DRL and MDRL are notable and represent important milestones for AI, at the same time we acknowledge there are also open questions in both (deep) single-agent learning <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b210">211,</ref><ref type="bibr" target="#b327">328]</ref> and multiagent learning <ref type="bibr" target="#b115">[116,</ref><ref type="bibr" target="#b171">172,</ref><ref type="bibr" target="#b194">195,</ref><ref type="bibr" target="#b241">242,</ref><ref type="bibr" target="#b244">245,</ref><ref type="bibr" target="#b359">360]</ref>. Our view is that there are practical issues within MDRL that hinder its scientific progress: the necessity of high compute power, complicated reproducibility (e.g., hyperparameter tuning), and the lack of sufficient encouragement for publishing negative results. However, we remain highly optimistic of the multiagent community and hope this work serves to raise those issues, encounter good solutions, and ultimately take advantage of the existing literature and resources available to move the area in the right direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Analysis of emergent behaviors (b) Learning communication (c) Learning cooperation (d) Agents modeling agents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Categories of different MDRL works. a Analysis of emergent behaviors: evaluate single-agent DRL algorithms in multiagent scenarios. b Learning communication: agents learn with actions and through messages. c Learning cooperation: agents learn to cooperate using only actions and (local) observations. d Agents modeling agents: agents reason about others to fulfill a task (e.g., cooperative or competitive). For a more detailed description see Sects. 3.3-3.6 and Tables 1, 2, 3 and 4</figDesc><graphic coords="4,49.59,56.03,340.36,224.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>22</head><label>22</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Multiagent object transportation (b) Pommerman 7 a A fully cooperative benchmark with two agents, Multiagent Object Trasportation. b A mixed cooperative-competitive domain with four agents, Pommerman. For more MDRL benchmarks see Sect. 4.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 . 4 and</head><label>44</label><figDesc>Albrecht et al. [6, Section 5.5]).-On the challenge of the combinatorial nature of MDRL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,49.59,55.55,340.36,207.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="20,49.59,56.15,340.84,186.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="22,49.59,55.58,340.36,328.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>These papers analyze emergent behaviors in MDRL</figDesc><table><row><cell>Work</cell><cell>Summary</cell><cell>Learning</cell><cell>Setting</cell></row><row><cell>Tampuu et al. [322]</cell><cell>Train DQN agents to play Pong.</cell><cell>VB</cell><cell>CO and CMP</cell></row><row><cell>Leibo et al. [188]</cell><cell>Train DQN agents to play sequential</cell><cell>VB</cell><cell>Mixed</cell></row><row><cell></cell><cell>social dilemmas.</cell><cell></cell><cell></cell></row><row><cell>Lerer and Peysakhovich [189]</cell><cell>Propose DRL agents able to</cell><cell>VB</cell><cell>Mixed</cell></row><row><cell></cell><cell>cooperate in social dilemmas.</cell><cell></cell><cell></cell></row><row><cell>Leibo et al. [187]</cell><cell>Propose Malthusian reinforcement</cell><cell>VB</cell><cell>Mixed</cell></row><row><cell></cell><cell>learning which extends self-play to</cell><cell></cell><cell></cell></row><row><cell></cell><cell>population dynamics.</cell><cell></cell><cell></cell></row><row><cell>Bansal et al. [24]</cell><cell>Train PPO agents in competitive</cell><cell>PG</cell><cell>CMP</cell></row><row><cell></cell><cell>MuJoCo scenarios.</cell><cell></cell><cell></cell></row><row><cell>Raghu et al. [264]</cell><cell>Train PPO, A3C, and DQN agents in</cell><cell>VB, PG</cell><cell>CMP</cell></row><row><cell></cell><cell>attacker-defender games.</cell><cell></cell><cell></cell></row><row><cell>Lazaridou et al. [183]</cell><cell>Train agents represented with NN to</cell><cell>PG</cell><cell>CO</cell></row><row><cell></cell><cell>learn a communication language.</cell><cell></cell><cell></cell></row><row><cell>Mordatch and Abbeel [225]</cell><cell>Learn communication with an</cell><cell></cell><cell></cell></row><row><cell></cell><cell>end-to-end differentiable model to</cell><cell></cell><cell></cell></row><row><cell></cell><cell>train with backpropagation.</cell><cell></cell><cell></cell></row></table><note><p><p>PG CO</p>Learning type is either value-based (VB) or policy gradient (PG). Setting where experiments were performed: cooperative (CO), competitive (CMP) or mixed. A detailed description is given in Sect. 3.3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>These papers propose algorithms for learning communication</figDesc><table><row><cell>Algorithm</cell><cell>Summary</cell><cell>Learning</cell><cell>Setting</cell></row><row><cell>Lazaridou et al. [183]</cell><cell>Train agents represented with NN to</cell><cell>PG</cell><cell>CO</cell></row><row><cell></cell><cell>learn a communication language.</cell><cell></cell><cell></cell></row><row><cell>Mordatch and Abbeel [225]</cell><cell>Learn communication with an</cell><cell>PG</cell><cell>CO</cell></row><row><cell></cell><cell>end-to-end differentiable model to</cell><cell></cell><cell></cell></row><row><cell></cell><cell>train with backpropagation.</cell><cell></cell><cell></cell></row><row><cell>RIAL [96]</cell><cell>Use a single network (parameter</cell><cell>VB</cell><cell>CO</cell></row><row><cell></cell><cell>sharing) to train agents that take</cell><cell></cell><cell></cell></row><row><cell></cell><cell>environmental and communication</cell><cell></cell><cell></cell></row><row><cell></cell><cell>actions.</cell><cell></cell><cell></cell></row><row><cell>DIAL [96]</cell><cell>Use gradient sharing during learning</cell><cell>VB</cell><cell>CO</cell></row><row><cell></cell><cell>and communication actions during</cell><cell></cell><cell></cell></row><row><cell></cell><cell>execution.</cell><cell></cell><cell></cell></row><row><cell>CommNet [312]</cell><cell>Use a continuous vector channel for</cell><cell>PG</cell><cell>CO</cell></row><row><cell></cell><cell>communication on a single</cell><cell></cell><cell></cell></row><row><cell></cell><cell>network.</cell><cell></cell><cell></cell></row><row><cell>BiCNet [253]</cell><cell>Use the actor-critic paradigm where</cell><cell>PG</cell><cell>Mixed</cell></row><row><cell></cell><cell>communication occurs in the latent</cell><cell></cell><cell></cell></row><row><cell></cell><cell>space.</cell><cell></cell><cell></cell></row><row><cell>MD-MADDPG [256]</cell><cell>Use of a shared memory as a means</cell><cell>PG</cell><cell>CO</cell></row><row><cell></cell><cell>to multiagent communication.</cell><cell></cell><cell></cell></row><row><cell>MADDPG-MD [173]</cell><cell>Extend dropout technique to</cell><cell></cell><cell></cell></row><row><cell></cell><cell>robustify communication when</cell><cell></cell><cell></cell></row><row><cell></cell><cell>applied in multiagent scenarios</cell><cell></cell><cell></cell></row><row><cell></cell><cell>with direct communication.</cell><cell></cell><cell></cell></row></table><note><p><p>PG CO</p>Learning type is either value-based (VB) or policy gradient (PG). Setting were experiments were performed: cooperative (CO) or mixed. A more detailed description is given in Sect. 3.4</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>These papers aim to learn cooperation</figDesc><table><row><cell>Algorithm</cell><cell>Summary</cell><cell cols="2">Learning Setting</cell></row><row><cell>Lerer and Peysakhovich [189]</cell><cell>Propose DRL agents able to cooperate in</cell><cell>VB</cell><cell>Mixed</cell></row><row><cell></cell><cell>social dilemmas</cell><cell></cell><cell></cell></row><row><cell>MD-MADDPG [256]</cell><cell>Use of a shared memory as a means to</cell><cell>PG</cell><cell>CO</cell></row><row><cell></cell><cell>multiagent communication</cell><cell></cell><cell></cell></row><row><cell>MADDPG-MD [173]</cell><cell>Extend dropout technique to robustify</cell><cell>PG</cell><cell>CO</cell></row><row><cell></cell><cell>communication when applied in multiagent</cell><cell></cell><cell></cell></row><row><cell></cell><cell>scenarios with direct communication</cell><cell></cell><cell></cell></row><row><cell>RIAL [96]</cell><cell>Use a single network (parameter sharing) to</cell><cell>VB</cell><cell>CO</cell></row><row><cell></cell><cell>train agents that take environmental and</cell><cell></cell><cell></cell></row><row><cell></cell><cell>communication actions</cell><cell></cell><cell></cell></row><row><cell>DIAL [96]</cell><cell>Use gradient sharing during learning and</cell><cell>VB</cell><cell>CO</cell></row><row><cell></cell><cell>communication actions during execution</cell><cell></cell><cell></cell></row><row><cell>DCH/PSRO [180]</cell><cell>Policies can overfit to opponents: better</cell><cell>VB</cell><cell>CO and CMP</cell></row><row><cell></cell><cell>compute approximate best responses to a</cell><cell></cell><cell></cell></row><row><cell></cell><cell>mixture of policies</cell><cell></cell><cell></cell></row><row><cell>Fingerprints [99]</cell><cell>Deal with ER problems in MDRL by</cell><cell>VB</cell><cell>CO</cell></row><row><cell></cell><cell>conditioning the value function on a</cell><cell></cell><cell></cell></row><row><cell></cell><cell>fingerprint that disambiguates the age of</cell><cell></cell><cell></cell></row><row><cell></cell><cell>the sampled data</cell><cell></cell><cell></cell></row><row><cell>Lenient-DQN [247]</cell><cell>Achieve cooperation by leniency, optimism</cell><cell>VB</cell><cell>CO</cell></row><row><cell></cell><cell>in the value function by forgiving</cell><cell></cell><cell></cell></row><row><cell></cell><cell>suboptimal (low-rewards) actions</cell><cell></cell><cell></cell></row><row><cell>Hysteretic-DRQN [244]</cell><cell>Achieve cooperation by using two learning</cell><cell>VB</cell><cell>CO</cell></row><row><cell></cell><cell>rates, depending on the updated values</cell><cell></cell><cell></cell></row><row><cell></cell><cell>together with multitask learning via policy</cell><cell></cell><cell></cell></row><row><cell></cell><cell>distillation</cell><cell></cell><cell></cell></row><row><cell>WDDQN [365]</cell><cell>Achieve cooperation by leniency, weighted</cell><cell>VB</cell><cell>CO</cell></row><row><cell></cell><cell>double estimators, and a modified</cell><cell></cell><cell></cell></row><row><cell></cell><cell>prioritized experience replay buffer</cell><cell></cell><cell></cell></row><row><cell>FTW [156]</cell><cell>Agents act in a mixed environment</cell><cell>PG</cell><cell>Mixed</cell></row><row><cell></cell><cell>(composed of teammates and opponents),</cell><cell></cell><cell></cell></row><row><cell></cell><cell>it proposes a two-level architecture and</cell><cell></cell><cell></cell></row><row><cell></cell><cell>population-based learning</cell><cell></cell><cell></cell></row><row><cell>VDN [313]</cell><cell>Decompose the team action-value function</cell><cell>VB</cell><cell>Mixed</cell></row><row><cell></cell><cell>into pieces across agents, where the pieces</cell><cell></cell><cell></cell></row><row><cell></cell><cell>can be easily added</cell><cell></cell><cell></cell></row><row><cell>QMIX [266]</cell><cell>Decompose the team action-value function</cell><cell>VB</cell><cell>Mixed</cell></row><row><cell></cell><cell>together with a mixing network that can</cell><cell></cell><cell></cell></row><row><cell></cell><cell>recombine them</cell><cell></cell><cell></cell></row><row><cell>COMA [98]</cell><cell>Use a centralized critic and a counter-factual</cell><cell>PG</cell><cell>Mixed</cell></row><row><cell></cell><cell>advantage function based on solving the</cell><cell></cell><cell></cell></row><row><cell></cell><cell>multiagent credit assignment</cell><cell></cell><cell></cell></row><row><cell cols="2">PS-DQN, PS-TRPO, PS-A3C [123] Propose parameter sharing for learning</cell><cell cols="2">VB, PG CO</cell></row><row><cell></cell><cell>cooperative tasks</cell><cell></cell><cell></cell></row><row><cell>MADDPG [206]</cell><cell>Use an actor-critic approach where the critic</cell><cell>PG</cell><cell>Mixed</cell></row><row><cell></cell><cell>is augmented with information from other</cell><cell></cell><cell></cell></row><row><cell></cell><cell>agents, the actions of all agents</cell><cell></cell><cell></cell></row></table><note><p><p>Learning type is either value-based (VB) or policy gradient (PG). Setting where experiments were performed: cooperative (CO), competitive (CMP) or mixed. A more detailed description is given in Sect.</p>3.5   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>These papers consider agents modeling agents</figDesc><table><row><cell>Algorithm</cell><cell>Summary</cell><cell>Learning</cell><cell>Setting</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We have noted inconsistency in abbreviations such as: D-MARL, MADRL, deep-multiagent RL and MA-DRL.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A Partially Observable Markov Decision Process (POMDP)<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b62">63]</ref> explicitly models environments where the agent no longer sees the true system state and instead receives an observation (generated from the underlying system state).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Action-dependant baselines had been proposed<ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b201">202]</ref>, however, a recent study by Tucker et al.<ref type="bibr" target="#b330">[331]</ref> found that in many works the reason of good performance was because of bugs or errors in the code, rather than the proposed method itself.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Before DQN, many approaches used neural networks for representing the Q-value function<ref type="bibr" target="#b73">[74]</ref>, such as Neural Fitted Q-learning<ref type="bibr" target="#b267">[268]</ref> and NEAT+Q<ref type="bibr" target="#b350">[351]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Double Q-learning<ref type="bibr" target="#b129">[130]</ref> originally proposed keeping two Q functions (estimators) to reduce the overestimation bias in RL, while still keeping the convergence guarantees, later it was extended to DRL in Double DQN<ref type="bibr" target="#b335">[336]</ref> (see Sect. 4.1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>In this setting each agent independently executes a policy, however, there are other cases where this does not hold, for example when agents have a coordinated exploration strategy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Counterfactual regret minimization is a technique for solving large games based on regret minimization<ref type="bibr" target="#b229">[230,</ref><ref type="bibr" target="#b367">368]</ref> due to a well-known connection between regret and Nash equilibria<ref type="bibr" target="#b38">[39]</ref>. It has been one of the reasons of successes in Poker<ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b223">224]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>This algorithm is similar to CFR-BR<ref type="bibr" target="#b158">[159]</ref> and has the main advantage that the current policy convergences rather than the average policy, so there is no need to learn the average strategy, which requires large reservoir buffers or many past networks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>TFT originated in an iterated prisoner's dilemma tournament and later inspired different strategies in MAL<ref type="bibr" target="#b257">[258]</ref>, its generalization, Godfather, is a representative of leader strategies<ref type="bibr" target="#b200">[201]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>The average strategy profile of fictitious players converges to a Nash equilibrium in certain classes of games, e.g., two-player zero-sum and potential games<ref type="bibr" target="#b221">[222]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>The vocabulary that agents use was arbitrary and had no initial meaning. To understand its emerging semantics they looked at the relationship between symbols and the sets of images they referred to<ref type="bibr" target="#b182">[183]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>There is a large body of research on coordinating multiagent teams by specifying communication protocols<ref type="bibr" target="#b114">[115,</ref><ref type="bibr" target="#b320">321]</ref>: these expect agents to know the team's goal as well as the tasks required to accomplish the goal.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>Elo uses a normal distribution for each player skill, and after each match, both players' distributions are updated based on measure of surprise, i.e., if a user with previously lower (predicted) skill beats a high skilled one, the low-skilled player is significantly increased.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>Nash equilibrium<ref type="bibr" target="#b228">[229]</ref> is a solution concept in game theory in which no agent would choose to deviate from its strategy (they are a best response to others' strategies). This concept has been explored in seminal MAL algorithms like Nash-Q learning<ref type="bibr" target="#b148">[149]</ref> and Minimax-Q learning<ref type="bibr" target="#b197">[198,</ref><ref type="bibr" target="#b198">199]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_14"><p>Johanson et al. [160]  also found "overfitting" when solving large extensive games (e.g., poker)-the performance in an abstract game improved but it was worse in the full game.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_15"><p>Bayesian policy reuse assumes an agent with prior experience in the form of a library of policies. When a novel task instance occurs, the objective is to reuse a policy from its library based on observed signals which correlate to policy performance<ref type="bibr" target="#b271">[272]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_16"><p>https://github.com/gjp1203/nui_in_madrl.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_17"><p>https://github.com/gjp1203/nui_in_madrl.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_18"><p>https://www.pommerman.com/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_19"><p>https://github.com/oxwhirl/smac.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_20"><p>https://github.com/oxwhirl/pymarl.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_21"><p>https://github.com/crowdAI/marlo-single-agent-starter-kit/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_22"><p>https://github.com/deepmind/hanabi-learning-environment.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_23"><p>https://github.com/YuhangSong/Arena-BuildingToolkit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_24"><p>https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_25"><p>https://github.com/openai/neural-mmo.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_26"><p>This idea was initially inspired by the Workshop "Critiquing and Correcting Trends in Machine Learning" at NeurIPS 2018 where it was possible to submit Negative results papers: "Papers which show failure modes of existing algorithms or suggest new approaches which one might expect to perform well but which do not. The aim is to provide a venue for work which might otherwise go unpublished but which is still of interest to the community." https://ml-critique-correct.github.io/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_27"><p>It is sometimes unclear in the literature what is the meaning of frame due to the "frame skip" technique. It is therefore suggested to refer to "game frames" and "training frames"<ref type="bibr" target="#b309">[310]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_28"><p>One recent effort by Beeching et al.<ref type="bibr" target="#b28">[29]</ref> proposes to use only "mid-range hardware" (8 CPUs and 1 GPU) to train deep RL agents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_29"><p>NeurIPS 2019 hosts the "MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors" where the primary goal of the competition is to foster the development of algorithms which can efficiently leverage human demonstrations to drastically reduce the number of samples needed to solve complex, hierarchical, and sparse environments<ref type="bibr" target="#b124">[125]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_30"><p>Cuccu, Togelius and Cudré-Mauroux achieved state-of-the-art policy learning in Atari games with only 6 to 18 neurons<ref type="bibr" target="#b74">[75]</ref>. The main idea was to decouple image processing from decision-making.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Chao Gao, Nidhi Hegde, Gregory Palmer, Felipe Leno Da Silva and Craig Sherstan for reading earlier versions of this work and providing feedback, to April Cooper for her visual designs for the figures in the article, to Frans Oliehoek, Sam Devlin, Marc Lanctot, Nolan Bard, Roberta Raileanu, Angeliki Lazaridou, and Yuhang Song for clarifications in their areas of expertise, to Baoxiang Wang for his suggestions on recent deep RL works, to Michael Kaisers, Daan Bloembergen, and Katja Hofmann for their comments about the practical challenges of MDRL, and to the editor and three anonymous reviewers whose comments and suggestions increased the quality of this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards characterizing divergence in deep Q-learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08894</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unifying temporal and structural credit assignment problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Agogino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th international conference on autonomous agents and multiagent systems</title>
		<meeting>17th international conference on autonomous agents and multiagent systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analyzing and visualizing multiagent rewards in dynamic and stochastic domains</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Agogino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="320" to="338" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling technique for markov chains using stochastic approximation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Ahamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Juneja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="489" to="504" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramamoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on autonomous agents and multi-agent systems</title>
		<meeting>the 12th international conference on autonomous agents and multi-agent systems<address><addrLine>Saint Paul, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autonomous agents modelling other agents: A comprehensive survey and open problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">258</biblScope>
			<biblScope unit="page" from="66" to="95" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning in multi-agent systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable planning and learning for multiagent POMDPs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<ptr target="https://blog.openai.com/ai-and-compute" />
		<title level="m">AI and compute</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized prioritized sweeping</title>
		<author>
			<persName><forename type="first">D</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1001" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hindsight experience replay</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gillhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07857</idno>
		<title level="m">RUDDER: Return decomposition for delayed rewards</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05866v2</idno>
		<title level="m">A brief survey of deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal control of Markov processes with incomplete state information</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Astrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="205" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The evolution of cooperation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="1390" to="1396" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maybe a few considerations in reinforcement learning research?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement learning for real life workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Surprising negative results for generative adversarial tree search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Critiquing and correcting trends in machine learning workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement learning through asynchronous advantage actor-critic on a GPU</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microscopic traffic simulation by cooperative multiagent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evolutionary algorithms in theory and practice: Evolution strategies, evolutionary programming, genetic algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Residual algorithms: Reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The mechanics of n-player differentiable games</title>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international conference on machine learning, proceedings of machine learning research</title>
		<meeting>the 35th international conference on machine learning, proceedings of machine learning research<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive policy gradient in multiagent learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international joint conference on Autonomous agents and multiagent systems</title>
		<meeting>the second international joint conference on Autonomous agents and multiagent systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="686" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emergent complexity via multiagent competition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00506</idno>
		<title level="m">The Hanabi challenge: A new frontier for AI research</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Teamwork with Limited Knowledge of Teammates</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh AAAI Conference on Artificial Intelligence<address><addrLine>Bellevue, WS, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="102" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Intrinsically motivated learning in natural and artificial systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<editor>M. Mirolli &amp; G. Baldassarre</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="17" to="47" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Intrinsic motivation and reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Solving transition independent decentralized Markov decision processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="423" to="455" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dibangoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simonin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01806</idno>
		<title level="m">Deep reinforcement learning on a budget: 3D Control and reasoning without a supercomputer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taïga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11530</idno>
		<title level="m">A geometric perspective on optimal representations for reinforcement learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Markovian decision process</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Mechanics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="679" to="684" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The complexity of decentralized control of Markov decision processes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Givan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="840" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dec-MCTS: Decentralized planning for multi-robot active perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mettu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="316" to="337" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lenient frequency adjusted Q-learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bloembergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaisers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Belgian/Netherlands artificial intelligence conference</title>
		<meeting>the 22nd Belgian/Netherlands artificial intelligence conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evolutionary dynamics of multi-agent learning: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bloembergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hennes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaisers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="659" to="697" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning, regret minimization, and equilibria</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Monsour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic game theory</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cooperative multi-agent policy gradient</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Dibangoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simonin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convergence problems of general-sum multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convergence and no-regret in multiagent learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Heads-up limit hold&apos;em poker is solved</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tammelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<biblScope unit="issue">6218</biblScope>
			<biblScope unit="page" from="145" to="149" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Coordination and adaptation in impromptu teams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mccracken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the nineteenth conference on artificial intelligence</title>
		<meeting>the nineteenth conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="53" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiagent learning using variable learning rate</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="250" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalization in reinforcement learning: Safely approximating the value function</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">R-max-a general polynomial time algorithm for near-optimal reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2002-10">2002. Oct</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<title level="m">OpenAI gym</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Iterative solution of games by fictitious play</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Activity Analysis of Production and Allocation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="374" to="376" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Superhuman AI for heads-up no-limit poker: Libratus beats top professionals</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sandholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6374</biblScope>
			<biblScope unit="page" from="418" to="424" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A survey of Monte Carlo tree search methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rohlfshagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bucilua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evolutionary computing in multi-agent environments: Operators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on evolutionary programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Evolution in multi-agent systems: Evolving communicating classifier systems for gait in a quadrupedal robot</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snaith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international conference on genetic algorithms</title>
		<meeting>the 6th international conference on genetic algorithms</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="382" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: An overview</title>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in multi-agent systems and applications -1</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="183" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Capture the Flag: The emergence of complex cooperative agents</title>
		<ptr target="https://deepmind.com/blog/capture-the-flag/" />
		<imprint>
			<date type="published" when="2018-09-07">2018. September 7, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m">Collaboration &amp; Credit Principles, How can we be good stewards of collaborative trust</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A cognitive hierarchy model of games</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Camerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Behavioural game theory: Thinking, learning and teaching</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Camerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in understanding strategic behavior</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="120" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Incorporating opponent models into adversary search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI/IAAI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="120" to="125" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Exact and approximate algorithms for partially observable Markov decision processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The representational capacity of actionvalue networks for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International conference on autonomous agents and multiagent systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06110</idno>
		<title level="m">Dopamine: A research framework for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multiagent learning in the presence of memory-bounded agents</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="213" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning and representation learning workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Offer: Off-environment reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ciosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Let&apos;s play again: Variability of deep reinforcement learning agents in Atari environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS critiquing and correcting trends workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The dynamics of reinforcement learning in cooperative multiagent systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th national conference on artificial intelligence</title>
		<meeting>the 15th national conference on artificial intelligence<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="746" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents</title>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sandholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="23" to="43" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Cognition and behavior in normal-form An experimental study</title>
		<author>
			<persName><forename type="first">Costa</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Broseta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1193" to="1235" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning to compete, coordinate, and cooperate in repeated games using reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="314" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Elevator group control using multiple reinforcement learning agents</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Crites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="235" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Playing Atari with six neurons</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cuccu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on autonomous agents and multiagent systems</title>
		<meeting>the 18th international conference on autonomous agents and multiagent systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="998" to="1006" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">How much does it help to know what she knows you know? An agent-based simulation study</title>
		<author>
			<persName><forename type="first">H</forename><surname>De Weerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verbrugge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Verheij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning to cooperate in multi-agent social dilemmas</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>De Cote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international conference on autonomous agents and multiagent systems</title>
		<meeting>the 5th international conference on autonomous agents and multiagent systems<address><addrLine>Hakodate, Hokkaido, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="783" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning: Pong from pixels</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m">Do I really have to cite an arXiv paper?</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Safely using predictions in general-sum normal form games</title>
		<author>
			<persName><forename type="first">S</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on autonomous agents and multiagent systems</title>
		<meeting>the 16th conference on autonomous agents and multiagent systems<address><addrLine>Sao Paulo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Human-level intelligence or animal-like abilities?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Darwiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Feudal reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Experience selection in deep reinforcement learning for control</title>
		<author>
			<persName><forename type="first">T</forename><surname>De Bruin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuška</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="347" to="402" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning multi-agent state space representations</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>De Hauwere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vrancx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international conference on autonomous agents and multiagent systems</title>
		<meeting>the 9th international conference on autonomous agents and multiagent systems<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="715" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Evolutionary computation: A unified approach</title>
		<author>
			<persName><forename type="first">Jong</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Potential-based difference rewards for multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Yliniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kudenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International conference on autonomous agents and multiagent systems</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MCS proceedings of the first international workshop on multiple classifier systems</title>
		<meeting><address><addrLine>Berlin Heidelberg, Cagliari, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02224</idno>
		<title level="m">Adapting auxiliary losses using gradient similarity</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Go-explore: A new approach for hard-exploration problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10995</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">The rating of chessplayers, past and present</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Elo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Arco Pub</publisher>
			<pubPlace>Nagoya</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">On a combinatorial game</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdös</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Selfridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="301" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Tree-based batch mode reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="503" to="556" />
			<date type="published" when="2005-04">2005. Apr</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning rates for Q-learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Even-Dar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2003-12">2003. Dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Beating the World&apos;s best at super smash Bros. with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06230</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2145" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Learning with opponent-learning awareness</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th international conference on autonomous agents and multiagent systems</title>
		<meeting>17th international conference on autonomous agents and multiagent systems<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Counterfactual multi-agent policy gradients</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Stabilising experience replay for deep multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The scientific method in the science of machine learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Forde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paganini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR debugging machine learning models workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">An introduction to deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>François-Lavet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="219" to="354" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Reinforcement learning in the presence of rare events</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on machine learning</title>
		<meeting>the 25th international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Fudenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tirole</surname></persName>
		</author>
		<title level="m">Game theory</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Addressing function approximation error in actor-critic methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Predicting and preventing coordination problems in cooperative Qlearning systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fulda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth international joint conference on artificial intelligence</title>
		<meeting>the twentieth international joint conference on artificial intelligence<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="780" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Skynet: A top deep RL agent in the inaugural pommerman team competition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th multidisciplinary conference on reinforcement learning and decision making</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">On hard exploration for reinforcement learning: A case study in pommerman</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI conference on artificial intelligence and interactive digital entertainment</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Hark side of deep learning-from grad student descent to automated machine learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gencoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Gils</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guldogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Süzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07633</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A framework for sequential planning in multiagent settings</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Gmytrasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="79" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Rational coordination in multi-agent environments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Gmytrasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Durfee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="319" to="350" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Approximate solutions to Markov decision processes</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Correlated Q-learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th international conference on autonomous agents and multiagent systems</title>
		<meeting>17th international conference on autonomous agents and multiagent systems<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Collaborative plans for complex group action</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="357" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Learning policy representations in multiagent systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Q-prop: Sample-efficient policy gradient with an off-policy critic</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3846" to="3855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Multiagent planning with factored MDPs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1523" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Efficient solution algorithms for factored MDPs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="399" to="468" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Coordinated reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Shaping as a method for accelerating reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gullapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1992 IEEE international symposium on intelligent control</title>
		<meeting>the 1992 IEEE international symposium on intelligent control</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="554" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Cooperative multi-agent control using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Egorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomous agents and multiagent systems</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Sukthankar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodriguez-Aguilar</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="66" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Cooperative Multi-agent Control using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Egorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive learning agents at AAMAS</title>
		<meeting><address><addrLine>Sao Paulo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Codel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Liebana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10079</idno>
		<title level="m">The MineRL competition on sample efficient reinforcement learning using human priors</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Reinforcement learning with deep energybased policies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international conference on machine learning</title>
		<meeting>the 34th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1352" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Reinforcement learning in feedback control</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="137" to="169" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Games with incomplete information played by &quot;Bayesian&quot; players, I-III part I. The basic model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Harsanyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="182" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Double Q-learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Deep recurrent Q-learning for partially observable MDPs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Value-function approximations for partially observable Markov decision processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="94" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Opponent modeling in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2675" to="2684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lemmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02286v2</idno>
		<title level="m">Emergence of locomotion behaviours in rich environments</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Fictitious self-play in extensive-form games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="805" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from self-play in imperfect-information games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01121</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning that matters</title>
		<author>
			<persName><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">TrueSkill TM : a Bayesian skill rating system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Learning against sequential opponents in repeated stochastic games. In The 3rd multi-disciplinary conference on reinforcement learning and decision making</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaisers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Towards a fast detection of opponents in repeated stochastic games</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaisers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomous agents and multiagent systems: AAMAS 2017 Workshops, Best Papers</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Sukthankar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodriguez-Aguilar</surname></persName>
		</editor>
		<meeting><address><addrLine>Sao Paulo, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05">2017. 8-12 May, 2017</date>
			<biblScope unit="page" from="239" to="257" />
		</imprint>
	</monogr>
	<note>selected papers</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaisers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baarslag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Munoz De Cote</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09183</idno>
		<title level="m">A survey of learning in multiagent environments-dealing with non-stationarity</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Agent modeling as auxiliary task for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI conference on artificial intelligence and interactive digital entertainment</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Identifying and tracking switching, non-stationary opponents: A Bayesian approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Munoz De Cote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiagent interaction without prior coordination workshop at AAAI</title>
		<meeting><address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Efficiently detecting switches against non-stationary opponents</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Munoz De Cote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="789" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS deep learning workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">A deep policy inference Q-network for multi-agent systems</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Shann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on autonomous agents and multiagent systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Nash Q-learning for general-sum stochastic games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Emergent cooperation for multiple agents using genetic programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Iba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on parallel problem solving from nature</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Janoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02553</idno>
		<title level="m">Are deep policy gradient algorithms truly policy gradient algorithms? CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international conference on machine learning</title>
		<meeting>the 32nd international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Selective experience replay for lifelong learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cosgun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Convergence of stochastic iterative dynamic programming algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Human-level performance in 3d multiplayer games with population-based reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Castañeda</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aau6249</idno>
		<ptr target="https://doi.org/10.1126/science.aau6249" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">6443</biblScope>
			<biblScope unit="page" from="859" to="865" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09846</idno>
		<title level="m">Population based training of neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Finding optimal abstract strategies in extensive-form games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-sixth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Accelerating best response calculation in large extensive games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-second international joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Computing robust counter-strategies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">The Malmo platform for artificial intelligence experimentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bignell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4246" to="4247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Unity: A general platform for intelligent agents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Juliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Berges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lange</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02627</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">FAQ-learning in matrix games: demonstrating convergence near Nash equilibria, and bifurcation of attractors in the battle of sexes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kaisers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Interactive Decision Theory and Game Theory</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">A natural policy gradient</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1531" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Rational learning leads to Nash equilibrium</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lehrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1019" to="1045" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Necessary and sufficient conditions for a solution of the bellman equation to be the value function: A general principle</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kamihigashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<ptr target="https://halshs.archives-ouvertes.fr/halshs-01159177" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Stochastic tree search with useful cycles for patrolling problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Karamouzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Guy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1289" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Using Monte Carlo tree search as a demonstrator within asynchronous deep RL</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI workshop on reinforcement learning in games</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Monte Carlo tree search with branch and bound for multi-robot task allocation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IJCAI-16 workshop on autonomous mobile service robots</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Evolutionary reinforcement learning for sample-efficient multiagent coordination</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khadka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Message-dropout: An efficient training method for multi-agent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Sparse cooperative Q-learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Autonomous shaping: Knowledge transfer in reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 23rd international conference on machine learning</title>
		<meeting>23rd international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Comparison of CMACs and radial basis functions for local function approximators in reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kretchmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on neural networks (ICNN&apos;97)</title>
		<meeting>international conference on neural networks (ICNN&apos;97)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="834" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3675" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="72" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">A unified game-theoretic approach to multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">An algorithm for distributed reinforcement learning in cooperative multi-agent systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventeenth international conference on machine learning</title>
		<meeting>the seventeenth international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">The world of independent learners is not Markovian</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fort-Piat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Knowledge-based and Intelligent Engineering Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Multi-agent cooperation and the emergence of (natural) language</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peysakhovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Exploiting open-endedness to solve problems through the search for novelty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALIFE</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00742</idno>
		<title level="m">Autocurricula and the emergence of innovation from social interaction: A manifesto for multi-agent intelligence research</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Malthusian reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wheelwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Marblestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duéñez-Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th international conference on autonomous agents and multiagent systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning in sequential social dilemmas</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on autonomous agents and multiagent systems</title>
		<meeting>the 16th conference on autonomous agents and multiagent systems<address><addrLine>Sao Paulo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">Maintaining cooperation in complex social dilemmas using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peysakhovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01068</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning: An overview</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07274</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>International conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Programming robots using reinforcement learning and teaching</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="781" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Self-improving reactive agents based on reinforcement learning, planning and teaching</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="293" to="321" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">What game are we playing? End-to-end learning in normal and extensive form games</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-seventh international joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Combating reinforcement learning&apos;s Sisyphean curse with intrinsic fear</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01211v8</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Troubling trends in machine learning scholarship</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Machine Learning Debates workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on machine learning</title>
		<meeting>the 11th international conference on machine learning<address><addrLine>New Brunswick, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Friend-or-foe Q-learning in general-sum games</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th international conference on autonomous agents and multiagent systems</title>
		<meeting>17th international conference on autonomous agents and multiagent systems<address><addrLine>Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Value-function reinforcement learning in Markov games</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Implicit negotiation in repeated games</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ATAL &apos;01: revised papers from the 8th international workshop on intelligent agents VIII</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Action-depedent control variates for policy optimization via stein&apos;s identity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Emergent coordination through competition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Computing approximate equilibria in sequential adversarial games by exploitability descent</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Timbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05614</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">On the pitfalls of measuring emergent communication</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th international conference on autonomous agents and multiagent systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6379" to="6390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Non-delusional Q-learning and value-iteration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9949" to="9959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">A comparative analysis of expected and distributional reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-third AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m">Multiagent Learning, Foundations and Recent Trends</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Lvd</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">2008. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Automatic programming of behavior-based robots using reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="311" to="365" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Independent reinforcement learners in cooperative Markov games: A survey regarding coordination problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Fort-Piat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of learning and motivation</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Bower</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989">1989</date>
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Safe strategies for agent modelling in games</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mccracken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI fall symposium</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">An analysis of reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Meyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="664" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Learning finite-state controllers for partially observable environments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meuleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifteenth conference on uncertainty in artificial intelligence</title>
		<meeting>the fifteenth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602v1</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Humanlevel control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Fictitious play property for games with identical interests</title>
		<author>
			<persName><forename type="first">D</forename><surname>Monderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Theory</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="258" to="265" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Prioritized sweeping: Reinforcement learning with less data and less time</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="130" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">DeepStack: Expertlevel artificial intelligence in heads-up no-limit poker</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moravčík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lisý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6337</biblScope>
			<biblScope unit="page" from="508" to="513" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Emergence of grounded compositional language in multi-agent populations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms for reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Moriarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="241" to="276" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Robust reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Warnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05676</idno>
		<title level="m">Deterministic implementations for reproducibility in deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Equilibrium points in n-person games</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="page" from="48" to="49" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">An introduction to counterfactual regret minimization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Neller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of model AI assignments, the fourth symposium on educational advances in artificial intelligence</title>
		<meeting>model AI assignments, the fourth symposium on educational advances in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth international conference on machine learning</title>
		<meeting>the sixteenth international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for multi-agent systems: A review of challenges, solutions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11794</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Game theory and multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nowé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vrancx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>De Hauwere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement learning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Wiering</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Van Otterlo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="441" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<monogr>
		<title level="m" type="main">ACKTR &amp; A2C</title>
		<author>
			<persName><forename type="first">Openai</forename><surname>Baselines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b234">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Open</surname></persName>
		</author>
		<author>
			<persName><surname>Five</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Interactive learning and decision making -foundations, insights &amp; challenges</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<monogr>
		<title level="m" type="main">A concise introduction to decentralized POMDPs</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">The parallel Nash memory for asymmetric games</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th annual conference on genetic and evolutionary computation</title>
		<meeting>the 8th annual conference on genetic and evolutionary computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Optimal and approximate Q-value functions for decentralized POMDPs</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="289" to="353" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Approximate solutions for factored Dec-POMDPs with many agents</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Spaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 international conference on Autonomous agents and multiagent systems</title>
		<meeting>the 2013 international conference on Autonomous agents and multiagent systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="563" to="570" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Influence-based abstraction for multiagent systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Witwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-sixth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hennes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00190</idno>
		<title level="m">Neural replicator dynamics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lespiau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">α-rank: Multi-agent evaluation by evolution</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">9937</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Deep decentralized multi-task multi-agent reinforcement learning under partial observability</title>
		<author>
			<persName><forename type="first">S</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international conference on machine learning</title>
		<meeting>the 34th international conference on machine learning<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00196</idno>
		<title level="m">Modeling friends and foes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Negative update intervals in deep multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International conference on autonomous agents and multiagent systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Lenient multi-agent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bloembergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on autonomous agents and multiagent systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Cooperative multi-agent learning: The state of the art</title>
		<author>
			<persName><forename type="first">L</forename><surname>Panait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="434" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Lenience towards teammates helps in cooperative multiagent learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Panait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international conference on autonomous agents and multiagent systems</title>
		<meeting>the 5th international conference on autonomous agents and multiagent systems<address><addrLine>Hakodate, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Theoretical advantages of lenient learners: An evolutionary game theoretic perspective</title>
		<author>
			<persName><forename type="first">L</forename><surname>Panait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="423" to="457" />
			<date type="published" when="2008-03">2008. Mar</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Papoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Christianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Albrecht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04737</idno>
		<title level="m">Dealing with non-stationarity in multi-agent deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10069</idno>
		<title level="m">Multiagent bidirectionallycoordinated nets for learning to play StarCraft combat games</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Pérez-Liébana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Gaina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ionita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08129</idno>
		<title level="m">The multi-agent reinforcement learning in Malmö (MARLÖ) competition. CoRR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Actor-critic fictitious play in simultaneous move multistage games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st international conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<monogr>
		<title level="m" type="main">Improving coordination in multi-agent deep reinforcement learning through memory-driven communication</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pesce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03887</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Robust adversarial reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno>JMLR. org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international conference on machine learning</title>
		<meeting>the 34th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2817" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Learning against opponents with bounded memory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international joint conference on artificial intelligence</title>
		<meeting>the 19th international joint conference on artificial intelligence<address><addrLine>Edinburg, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="817" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">A general criterion and an algorithmic framework for learning in multi-agent systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="45" to="76" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventeenth international conference on machine learning</title>
		<meeting>the seventeenth international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<monogr>
		<title level="m" type="main">Markov decision processes: Discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Decision tree function approximation in reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pyeatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third international symposium on adaptive systems: Evolutionary computation and probabilistic graphical models</title>
		<meeting>the third international symposium on adaptive systems: Evolutionary computation and probabilistic graphical models<address><addrLine>Cuba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Machine theory of mind</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Can deep reinforcement learning solve Erdos-Selfridge-spencer games?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international conference on machine learning</title>
		<meeting>the 35th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Modeling others using oneself in multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">QMIX -monotonic value function factorisation for deep multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>De Witt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07124</idno>
		<title level="m">Pommerman: A multi-agent playground</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Neural fitted Q iteration-first experiences with a data efficient neural reinforcement learning method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<monogr>
		<title level="m" type="main">Learning to learn without forgetting by maximizing transfer and minimizing interference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11910</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">The file drawer problem and tolerance for null results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">638</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">New methods for competitive coevolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hawasly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramamoorthy</surname></persName>
		</author>
		<title level="m">Bayesian policy reuse. Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="99" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Policy distillation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Coevolving game-playing agents: Measuring performance and intransitivities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Runarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="226" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>De Witt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G J</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04043</idno>
		<title level="m">The StarCraft multi-agent challenge</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning in the iterated prisoner&apos;s dilemma</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Sandholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Crites</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosystems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="147" to="166" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">A possibility for implementing curiosity and boredom in model-building neural controllers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on simulation of adaptive behavior: From animals to animats</title>
		<meeting>the international conference on simulation of adaptive behavior: From animals to animats</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="222" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">Critique of Paper by &quot;Deep Learning Conspiracy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://people.idsia.ch/~juergen/deep-learning-conspiracy.html" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06440</idno>
		<title level="m">Equivalence between policy gradients and soft Q-learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st international conference on machine learning</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Winner&apos;s curse? On pace, progress, and empirical rigor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Dynamic fictitious play, dynamic gradient play, and distributed convergence to Nash equilibria</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Arslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="327" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<monogr>
		<title level="m" type="main">Loss is its own reward: Self-supervision for reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mahmoudieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR workshops</note>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">If multi-agent learning is the answer, what is the question?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">A survey on transfer learning for multiagent reinforcement learning systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="645" to="703" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">Convergence results for single-step on-policy reinforcement-learning algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="308" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Nash convergence of gradient dynamics in general-sum games</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth conference on uncertainty in artificial intelligence</title>
		<meeting>the sixteenth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Transfer of learning by composing solutions of elemental sequential tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="323" to="339" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Convergence of multi-agent learning with a finite step size in general-sum games</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International conference on autonomous agents and multiagent systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<monogr>
		<title level="m" type="main">Arena: A general evaluation platform and building toolkit for multi-agent intelligence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08085</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">Randomization, derandomization and antirandomization: three games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Spencer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Actor-critic policy optimization in partially observable multiagent environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3422" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">Reinforcement learning in pomdps with memoryless options and option-observation initiation sets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Steckelmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roijers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vrancx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Plisnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nowé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">Learning to cooperate in a social dilemma: A satisficing approach to bargaining</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Stimpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on machine learning (ICML-03</title>
		<meeting>the 20th international conference on machine learning (ICML-03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="728" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">Ad Hoc autonomous agent teams: Collaboration without pre-coordination</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaminka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Rosenschein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI conference on artificial intelligence</title>
		<meeting><address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1504" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">Multiagent systems -a survey from a machine learning perspective</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="383" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<monogr>
		<title level="m" type="main">Accelerated methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02811</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">An analysis of model-based interval estimation for Markov decision processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1331" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00784</idno>
		<title level="m">Neural MMO: A massively multiagent game environment for training and evaluating intelligent agents</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<monogr>
		<title level="m" type="main">Influencebased abstraction in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suau De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Congeduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Starre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In Adaptive, learning agents workshop</note>
</biblStruct>

<biblStruct xml:id="b309">
	<monogr>
		<title level="m" type="main">Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06567</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">Rule-injection hints as a means of improving network performance and learning time</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Suddarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kergosien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">Value-decomposition networks for cooperative multi-agent learning based on team reward</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th international conference on autonomous agents and multiagent systems</title>
		<meeting>17th international conference on autonomous agents and multiagent systems<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">Generalization in reinforcement learning: Successful examples using sparse coarse coding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1038" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b314">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement learning: An introduction</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Pilarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 10th international conference on autonomous agents and multiagent systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="761" to="768" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Algorithms for reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="103" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">A unified analysis of value-function-based reinforcementlearning algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2017" to="2060" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2154" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">Towards flexible teamwork</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tambe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="83" to="124" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main">Multiagent cooperation and competition with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kodelja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kuzovkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Korjus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">172395</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning proceedings 1993 proceedings of the tenth international conference, University of Massachusetts</title>
		<meeting><address><addrLine>Amherst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-06">1993. June, 1993</date>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b323">
	<analytic>
		<title level="a" type="main">Transfer learning for reinforcement learning domains: A survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1633" to="1685" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main">Temporal difference learning and TD-Gammon</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">Extending Q-learning to general adaptive multi-agent systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="871" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">MuJoCo -A physics engine for model-based control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent robots and systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for general video game AI</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Torrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perez-Liebana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02448</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic approximation and Q-learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="202" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<analytic>
		<title level="a" type="main">Analysis of temporal-diffference learning with function approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1075" to="1081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">The mirage of action-dependent baselines in reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main">Distributed agent-based air traffic flow management</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agogino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international conference on autonomous agents and multiagent systems</title>
		<meeting>the 6th international conference on autonomous agents and multiagent systems<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main">Multiagent learning: Basics, challenges, and prospects</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02648</idno>
		<title level="m">Deep reinforcement learning and the deadly triad</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b334">
	<analytic>
		<title level="a" type="main">Coordinated deep reinforcement learners for traffic light control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of learning, inference and control of multi-agent systems at NIPS</title>
		<meeting>learning, inference and control of multi-agent systems at NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double Q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b336">
	<analytic>
		<title level="a" type="main">A theoretical and empirical analysis of Expected Sarsa</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE symposium on adaptive dynamic programming and reinforcement learning</title>
		<meeting><address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main">FeUdal networks for hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Apps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<ptr target="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" />
		<title level="m">AlphaStar: Mastering the real-time strategy game StarCraft II</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<analytic>
		<title level="a" type="main">On Monte Carlo tree search and reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vodopivec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="881" to="936" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b340">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Morgenstern</surname></persName>
		</author>
		<title level="m">Theory of games and economic behavior</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Bulletin of the American Mathematical Society</publisher>
			<date type="published" when="1945">1945</date>
			<biblScope unit="volume">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b341">
	<analytic>
		<title level="a" type="main">Analyzing complex strategic interactions in multi-agent systems</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Kephart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-02 workshop on game-theoretic and decision-theoretic agents</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b342">
	<monogr>
		<title level="m" type="main">On the origin of deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07800</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b343">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01224</idno>
		<title level="m">Sample efficient actor-critic with experience replay</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b344">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main">Learning from delayed rewards</title>
		<author>
			<persName><forename type="first">J</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">King&apos;s College</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b346">
	<analytic>
		<title level="a" type="main">Lenient learning in independent-learner stochastic cooperative games</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b347">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09817</idno>
		<title level="m">Multiagent soft Q-learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b348">
	<analytic>
		<title level="a" type="main">Best-response multiagent learning in non-stationary environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Rosenschein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd international conference on autonomous agents and multiagent systems</title>
		<meeting>the 3rd international conference on autonomous agents and multiagent systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
		<title level="m">Multiagent systems. Intelligent robotics and autonomous agents series</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main">Evolutionary function approximation for reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="877" to="917" />
			<date type="published" when="2006-05">2006. May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b351">
	<analytic>
		<title level="a" type="main">Protecting against evaluation overfitting in empirical reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE symposium on adaptive dynamic programming and reinforcement learning (ADPRL)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b352">
	<monogr>
		<title level="m" type="main">Reinforcement learning. Adaptation, learning, and optimization</title>
		<editor>Wiering, M., &amp; van Otterlo, M.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="volume">12</biblScope>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b353">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Optimal payoff functions for members of collectives</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling complexity in economic and social systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="355" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b355">
	<analytic>
		<title level="a" type="main">General principles of learning-based multi-agent systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third international conference on autonomous agents</title>
		<meeting>the third international conference on autonomous agents</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b356">
	<analytic>
		<title level="a" type="main">Classes of multiagent Q-learning dynamics with epsilon-greedy exploration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wunder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international conference on machine learning</title>
		<meeting>the 35th international conference on machine learning<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1167" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">Towards efficient detection and optimal response against sophisticated opponents</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<analytic>
		<title level="a" type="main">Recurrent deep multiagent Q-learning for autonomous brokers in smart grid</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Strbac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-seventh international joint conference on artificial intelligence</title>
		<meeting>the twenty-seventh international joint conference on artificial intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b359">
	<analytic>
		<title level="a" type="main">Mean field multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international conference on machine learning</title>
		<meeting>the 35th international conference on machine learning<address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b360">
	<analytic>
		<title level="a" type="main">Towards sample efficient reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5739" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b361">
	<analytic>
		<title level="a" type="main">Graying the black box: Understanding DQNs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ben-Zrihem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1899" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b362">
	<analytic>
		<title level="a" type="main">Multi-agent with policy prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-fourth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b363">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for sponsored search real-time bidding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b364">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08534</idno>
		<title level="m">Weighted double deep multiagent reinforcement learning in stochastic cooperative environments</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b365">
	<analytic>
		<title level="a" type="main">A deep bayesian policy reuse approach against non-stationary agents</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="962" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b366">
	<analytic>
		<title level="a" type="main">Cyclic equilibria in Markov games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1641" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b367">
	<analytic>
		<title level="a" type="main">Regret minimization in games with incomplete information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Piccione</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1729" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
