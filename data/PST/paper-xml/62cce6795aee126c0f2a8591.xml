<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Large-scale Universal User Representation with Sparse Mixture of Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-11">11 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Caigao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siqiao</forename><surname>Xue</surname></persName>
							<email>&lt;siqiao.xsq@alibaba-inc.com&gt;</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Zhang</surname></persName>
							<email>&lt;james.z@antgroup.com&gt;.</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingyue</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhibo</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyan</forename><surname>Hao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">James Zhang</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Large-scale Universal User Representation with Sparse Mixture of Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-11">11 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.04648v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning user sequence behaviour embedding is very sophisticated and challenging due to the complicated feature interactions over time and high dimensions of user features. Recent emerging foundation models, e.g., BERT and its variants, encourage a large body of researchers to investigate in this field. However, unlike natural language processing (NLP) tasks, the parameters of user behaviour model come mostly from user embedding layer, which makes most existing works fail in training a universal user embedding of large scale. Furthermore, user representations are learned from multiple downstream tasks, and the past research work do not address the seesaw phenomenon. In this paper, we propose SUPER-MOE, a generic framework to obtain high quality user representation from multiple tasks. Specifically, the user behaviour sequences are encoded by MoE transformer, and we can thus increase the model capacity to billions of parameters, or even to trillions of parameters. In order to deal with seesaw phenomenon when learning across multiple tasks, we design a new loss function with task indicators. We perform extensive offline experiments on public datasets and online experiments on private real-world business scenarios. Our approach achieves the best performance over stateof-the-art models, and the results demonstrate the effectiveness of our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent works have demonstrated that the pre-trained model plays a critical role on a wide range of applications, e.g., <ref type="bibr" target="#b3">(Devlin et al., 2018;</ref><ref type="bibr" target="#b4">Dosovitskiy et al., 2020;</ref><ref type="bibr">Riquelme First Workshop of Pre-training: Perspectives, Pitfalls, and</ref><ref type="bibr">Paths Forward at ICML 2022, Baltimore, Maryland, USA, PMLR 162, 2022.</ref> Copyright 2022 by the author(s). <ref type="bibr">et al., 2021;</ref><ref type="bibr">Bommasani et al., 2021b;</ref><ref type="bibr" target="#b6">Geng et al., 2022;</ref><ref type="bibr" target="#b14">Sun et al., 2019;</ref><ref type="bibr" target="#b10">Qiu et al., 2020;</ref><ref type="bibr">Khan et al., 2021;</ref><ref type="bibr" target="#b16">Wu et al., 2020;</ref><ref type="bibr" target="#b18">Xiao et al., 2021;</ref><ref type="bibr">Zeng et al., 2021)</ref>. To improve the efficiency and effectiveness of these models, many researchers attempt to exploit transformer in order to capture chronological pattern and dynamics of user intentions <ref type="bibr">(Zeng et al., 2021;</ref><ref type="bibr">Xue et al., 2021)</ref>. With the remarkable achievements of pre-trained models, especially BERT-based models <ref type="bibr" target="#b11">(Qiu et al., 2021)</ref>, the transformer backbone has been utilized to address user data sparsity and cold-start problems in downstream applications <ref type="bibr" target="#b21">(Yuan et al., 2020;</ref><ref type="bibr">Zhang et al., 2020a)</ref>. In addition, DNN-based self-supervised learning (SSL) model is designed to improve semantic representations for highly-skewed data distribution, with inadequate explicit user feedback in user behaviour sequence interactions via unlabeled data <ref type="bibr" target="#b20">(Yao et al., 2021;</ref><ref type="bibr" target="#b13">Shin et al., 2021;</ref><ref type="bibr">Zhang et al., 2020b)</ref>.</p><p>However, the existing pre-trained model suffers from many difficulties in achieving good user representations, e.g., only a few behaviour channels are used in the model due to the huge sizes of vocabularies and the resulting low training efficiency. In AETN <ref type="bibr">(Zhang et al., 2020a)</ref>, only three behaviour channels are utilized, yielding sub-optimal user representations. Therefore, the motivations of our work are threefold, supported by our practical observations in online production system. Firstly, most of model parameters come from feature embedding of ID and categorical features, which usually dominate GPU memory usage <ref type="bibr" target="#b8">(Lian et al., 2021)</ref>. For example, the number of user IDs are often in the scale of billions, resulting in parameter size of numberIDs ? embeddingDIMs. Secondly, the front embedding layer accounts for the majority of the model's size, while the rest of model layers are extremely computationally expensive. Consequently, training feature embedding layer and main neural networks simultaneously and synchronously for model of large scale is challenging, which calls for efficient model training algorithm for sparsity. Finally, there are multiple training objectives no matter in model pre-training stage or in fine-tuning stage, which often causes pre-trained user embedding models with sub-optimal performance when using simple bottom-shared mechanism for the reason of seesaw phenomenon <ref type="bibr" target="#b15">(Tang et al., 2020)</ref> and negative transfer <ref type="bibr" target="#b9">(Ma et al., 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Large channels Sequential Temporal Multi-task learning Scalability up to Trillions MTL <ref type="bibr" target="#b15">(Tang et al., 2020</ref>) </p><formula xml:id="formula_0">? ? ? ? ? PERSIA(Lian et al., 2021) ? ? ? ? ? MTSSL(Yao et al., 2021) ? ? ? ? ? BERT(Sun et al., 2019) ? ? ? ? ? AETN(Zhang et al., 2020a) ? ? ? ? ? OURS ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Statement</head><p>Generally, we denote a typical one-channel user behaviour sequence as s = [s 1 , s 2 , ..., s i , ..., s N ], where s i indicates the i th user behaviour for this channel, which has length of N . A multi-channel user behaviour sequence is denoted as S = {[s j 1 , s j 2 , ..., s j i ..., s j N ]}, and [s j 1 , s j 2 , ..., s j i ..., s j N ] is the j th channel of user behaviour sequence corresponding to M behaviour channels. Each instance S in each task contains a userID u ? U , and three types of sequence channels, namely, category channel S category , ID channel S ID and dense channel S dense . Therefore, given a set of N tasks T = {t 1 , t 2 , ..., t n } with corresponding supervised label Y = {y 1 , y 2 , ..., y n }, our goal is to learn the base user representations across these tasks in order to apply them to downstream applications. Following the two-stage training paradigm <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, we pre-train a base model firstly on the huge pre-training dataset and then fine-tune a new model on downstream target dataset with parameters initialized as the pre-trained model. After the training, our base representation model should be able to produce universal representation H to serve all downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">User Embedding Pre-training Framework</head><p>Pre-training Tasks. Similar to the pre-training task in <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, a new user representation pre-training task is designed to cater to the attribution of user behaviour data, i.e., masked channel prediction (MCP) task. Slightly different from masked language modeling (MLM) task in NLP, not all of the features are masked due to multi-channel problem in user behaviour data which would produce too many feature vocabularies. Theoretically, in the MCP task, some channel elements in the behaviour sequence are randomly masked with special token [M ASK] at pre-training stage. Therefore, an MCP task of one feature channel is elaborated as input = [s 1 , s 2 , ..., [M ASK] i ..., s N ], with label = [M ASK] i . However, only a few channels are selected to be MCP tasks due to our belief that the more important a user behaviour sequence is, the more likely the sequence is selected as MCP task. In order to preserve essential information of user behaviours, we choose user ID, location, time interval, payment tool, product, trade amount, super position model (SPM) trace, click, and conversion etc.</p><p>Pre-training Objectives. Formally, we denote s mask as the probability of the estimated activity, and the probability p(s mask ; ?) is represented by the product of the conditional distributions over the masked sequence:</p><formula xml:id="formula_1">p(s mask ; ?) = N i=1 p(s mask |s 1 , s 2 , ..., [M ASK]..., s N ; ?) (1)</formula><p>Our objective is to maximize p(s mask ; ?), which is equivalent to minimizing the following loss function: . . where S i is the set of positions of masked elements of the i th MCP task, and ?j and s j are the predicted user behaviour and the ground-truth behaviour, respectively. Notably, user behaviours are of very different statistical characteristics from NLP or CV, e.g., the click and conversion task are sequential tasks. Hence, we propose a new training objective function:</p><formula xml:id="formula_2">L i mcp = - 1 |S i | j?Si -logp( ?j = s j ) ,<label>(2)</label></formula><formula xml:id="formula_3">? !" " ? !" # ? !" $ ? ? !" % ? !# " ? !# # ? !# $ ? ? !# % ? !$ " ? !$ # ? !$ $ ? ? !$ % ? !&amp; " ? !&amp; # ? !&amp; $ ? ? !&amp; % ? !" " ? !" # ? ? !" % ? !# " ? !# # ? ? !# % ? !$ " ? !$ # ? !$ % ? ? ? !&amp; % . . . ?' " ?' # ?' $ ?' % . . . ?() " ?() # ? () $ ?() %</formula><formula xml:id="formula_4">L k (? k ) = 1 i ? i k i ? i k loss k ( s i k (? k ), s i k ) , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where ? is the indicator of training samples among k tasks. Each MFP layer in the encoder block is followed by a layer normalization and nonlinear activation layer. The operator split(?) is a model parallel operation, implemented by the whale framework <ref type="bibr" target="#b17">(Xianyan Jia, 2022)</ref>. Note that the splitting of MFP layer addresses the issue of GPU memory explosion, which successfully integrate hundreds of behaviour channels into model training. An MoE-MSA layer takes the output of MFP y m pf as input, formulated as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre</head><formula xml:id="formula_6">y msa = sof tmax( (qw q G q (q))(kw k G k (k)) T ? d k )(vw v G v (v)) ,</formula><p>(5) where q, k, v is the output of an MFP layer, and y msa is output of an MoE-MSA layer, connected by two MoE-FNN layers. Lastly, the point-wise MoE-FNN <ref type="bibr" target="#b5">(Fedus et al., 2021)</ref> can be formulated as:</p><formula xml:id="formula_7">y f f n = E e=1 G e (x) ? FFN e (x) ,<label>(6)</label></formula><p>with FFN e (x) = w oe ? Relu(w ie ? x),G e (x) = sof tmax(TopK(h e (x), k)) , where w o and w i are the standard feed-forward networks with the same parameters. We choose top 1 strategy <ref type="bibr" target="#b5">(Fedus et al., 2021)</ref> for TopK(?) function. In summary, y f f n is the output of backbone of an MoE transformer. Formally, a series of MoE transformer blocks can be described as:</p><formula xml:id="formula_8">y moe = MoETransformer([s category , s ID , s dense ]) , (7)</formula><p>where MoETransformer =MoE F F N (MoE M SA (MFP(?))).</p><p>The overall pre-training architecture is shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">User Embedding Fine-tuning Framework</head><p>After pre-training, we adapt the learned user representations to specific downstream tasks, instead of using pre-trained representations directly, which is somehow unrelated to our defacto targets in production environment. Therefore, we need to develop a new model to fine-tune our user behaviour model across multiple downstream tasks with a unified framework. Assuming that we have restored and initialized the parameters of the previous pre-trained model, the fine-tuning model shares the parameters of the pre-trained part, and a linear classification layer is placed on the top of the final output without activation function. Denoting h o as the output of the final MoE-transformer, we have:</p><formula xml:id="formula_9">y i = Tower i (MaxPooling(h o )) , (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>and the T ower i is a linear classification layer of the i th fine-tuning task. Note that the user representation H = MaxPooling(h o ). The overall architecture of our fine-tuning framework is shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-task Training Optimization</head><p>In order to address seesaw and negative transfer problems and to improve learning from multiple tasks, such as regression and classification, we leverage a multi-task optimization strategy, i.e., jointly optimize across multiple tasks, which can be applied in both pre-training stage and fine-tuning stage. Mathematically, we get k training objectives from equation ( <ref type="formula">5</ref>), and therefore, the total loss can be formulated as:</p><formula xml:id="formula_11">Loss(?) = ? 1 * l 1 ( s 1 (? 1 ), s 1 )+ ? 2 * l 2 ( s 2 (? 2 ), s) + ... + ? k * l k ( s k (? k ), s k ) ,<label>(9)</label></formula><p>where Loss(?) denotes the total loss and ? k is the regularization strength of the k th loss. Recall that our objective is actually to maximize Area Under Curve (AUC) score, we consider the following bi-level optimization problem:</p><formula xml:id="formula_12">M ax AU C val (? ? , ?) s.t.? ? = arg min ? Loss(?, ?) ,<label>(10)</label></formula><p>where AU C val is the AUC score on validation dataset while training. However, AU C val (? ? , ?) is non-differentiable with the indicator function I(f (?, x + i ) &lt; f (?, x - j )), and x + i and x - j are the positive and negative samples, respectively. We therefore employ max{0, 1 -(f (?, x + i )f (?, x - j ))} as a differentiable convex surrogate of the above indicator function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Methodology</head><p>In this section, we demonstrate the online and offline performance of SUPERMOE in generating general embedding for user behaviour sequence. We evaluate our model in four different real world test datasets, and one for public and three for private datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">DATASET DESCRIPTION</head><p>We evaluate the performance of our model on four different downstream applications, i.e., SIUPD, Paytool, MCP, and Fortune. SIUPD dataset comes from the IJCAI17 contest<ref type="foot" target="#foot_0">1</ref> , which contains 139,6245 users' shopping logs on Alipay platform. Paytool is a user payment preference dataset, which describes the behaviour of using payment tools for online users. In MCP dataset, we use 103 channels of subscription and redemption behaviour sequences for users. Fortune dataset includes users "impression?click" and "click?purchase" behaviours. All these four datasets are split into training/test sets with the ratio of 0.8/0.2. The statistics of the datasets can be found in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">BASELINES</head><p>We fine-tune and evaluate our model against four other representative models: MMOE <ref type="bibr" target="#b9">(Ma et al., 2018)</ref>, a classical multi-task recommendation model, PLE <ref type="bibr" target="#b15">(Tang et al., 2020)</ref>, an extension of MMOE with multiple progressive extraction layers, BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, a well-famed sequence model widely used in large scale representation learning, especially in NLP and AETN <ref type="bibr">(Zhang et al., 2020a)</ref>, a user representation learning model, which combines multi-head attention and Denoising Autoencoder(DAE) model to generate user embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Offline Evaluation Results</head><p>In order to show the advantages of our model, we conduct the following intrinsic experiments to evaluate offline and online performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">OFFLINE MODEL PERFORMANCE</head><p>In this section, we present the results of offline model performance in the downstream tasks. Table <ref type="table" target="#tab_1">2</ref> summarizes the overall AUC scores of different models across all datasets. Taking the evaluation results of SIUPD dataset as an example, it is obvious that our model improves the baseline method MMoE by gains of 2.7 and 1.8, respectively, in two combined tasks, for the reason that our model utilizes more abundant chronological user behaviours to address the behaviour sparsity issue. Moreover, we outperform the other two sequential models with gains of 0.63 and 0.19, respectively, benefiting from of our multi-task optimization. Similar performances can be observed in other three datasets. It is worth mentioning that our methods all achieve the state-of-the-art performances with significant gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">OFFLINE EMBEDDING PERFORMANCE</head><p>To evaluate the user embedding quality and efficiency of our model, we conduct six different experiments for comparison, and analyze the effects of different embedding methods, as well as different model capacities. We select the user's payment switching task in PAYTOOL dataset to report AUC score, Recall@85 and Recall@50 respectively. The results are illustrated in Table <ref type="table" target="#tab_2">4</ref>. Notably, all sequential embedding methods are better than PLE-only model, which demonstrates the advantage of user embedding. Furthermore, our embedding is more effective than other two sequential models, which takes the same model size of 1 billion. We also investigate the performance of different model capacities, and it can be seen in Table <ref type="table" target="#tab_2">4</ref> that MoE with 20 billions parameters performs much better than MoE with 1 billion, which generates gains of 0.67 AUC, 2.01 recall@85, and 1.39 recall@50, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">ONLINE A/B TESTING</head><p>To further investigate the quality and effectiveness of our user embeddings, we conduct two A/B testing experiments against online baseline model. "Online1" experiment is a payment switching scenario operating on real-world Alipay platform. In this experiment, our model brings on gains of 13.41% pv, 1.97% in conversion and 21.36% GMV. In addition, our model achieves gains of 4.95%,9.11% and 25.19%, respectively, in "Online2" experiment, which is a fund subscription and redemption scenario. These results are summarized in Table <ref type="table" target="#tab_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we investigated the utilization of multi-layer MoE networks as a practical way to massively increase model capacity and to deal with seesaw phenomenon and negative transfer problem. To complete this research, we introduce an user behaviour representation pre-training and fine-tuning model using sparse MoE. We have shown that it is possible to learn large scale user embeddings, while capturing ubiquitous high order correlations using sparse MoE, with our meticulous model architecture. Moreover, we formulated a bi-level optimization method in order to address multi-task optimization. Extensive empirical experiments demonstrated the overwhelming superiority of our method on various real-world datasets comparing to other state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The SuperMoE framework consists of three different stages. In the multi-channel feature projection stage, all the channel features are embedded as dense vectors. During the pre-training stage, a series of masked channel prediction(MCP) tasks are utilized in order to achieve general user representations. The upper right shows the finetune stage, which freezes the parameters of pre-training model as an initialization. The upper left depicts a standard MoE transformer unit with dynamic routing mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-training Model Framework. The main architecture ingredients of pre-training model are a stack of MoE transformers. Basically, our MoE transformer's backbone has a simple structure which consists of a multi-channel feature projection (MFP) layer, a MoE multi-head self-attention (MoE-MSA) layer and two MoE feed-forward network (MoE-FFN) layers. MFP layer takes the following form: y mpf = [s category * w category , split(s ID * w ID ), s dense ] , (4) where [?] means the concatenation operator of all vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Advantages and limitations of the proposed model and the other models</figDesc><table><row><cell>In this paper, we propose SUPERMOE, a general frame-</cell></row><row><cell>work for user sequence behaviour representation and predic-</cell></row><row><cell>tion using sparse MoE transformer. Intuitively, transformer</cell></row><row><cell>demonstrates the importance of capturing long range depen-</cell></row><row><cell>dencies and pairwise or higher order interactions between</cell></row><row><cell>elements (Bommasani et al., 2021a). The sparse gating</cell></row><row><cell>mechanism, such as MoE, has shown its great advantages in</cell></row><row><cell>multi-objective learning in user recommendation systems.</cell></row><row><cell>Therefore, embedding the gating function in transformer</cell></row><row><cell>would be a good alternative to conventional models in user</cell></row><row><cell>representation learning. The comparison of advantages and</cell></row><row><cell>limitations of the proposed model and the other models is</cell></row><row><cell>listed in Table 1.</cell></row><row><cell>Our contributions can thus be summarized as follows: 1)</cell></row><row><cell>We propose a sparse MoE transformer model to deal with</cell></row><row><cell>huge amount of user behaviour sequence data with high</cell></row><row><cell>dimensions. 2) We propose a novel multi-task optimization</cell></row><row><cell>algorithm in order to address seesaw problem and negative</cell></row><row><cell>transfer problem across multiple tasks. 3) We devise a novel</cell></row><row><cell>method to split feature projection layer in order to address</cell></row><row><cell>the issue of GPU memory explosion, which successfully</cell></row><row><cell>integrates hundreds of behaviour channels into model train-</cell></row><row><cell>ing. 4) Our method significantly outperforms existing user</cell></row><row><cell>behaviour representation learning methods.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Overall AUC performance for different models</figDesc><table><row><cell>Model</cell><cell></cell><cell>SIUPD</cell><cell></cell><cell></cell><cell></cell><cell>PAYTOOL</cell><cell></cell><cell></cell><cell>MCP</cell><cell>FORTUNE</cell></row><row><cell></cell><cell cols="10">Category1 Category2 Category1 Category2 Category3 Category4 Category5 subscription CTR</cell><cell>CVR</cell></row><row><cell cols="2">MMOE 80.758</cell><cell cols="3">79.172 87.691</cell><cell>55.016</cell><cell>92.166</cell><cell>61.019</cell><cell>90.581</cell><cell>67.843</cell><cell>80.988 90.765</cell></row><row><cell>PLE</cell><cell>81.819</cell><cell cols="3">79.798 87.762</cell><cell>55.269</cell><cell>92.803</cell><cell>61.267</cell><cell>91.924</cell><cell>68.351</cell><cell>81.719 91.751</cell></row><row><cell>BERT</cell><cell>83.021</cell><cell cols="3">80.319 88.908</cell><cell>56.081</cell><cell>93.217</cell><cell>62.832</cell><cell>93.657</cell><cell>70.092</cell><cell>82.683 92.014</cell></row><row><cell>AETN</cell><cell>82.828</cell><cell cols="3">80.774 89.293</cell><cell>55.961</cell><cell>93.229</cell><cell>62.706</cell><cell>92.899</cell><cell>70.055</cell><cell>82.952 91.817</cell></row><row><cell>OURS</cell><cell>83.453</cell><cell cols="2">80.971</cell><cell>89.598</cell><cell>56.192</cell><cell>93.461</cell><cell>63.574</cell><cell>94.356</cell><cell>71.218</cell><cell>83.791 92.331</cell></row><row><cell></cell><cell cols="4">Table 3. Dataset Descriptions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Dataset Training Test Channels AverageLength</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIUPD</cell><cell>16M</cell><cell>4M</cell><cell>11</cell><cell></cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Paytool</cell><cell>240M</cell><cell>60M</cell><cell>12</cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MCP</cell><cell>80M</cell><cell>20M</cell><cell>103</cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fortune</cell><cell>32M</cell><cell>8M</cell><cell>786</cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Embedding Evaluation in PAYTOOL</figDesc><table><row><cell>Model</cell><cell cols="3">AUC Score Recall@85 Recall@50</cell></row><row><cell>PLE</cell><cell>92.183</cell><cell>19.583</cell><cell>46.581</cell></row><row><cell>PLE+BERT</cell><cell>94.067</cell><cell>28.751</cell><cell>50.673</cell></row><row><cell>PLE+AETN</cell><cell>94.143</cell><cell>29.033</cell><cell>50.894</cell></row><row><cell>PLE+MoE1B</cell><cell>95.721</cell><cell>30.628</cell><cell>53.766</cell></row><row><cell>PLE+MoE10B</cell><cell>96.169</cell><cell>31.193</cell><cell>54.938</cell></row><row><cell>PLE+MoE20B</cell><cell>96.395</cell><cell>32.640</cell><cell>55.174</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Online Comparison of Different Models</figDesc><table><row><cell>Scenario</cell><cell>Models</cell><cell>PV</cell><cell>PVCVR</cell><cell>GMV</cell></row><row><cell>Online1</cell><cell>PLE+BERT OURS</cell><cell>0 13.41%</cell><cell>0 1.97%</cell><cell>0 21.36%</cell></row><row><cell>Online2</cell><cell>PLE+BERT OURS</cell><cell>0 4.95%</cell><cell>0 9.11%</cell><cell>0 25.19%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://tianchi.aliyun.com/dataset/dataDetail?dataId=58</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Behavior sequence transformer for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data</title>
		<meeting>the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13366</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformers in vision: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Persia: A hybrid system scaling deep learning based recommenders up to 100 trillion parameters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05897</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1930" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1872" to="1897" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pre-training user representations for improved recommendation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><surname>U-Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI Conference on Artificial Intelligence</title>
		<meeting>of the AAAI Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA, AAAI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Susano Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Ramstrom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11294</idno>
		<title level="m">Scaling law for recommendation models: Towards general-purpose user representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
		<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><surname>Ptum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01494</idno>
		<title level="m">Pre-training user model from unlabeled user behaviors via self-supervision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Whale: Efficient giant model training over heterogeneous GPUs</title>
		<author>
			<persName><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W W X Z S J Z X L L C Y L Z Z X L W L</forename></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/atc22/presentation/jia-xianyan" />
	</analytic>
	<monogr>
		<title level="m">2022 USENIX Annual Technical Conference (USENIX ATC 22)</title>
		<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Uprec: User-aware pre-training for recommender systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10989</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A graph regularized point process model for event propagation sequence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN52387.2021.9533830</idno>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selfsupervised learning for large-scale item recommendations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tjoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4321" to="4330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameterefficient transfer from sequential behaviors for user modeling and recommendation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1469" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge transfer via pre-training for recommendation: A review and prospect</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in big Data</title>
		<imprint>
			<biblScope unit="page" from="4" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">General-purpose user embeddings based on mobile app usage</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2831" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gcn-based user representation learning for unifying robust recommendation and fraudster detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V N</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 43rd international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
