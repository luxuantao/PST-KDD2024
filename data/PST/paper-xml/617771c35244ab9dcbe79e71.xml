<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gophormer: Ego-Graph Transformer for Node Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-25">25 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
							<email>jzhao8@nd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qianlong</forename><surname>Wen</surname></persName>
							<email>qwen@nd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Hao</roleName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
							<email>yumliu@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
							<email>hasun@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Microsoft Research Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<address>
									<addrLine>Woodstock &apos;18</addrLine>
									<postCode>03-05, 2018</postCode>
									<settlement>June, Woodstock</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<address>
									<addrLine>Woodstock &apos;18</addrLine>
									<postCode>03-05, 2018</postCode>
									<settlement>June, Woodstock</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gophormer: Ego-Graph Transformer for Node Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-25">25 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:2110.13094v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transformers</term>
					<term>Graph Neural Networks</term>
					<term>Node Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have achieved remarkable performance in a myriad of fields including natural language processing and computer vision. However, when it comes to the graph mining area, where graph neural network (GNN) has been the dominant paradigm, transformers haven't achieved competitive performance, especially on the node classification task. Existing graph transformer models typically adopt fully-connected attention mechanism on the whole input graph and thus suffer from severe scalability issues and are intractable to train in data insufficient cases. To alleviate these issues, we propose a novel Gophormer model which applies transformers on ego-graphs instead of full-graphs. Specifically, Node2Seq module is proposed to sample ego-graphs as the input of transformers, which alleviates the challenge of scalability and serves as an effective data augmentation technique to boost model performance. Moreover, different from the feature-based attention strategy in vanilla transformers, we propose a proximity-enhanced attention mechanism to capture the fine-grained structural bias. In order to handle the uncertainty introduced by the ego-graph sampling, we further propose a consistency regularization and a multi-sample inference strategy for stabilized training and testing, respectively. Extensive experiments on six benchmark datasets are conducted to demonstrate the superiority of Gophormer over existing graph transformers and popular GNNs, revealing the promising future of graph transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, graph neural networks (GNNs), have shown excellency in graph mining and are widely applied to a variety of applications such as node classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>, graph classification <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38]</ref>, and recommendation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. Most GNN methods follow a message-passing scheme where the node representation is learned by aggregating and transforming its neighborhood embeddings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>. Despite the prevailing adoption of the messagepassing scheme, there is a growing recognition of the inherent limitations of this paradigm. On one hand, due to the repeated aggregation of local information, the message-passing scheme suffers from the over-smoothing issue, i.e. representations of different nodes become indistinguishable when stacking too many feature propagation layers <ref type="bibr" target="#b5">[6]</ref>. On the other hand, due to the exponential blow-up in computation paths as the model depth increases <ref type="bibr" target="#b0">[1]</ref>, GNNs also experience difficulties in capturing long-range interactions (the over-squashing problem).</p><p>Meanwhile, in the fields of natural language processing and computer vision, the transformer architecture has shown dominant performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> thanks to its powerful feature learning capacity and low inductive bias. In light of its amazing performance and the limitations of GNNs, attempts have been made to introduce transformer into graph learning to replace the message-passing scheme. The majority of these methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref> apply transformers on the entire graph, treating all nodes as fully-connected, and enhance the vanilla feature-based attention mechanism with structural encoding and topology-enhanced attention mechanism. For example, SAN <ref type="bibr" target="#b19">[20]</ref> learns Laplacian positional encodings and uses different attention mechanism for connected and unconnected nodes. Graphormer <ref type="bibr" target="#b33">[34]</ref> achieves state-of-the-art graph classification performance via transformers with centrality, spatial, and edge encodings. However, there are several limitations of these methods. First, the full-graph attention schema suffers from the quadratic dependency (mainly in terms of memory) on the graph size due to the full attention mechanism, leading to the poor scalability especially on large graphs. Moreover, the full-attention mechanism views the entire input graph as a fully-connected graph and fuses information from all the nodes, which potentially introduces noise from numerous irrelevant long-distance neighbors. Last but not least, unlike GNNs with few learnable parameters, the transformers contain much more parameters, which are intractable to be fully trained with scarce annotations and are also more vulnerable to overfitting.</p><p>In light of the potential noise from the long-distance neighbors and the challenge of scalability, we propose to use the ego-graph instead of the entire graph as the transformer input. Specifically, we propose a novel module named Node2Seq to sample an egograph for each node as the input of the transformer. Then, node representations are learned based on the features within the sampled ego-graphs instead of node features of the entire graph. This strategy apparently alleviates the aforementioned problems: First, the challenge of scalability is eased since the input size is reduced from |𝑉 | (number of nodes in the whole graph) to |𝑉 𝐸 | (average number of nodes in ego-graphs) with |𝑉 | ≫ |𝑉 𝐸 |. In this way, transformers can be easily applied to large graphs. Second, the ego-graph preserves the localized contextual information within a pre-defined order, which consequently filters the high-order noise. Last but not least, given the data-hungry <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref> (more data leads to better performance) characteristic of transformers, the sampling process can be viewed as an effective data augmentation operator, which greatly boosts the performance of graph transformers. Despite these benefits, several challenges still need to be addressed when using ego-graph-based transformer. To start with, due to the fully-connected nature of the attention mechanism, the structural information is lost in the construction of sequential input. Therefore, we need to design an effective attention mechanism to incorporate the vital structural information. Moreover, though more scalable, the constructed ego-graph of each node aggregates information solely from its local neighbors and neglects the high-order information of graphs. Hence we need to design another strategy to incorporate the global information as complementary. Last but not least, the sampled ego-graphs of a center node is essentially a subset of this node's full-neighbor ego-graph, which may lost important information and renders potentially unstable performance.</p><p>To address the aforementioned challenges, in this paper we propose a novel model dubbed Ego-graph Transformer (Gophormer) to learn desirable node representations. We demonstrate that using sampled ego-graphs instead of conventional full-graph greatly boosts the transformer's performance. Specifically, Gophormer constructs ego-graphs integrated with global nodes by Node2Seq to capture both local and global structural information. After that, proximity-enhanced attention mechanism is proposed to incorporate both feature and structural proximity to learn node embeddings. To alleviate the uncertainty caused by sampling module, the egograph consistency regularization is applied in the training phase to regularize different samples, and the multi-sample inference strategy is proposed to stabilize predictions. It is noteworthy to highlight our contributions as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Given an input graph 𝐺 = (𝑉 , 𝐸) composed of a node set 𝑉 and an edge set 𝐸, its graph structure information can be represented as an adjacency matrix A ∈ {0, 1} </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Network</head><p>GNN model was first proposed by Scarselli et al. to handle both node and graph level tasks <ref type="bibr" target="#b24">[25]</ref>. Then, encouraged by the success of convolutional neural networks (CNNs) <ref type="bibr" target="#b20">[21]</ref> in the computer vision domain, CNNs was generalized to graph-structured data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. Kipf and Welling simplified the spectral GNN model and proposed graph convolutional networks (GCNs) <ref type="bibr" target="#b16">[17]</ref>. After that, numerous GNN variants have been proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>. Due to its remarkable performance, GNNs have been widely used on many graph applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. Despite the great success of existing GNN methods, the messagepassing schema of GNNs also suffers from some inherent problems: To start with, most GNNs suffer from the over-smoothing problem. It has been observed that deeply stacking the layers often results in significant performance drop for GNNs, such as GCN <ref type="bibr" target="#b16">[17]</ref> and GAT <ref type="bibr" target="#b27">[28]</ref>, even beyond just a few (2-4) layers <ref type="bibr" target="#b38">[39]</ref>. The reason is that, as the graph convolution operation have been shown as a special form of Laplacian smoothing <ref type="bibr" target="#b21">[22]</ref>, stacking many GNN layers repeats the Laplacian smoothing operation and eventually makes node embeddings indistinguishable. Another key issue with existing GNNs is the over-squashing problem <ref type="bibr" target="#b0">[1]</ref>. Due to the exponential blow-up in computation path as the model depth increase, it is hard for GNNs to pass information to distant neighbors with  </p><formula xml:id="formula_0">N u Q = " &gt; A A A B 7 3 i c b V D J S g N B E K 2 J W 4 x b 1 K O X x i D k F G Z E T X I Q A l 4 8 e I h g F k i G 0 N P p J E 1 6 F r t r h D D k J / T g Q R G v f o A / 4 s 2 / s b P g / q D g 8 V 4 V V f W 8 S A q N t v 1 u p R Y W l 5 Z X 0 q u Z t f W N z a 3 s 9 k 5 d h 7 F i v M Z C G a q m R z W X I u A 1 F C h 5 M 1 K c + p 7 k D W 9 4 N v E b N 1 x p E Q Z X O I q 4 6 9 N + I H q C U T R S 8 4 K 0 U f h c d 7 I 5 u 2 B P Q f 4 S Z 0 5 y l f y p f D 2 G u 2 o n + 9 b u h i z 2 e Y B M U q 1 b j h 2 h m 1 C F g k k + z r R j z S P K h r T P W 4 Y G 1 C x x k + m 9 Y 3 J g l C 7 p h c p U g G S q f p 9 I q K / 1 y P d M p 0 9 x o H 9 7 E / E / r x V j r + Q m I o h i 5 A G b L e r F k m B I J s + T r l C c o R w Z Q p k S 5 l b C B l R R h i a i j A n B + f q 9 b F A 8 m p O y 8 x l C / b D g n B S O L k 0 a J Z g h D X u w D 3 l w o A g V O I c q 1 I C B h F t 4 g E f r 2 r q 3 n q z n W W v K m s /</formula><formula xml:id="formula_1">J H g E r X h q K 2 p J j k q c v 0 F v c r X Q = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B q P g K e x K M M k t 4 M V j B P O A Z A m z k 9 l k z O z M M j M r h C X / 4 M W D I l 7 9 H 2 / + j Z N N 8 F 3 Q U F R 1 0 9 0 V x J x p 4 7 r v T m 5 l d W 1 9 I 7 9 Z 2 N r e 2 d 0 r 7 h + 0 t U w U o S 0 i u V T d A G v K m a A t w w y n 3 V h R H A W c d o L J 5 d z v 3 F G l m R Q 3 Z h p T P 8 I j w U J G s L F S u 9 8 c s 4 E 7 K J b c s p s B / S X e k p Q a J 5 C h O S i + 9 Y e S J B E V h n C s d c 9 z Y + O n W B l G O J 0 V + o m m M S Y T P K I 9 S w W O q P b T 7 N o Z O r X K E I V S 2 R I G Z e r 3 i R R H W k + j w H Z G 2 I z 1 b 2 8 u / u f 1 E h P W / J S J O D F U k M W i M O H I S D R / H Q 2 Z o s T w q S W Y K G Z v R W S M F S b G B l S w I X h f v 9 c t q p U l q X u f I b T P y 9 5 F u X J d K T V q i z Q g D 0 d w D G f g Q R U a c A V N a A G B W 7</formula><p>i H R 3 h y p P P g P D s v i 9 a c s 5 w 5 h B 9 w X j 8 A t e y P g A = = &lt; / l a t e x i t &gt; 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d w A + o g I 5 X z K 7 L 5 R c p P F F 5 w Q e a fixed vector size. Hence, GNNs perform poorly if the prediction task is depended on long-range interactions.</p><formula xml:id="formula_2">t n o = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B q P g K e x K M M k t 4 M V j B P O A Z A m z k 9 l k z O z M M j M r h C X / 4 M W D I l 7 9 H 2 / + j Z N N 8 F 3 Q U F R 1 0 9 0 V x J x p 4 7 r v T m 5 l d W 1 9 I 7 9 Z 2 N r e 2 d 0 r 7 h + 0 t U w U o S 0 i u V T d A G v K m a A t w w y n 3 V h R H A W c d o L J 5 d z v 3 F G l m R Q 3 Z h p T P 8 I j w U J G s L F S u 9 8 c s 4 E 3 K J b c s p s B / S X e k p Q a J 5 C h O S i + 9 Y e S J B E V h n C s d c 9 z Y + O n W B l G O J 0 V + o m m M S Y T P K I 9 S w W O q P b T 7 N o Z O r X K E I V S 2 R I G Z e r 3 i R R H W k + j w H Z G 2 I z 1 b 2 8 u / u f 1 E h P W / J S J O D F U k M W i M O H I S D R / H Q 2 Z o s T w q S W Y K G Z v R W S M F S b G B l S w I X h f v 9 c t q p U l q X u f I b T P y 9 5 F u X J d K T V q i z Q g D 0 d w D G f g Q R U a c A V N a A G B W 7 i H R 3 h y p P P g P D s v i 9 a c s 5 w 5 h B 9 w X j 8 A t 3 C P g Q = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F Q c s J G 0 y f T m h 9 r 3 H B 9 W k R r J X D 5 A = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B q v g x p J I s O 2 u 4 M a N U M E + s A 1 l M p 2 0 Q y e T M D M R S u h f u H G h i F v / x p 1 / 4 z Q t v g 9 c O J x z L / f e 4 8 e c K W 3 b 7 1 Z u a X l l d S 2 / X t j Y 3 N r e K e 7 u t V S U S E K b J O K R 7 P h Y U c 4 E b W q m O e 3 E k u L Q 5 7 T t j y 9 m f v u O S s U i c a M n M f V C P B Q s Y A R r I 9 3 2 G i P W T 6 9 O n W m / W L L L d g b 0 l z g L U q o f Q Y Z G v / j W G 0 Q k C a n Q h G O l u o 4 d a y / F U j P C 6 b T Q S x S N M R n j I e 0 a K n B I l Z d m F 0 / R s V E G K I i k K a F R p n 6 f S H G o 1 C T 0 T W e I 9 U j 9 9 m b i f 1 4 3 0 U H V S 5 m I E 0 0 F m S 8 K E o 5 0 h G b v o w G T l G g + M Q Q T y c y t i I y w x E S b k A o m B O f r 9 5 p B x V 2 Q m v M Z Q u u s 7 J y X 3 W u 3 V K / O 0 4 A 8 H M A h n I A D F a j D J T S g C Q Q E 3 M M j P F n K e r C e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Transformers</head><p>Transformers <ref type="bibr" target="#b26">[27]</ref> are powerful encoders composed of two major components: a multi-head self-attention (MHA) module and a position-wise feed-forward network (FFN). The MHA module works as follows: Given an input sequence of</p><formula xml:id="formula_3">H = [𝒉 1 , • • • , 𝒉 𝑛 ] ⊤ ∈ R 𝑛×𝑑</formula><p>where 𝑑 is the hidden dimension and 𝒉 𝑖 ∈ R 𝑑×1 is the hidden representation at position 𝑖, the MHA module firstly projects the input H to query-, key-, value-spaces, denoted as Q, K, V, using three matrices</p><formula xml:id="formula_4">W 𝑄 ∈ R 𝑑×𝑑 𝐾 , W 𝐾 ∈ R 𝑑×𝑑 𝐾 and W 𝑉 ∈ R 𝑑×𝑑 𝑉 : Q = HW 𝑄 , K = HW 𝐾 , V = HW 𝑉 .<label>(1)</label></formula><p>Then, in each head ℎ ∈ {1, 2, . . . , 𝐻 }, the scaled dot-product attention mechanism is applied to the corresponding ⟨Q ℎ , K ℎ , V ℎ ⟩:</p><formula xml:id="formula_5">head ℎ = Softmax Q ℎ K 𝑇 ℎ √︁ 𝑑 𝐾 V ℎ ,<label>(2)</label></formula><p>and the outputs from different heads are further concatenated and transformed to obtain the final output of MHA:</p><formula xml:id="formula_6">MHA(H) = Concat ( head 1 , . . . , head 𝐻 ) W 𝑂 ,<label>(3)</label></formula><p>where W 𝑂 ∈ R 𝑑×𝑑 . In this work, we employ 𝑑 𝐾 = 𝑑 𝑉 = 𝑑/𝐻 . Due to its remarkable performance, the aforementioned transformer <ref type="bibr" target="#b26">[27]</ref> paradigm has become the go-to architecture in many fields such as natural language processing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref> and computer vision <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>. In observation of these impressive achievements, a natural idea is to introduce transformer to graphs. Yet, directly applying such feature-based transformers on graphs will lead to the loss of structural information, whereas the structural information is vital in graph mining tasks. Therefore, the key bottleneck of the performance of graph transformer lies in the effective utilization of graph structures.</p><p>To overcome this bottleneck, several structural preserving techniques have been proposed. For example, GT <ref type="bibr" target="#b11">[12]</ref> proposes a generalization of transformer on graph and uses Laplacian eigen-vectors as positional encoding to enhance the node features. SAN <ref type="bibr" target="#b19">[20]</ref> replaces the static Laplacian eigen-vectors with learnable positional encodings and designs an attention mechanism that distinguishes local connectivity. Graphormer <ref type="bibr" target="#b33">[34]</ref> utilizes centrality encoding to enhance the node feature and uses spatial encoding (SPD-indexed attention bias) along with edge encoding to incorporate structural inductive bias to the attention mechanism.</p><p>However, there are several inherent limitations of existing graph transformers. Above all, the graph is essentially treated as fullyconnected with the MHA mechanism calculating attention for all node pairs, causing severe scalability problem. What's more, since the entire graph is treated as only one sequence, this scheme suffers from data scarcity problem if graphs are few (especially for node classification tasks where only one graph is provided in most cases). Therefore, better means of incorporating structural information is required in developing graph transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED METHOD</head><p>In light of the limitations of the full-graph transformers, we propose Gophormer, shown in Figure <ref type="figure">1</ref>, with two simple yet effective designs to incorporate the structural information into traditional transformer. On one hand, Gophormer utilizes the graph topology to generate training inputs via the Node2Seq module, where the sampled ego-graph induced by each center (training) node is converted to a sequential input. Therefore, the local structural information is explicitly preserved in the ego-graphs. On the other hand, Gophormer leverages proximity-enhanced transformer, which utilizes the proximity encoding composed by different views of structural information to calculate attention values. To perform node classification, the encoded ego-graph representations are read out and mapped by MLP classifier to the final prediction. The model is optimized by supervised loss and consistency regularization loss. We first introduce how Gophormer prepares sequential data for transformer using the Node2Seq module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Node2Seq</head><p>As stated in Section 2.2, current graph transformer methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref> utilize the entire graph to generate input sequences. However, this paradigm is not only memory-consuming but also hard to train, leading to poor performance (further discussed in Section 4.3). To alleviate such limitations of full-graph-based transformers, we propose to sample ego-graphs as input sequences to capture the local contextual information. Specifically, in each epoch, we sample 𝑆 ego-graphs for each training node. In this paper, we use Graph-SAGE sampling <ref type="bibr" target="#b14">[15]</ref>, which uniformly samples the neighbors of each layer to iteratively construct the ego-graphs with pre-defined maximum depth 𝐷. Therefore, the local contextual information within 𝐷-hop is randomly selected as sequential input for the transformer encoder. Note that, users may flexibly design other sampling method to determine the range of contextual nodes. For example, a sampling strategy focusing on high-order neighbors could be a good choice for heterophilic graphs <ref type="bibr" target="#b39">[40]</ref>.</p><p>This strategy is beneficial in several aspects: <ref type="bibr" target="#b0">(1)</ref> The ego-graphbased attention can be viewed as full-graph-based attention with random hard attention masks focusing on local information, i.e. nodes outside the sampled ego-graphs are with multiplicative mask value −∞, in this way the structural information is incorporated. (2) Using sampled ego-graphs instead of full-graph can be viewed as a data augmentation technique which not only enjoys performance gain due to transformers' data hungry nature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref>  So far our proposal only focuses on the local structural contexts, i.e. neighbors within pre-defined depth 𝐷, while the high-order neighbors are neglected. Unfortunately, the importance of such global contextual information has been widely recognized <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref>. To alleviate this problem, inspired by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>, we propose to use global nodes to store global context information. Specifically, we add 𝑛 𝑔 global nodes to each sampled ego-graph with learnable features C = {c 𝑖 ∈ R 𝑑×1 , 𝑖 ∈ 1, . . . , 𝑛 𝑔 }. These global nodes are shared across all the ego-graphs, which preserves the global context information that assist the node classification task. Hence, in each Gophormer layer, each node not only fuses the information of local neighbors but also the global context information.</p><p>In sum, for each epoch, a center node 𝑣 𝑐 and its induced training ego-graphs G 𝑐 = {𝐺 (𝑠) 𝑐 | 𝑠 ∈ {1, 2, ..., 𝑆 }} consist of sampled nodes and global nodes, the input of transformer H 0 𝑐,𝑠 is defined as:</p><formula xml:id="formula_7">H 0 𝑐,𝑠 = Concat X (𝑠) 𝑐 , C ,<label>(4)</label></formula><p>where X</p><p>(𝑠)</p><p>𝑐 denotes the node features of 𝐺 (𝑠) 𝑐 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proximity-Enhanced Transformer</head><p>In this section, we introduce the proposed attention mechanism of Gophormer. As introduced in 2.2, the attention score of traditional transformers <ref type="bibr" target="#b26">[27]</ref> is calculated by the dot product between encoded query and key embeddings. That is to say, if directly applied, the attention scores between different nodes in each ego-graph are calculated by measuring the similarity between encoded features whereas the vital structural information is ignored. To tackle this, one way to introduce structural inductive bias is to use SPDindexed attention bias <ref type="bibr" target="#b33">[34]</ref>. However, this method has two inherent limitations: First, as the name suggests, the SPD only captures the shortest-path relationship, neglecting other structural relationships, e.g. identity <ref type="bibr" target="#b35">[36]</ref> and higher-order relationships <ref type="bibr" target="#b23">[24]</ref>. Moreover, the SPD-indexed attention bias only reflects the shortest path connectedness between a node pair, neglecting the fine-grained probabilities of connectedness. For example, given two pairs of nodes with SPD as 2, the node pair with more 2-hop path instances apparently have stronger relationships compared to the pair with fewer path instances. Thus, these two pairs of nodes should not be equally treated even if their SPD values are identical.</p><p>We propose proximity-enhanced multi-head attention (PE-MHA) to overcome these limitations. Through depiction of the proposed PE-MHA mechanism, we omit the following notations for brevity: 𝑐 for center node, 𝑠 for sample index, and 𝑙 for transformer layers; since the attention mechanism are identical in each layer and in each ego-graph. Specifically, for a node pair 𝑣 𝑖 , 𝑣 𝑗 , 𝑀 views of structural information is encoded as a proximity encoding vector, denoted as 𝝓 𝑖 𝑗 ∈ R 𝑀×1 , to enhance the attention mechanism. The proximity-enhanced attention score 𝛼 𝑖 𝑗 is defined as:</p><formula xml:id="formula_8">𝛼 𝑖 𝑗 = 𝒉 𝑖 W 𝑄 𝒉 𝑗 W 𝐾 𝑇 √ 𝑑 + 𝝓 𝑇 𝑖 𝑗 𝒃,<label>(5)</label></formula><p>where 𝒃 ∈ R 𝑀×1 stands for the learnable parameters that calculate the bias of different structural information. The proximity encoding is calculated by 𝑀 structural encoding functions defined as:</p><formula xml:id="formula_9">𝝓 𝑖 𝑗 = Concat(Φ 𝑚 (𝑣 𝑖 , 𝑣 𝑗 )|𝑚 ∈ 0, 1, .., 𝑀 − 1),<label>(6)</label></formula><p>where each structural encoding function Φ 𝑚 encodes a view of structural information. In this paper, we consider two aspects of structural information for each node pair: (1) whether the node pairs are connected at specific order, (2) whether global node exists in this node pair. The proximity encoding functions are defined as:</p><formula xml:id="formula_10">Φ 𝑚 (𝑣 𝑖 , 𝑣 𝑗 ) = Ã𝑚 [𝑖, 𝑗], if 𝑚 &lt; 𝑀 − 1 I(𝑖, 𝑗), if 𝑚 = 𝑀 − 1 ,<label>(7)</label></formula><p>where I(𝑖, 𝑗) = 1 if global nodes exists in 𝑣 𝑖 , 𝑣 𝑗 , otherwise 0, Ã = Norm(A + I) denotes the normalized adjacency matrices with self-loop. In this way, the first 𝑀 − 1 dimensions of 𝝓 𝑖 𝑗 encodes the reachable probabilities from 0-order (identity relationship <ref type="bibr" target="#b35">[36]</ref>), i.e. whether 𝑣 𝑖 is 𝑣 𝑗 , to (𝑀 − 2)-order between node 𝑣 𝑖 and 𝑣 𝑗 . Hence, the fine-grained proximity that reflects different orders of structural information is preserved for each node pair. The aforementioned proximity encoding scheme enables the proposed Gophormer with strong expressiveness to handle structural data. In fact, the attention mechanism of Graphormer with SPD-indexed bias can be viewed as a special case of proximityenhanced attention mechanism with one-hot proximity-encoding of the shortest-path-order. Moreover, as the attention mechanism of Graphormer can be viewed as a special case of PE-MHA, Gophormer also enjoys the merits of representing the aggregation and combination steps in popular GNN models <ref type="bibr" target="#b33">[34]</ref>.</p><p>We follow the GT <ref type="bibr" target="#b11">[12]</ref> framework to obtain the output of the 𝑙-th transformer layer, denoted as H 𝑙 :</p><formula xml:id="formula_11">Ĥ𝑙 = Norm PE-MHA H 𝑙−1 + H 𝑙−1 , H 𝑙 = Norm FFN Ĥ𝑙 + Ĥ𝑙 ,<label>(8)</label></formula><p>where Norm(•) denotes the layer-norm function. By stacking 𝐿 layers, Gophormer encodes each node inside the sampled ego-graphs</p><formula xml:id="formula_12">𝐺 (𝑠)</formula><p>𝑐 ∈ G 𝑐 and obtain the embedding of the center node 𝑣 𝑐 , denoted as 𝒛 (𝑠) 𝑐 , via a readout function:</p><formula xml:id="formula_13">𝒛 (𝑠) 𝑐 = Readout 𝒉 𝐿 𝑖,𝑐,𝑠 | 𝑣 𝑖 ∈ 𝐺 (𝑠) 𝑐 .<label>(9)</label></formula><p>The readout function can be implemented by graph pooling functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>. Since we are interested in the node property of the center node 𝑣 𝑐 , in this paper, we simply use the center node representation as the final node embedding, i.e. 𝒛 (𝑠) 𝑐 = 𝒉 𝐿 𝑐,𝑐,𝑠 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>In this section, we introduce how to optimize the Gophormer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given the center node representation 𝒛 (𝑠)</head><p>𝑐 of a sampled ego-graph 𝐺 (𝑠) 𝑐 ∈ G 𝑐 , Gophormer first adopts a MLP (Multi-Layer Perceptron) function 𝑓 𝑚𝑙𝑝 with parameters Θ to predict the node class:</p><formula xml:id="formula_14">𝒚 (𝑠) 𝑐 = 𝑓 𝑚𝑙𝑝 𝒛 (𝑠) 𝑐 , Θ ,<label>(10)</label></formula><p>where 𝒚 (𝑠) 𝑐 ∈ R 𝐶×1 stands for the classification result, 𝐶 stands for the number of classes. The supervised loss is achieved by the average cross entropy loss of labeled training nodes 𝑉 𝐿 :</p><formula xml:id="formula_15">L sup = − 1 𝑆 ∑︁ 𝑣 𝑐 ∈𝑉 𝐿 𝑆 ∑︁ 𝑠=1 𝒚 𝑇 𝑐 log 𝒚 (𝑠) 𝑐 ,<label>(11)</label></formula><p>where 𝒚 𝑐 ∈ R 𝐶×1 is the ground truth label of center node 𝑣 𝑐 . We use consistency regularization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref> to enforce the model make similar predictions for ego-graphs induced by the same center node:</p><formula xml:id="formula_16">L con = 1 𝑆 ∑︁ 𝑣 𝑐 ∈𝑉 𝐿 𝑆 ∑︁ 𝑠=1 𝒚 ′ 𝑐 − 𝒚 (𝑠) 𝑐 2 2 ,<label>(12)</label></formula><p>where 𝒚 ′ 𝑐 is the sharpened average distribution of ego-graphs (readers may refer to <ref type="bibr" target="#b13">[14]</ref> for more details). The overall loss is obtained by fusing supervised loss L 𝑠𝑢𝑝 and consistency regularization loss L con with coefficient 𝜆:</p><formula xml:id="formula_17">L = L 𝑠𝑢𝑝 + 𝜆L con .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>Although sampling ego-graphs is an effective data augmentation technique, each sampled ego-graph only preserves partial information within 𝐷-hop. Therefore, chances are that the important information is lost when sampling testing ego-graphs, leading to inferior results. A straight-forward method to solve this problem is to infer node embeddings based on the full-ego-graph <ref type="bibr" target="#b14">[15]</ref>, i.e. get all the induced nodes within the maximum depth 𝐷. However, we empirically find that this method leads to inferior results (further discussed in Section 4.4.4). The potential reason is that the model is trained on ego-graphs while tested on the full-ego-graphs. Therefore, during test time, the model is asked to inference on much larger graphs, leading to performance downgrade <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>To bridge the gap between training and testing graphs while alleviating information loss at the same time, we propose a simple yet effective inference method dubbed multi-sample inference: During testing time, the model samples 𝑆 ′ ego-graphs for each nodes with the same sampling strategy in training time. The final prediction of a center node 𝑣 𝑐 , denoted as 𝒚 𝑐 , is obtained by the average of the predictions of 𝑆 ′ sampled ego-graphs:</p><formula xml:id="formula_18">𝒚 𝑐 = 1 𝑆 ′ 𝑆 ′ ∑︁ 𝑠=1 𝒚 (𝑠) 𝑐 . (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>Through this way, the ego-graph sampling method is consistent during training and testing and the loss of information can be alleviated by using larger 𝑆 ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct extensive experiments to comprehensively evaluate the proposed Gophormer model. Following previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>, node classification is selected as the downstream task to evaluate the effectiveness of our proposal. After that, ablation studies are conducted to demonstrate the importance of different components. Finally, the sensitivity of the model performance on several core parameters are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>4.1.1 Datasets. To comprehensively evaluate the effectiveness of Gophormer, we conduct experiments on the six benchmark datasets including four citation network datasets (i.e. Cora, Citeseer, Pubmed <ref type="bibr" target="#b16">[17]</ref> and DBLP <ref type="bibr" target="#b40">[41]</ref>), and two social network datasets (i.e. Blogcatalog and Flickr <ref type="bibr" target="#b15">[16]</ref>). We set the train-validation-test split as 60%/20%/20%. The statistics of datasets are shown in Table <ref type="table" target="#tab_4">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines.</head><p>To evaluate the effectiveness of Gophormer in graph mining area, we compare it with 8 baseline methods, including five popular GNN methods, i.e. GCN <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b27">[28]</ref>, Graph-SAGE <ref type="bibr" target="#b14">[15]</ref>, JKNet <ref type="bibr" target="#b32">[33]</ref>, and APPNP <ref type="bibr" target="#b18">[19]</ref>, along with four stateof-the-art graph transformers, i.e. GT-Sparse <ref type="bibr" target="#b11">[12]</ref>, GT-Full <ref type="bibr" target="#b11">[12]</ref>, Graphormer <ref type="bibr" target="#b33">[34]</ref>, and SAN <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details.</head><p>We implement Gophormer with Python (3.8.5), Pytorch (1.9.1) and DGL (0.7.1). The code will be made public upon paper publication. We use a  <ref type="table">2</ref>: Node classification performance (mean±std%, the best results are bolded and the runner-ups are underlined).</p><p>Adam as optimizer and adopt the ReduceLROnPlateau scheduler of Pytorch. Specifically, the learning rate first goes through a warm-up process, where the learning rate increase linearly from zero to the peak learning rate and then decay with the decay factor according to the validation performance until it reaches the end learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Parameter Settings.</head><p>We fix some hyper-parameters for the convenience of tuning work: the dropout is set to 0.5, the weight decay is set to 1e-5, the local contextual information of maximum depth 𝐷 is set as 2, the hidden dimension 𝑑 is set to 64 and the number of attention head 𝐻 is set as 8, the end learning rate is set to 1e-9. Other important hyper-parameters are tuned on each dataset by grid search. The search space of learning rate, batch size, number of layers, number global nodes, the number of structural views 𝑀 for proximity encoding, and the regularization balancing coefficient 𝜆 in Eq.13, are {0.0001, 0.0002}, {32, 64, 128}, {1, 2, 3, 4, 5}, {0, 1, 2, 3, 4}, {2, 3, 4}, {0.5, 0.75, 1}, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Node Classification Performance</head><p>The node classification performance of all models are shown in Table <ref type="table">2</ref>, from which we have the following observations: (1) Graph transformer baselines are generally outperformed by GNNs especially on the small scale datasets (i.e. Cora and Citeseer). This phenomenon is probably caused by two reasons: First, transformers have much more parameters compared with GNNs and thus they are harder to train especially on small datasets. Second, in most of these datasets, local information is quite important while the attention over the entire graph essentially introduces more noise to the model. ( <ref type="formula" target="#formula_5">2</ref>) Gophormer achieves state-of-the-art results and outperforms all graph transformer baselines on all six datasets, proving the effectiveness of our proposed model. Notably, Gophormer achieves better performance than the GNN-based baselines on the small scale datasets (i.e. Cora and Citeseer), indicating the good generalization ability of our proposed model. (3) Performance gains of Gophormer on graphs with more nodes or edges (e.g., Flickr, Pubmed and DBLP) are more significant than the ones on the small graphs (e.g., Cora and Citeseer). This is probably due to the datahungry <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref> nature of transformers. Since more nodes and edges lead to more data and more possible data augmentation samples, the superiority of Gophormer is more obvious in these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of Node2Seq</head><p>In this section, we investigate the effectiveness of the proposed graph transformer training paradigm Node2Seq by comparing it with the traditional full-graph training paradigm <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>. We first take a closer look at the training and validation performance through training. As shown in Figure <ref type="figure">2</ref>, it is apparently more intractable to train the full-graph based model according to the observed training trajectories. This is reasonable as full-graph training has the penitential to introduce noise from long-distance neighbors, which may disrupt and distract the training process. Meanwhile, compared to the full-graph training, the trajectories of Node2Seq are more smooth and Node2Seq consistently outperforms the baseline from the early beginning, which demonstrates the superiority of the proposed ego-graph training mechanism.</p><p>To further investigate the generality of the Node2Seq technique, we study whether the proposed data augmentation strategy could improve the performance of the two graph transformer baselines. We integrate the proposed Node2Seq into the SAN and Graphormer models denoted as SAN-Node2Seq and Graphormer-Node2Seq. Figure <ref type="figure">3</ref> presents the classification results of different models. By enjoying the merits of Node2Seq, SAN-Node2Seq and Graphormer-Node2Seq consistently outperform their vanilla versions by a large margin. Specifically, SAN-Node2Seq outperforms its vanilla version by 6.21%, 4.81%, 14.77%, 2.10%, 1.77%, and 12.86% on the six datasets, respectively. Graphormer-Node2Seq achieves 4.56%, 7.65%, 15.37%, 3.11%, 1.25%, 14.38% gains on the six datasets, respectively. Such a huge performance gain further demonstrates the effectiveness and generality of the proposed sampled ego-graph training paradigm. Furthermore, the proposed Gophormer model still consistently outperforms graph transformer baselines with Node2Seq techniques, which proves the effectiveness of other designs within the Gophormer model, e.g. proximity-enhanced transformer, CR loss, and multi-sample inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In order to verify the effectiveness of different modules in Gophormer, we design five sets of experiments to compare the node classification performance of Gophormer and its variants. The results are shown in Figure <ref type="figure" target="#fig_3">4</ref>. 4.4.1 Proximity Encoding Attention Bias. The vanilla transformer calculates the attention scores purely relying on the node attributes, in which the structural information is ignored in the construction of fully-connected graphs. In light of this, the proposed Gophormer model incorporates a proximity-based attention mechanism to better capture the structural bias by considering both feature and structural proximity to learn desirable node representations. To verify the effectiveness of the proximity-based attention mechanism, we design an ablation model Gophormer-w/o-PE without the proximity-based attentions. From Figure <ref type="figure" target="#fig_3">4</ref>, we can observe that Gophormer consistently outperforms Gophormer-w/o-PE on all the six datasets, demonstrating the importance for graph transformer to properly incorporate the structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Consistency Regularization.</head><p>As discussed in Section 3.3, in order to alleviate the uncertainty of ego-graph sampling, Gophormer adopts the consistency regularization loss to enforce the classification distribution learned from different ego-graphs sampled from the same node to be similar. To investigate the effectiveness of this design, we propose a variant of Gophormer without the ego-graph consistency regularization strategy, namely Gophormer-w/o-CR, and compare its node classification performance with Gophormer. From Figure <ref type="figure" target="#fig_3">4</ref>, one can clearly see that model performance drops after removing the consistency regularization. Without the consistency regularization, the sampled ego-graphs of an identical node could be distinct, leading to the different predictions and inferior classification performance. Our proposal can alleviate the uncertainty of sampling ego-graphs and thus boost the performance of Gophormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Global Nodes.</head><p>As discussed in Section 3.2, Gophormer utilizes a set of shared global nodes across different ego-graphs to convey global contextual information. To evaluate the importance of these global nodes, we design another ablation model Gophormerw/o-GN without the global nodes. Figure <ref type="figure" target="#fig_3">4</ref> shows the classification performance of this variation model. After removing the global nodes, the performance consistently exhibits a significant decline over all datasets. The ego-graphs are generated within a fixed hops from the center nodes and then fed into graph transformer. Without adding global nodes, the receptive field of model is limited to a localized small area and further degrades the model preference. Global nodes can be viewed as the intermediaries to exchange global information across different ego-graphs, which are capable of providing external global context as complementary to the local information preserved in the ego-graphs. By simultaneously enjoying the merits of local and global information, Gophormer achieves desirable performance over all datasets. 4.4.4 Multi-sample Inference. As discussed in section 3.4, we propose a multi-sample inference strategy to handle uncertainty and relieve the structural inconsistency between the training and testing sets. To verify these claims, another ablation model Gophormer-FullInf without the multi-sample strategy is designed. Gophormer-FullInf is trained on sampled ego-graphs and tested on the full-egograph without sampling. From the results shown in Figure <ref type="figure" target="#fig_3">4</ref>, we empirically find that the multi-sample schema contributes to boosting the classification performance compared to the Gophormer-FullInf model. The results reveal that the consistent graph sampling method between training and testing sets is crucial to achieve desirable performance, and the proposed multi-sample inference strategy is capable of achieving promising results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Evaluation on Inference Strategies.</head><p>We demonstrated the effectiveness of the proposed multi-sample inference schema in section 4.4.4. In this section, we aim to further reveal the potential reasons why this strategy will work. Two types of ablation models are proposed, including the FullInf which infers the predictions based on the full-ego-graphs, and MSI-𝑘 denotes the proposed multisample inference strategy with 𝑘 samples. From the results shown in Figure <ref type="figure" target="#fig_4">5</ref>, we can see that FullInf and MSI-1-sample settings achieve the worst performance. This is reasonable as FullInf cannot handle the structural inconsistency between graph samples in training set and testing set, while MSI-1-sample makes decisions solely based on a single sample and thus its performance is severely hindered by the uncertainty. Other three MSI variations with larger number of samples achieve better performance. With the increase of 𝑘, the performance first increases and then keeps steady. It means that the incorporation of more sampled ego-graphs in the inference phase contributes to better classification performance at the beginning. When the real category distribution is fully revealed, more samples cannot further improve model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Analysis</head><p>In this section, we study the performance sensitivity of the proposed Gophormer model on two core hyper-parameters: the number of global nodes 𝑛 𝑔 and the number of transformer layers 𝐿.  Here we aim to study the impact of varying the number of global nodes. Figure <ref type="figure" target="#fig_6">6</ref> presents the node classification performance of Gophormer with 𝑛 𝑔 varying from 0 to 4. From the results, one can see that with the increase of 𝑛 𝑔 , the performance increases until reaches at a peak and then decreases. This is reasonable as suitable number of global nodes are capable of incorporating proper global information, while excessive global nodes may introduce too many parameters which slow down the training speed and hurt the generalization ability. Thus, the number of global nodes should be carefully decided to achieve the optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Number of Gophormer</head><p>Layers. We further study the influence of the number of Gophormer layers 𝐿 on the classification performance. We vary 𝐿 from 1 to 5 and exhibit the results in Figure <ref type="figure" target="#fig_7">7</ref>. It is obvious that with the increase of 𝐿, the performance significantly increases at the beginning, since stacking Gophormer layers enlarges representation learning capacity which is beneficial in capturing sophisticated structural patterns. However, deeper models also suffer from serious challenge of over-fitting, which is responsible for the performance decline with 5 transformer layers over all the datasets. Gophormer on small datasets, like Cora and Citeseer, achieves best performance with 𝐿 set to 2 or 3. Meanwhile on other larger datasets, the best performance is often achieved when 𝐿 is set to 3 or 4. Hence, the number of transformer layers should be carefully chosen based on the graph scale and characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>The transformer architecture has shown dominant performance on CV and NLP tasks, but not yet in the graph field especially on node classification tasks. In this work, we propose a novel ego-graphbased transformer model dubbed Gophormer, which effectively incorporates the structural information by Node2Seq module and the proximity-enhanced attention mechanism. We also design consistency regularization loss and multi-sample inference to alleviate the negative impacts of sampling. Extensive experiments are conducted to show the effectiveness of the proposed Gophormer model, revealing the promising future of the emerging field of graph transformers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Figure 1 :</head><label>11</label><figDesc>Figure 1: Model framework of Gophormer. (a) A sample graph with two example nodes colored red and blue. (b) The Node2Seq process: ego-graphs are sampled from the original graph and converted to sequential data. White nodes are context nodes, yellow nodes are global nodes to store graph-level context. In the example two sample ego-graphs are drawn for each node. (c) The proximity encoding process. (d) The main graph transformer framework of Gophormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>but also alleviates the risk of overfitting. Intuitively speaking, since the sampled ego-graphs from the same node can be different in different epochs, it is harder for the model to fit the training data, enforcing training more generalized model. (3) This strategy apparently enables more scalable training with attention values calculated inside ego-graphs instead of full-graphs. Since the input size is reduced from |𝑉 | to |𝑉 𝐸 | (average number of nodes in ego-graphs) with |𝑉 | ≫ |𝑉 𝐸 |, the memory requirement (quadratic to input size) is greatly reduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The performances of full-graph and Node2Seq data augmentation through training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance evaluation of variants of Gophormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The impact of varying number of samples used multi-sample inference settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The classification performance of Gophormer with different global nodes number 𝑛 𝑔 .4.5.1 Global Nodes.The ego-graph contains only the local structural contexts while neglects the high-order information. Therefore, we propose to add 𝑛 𝑔 shared global nodes into the sampled egographs to introduce the global structural information. Here we aim to study the impact of varying the number of global nodes. Figure6presents the node classification performance of Gophormer with 𝑛 𝑔 varying from 0 to 4. From the results, one can see that with the increase of 𝑛 𝑔 , the performance increases until reaches at a peak and then decreases. This is reasonable as suitable number of global nodes are capable of incorporating proper global information, while excessive global nodes may introduce too many parameters which slow down the training speed and hurt the generalization ability. Thus, the number of global nodes should be carefully decided to achieve the optimal performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The classification performance of Gophormer with different numbers of network layer 𝐿.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the datasets.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="4">#Nodes #Edges #Classes #Features</cell><cell>Type</cell></row><row><cell></cell><cell>Cora</cell><cell>2,708</cell><cell>5,278</cell><cell>7</cell><cell>1,433</cell><cell>Citation network</cell></row><row><cell></cell><cell>Citeseer</cell><cell>3,327</cell><cell>4,522</cell><cell>6</cell><cell>3,703</cell><cell>Citation network</cell></row><row><cell></cell><cell>DBLP</cell><cell>17,716</cell><cell>52,864</cell><cell>4</cell><cell>1,639</cell><cell>Citation network</cell></row><row><cell></cell><cell>Pubmed</cell><cell>19,717</cell><cell>44,324</cell><cell>3</cell><cell>500</cell><cell>Citation network</cell></row><row><cell cols="2">Blogcatalog</cell><cell>5,196</cell><cell>171,743</cell><cell>6</cell><cell>8,189</cell><cell>Social network</cell></row><row><cell></cell><cell>Flickr</cell><cell>7,575</cell><cell>239,738</cell><cell>9</cell><cell>12,047</cell><cell>Social network</cell></row><row><cell>Model</cell><cell>Cora</cell><cell cols="2">Citeseer</cell><cell>Blogcatalog</cell><cell>Pubmed</cell><cell>DBLP</cell><cell>Flickr</cell></row><row><cell>GCN</cell><cell cols="3">87.33±0.38 79.43±0.26</cell><cell>78.81±0.29</cell><cell cols="3">84.86±0.19 83.62±0.13 61.49±0.61</cell></row><row><cell>GAT</cell><cell cols="3">86.29±0.53 80.13±0.62</cell><cell>73.20±1.46</cell><cell cols="3">84.40±0.05 84.19±0.19 54.29±2.56</cell></row><row><cell cols="4">GraphSAGE 86.90±0.84 79.23±0.53</cell><cell>76.73±0.39</cell><cell cols="3">86.19±0.18 84.73±0.28 60.37±0.27</cell></row><row><cell>APPNP</cell><cell cols="3">87.15±0.43 79.33±0.35</cell><cell>95.63±0.23</cell><cell cols="3">87.04±0.17 84.40±0.17 93.25±0.24</cell></row><row><cell>JKNet</cell><cell cols="3">87.70±0.65 78.43±0.31</cell><cell>78.46±1.74</cell><cell cols="3">87.64±0.26 84.57±0.28 53.66±0.40</cell></row><row><cell>GT-full</cell><cell cols="3">63.40±0.94 58.75±1.06</cell><cell>65.32±0.46</cell><cell cols="3">77.29±0.50 78.15±0.41 60.77±0.82</cell></row><row><cell>GT-sparse</cell><cell cols="3">71.84±0.62 67.38±0.76</cell><cell>70.65±0.47</cell><cell cols="3">82.11±0.39 81.04±0.27 68.59±0.64</cell></row><row><cell>SAN</cell><cell cols="3">74.02±1.01 70.64±0.97</cell><cell>74.98±0.52</cell><cell cols="3">86.22±0.43 83.11±0.32 70.26±0.73</cell></row><row><cell cols="4">Graphormer 72.85±0.76 66.21±0.83</cell><cell>71.84±0.33</cell><cell cols="3">82.76±0.24 80.93±0.39 66.16±0.24</cell></row><row><cell cols="8">Gophormer 87.85±0.10 80.23±0.09 96.03±0.28 89.40±0.14 85.20±0.20 91.51±0.28</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1-Layer-MLP as the 𝑓 𝑚𝑙𝑝 in Eq.10 to predict the classes. The global nodes are added to the input of first Gophormer layer's FFN layer since the feature dimension 𝑑 𝐹 might unequal with the hidden dimension 𝑑. We use</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the Bottleneck of Graph Neural Networks and its Practical Implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Accurate Learning of Graph Representations with Graph Multiset Pooling</title>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minki</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MixMatch: A Holistic Approach to Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Generalization of Transformer Networks to Graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Yihong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph Random Neural Networks for Semi-Supervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Label informed attributed network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Few More Examples May Be Worth Billions of Parameters</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Kirstain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04374</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rethinking Graph Transformers with Spectral Attention</title>
		<author>
			<persName><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>CoRR abs/2103.14030</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Asymmetric Transitivity Preserving Graph Embedding</title>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Investigating and Mitigating Degree-Related Biases in Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12386</idno>
		<title level="m">Non-IID Graph Neural Networks</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR abs/1901.00596</idno>
		<title level="m">A Comprehensive Survey on Graph Neural Networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Do Transformers Really Perform Bad for Graph Representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Identity-aware Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Gomes-Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Big Bird: Transformers for Longer Sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2006.04131" />
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
