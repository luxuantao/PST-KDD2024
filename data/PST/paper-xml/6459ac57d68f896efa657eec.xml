<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting</title>
				<funder>
					<orgName type="full">Ministry of Education, Singapore</orgName>
				</funder>
				<funder ref="#_jV6mR9c">
					<orgName type="full">Academic Research Fund Tier 2 (Proposal ID</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-05">5 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
							<email>yfang@smu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-05">5 May 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3539618.3591641</idno>
					<idno type="arXiv">arXiv:2305.03324v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text classification</term>
					<term>graph neural networks</term>
					<term>low-resource learning</term>
					<term>pre-training</term>
					<term>prompt-tuning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text classification is a fundamental problem in information retrieval with many real-world applications, such as predicting the topics of online articles and the categories of e-commerce product descriptions. However, low-resource text classification, with few or no labeled samples, poses a serious concern for supervised learning. Meanwhile, many text data are inherently grounded on a network structure, such as a hyperlink/citation network for online articles, and a user-item purchase network for e-commerce products. These graph structures capture rich semantic relationships, which can potentially augment low-resource text classification. In this paper, we propose a novel model called Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource text classification in a two-pronged approach. During pre-training, we propose three graph interaction-based contrastive strategies to jointly pre-train a graph-text model; during downstream classification, we explore prompting for the jointly pre-trained model to achieve low-resource classification. Extensive experiments on four real-world datasets demonstrate the strength of G2P2 in zero-and few-shot low-resource text classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Content analysis and feature selection; Clustering and classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Text classification is a fundamental research problem with many important applications in information retrieval. For example, predicting the topics of online articles can help readers easily search and navigate within the website or portal <ref type="bibr" target="#b28">[29]</ref>, and classifying the category of e-commerce item descriptions enables businesses to structure their inventory efficiently and improve users' search experience <ref type="bibr" target="#b59">[60]</ref>. Advances in supervised deep learning in the last decade have achieved remarkable success for text classification, especially when there are large-scale and high-quality labeled data. However, data labeling is often costly and time-consuming, making low-resource classification, in which no or few labeled samples are available, an appealing alternative.</p><p>To address low-resource text classification, one approach is to utilize pre-trained language models (PLMs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref>, many of which are based on the transformer architecture <ref type="bibr" target="#b47">[48]</ref> due to its powerful ability of encoding texts. A PLM can be adapted to different tasks by fine-tuning the model parameters to task-specific objectives. While the "pre-train, fine-tune" paradigm requires fewer labeled data than traditional supervised learning, it suffers from two drawbacks. First, state-of-the-art PLMs typically have a huge model size, e.g., GPT-3 has 175 billion parameters <ref type="bibr" target="#b2">[3]</ref>, making fine-tuning prohibitively expensive <ref type="bibr" target="#b18">[19]</ref>. Second, fine-tuning still needs a reasonable amount of labeled data due to the gap between pre-training and fine-tuning objectives, and thus struggles with low-resource scenarios including zero-and few-shot classification. To overcome the problem of pretraining and fine-tuning, prompting <ref type="bibr" target="#b2">[3]</ref> has been proposed. It uses a natural language instruction or "prompt" to give a hint of the downstream task, whilst freezing the parameters of a large PLM. In other words, no fine-tuning or additional training is required at all for a new task. However, discrete natural language prompts can be difficult to design and may result in suboptimal performance compared to fine-tuning <ref type="bibr" target="#b17">[18]</ref>. More recently, prompt tuning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> formulates a continuous prompt as a learnable embedding, which is optimized during task adaptation without updating the PLM.</p><p>Meanwhile, text data are frequently grounded on network structures, such as hyperlink or citation networks for online articles, and user-item interaction graphs for e-commerce. These graph structures expose valuable relationships between articles or items, which can be used to augment low-resource text classification. While existing PLMs and prompting do not exploit these relationships, graph neural networks (GNNs) <ref type="bibr" target="#b57">[58]</ref> are designed to learn from graph structures based on a message-passing architecture. However, traditional end-to-end training of GNNs heavily relies on abundant task-specific labels, which motivates self-supervised GNNs <ref type="bibr" target="#b56">[57]</ref> that employ well-designed pretext tasks on a label-free graph <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">50]</ref>. Unfortunately, the treatment of text features in GNNs remains rudimentary. Typically, a simple bag-of-words representation <ref type="bibr" target="#b61">[62]</ref> or aggregation of shallow word embedding vectors <ref type="bibr" target="#b29">[30]</ref> is fed into GNNs as the initial node features, which are further propagated along graph structures. Hence, the modeling of texts in GNNs is coarse-grained, unable to fully capture the subtle semantic differences and similarities within texts.</p><p>Challenges and present work. To overcome the limitations of existing text-and graph-based solutions, we must address two open questions as follows.</p><p>Firstly, how do we capture fine-grained textual semantics, while leveraging graph structure information jointly? A na?ve approach is to use a language model to generate features from raw texts as input, and then train a GNN. However, in this way, the texts and graph are only loosely coupled, lacking an explicit pairing to complement each other. In this paper, we propose graph-grounded contrastive pre-training, to maximize the alignment between text and graph representations based on three types of graph interaction, namely, text-node, text-summary, and node-summary interactions.</p><p>Secondly, how do we augment low-resource text classification given a jointly pre-trained graph-text model? We propose a novel approach of "prompting" a jointly pre-trained graph-text model instead of fine-tuning it. This allows us to leverage the most relevant structural and semantic information from the pre-trained model, making the process friendlier to low-resource scenarios. More specifically, we use handcrafted discrete prompts for zero-shot classification, and continuous prompts for few-shot settings based on automatic prompt-tuning. Due to the significantly fewer parameters involved, prompt-tuning is more label-and computation-efficient than finetuning the pre-trained model. Furthermore, we propose a contextbased initialization for prompt-tuning that considers graph structures between texts to provide a more informative starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>To summarize, we make the following contributions in this work. <ref type="bibr" target="#b0">(1)</ref> This is the first attempt to pre-train text and graph encoders jointly for low-resource text classification. <ref type="bibr" target="#b1">(2)</ref> We propose a novel model called Graph-Grounded Pre-training and Prompting (G2P2), with three graph interaction-based constrastive strategies in pre-training, and a prompting approach for the jointly pre-trained graph-text model in downstream tasks. <ref type="bibr" target="#b2">(3)</ref> We conduct extensive experiments on four real-world datasets to demonstrate the strength of G2P2 in zero-and few-shot text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph neural networks. Inspired by the success of convolutional networks in computer vision, GNNs have emerged to handle non-Euclidean relational data <ref type="bibr" target="#b57">[58]</ref>, ranging from early semi-supervised models such as GCN <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b48">[49]</ref> and GIN <ref type="bibr" target="#b60">[61]</ref>, to the more recent self-supervised pre-training paradigm <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b49">50]</ref>. Besides their widespread success in graph tasks, they have also been leveraged to improve text-based tasks through knowledge graphs <ref type="bibr" target="#b3">[4]</ref> and heterogeneous graphs <ref type="bibr" target="#b19">[20]</ref>, or multi-modal learning <ref type="bibr" target="#b25">[26]</ref>. However, these approaches either employ coarse-grained text treatment, or have decoupled graph and text encoders without fully exploiting the intrinsic relationship between them. Although a more recent approach called GLEM <ref type="bibr" target="#b67">[68]</ref> integrates both the text and graph structure information by fusing language models and GNNs, it is not designed for low-resource learning.</p><p>Language pre-training and prompting. Pre-trained language models <ref type="bibr" target="#b9">[10]</ref> have become the most popular backbone in natural langauge processing (NLP). While earlier PLMs such as GPT <ref type="bibr" target="#b36">[37]</ref>, BERT <ref type="bibr" target="#b15">[16]</ref>, XLNet <ref type="bibr" target="#b62">[63]</ref> and RoBERTa <ref type="bibr" target="#b24">[25]</ref> still have affordable model size, recent introductions such as T5 <ref type="bibr" target="#b37">[38]</ref> and GPT-3 <ref type="bibr" target="#b2">[3]</ref> produce massive models with billions of parameters. To avoid the high cost of fine-tuning on these large models, prompting <ref type="bibr" target="#b21">[22]</ref> starts to receive more attention in the community. A prompt is a special template to pad the task input, with the goal of extracting useful knowledge from PLMs to flexibly adapt to downstream tasks. Fueled by the success of GPT-3, numerous prompting methods including discrete natural language prompt <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> and continuous prompt <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b68">69]</ref> have emerged. The strength of prompting has been validated in a wide range of NLP applications, including text classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b65">66]</ref>, machine translation <ref type="bibr" target="#b45">[46]</ref> and relation extraction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref>. More recently, prompting has also been applied to GNNs for node classification <ref type="bibr" target="#b43">[44]</ref>.</p><p>Zero-or few-shot paradigms. Broadly speaking, our setting is also related to other learning paradigms. For example, in semisupervised learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b58">59]</ref>, each class may only have a few examples, but all classes must be seen in training and they cannot handle any novel class during testing. Meta-learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b69">70]</ref> is another popular paradigm that supports few-shot learning. However, large-scale labeled data are still required in a so-called "meta-training" phase, to support the few-shot learning of novel classes during "meta-testing". In contrast, we only need label-free data for pre-training, without requiring any meta-training phase that would consume large-scale labeled data. Separately, there also exists joint consideration of image and text data using a contrastive pre-training strategy for zero-or few-shot classification <ref type="bibr" target="#b35">[36]</ref>. In our work, graph data are significantly different from images, which provide various types of interaction between texts. On graphs, zero-shot node classification has also been done <ref type="bibr" target="#b52">[53]</ref>. It relies heavily on the availability of Wikipedia pages or other side information to generate class prototype embeddings. However, it is very labor intensive to find and curate the right side information, especially when there are a large number of classes and/or novel classes emerge frequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>In this section, we introduce our approach G2P2 for low-resource text classification. We start with some preliminaries and an overview, and then present the details of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Graph-grounded text corpus. Consider a set of documents D, which is grounded on a graph G = (D, E, X) such that each document ? ? ? D is a node ? ? in the graph. The documents are linked via edges in E, which are formed based on the application (e.g., if each document represents an article, the edges could be citations between articles). Each node ? ? is also associated with a feature vector x ? , given by the input feature matrix X. Finally, each document/node<ref type="foot" target="#foot_0">1</ref> has a class label (e.g., the topic of the article).   Low-resource classification. A low-resource task consists of a support set S and a query set Q. The support set S contains ? classes, and each class has ? labeled examples where ? is a small number (e.g., 1 or 5), known as ? -way ?-shot classification. The query set Q contains one or more unlabeled instances belonging to the ? classes in the support set. Our goal is to classify the instances in the query set based on the labeled examples in the support set. Unlike episodic few-shot meta-learning <ref type="bibr" target="#b6">[7]</ref> which has both training tasks and testing tasks, we only have testing tasks; in the training stage, we perform self-supervised pre-training on label-free data only. As a special case, tasks with ? = 0 are known as zero-shot classification, which means that there is no labeled example at all and we can only rely on class metadata (e.g., class label text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of G2P2</head><p>As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, our model consists of two stages: (a) graphgrounded constrastive pre-training, and (b) graph-grounded prompttuning for low-resource classification.</p><p>During pre-training, we learn a dual-modal embedding space by jointly training a text encoder and graph encoder in a selfsupervised fashion, since a document also exists as a node on the graph. More specifically, we use a transformer-based text encoder and a GNN-based graph encoder. The transformer takes the text on each node (i.e., document) as the input, and outputs a text embedding vector t ? for node ? ? . On the other hand, the GNN takes the graph and node features as input, and generates a node embedding vector z ? for node ? ? . Subsequently, in the dual-modal embedding space, we align the text and graph representations on the same or related nodes through three contrastive strategies based on different types of interaction on the graph.</p><p>In downstream testing, we employ prompting on our jointly pre-trained graph-text model for zero-or few-shot classification. For zero-shot classification, we use handcrafted discrete prompts together with the label text. For few-shot classification, we use continuous prompts to pad the label text. In particular, for prompttuning, we initialize the continuous prompt embeddings based on graph contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph-grounded contrastive pre-training</head><p>The graph-grounded pre-training learns a dual-modal embedding space by jointly training a text encoder and a graph encoder, based on three types of interaction on the underlying graph.</p><p>Dual encoders. The text encoder is a transformer <ref type="bibr" target="#b47">[48]</ref>, which we denote ? ? . Given a document ? ? , the text encoder<ref type="foot" target="#foot_1">2</ref> outputs the ?-dimensional embedding vector of ? ? , denoted t ? ? R ? :</p><formula xml:id="formula_0">t ? = ? ? (? ? ; ? ? ),<label>(1)</label></formula><p>where ? ? represents the parameter set of the transformer. Correspondingly, let T ? R | D |?? represent the text embedding matrix for all documents. At the same time, a document ? ? is also a node ? ? in the graph. We choose a classic GNN called graph convolutional network (GCN) <ref type="bibr" target="#b16">[17]</ref> as the graph encoder, denoted ? ? . It similarly outputs an embedding vector z ? ? R ? for a given node ? ? :</p><formula xml:id="formula_1">z ? = ? ? (? ? ; ? ? ),<label>(2)</label></formula><p>where ? ? represents the parameter set of the GCN. Likewise, let Z ? R | D |?? represent the graph embedding matrix for all nodes.</p><p>Text-node interaction. Our graph-grounded texts naturally implies a bijection between nodes and texts, where each document ? ? corresponds to the node ? ? in the graph. Inspired by the pairing of image and its caption text <ref type="bibr" target="#b35">[36]</ref> and the mapping of content and node sequences <ref type="bibr" target="#b20">[21]</ref>, we design a pre-training strategy to predict which text document matches which node in the graph. Specifically, given ? documents and the corresponding ? nodes, there are ? 2 possible document-node pairs {(? ? , ? ? ) | ?, ? = 1, . . . , ?}.</p><p>Among them, only ? pairs with ? = ? are true matching, whereas the remaining ? 2 -? pairs are false matching. As our first contrastive strategy, we exploit the bijective interaction between texts and nodes on the graph, to maximize the cosine similarity of the ? matching pairs, while minimizing the cosine similarity of the ? 2 -? unmatching pairs. To compute the cosine similarity for the ? 2 pairs, we first perform a row-wise L2 normalization on embedding matrices T and Z to obtain T and Z, respectively. We then compute a node-text similarity matrix ? 1 ? R ??? to capture pairwise cosine similarity, as follows.</p><formula xml:id="formula_2">? 1 = Z T? ? exp(?),<label>(3)</label></formula><p>where ? ? R is a trainable temperature parameter to scale the similarity values <ref type="bibr" target="#b35">[36]</ref>.</p><p>Remark. Although ? 1 ? R ??? is a dense matrix, it is constructed batchwise for practical implementation. That is, ? is not the total number of documents, but the relatively small batch size, and thus the overhead is negligible. ? 2 and ? 3 will be introduced later following the same treatment. ? To formulate the contrastive loss based on the text-node bijective interaction, we adapt the multi-class N-pair loss <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b66">67]</ref>, by considering both the row-wise and column-wise cross entropy loss w.r.t. the row or column index. For example, the ?-th row of ? 1 represents the similarity scores between node ? ? and every document, in which the row index ? indicates the ground truth document ? ? that matches ? ? .</p><formula xml:id="formula_3">L 1 = 1 2 CE(? 1 , y) + CE(? ? 1 , y) ,<label>(4)</label></formula><p>where y = (1, 2, . . . , ?) ? is the label vector for contrastive training, and CE denotes the cross entropy loss applied to the input matrix ? 1 or ? ? 1 in a row-wise manner. Text-summary interaction. Apart from the bijective text-node interaction, we further exploit higher-order interactions on the graph. In particular, each document has a set of neighboring documents defined by graph topology. The neighboring documents can be understood as a summary of the target document given the semantic relatedness between them. For example, on an e-commerce network, the products purchased by a user naturally portray a summary of the user and vice versa. Without loss of generality, we employ a simple mean pooling to generate the summary embedding s ? ? R ? as follows.</p><formula xml:id="formula_4">s ? = 1 | N ? | ? ?N ? t ? .<label>(5)</label></formula><p>For efficiency, we only sample a fixed number of neighboring documents to generate the summary. Then, let S ? R ??? denote the summary text embedding matrix for all documents. Hence, as our second contrastive strategy, we seek to align the text embedding of each document and its corresponding summary text embedding, based on the text-summary interaction derived from graph neighborhood. In other words, we maximize the cosine similarity of the ? matching pairs of document and its neighborhood-based summary, while minimizing the cosine similarity of the ? 2 -? unmatching pairs. Specifically, we first follow Eq. ( <ref type="formula" target="#formula_2">3</ref>) to construct a text-summary similarity matrix ? 2 ? R ??? : calculate the similatity matrices ? 1 , ? 2 , ? 3 ; ? Eqs. ( <ref type="formula" target="#formula_2">3</ref>), ( <ref type="formula" target="#formula_5">6</ref>), <ref type="bibr" target="#b7">(8)</ref> 11:</p><formula xml:id="formula_5">? 2 = TS ? ? exp(?). (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>calculate the contrastive losses L 1 , L 2 , L 3 ; ? Eqs. ( <ref type="formula" target="#formula_3">4</ref>), ( <ref type="formula" target="#formula_7">7</ref>), <ref type="bibr" target="#b8">(9)</ref> 12:</p><p>update the overall loss L; ? Eq. ( <ref type="formula" target="#formula_11">10</ref>)</p><p>13:</p><p>? 0 ? , ? 0 ? ? update via backpropagation 14:</p><p>end for 15: end while 16: return ? 0 ? , ? 0 ? .</p><p>Subsequently, we apply the same contrastive loss following Eq. ( <ref type="formula" target="#formula_3">4</ref>), as follows.</p><formula xml:id="formula_7">L 2 = 1 2 CE(? 2 , y) + CE(? ? 2 , y) ,<label>(7)</label></formula><p>Node-summary interaction. The neighborhood-based summary for document ? ? also serves as a semantic description of node ? ? .</p><p>Mirroring the text-summary interaction, as our third contrastive strategy, we seek to align the node embedding and its neighborhoodbased summary text embedding. In the following, we similarly compute a node-summary similarity matrix ? 3 ? R ??? , and formulate the corresponding contrastive loss L 3 .</p><formula xml:id="formula_8">? 3 = ZS ? ? exp(?),<label>(8)</label></formula><formula xml:id="formula_9">L 3 = 1 2 CE(? 3 , y) + CE(? ? 3 , y) . (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>Overall pre-training objective. Finally, we integrate the three contrastive losses based on the text-node, text-summary and nodesummary interactions. We obtain a pre-trained model ? 0 = (? 0 ? , ? 0 ? ) consisting of the parameters of the dual encoders, given by</p><formula xml:id="formula_11">? 0 = arg min ? ? ,? ? L 1 + ?(L 2 + L 3 ),<label>(10)</label></formula><p>where ? ? R + is a hyperparameter to balance the contribution from summary-based interactions. The pre-training procedure is outlined in Algorithm 1, which has the following complexity per epoch. Let |D| be the number of documents, ? be the number of neighbors sampled to generate the summary embedding in Eq. ( <ref type="formula" target="#formula_4">5</ref>), and ? be the batch size. First, the cost of generating the three types of embeddings (lines 5-8) per epoch is ? (|D|?), given that calculating the summary embedding needs go through ? neighbors. Second, the cost of calculating the three similarity matrices in each batch is ? (? 2 ), and the total cost</p><formula xml:id="formula_12">per epoch is ? | D | ? ? 2 = ? (|D|?) given | D |</formula><p>? batches in an epoch. Thus, the overall complexity is ? (|D|(? + ?)), which is linear in the number of documents, since ? and ? are small constants. In our implementation, we set ? = 3 and ? = 64. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prompting joint graph-text model</head><p>After pre-training our graph-text model, it is non-trivial to apply it to low-resource classification. To narrow the gap between pre-training and downstream tasks, the traditional "pre-train, finetune" paradigm typically introduces a new projection head for the downstream task, which will be fine-tuned together with the whole pre-trained model. However, in a low-resource setting, it is neither effective nor efficient to update the entire model with a huge number of parameters. Without updating massive PLMs, prompting has recently emerged as a powerful alternative to fine-tuning in NLP <ref type="bibr" target="#b21">[22]</ref>. However, prompting has not been explored for graphtext models, where structural and textual information have been jointly pre-trained. In the following, we elaborate on our prompting strategies for zero-and few-shot classification.</p><p>Zero-shot classification. In the zero-shot setting, we can only use handcrafted discrete prompts, as the absence of labeled data in zero-shot tasks cannot support learnable prompts.</p><p>In ? -way zero-shot classification, out of ? classes, we predict the class which has the highest similarity to the given node. As illustrated by the diagram in Fig. <ref type="figure" target="#fig_2">2</ref>, the classification weights can be generated by the text encoder based on the class label texts <ref type="bibr" target="#b50">[51]</ref>, without requiring any labeled sample for the classification task. Specifically, the weight vector w ? for class ? ? {1, 2, . . . , ? } is the output of the pre-trained text encoder, i.e.,</p><formula xml:id="formula_13">w ? = ? ? ("prompt [CLASS]"; ? 0 ? ).<label>(11)</label></formula><p>Here "prompt [CLASS]" is a prompt template, where [CLASS] refers to the label text of the target class ? (e.g., "NLP" for paper area classification), and prompt is a manually engineered sequence of natural language tokens to signal the relevance of the label text (e.g., "paper of NLP" helps focus on the topic of the paper). In the simplest case, "prompt" can be an empty string so that we only rely on the label text. Then, the class distribution given node representation z ? is predicted as</p><formula xml:id="formula_14">? (? | z ? ) = exp ?z ? , w ? ? ? ?=1 exp ?z ? , w ? ? ,<label>(12)</label></formula><p>where ??, ?? is the cosine similarity.</p><p>Few-shot classification. The problem with discrete prompts is that they are difficult to optimize, given that PLMs are intrinsically continuous. Substituting discrete natural language prompts with learnable continuous prompts, prompt tuning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> can automate the optimization of prompts when some labeled data are available. Hence, in the few-shot setting, we explore prompt tuning to cue in the relevant structural and semantic information from our jointly pre-trained graph-text model. Specicifally, instead of a sequence of discrete tokens, we take a sequence of continuous embeddings [h 1 , ? ? ? , h ? , h CLASS ] as the prompt, where ? is a hyperparameter indicating the number of context tokens, each h ? (? ? ?) is a trainable vector, and h CLASS is the word embedding sequence of the target class label. The continuous prompt is fed as input to the text encoder to generate the classification weights for each class ?:</p><formula xml:id="formula_15">w ? = ? ? ([h 1 , ? ? ? , h ? , h CLASS ]; ? 0 ? ),<label>(13)</label></formula><p>where each h ? (? ? ?) has the same dimension as the input word embeddings to the text encoder.</p><p>Using the same softmax layer in Eq. ( <ref type="formula" target="#formula_14">12</ref>), we further update the continuous prompt embeddings using the labeled support set of the few-shot task by minimizing a cross entropy loss, whilst freezing the parameters of the dual encoders. This prompt tuning process is both data-and computation-efficient, given the small number of learnable parameters in the prompt.</p><p>Furthermore, existing prompt tuning methods either initialize the prompt embeddings randomly <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> or using the word embeddings of handcrafted discrete prompts <ref type="bibr" target="#b70">[71]</ref>. While random initialization is non-informative and more prone to local optimum, it is still difficult to pick the right discrete prompts for initialization. Therefore, we take the advantage of graph structures to initialize the prompt embeddings.</p><p>Specifically, given a node ? ? , we define its graph contexts as its neighbor set {? ? | ? ? N ? }. Due to the underlying semantic relatedness, the graph contexts of the few-shot examples carry strong signals about the task, which can be exploited to improve the initialization. For each document/node ? ? in the task support set, we sample ? nodes from its graph contexts. For ? ? itself and each context node sampled, we truncate its corresponding document to ? words, and convert it to a sequence of ? word embedding vectors, each having the same dimension as the vector h ? (? ? ?) in our continuous prompt. Hence, for each support node, we would obtain ? +1 such sequences; in an ? -way ?-shot task, there is a total of ? ? (? + 1) sequences. We take the average of these embedding sequences to initialize the learnable prompt vectors h 1 , . . . , h ? , which is derived from graph contexts and thus could provide a more informative starting point than random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct extensive experiments to evaluate our proposed approach G2P2, with comparison to state-of-the-art baselines and model analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Datasets. Four public graph-grounded text corpora are used, as summarized in Tab. 1.</p><p>? Cora: Known as the "Cora Research Paper Classification" dataset <ref type="bibr" target="#b27">[28]</ref>, it is a collection of research papers that are linked to each ? Art, Industrial and Music Instruments (M.I.) are three Amazon review datasets <ref type="bibr" target="#b32">[33]</ref>, respectively from three broad areas, namely, arts, crafts and sewing (Art), industrial and scientific (Industrial), and musical instruments (M.I.). The description of each product is deemed a text document, whereas the reviews of a user are combined into one document to reflect the user's preferences.</p><p>If a user has reviewed a product, a link is constructed between them. The product subcategories within a broad area represent the classes, which are fine-grained and may involve thousands of classes with subtle differences. The classification is only performed on product descriptions, whereas the user reviews only serve to enrich the text semantics.</p><p>For all datasets, we employ the word2vec algorithm <ref type="bibr" target="#b29">[30]</ref> to obtain the 128-dimensional word embeddings of each word in the text documents. Then, for each node, we average the word embedding vectors of all the words in its document, and the averaged vector is used as the node's input features for the GNN-based methods.</p><p>Task construction. We perform zero-or few-shot text classification. We adopt a 5-way setting, i.e., we sample five classes from all the classes to construct a task. In each task, we construct a ?shot support set by further sampling ? examples from each class for ? ? {0, 1, . . . , 5}, and a validation set of the same size as the support set. The remaining examples form the query set. Note that the support set is labeled and serves as task training data, whereas the query set is unlabeled and used for evaluation. Note that in our experiments, all the classes are used-it is only that each task involves five classes, and we have multiple tasks during testing to cover all the classes. This is a typical task setup <ref type="bibr" target="#b6">[7]</ref>, which allows for a comprehensive evaluation under different class combinations. The reported results are averaged over all the tasks on each dataset.</p><p>Baselines for few-shot classification. We consider competitive baselines from four categories.</p><p>(1) End-to-end GNNs, which are graph neural networks trained in a supervised, end-to-end manner from random initialization.</p><p>? GCN <ref type="bibr" target="#b16">[17]</ref>: an extension of the convolutional neural network that operates on the graph. ? SAGE sup <ref type="bibr" target="#b8">[9]</ref>: the supervised version of GraphSAGE, an inductive GNN that generates node embeddings by sampling and aggregating features from a node's local neighborhood.</p><p>? TextGCN <ref type="bibr" target="#b63">[64]</ref>: a GCN-based model on a text graph constructed from word co-occurrence and document-word relations, which jointly learns the embeddings of both words and documents.</p><p>(2) Pre-trained/self-supervised GNNs, these GNNs are pre-trained using pretext tasks without labeled data, followed by fine-tuning or fitting a classification head while freezing the model parameters.</p><p>? GPT-GNN <ref type="bibr" target="#b14">[15]</ref>: a GNN pre-training approach by a self-supervised graph generation task, including node attribute generation and edge generation. It follows the "pre-train, fine-tune" paradigm.</p><p>? DGI <ref type="bibr" target="#b49">[50]</ref>: a GNN pre-training approach that maximizes the mutual information between local and global representations. As an unsupervised method, it also freezes the model parameters and fits a simple logistic regression model for the downstream few-shot classification, after pre-training. ? SAGE self <ref type="bibr" target="#b8">[9]</ref>: the self-supervised version of GraphSAGE, encouraging similar embeddings for neighboring nodes and distinct embeddings for non-adjacent nodes. After pre-training, it follows the same approach of DGI for the downstream classification.</p><p>(3) Pre-trained transformers, which are pre-trained using masked language modeling <ref type="bibr" target="#b15">[16]</ref>, and then fine-tuned together with a randomly initialized classification head (e.g., a fully connected layer), for the downstream few-shot classification task.</p><p>? BERT <ref type="bibr" target="#b15">[16]</ref>: a bidirectionally trained transformer using masked language modeling, which learns from unlabeled text by being jointly conditioned on both left and right contexts in all layers. ? RoBERTa <ref type="bibr" target="#b24">[25]</ref>: a replication of BERT that carefully measures the impact of many key hyperparameters and training data size during training. ? BERT * and RoBERTa * : variants of BERT and RoBERTa, which are obtained by fine-tuning the pre-trained BERT and RoBERTa, respectively, using masked language modeling on our datasets, to mitigate the domain gap between our datasets and the datasets used for pre-training BERT and RoBERTa.</p><p>(4) Prompt tuning: P-Tuning v2 <ref type="bibr" target="#b22">[23]</ref>, is a version of prefix-tuning <ref type="bibr" target="#b18">[19]</ref> optimized and adapted for natural language. It uses deep prompt tuning, which applies continuous prompts for every layer of the pre-trained language model. Note that our setting is different from few-shot learning under the meta-learning paradigm <ref type="bibr" target="#b6">[7]</ref>, since there are no few-shot tasks for the meta-training phase. Hence, we cannot use state-of-the-art meta-learning models for comparison. Besides, two of the baselines we compared, DGI and SAGE self , have adopted a form of linear probe which is known to be a strong few-shot learner <ref type="bibr" target="#b46">[47]</ref>.</p><p>Baselines for zero-shot classification. We only compare with PLMs, as all other methods require at least one shot to work. For each method, we use the discrete prompt [CLASS] (i.e., the label text alone). We also evaluate handcrafted prompts "prompt [CLASS]", where prompt is a sequence of tokens found by prompt engineering, and annotate the model name with "+d". Essentially, we compute the similarity between the target document and the label text of each class (with or without additional tokens), and predict the most similar class following Fig. <ref type="figure" target="#fig_2">2</ref>.</p><p>Settings of G2P2 and baselines. For G2P2, the text encoder is a transformer <ref type="bibr" target="#b47">[48]</ref>. Following CLIP <ref type="bibr" target="#b35">[36]</ref>, we use a 63M-parameter, 12-layer 512-wide model with 8 attention heads. It operates on a lower-cased byte pair encoding (BPE) representation of the texts with a 49,152 vocabulary size <ref type="bibr" target="#b40">[41]</ref>. The maximum sequence length is capped at 128. The graph encoder employs a GCN <ref type="bibr" target="#b16">[17]</ref>, using two layers <ref type="bibr" target="#b8">[9]</ref> with LeakyReLU activation, each with 128 dimensions <ref type="bibr" target="#b33">[34]</ref>. The pre-training of our model starts from scratch without initializing the graph and text encoders with previously pre-trained weights. ? in Eq. ( <ref type="formula" target="#formula_11">10</ref>) is set to 0.1 on Cora, and set to 10 on the three Amazon review datasets, which were chosen from {0.01, 0.1, 1, 10, 100} according to the accuracy on the validation data. The number of learnable prompt tokens, ? in Eq. ( <ref type="formula" target="#formula_15">13</ref>), is set to 4, which was chosen from {2, 4, 8, 16, 32} based on the validation data. We use the Adam optimizer with the learning rate 2 ? 10 -5 with 2 training epochs, and a batch size of 64 in pre-training, referring to Hugging Face's <ref type="bibr" target="#b55">[56]</ref> example settings. The text embedding size is 128, the same as the output of the graph encoder. To generate the summary embedding and the context-based prompt initialization, the number of neighboring nodes sampled is 3. For prompt tuning, we set the learning rate as 0.01, which was chosen from {0.0001,0.001,0.01,0.1} according to the accuracy on validation data. For all the GNN methods, including the GNN component in G2P2, we use the 128-dimensional word2vec embeddings <ref type="bibr" target="#b29">[30]</ref> averaged over the words in the raw texts as the input node features. We use a two-layer architecture, and set the hidden dimension to be 128, except for GCN and SAGE sup whose hidden dimension is set to 32 <ref type="bibr" target="#b16">[17]</ref> which gives better empirical performance. For all GNN pre-training baselines, we use 0.01 as the learning rate. For BERT, RoBERTa and G2P2, we adopt 0.00002 as the learning rate. Our implementations of BERT, RoBERTa and their masked language modeling are based on Hugging Face's transformers <ref type="bibr" target="#b55">[56]</ref>. For both BERT and RoBERTa, we use their base versions, given that our model G2P2 uses just a small 63M-parameter model, following previous work <ref type="bibr" target="#b35">[36]</ref>. For P-Tuning v2, we use the original code on the RoBERTa backbone, and take the recommended 0.005 as the learning rate for prompt tuning. For G2P2, the learning rate for prompt tuning is set to 0.01.</p><p>We conduct all experiments on a server with 4 units of GeForce RTX 3090 GPU. Pre-training G2P2 takes about 0.5/6/9/10 hours on Cora/M.I./Industrial/Art, respectively, on a single GPU. The inference (with prompt-tuning) is carried out with five different splits generated from five random seeds {1, 2, 4, 8, 16}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance of low-resource classification</head><p>We evaluate the classification performance under various shots.</p><p>Five shots. In Tab. 2, we first compare the performance of G2P2 with baselines under the 5-shot setting. G2P2 emerges as the winner consistently, outperforming the best baseline by around 2-8% with statistical significance.</p><p>We also make a few more observations. Firstly, among the GNNs, pre-trained/self-supervised models tend to perform better than the end-to-end approaches, since the latter heavily rely on labeled data. Among the former, DGI and SAGE self perform better as they are a form of linear probe, known to be a strong few-shot learner <ref type="bibr" target="#b46">[47]</ref>. Note that, instead of using word2vec embeddings <ref type="bibr" target="#b29">[30]</ref> of raw texts as node features, we also tired using the pre-trained RoBERTa <ref type="bibr" target="#b24">[25]</ref> to generate the node features for DGI and SAGE self . However, doing so does not bring any improvement, showing that it is ineffective to simply combine a language model and GNN in a decoupled manner. In contrast, our proposed model jointly learns the text and graph encoders through three graph-grounded contrastive strategies. Secondly, PLMs are generally superior to GNNs, illustrating the importance of leveraging texts in a fine-grained way. Additionally, RoBERTa outperforms BERT owing to an improved pre-training procedure <ref type="bibr" target="#b24">[25]</ref>. However, further fine-tuning PLMs on our text data gives mixed results: RoBERTa * slightly outperforms RoBERTa but BERT * is much worse than BERT. In other words, it is not straightforward to mitigate the domain gap by simply     fine-tuning with the domain texts. Thirdly, the continuous prompt approach P-Tuning v2 achieves competitive results compared to fine-tuning, while having the advantage of being much cheaper than fine-tuning. However, our model G2P2 still significantly outperforms it. Furthermore, G2P2-p without prompt tuning is inferior to G2P2, showing the benefit of continuous prompts.</p><p>Fewer shots. In addition to the 5-shot setting, in Fig. <ref type="figure" target="#fig_7">3</ref> we also study the impact of fewer shots on G2P2 and several representative baselines. G2P2 generally performs the best across different shots. In general, the performance of all approaches decreases as the number of shots is reduced. However, the baselines suffer significantly under extreme low-resource (e.g., 1-or 2-shot) settings. In contrast, G2P2 remains robust, reporting a relatively small decrease in performance even with just 1 or 2 shots.</p><p>The results demonstrate the practical value of our proposed model especially when labeled data are difficult or costly to obtain in time. On the other hand, traditional approaches constantly face the challenge of the inability to keep up with the rapid growth of emerging classes in dynamic and open environments <ref type="bibr" target="#b52">[53]</ref>. For example, labeling a large volume of texts for novel topics in online articles, or new product categories in open-ended e-commerce platforms, can suffer a substantial time lag.</p><p>Zero shot. Finally, we report the zero-shot performance in Tab. 3, where our models G2P2 and G2P2+d significantly outperform the baselines. The results particularly demonstrate the effectiveness of our graph-grounded contrastive pre-training in the absence of labeled data, which is crucial to handling evolving classes without any labeled sample in many real-world scenarios. Moreover, handcrafted discrete prompts (i.e., BERT * +d and G2P2+d) can be superior to using label text only (i.e., BERT * and G2P2), showing the effectiveness of additional prompt tokens.</p><p>However, finding the optimal discrete prompts often requires significant engineering work. Specifically, for the three approaches with discrete prompts, namely, RoBERTa * +d, BERT * +d and G2P2+d, we explored more than 10 handcrafted prompt templates on each dataset, which are typically relevant to the corresponding dataset and require some domain knowledge to devise. While discrete </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model analyses</head><p>We conduct more in-depth studies on G2P2. Unless otherwise stated, we report the classification accuracy under the 5-shot setting.</p><p>Ablation study. We first evaluate the contribution from each of the three graph interaction-based contrastive strategies, by employing different combinations of the proposed loss terms L 1 , L 2 and L 3 .</p><p>As shown in Tab. 4, strategies without L 1 have performed quite poorly, demonstrating that the bijective text-node interaction is the fundamental component of our pre-training. That being said, when further adding L 2 or L 3 to L 1 , we still observe a noticeable performance improvement, showing the benefit of incorporating additional graph-based interactions for text data. Lastly, G2P2 with all three loss terms outperforms all 1-or 2-combinations of the losses, demonstrating that the three contrastive strategies are all useful and they are well integrated. Overall, the results reveal that graph information is vital to low-resource text classification, since graph structures reveal rich relationships between documents. Next, we evaluate the contribution from our prompt-tuning approach. Specifically, we compare G2P2 with two ablated variants: using label text only without trainable prompt vectors, and randomly initializing the prompt vectors. As reported in Tab. 4, only using label text clearly degrades classification performance, implying the importance of learning continuous prompts through prompt-tuning. Furthermore, our approach G2P2 with contextbased initialization for prompt vectors shows a small but consistent advantage over random initialization, which implies the usefulness of considering graph structures in prompt-tuning.</p><p>Hyperparameter study. We first investigate the impact of the interaction coefficient ? in Fig. <ref type="figure" target="#fig_8">4</ref>(a), which balances the high-order  contrastive losses (L 2 , L 3 ). The performance is generally better and stable when ? is slightly bigger (e.g., ? 10), indicating the significance of the high-order text-summary and node-summary interactions. Next, we study the prompt length ? in Fig. <ref type="figure" target="#fig_8">4</ref>(b), which refers to the number of trainable prompt vectors in Sect. 3.4. The performance is relatively unaffected by the prompt length, and thus it is robust to choose a small ? (e.g., 4) for efficiency.</p><p>Efficiency of prompt tuning. In our work, the continuous prompts are optimized by prompt tuning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b70">71]</ref> without updating the pretrained model. In this experiment, we investigate the efficiency of prompt-tuning in G2P2 compared to the efficiency of traditional fine-tuning. As G2P2 has a transformer component, we compare it with four transformer based models, all of which follow the classical "pre-train, fine-tune" paradigm <ref type="bibr" target="#b15">[16]</ref>.</p><p>As shown in Tab. 5, "Tuning time per task" refers to the average time required per task for prompt-tuning in G2P2 or fine-tuning in the baselines, while "Param. size" refers to the number of parameters that require updating. The results demonstrate that prompt tuning in G2P2 is much more efficient than fine-tuning in the baselines, achieving 2.1?18.8x speedups. The reason is that prompt tuning updates far fewer parameters. In G2P2, we used 4 trainable 512dimensional prompt vectors, totaling 2048 parameters only, while fine-tuning in the baselines needs to update the whole pre-trained model with more than 100M parameters. Note that the speedup is not linear w.r.t. the parameter size, due to the overhead in the data loader and the optimizer. Overall, our prompt tuning is not only effective under low-resource settings, but also parameter-and computation-efficient.</p><p>Generalization study. Our previous experiments can be considered "transductive" as both the pre-training of the text encoder and the downstream classification are conducted on the whole corpus. To further evaluate the generalization ability of our model, we adopt an "inductive" setting, whereby we pre-train the text encoder only on a subset of the corpus and perform downstream classification on a disjoint subset. Particularly, in the three Amazon datasets, since user texts have no labels and item texts have labels, it is natural for us to pre-train with only user texts and classify only item texts downstream. We also employ masked language modeling on only the user texts for BERT and RoBERTa, to get BERT * and RoBERTa * . As shown in Tab. 6, G2P2 still performs very well in the inductive setting, illustrating the strong generalization ability of our pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we studied the problem of low-resource text classification. Given that many text documents are related through an underlying network, we proposed a novel model called Graph-Grounded Pre-training and Prompting (G2P2). It consists of three graph interaction-based contrastive strategies in pre-training, and a prompting mechanism for the jointly pre-trained graph-text model in downstream classification. We conducted extensive experiments and showed the advantages of G2P2 in zero-and few-shot text classification.</p><p>A limitation of this work is the need of a graph to complement the texts. Although graphs are ubiquitous in information retrieval applications, in the case that an organic graph is unavailable, a potential solution is to construct synthetic graphs based on word co-occurrences or other relations, e.g., linking up news articles in close time periods and locations. We leave further explorations to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall framework of G2P2. (a) During pre-training, it jointly trains a text and a graph encoder through three contrastive strategies. (b) During testing, it performs prompt-assisted zero-or few-shot classification (the figure only shows prompt-tuning for few-shot classification, while zero-shot inference adopts a simplified scheme).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>"Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic diagram for zero-shot classification. The pre-trained models ? 0 ? and ? 0 ? are obtained from Fig. 1(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc># of shots, M.I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance on different shots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Hyperparameter study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Pre-training Procedure of G2P2 for each node ? ? /document ? ? in the batch do</figDesc><table><row><cell cols="3">Ensure: Pre-trained weights of text encoder ? 0 ? , graph encoder ? 0 ? .</cell></row><row><cell cols="2">1: ? 0 ? , ? 0 ? ? parameters initialization;</cell><cell></cell></row><row><cell cols="2">2: while not converged do</cell><cell></cell></row><row><cell>3:</cell><cell>sample batches of documents from D;</cell><cell></cell></row><row><cell>4:</cell><cell>for each batch do</cell><cell></cell></row><row><cell>5:</cell><cell></cell><cell></cell></row><row><cell>6:</cell><cell>calculate ? ? 's text embedding t ? ;</cell><cell>? Eq. (1)</cell></row><row><cell>7:</cell><cell>calculate ? ? 's node embedding z ? ;</cell><cell>? Eq. (2)</cell></row><row><cell>8:</cell><cell>calculate ? ? 's summary embedding s ? ;</cell><cell>? Eq. (5)</cell></row><row><cell>9:</cell><cell>end for</cell><cell></cell></row><row><cell>10:</cell><cell></cell><cell></cell></row></table><note><p>Require: A graph-grounded text corpus G = ( D, E, X).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell>Cora</cell><cell cols="2">Art Industrial</cell><cell>M.I.</cell></row><row><cell># Documents</cell><cell cols="2">25,120 1,615,902</cell><cell>1,260,053</cell><cell>905,453</cell></row><row><cell># Links</cell><cell cols="2">182,280 4,898,218</cell><cell cols="2">3,101,670 2,692,734</cell></row><row><cell># Avg. doc length</cell><cell>141.26</cell><cell>54.23</cell><cell>52.15</cell><cell>84.66</cell></row><row><cell># Avg. node deg</cell><cell>7.26</cell><cell>3.03</cell><cell>2.46</cell><cell>2.97</cell></row><row><cell># Total classes</cell><cell>70</cell><cell>3,347</cell><cell>2,462</cell><cell>1,191</cell></row><row><cell cols="5">other through citations. The abstract of a paper is deemed a text</cell></row><row><cell cols="5">document. The papers are classified into a topic hierarchy with</cell></row><row><cell cols="5">73 leaves. After removing papers with no content or label, the</cell></row><row><cell cols="5">resulting hierarchy has 70 leaf topics. Note that we are using a</cell></row><row><cell cols="5">more comprehensive version of the Cora dataset, which is larger</cell></row><row><cell cols="5">and has more classes than the version used elsewhere [17].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Five-shot classification performance (percent) with 95% confidence intervals. each column, the best result among all methods is bolded and the best among the baselines is underlined. Improvement by G2P2 is calculated relative to the best baseline. * indicates that our model significantly outperforms the best baseline based on the two-tail ? -test (? &lt; 0.05).</figDesc><table><row><cell cols="3">In Cora</cell><cell>Art</cell><cell></cell><cell cols="2">Industrial</cell><cell>M.I.</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>Macro-F1</cell><cell>Accuracy</cell><cell>Macro-F1</cell><cell>Accuracy</cell><cell>Macro-F1</cell><cell>Accuracy</cell><cell>Macro-F1</cell></row><row><cell>GCN</cell><cell>41.15?2.41</cell><cell>34.50?2.23</cell><cell>22.47?1.78</cell><cell>15.45?1.14</cell><cell>21.08?0.45</cell><cell>15.23?0.29</cell><cell>22.54?0.82</cell><cell>16.26?0.72</cell></row><row><cell>SAGE sup</cell><cell>41.42?2.90</cell><cell>35.14?2.14</cell><cell>22.60?0.56</cell><cell>16.01?0.28</cell><cell>20.74?0.91</cell><cell>15.31?0.37</cell><cell>22.14?0.80</cell><cell>16.69?0.62</cell></row><row><cell>TextGCN</cell><cell>59.78?1.88</cell><cell>55.85?1.50</cell><cell>43.47?1.02</cell><cell>32.20?1.30</cell><cell>53.60?0.70</cell><cell>45.97?0.49</cell><cell>46.26?0.91</cell><cell>38.75?0.78</cell></row><row><cell>GPT-GNN</cell><cell>76.72?2.02</cell><cell>72.23?1.17</cell><cell>65.15?1.37</cell><cell>52.79?0.83</cell><cell>62.13?0.65</cell><cell>54.47?0.67</cell><cell>67.97?2.49</cell><cell>59.89?2.51</cell></row><row><cell>DGI</cell><cell>78.42?1.39</cell><cell>74.58?1.24</cell><cell>65.41?0.86</cell><cell>53.57?0.75</cell><cell>52.29?0.66</cell><cell>45.26?0.51</cell><cell>68.06?0.73</cell><cell>60.64?0.61</cell></row><row><cell>SAGE self</cell><cell>77.59?1.71</cell><cell>73.47?1.53</cell><cell>76.13?0.94</cell><cell>65.25?0.31</cell><cell>71.87?0.61</cell><cell>65.09?0.47</cell><cell>77.70?0.48</cell><cell>70.87?0.59</cell></row><row><cell>BERT</cell><cell>37.86?5.31</cell><cell>32.78?5.01</cell><cell>46.39?1.05</cell><cell>37.07? 0.68</cell><cell>54.00?0.20</cell><cell>47.57?0.50</cell><cell>50.14?0.68</cell><cell>42.96?1.02</cell></row><row><cell>BERT  *</cell><cell>27.22?1.22</cell><cell>23.34?1.11</cell><cell>45.31?0.96</cell><cell>36.28?0.71</cell><cell>49.60?0.27</cell><cell>43.36?0.27</cell><cell>40.19?0.74</cell><cell>33.69?0.72</cell></row><row><cell>RoBERTa</cell><cell>62.10?2.77</cell><cell>57.21?2.51</cell><cell>72.95?1.75</cell><cell>62.25?1.33</cell><cell>76.35?0.65</cell><cell>70.49?0.59</cell><cell>70.67?0.87</cell><cell>63.50?1.11</cell></row><row><cell>RoBERTa  *</cell><cell>67.42?4.35</cell><cell>62.72?3.02</cell><cell>74.47?1.00</cell><cell>63.35?1.09</cell><cell>77.08?1.02</cell><cell>71.44?0.87</cell><cell>74.61?1.08</cell><cell>67.78?0.95</cell></row><row><cell>P-Tuning v2</cell><cell>71.00?2.03</cell><cell>66.76?1.95</cell><cell>76.86?0.59</cell><cell>66.89?1.14</cell><cell>79.65?0.38</cell><cell>74.33?0.37</cell><cell>72.08?0.51</cell><cell>65.44?0.63</cell></row><row><cell>G2P2-p</cell><cell>79.16?1.23</cell><cell>74.99?1.35</cell><cell>79.59?0.31</cell><cell>68.26?0.43</cell><cell>80.86?0.40</cell><cell>74.44?0.29</cell><cell>81.26?0.36</cell><cell>74.82?0.45</cell></row><row><cell>G2P2</cell><cell>80.08  *  ?1.33</cell><cell>75.91  *  ?1.39</cell><cell>81.03  *  ?0.43</cell><cell>69.86  *  ?0.67</cell><cell>82.46  *  ?0.29</cell><cell>76.36  *  ?0.25</cell><cell>82.77  *  ?0.32</cell><cell>76.48  *  ?0.52</cell></row><row><cell>(improv.)</cell><cell>(+2.12%)</cell><cell>(+1.78%)</cell><cell>(+5.43%)</cell><cell>(+4.44%)</cell><cell>(+3.53%)</cell><cell>(+2.7%)</cell><cell>(+6.53%)</cell><cell>(+7.92%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Zero-shot classification accuracy (percent). Table2for explanations on entry styles. In Tab. 3, we simply report the performance of the best handcrafted template for each approach and each dataset. It is also worth noting that the same prompt can sometimes generate opposite results on different models. For instance, in Cora dataset, while "a model of [CLASS]" is the best prompt for RoBERTa * +d, it is a bad choice for G2P2+d. Moreover, some prompts without any semantic meaning, like "a [CLASS]", can be the best choice sometimes. The observations imply that prompt engineering involves labor-intensive work, and the outcomes contain much uncertainty on what the optimal discrete prompt would be. Therefore, using only the label text is still a reasonably good choice.</figDesc><table><row><cell></cell><cell>See Cora</cell><cell>Art</cell><cell>Industrial</cell><cell>M.I.</cell></row><row><cell>RoBERTa</cell><cell>30.46?2.01</cell><cell>42.80?0.94</cell><cell>42.89?0.97</cell><cell>36.40?1.20</cell></row><row><cell>RoBERTa  *</cell><cell>39.58?1.26</cell><cell>34.77?0.65</cell><cell>37.78?0.32</cell><cell>32.17?0.68</cell></row><row><cell cols="2">RoBERTa  *  +d 45.53?1.33</cell><cell>36.11?0.66</cell><cell>39.40?1.22</cell><cell>37.65?0.33</cell></row><row><cell>BERT</cell><cell>23.58?1.88</cell><cell>35.88?1.44</cell><cell>37.32?0.85</cell><cell>37.42?0.80</cell></row><row><cell>BERT  *</cell><cell>23.38?1.96</cell><cell>54.27?1.85</cell><cell>56.02?1.22</cell><cell>50.19?0.72</cell></row><row><cell>BERT  *  +d</cell><cell>26.65?1.71</cell><cell>56.61?1.76</cell><cell>55.93?0.96</cell><cell>52.13?0.88</cell></row><row><cell>G2P2</cell><cell>63.52?2.89</cell><cell>76.52?0.59</cell><cell>76.66?0.31</cell><cell>74.60?0.62</cell></row><row><cell>G2P2+d</cell><cell cols="4">65.28  *  ?3.12 76.99  *  ?0.60 77.43  *  ?0.27 75.86  *  ?0.69</cell></row><row><cell>(improv.)</cell><cell>(+45.38%)</cell><cell>(+36.00%)</cell><cell>(+38.22%)</cell><cell>(+45.52%)</cell></row><row><cell cols="5">prompts are generally helpful to zero-shot classification, their effec-</cell></row><row><cell cols="2">tiveness varies.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cora</cell><cell>Art</cell><cell>Industrial</cell><cell>M.I.</cell></row><row><cell></cell><cell></cell><cell cols="2">Only L 3</cell><cell cols="3">74.66?1.80 52.56?1.09 45.97?0.81 49.05?0.54</cell></row><row><cell></cell><cell></cell><cell cols="2">Only L 2</cell><cell cols="3">77.01?1.30 58.90?0.55 52.99?0.46 59.41?0.85</cell></row><row><cell></cell><cell></cell><cell cols="2">Only L 1</cell><cell cols="3">79.50?1.19 77.37?0.72 78.10?0.34 79.70?0.56</cell></row><row><cell></cell><cell></cell><cell cols="2">L 2 +L 3</cell><cell cols="3">70.04?2.89 49.91?1.57 50.07?0.50 56.14?1.01</cell></row><row><cell></cell><cell></cell><cell cols="2">L 1 +L 3</cell><cell cols="3">79.73?0.89 78.60?0.40 79.97?0.43 80.42?0.45</cell></row><row><cell></cell><cell></cell><cell cols="2">L 1 +L 2</cell><cell cols="3">79.42?1.04 80.55?0.52 81.06?0.33 82.39?0.41</cell></row><row><cell cols="7">Only label text 79.16?1.23 79.59?0.31 80.86?0.40 81.26?0.36</cell></row><row><cell></cell><cell cols="3">Random init.</cell><cell cols="3">80.03?0.99 80.85?0.43 82.43?0.33 82.64?0.21</cell></row><row><cell></cell><cell></cell><cell>G2P2</cell><cell></cell><cell cols="3">80.08?1.33 81.03?0.43 82.46?0.35 82.77?0.32</cell></row><row><cell></cell><cell>86</cell><cell></cell><cell></cell><cell>Cora</cell><cell cols="2">Industrial</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Art</cell><cell>M.I.</cell></row><row><cell></cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Acc. (%)</cell><cell>80 82</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>76</cell><cell>0.01</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Tuning time and parameter size. RoBERTa 45.47?2.38 64.22?3.62 43.46?2.99 44.99?2.58 123 M RoBERTa * 39.38?2.01 59.56?3.55 35.10?2.75 38.84?2.39 123 M BERT 32.23?1.71 51.77?2.00 31.72?1.77 33.55?2.39 110 M BERT * 34.82?1.68 55.16?2.32 31.11?1.74 29.00?2.23 110 M G2P2 2.42?0.41 22.03?1.39 14.63?1.26 12.72?1.17 2048</figDesc><table><row><cell></cell><cell cols="2">Tuning time per task (in seconds)</cell><cell></cell><cell>Param.</cell></row><row><cell>Cora</cell><cell>Art</cell><cell>Industrial</cell><cell>M.I.</cell><cell>size</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Inductive performance on text classification.</figDesc><table><row><cell></cell><cell>Art</cell><cell>Industrial</cell><cell>M.I.</cell></row><row><cell>BERT  *</cell><cell>43.66?0.90</cell><cell>48.35?0.25</cell><cell>39.24?0.88</cell></row><row><cell>RoBERTa  *</cell><cell>69.55?1.14</cell><cell>73.65?0.86</cell><cell>71.96?1.44</cell></row><row><cell>G2P2</cell><cell>79.81?0.22</cell><cell>81.29?0.32</cell><cell>81.85?0.33</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We will use "node" and "document" interchangeably given their one-one correspondence in our context.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Technically, the input to the text encoder is a sequence of continuous embeddings; the tokens in a document are first converted to word embeddings.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research / project is supported by the <rs type="funder">Ministry of Education, Singapore</rs>, under its <rs type="funder">Academic Research Fund Tier 2 (Proposal ID</rs>: <rs type="grantNumber">T2EP20122-0041</rs>). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the <rs type="institution">Ministry of Education, Singapore</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jV6mR9c">
					<idno type="grant-number">T2EP20122-0041</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks</title>
		<author>
			<persName><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishikesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="522" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Few-shot Text Classification with Distributional Signatures</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurlPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DEKR: description enhanced knowledge graph for machine learning method recommendation</title>
		<author>
			<persName><forename type="first">Xianshuai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongmin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2147" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowprompt: Knowledge-aware prompttuning with synergistic optimization for relation extraction</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2778" to="2788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making Pre-trained Language Models Better Few-shot Learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurlPS</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pre-trained models: Past, present and future</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="225" to="250" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<title level="m">Ptr: Prompt tuning with rules for text classification</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2225" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Kristina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toutanova</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Power of Scale for Parameter-Efficient Prompt Tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Heterogeneous graph attention networks for semi-supervised short text classification</title>
		<author>
			<persName><forename type="first">Tianchi</forename><surname>Hu Linmei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno>EMNLP. 4821-4830</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Content to node: Selftranslation network embedding</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1794" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">GPT understands, too</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aixin Sun, and Chunyan Miao. 2021b. Pre-training graph transformer with multimodal side information for recommendation</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<biblScope unit="page" from="2853" to="2861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to pre-train graph neural networks</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4276" to="4284" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automating the Construction of Internet Portals with Machine Learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
		<ptr target="www.research.whizbang.com/data" />
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Noisy Channel Language Model Prompting for Few-Shot Text Classification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5316" to="5330" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adversarial Training Methods for Semi-Supervised Text Classification. In ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fine-grained aspects</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ander</forename><surname>Barrena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1199" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurlPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gppt: Graph pre-training and prompt tuning to generalize graph neural networks</title>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1717" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">NSP-BERT: A Promptbased Zero-Shot Learner Through an Original Pre-training Task-Next Sentence Prediction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangping</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03564</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators</title>
		<author>
			<persName><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6131" to="6142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurlPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax. ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint Embedding of Words and Labels for Text Classification</title>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2321" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph few-shot learning with attribute matching</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Yuchen Guo, and Zhiguo Gong. 2021. Zero-shot node classification with decomposed graph prototype network</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<biblScope unit="page" from="1769" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generalizing graph neural network across graphs and time</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1214" to="1215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Meta-inductive node classification across graphs</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zemin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Selfsupervised learning on graphs: Contrastive, generative, or predictive</title>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurlPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6256" to="6268" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Open-world learning and application to product classification</title>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3413" to="3419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurlPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Diverse Few-Shot Text Classification with Multiple Metrics</title>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ai Foundations-Learning</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1206" to="1215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Aspect Sentiment Quad Prediction as Paraphrase Generation</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9209" to="9219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00747</idno>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning on Large-scale Text-attributed Graphs via Variational Inference</title>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=q0nmYciuuZN" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Factual Probing Is [MASK]: Learning vs. Learning to Recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5017" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Meta-gnn: On few-shot node classification in graph meta-learning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goce</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2357" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
