<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-07">7 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310030</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
								<address>
									<postCode>310030</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310030</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
								<address>
									<postCode>310030</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jintao</forename><surname>Chen</surname></persName>
							<email>chenjintao@zju.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bozhen</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310030</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
								<address>
									<postCode>310030</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>stan.zq.li@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310030</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
								<address>
									<postCode>310030</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-07">7 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3512156</idno>
					<idno type="arXiv">arXiv:2202.03104v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph neural networks</term>
					<term>graph self-supervised learning</term>
					<term>contrastive learning</term>
					<term>graph representation learning</term>
					<term>robustness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph contrastive learning (GCL) has emerged as a dominant technique for graph representation learning which maximizes the mutual information between paired graph augmentations that share the same semantics. Unfortunately, it is difficult to preserve semantics well during augmentations in view of the diverse nature of graph data. Currently, data augmentations in GCL that are designed to preserve semantics broadly fall into three unsatisfactory ways. First, the augmentations can be manually picked per dataset by trial-and-errors. Second, the augmentations can be selected via cumbersome search. Third, the augmentations can be obtained by introducing expensive domain-specific knowledge as guidance. All of these limit the efficiency and more general applicability of existing GCL methods. To circumvent these crucial issues, we propose a Simple framework for GRAph Contrastive lEarning, SimGRACE for brevity, which does not require data augmentations. Specifically, we take original graph as input and GNN model with its perturbed version as two encoders to obtain two correlated views for contrast. SimGRACE is inspired by the observation that graph data can preserve their semantics well during encoder perturbations while not requiring manual trial-and-errors, cumbersome search or expensive domain knowledge for augmentations selection. Also, we explain why SimGRACE can succeed. Furthermore, we devise adversarial training scheme, dubbed AT-SimGRACE, to enhance the robustness of graph contrastive learning and theoretically explain the reasons. Albeit simple, we show that SimGRACE can yield competitive or better performance compared with state-of-the-art methods in terms of generalizability, transferability and robustness, while enjoying unprecedented degree of flexibility and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs), inheriting the power of neural networks and utilizing the structural information of graph data simultaneously, have achieved overwhelming accomplishments in various graph-based tasks, such as node, graph classification or graph generation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50]</ref>. However, most existing GNNs are trained in a supervised manner and it is often resource-and time-intensive to collect abundant labeled data <ref type="bibr" target="#b46">[47]</ref>. To remedy this issue, tremendous endeavors have been devoted to graph selfsupervised learning that learns representations from unlabeled graphs. Among many, graph contrastive learning (GCL) <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref> follows the general framework of contrastive learning in computer vision domain <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b45">46]</ref>, in which two augmentations are generated for each graph and then maximizes the mutual information between these two augmented views. In this way, the model can learn representations that are invariant to perturbations. For example, GraphCL <ref type="bibr" target="#b53">[54]</ref> first designs four types of general augmentations (node dropping, edge perturbation, attribute masking and subgraph) for GCL. However, these augmentations are not suitable for all scenarios because the structural information and semantics of the graphs varies significantly across domains. For example, GraphCL <ref type="bibr" target="#b53">[54]</ref> finds that edge perturbation benefits social networks but hurt some biochemical molecules in GCL. Worse still, these augmentations may alter the graph semantics completely even if the perturbation is weak. For example, dropping a carbon atom in the phenyl ring will alter the aromatic system and result in an alkene chain, which will drastically change the molecular properties <ref type="bibr" target="#b39">[40]</ref>.</p><p>To remedy these issues, several strategies have been proposed recently. Typically, GraphCL <ref type="bibr" target="#b53">[54]</ref> manually picks data augmentations per dataset by tedious trial-and-errors, which significantly limits the generality and practicality of their proposed framework. To get rid of the tedious dataset-specific manual tuning of GraphCL, JOAO <ref type="bibr" target="#b52">[53]</ref> proposes to automate GraphCL in selecting augmentation pairs. However, it suffers more computational overhead to Table <ref type="table">1</ref>: Comparison between state-of-the-art GCL methods (graph-level representation learning) and SimGRACE.</p><p>No manual trial-and-errors No domain knowledge Preserving semantics No cumbersome search Generality GraphCL <ref type="bibr" target="#b53">[54]</ref> MoCL <ref type="bibr" target="#b39">[40]</ref> JOAO(v2) <ref type="bibr" target="#b52">[53]</ref> SimGRACE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GraphCL MoCL SimGRACE</head><p>Figure <ref type="figure">1</ref>: Comparison of GraphCL <ref type="bibr" target="#b53">[54]</ref>, MoCL <ref type="bibr" target="#b39">[40]</ref> and Sim-GRACE on MUTAG dataset. The samples of two classes are distinguished by colors (blue &amp; orange). We first train three GNN encoders with these methods respectively and visualise the representations of original graphs with t-SNE in the upper row. Then, we perturb graphs or encoders in their respective ways (edge perturbation for GraphCL, replacing functional group with bioisosteres of similar properties for MoCL, encoder perturbation for SimGRACE) and visualise the representations of perturbed (GraphCL, MoCL) or original (SimGRACE) graphs in the below row. Unlike GraphCL, SimGRACE and MoCL can preserve the class identity semantics well after perturbations. However, MoCL requires expensive domain knowledge as guidance.</p><p>search suitable augmentations and still relies on human prior knowledge in constructing and configuring the augmentation pool to select from. To avoid altering the semantics in the general augmentations adopted in GraphCL and JOAO(v2), MoCL <ref type="bibr" target="#b39">[40]</ref> proposes to replace valid substructures in molecular graph with bioisosteres that share similar properties. However, it requires expensive domain knowledge as guidance and can not be applied in other domains like social graphs. Hence, a natural question emerges: Can we emancipate graph contrastive learning from tedious manual trial-and-errors, cumbersome search or expensive domain knowledge ?</p><p>To answer this question, instead of devising more advanced data augmentations strategies for GCL, we attempt to break through state-of-the-arts GCL framework which takes semantic-preserved data augmentations as prerequisite. More specifically, we take original graph data as input and GNN model with its perturbed version as two encoders to obtain two correlated views. And then, we maximize the agreement of these two views. With the encoder perturbation as noise, we can obtain two different embeddings for same input as "positive pairs". Similar to previous works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b53">54]</ref>,</p><p>we take other graph data in the same mini-batch as "negative pairs". The idea of encoder perturbation is inspired by the observations in Figure <ref type="figure">1</ref>. The augmentation or perturbation of MoCL and our SimGRACE can preserve the class identity semantics well while GraphCL can not. Also, we explain why SimGRACE can succeed. Besides, GraphCL <ref type="bibr" target="#b53">[54]</ref> shows that GNNs can gain robustness using their proposed framework. However, (1) they do not explain why GraphCL can enhance the robustness; (2) GraphCL seems to be immunized to random attacks well while performing unsatisfactory against adversarial attacks. GROC <ref type="bibr" target="#b18">[19]</ref> first integrates adversarial transformations into the graph contrastive learning framework and improves the robustness against adversarial attacks. Unfortunately, as the authors pointed out, the robustness of GROC comes at a price of much longer training time because conducting adversarial transformations for each graph is time-consuming. To remedy these deficiencies, we propose a novel algorithm AT-SimGRACE to perturb the encoder in an adversarial way, which introduces less computational overhead while showing better robustness. Theoretically, we explain why AT-SimGRACE can enhance the robustness. We highlight our contributions as follows:</p><p>• Significance: We emancipate graph contrastive learning from tedious manual trial-and-errors, cumbersome search or expensive domain knowledge which limit the efficiency and more general applicability of existing GCL methods. The comparison between SimGRACE and state-of-the-art GCL methods can be seen in Table <ref type="table">1</ref> Inspired by the success of self-supervised learning in computer vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> and natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>, tremendous endeavors have been devoted to graph self-supervised learning that learns representations in an unsupervised manner with designed pretext tasks. Initially, Hu et al. <ref type="bibr" target="#b15">[16]</ref> propose two pretext tasks, i.e, predicting neighborhood context and node attributes to conduct node-level pre-training. Besides, they utilize supervised graph-level property prediction and structure similarity prediction as pretext tasks to perform graph-level pre-training. GPT-GNN <ref type="bibr" target="#b16">[17]</ref> designs generative task in which node attributes and edges are alternatively generated such that the likelihood of a graph is maximized. Recently, GROVER <ref type="bibr" target="#b33">[34]</ref> incorporates GNN into a transformer-style architecture and learns node embedding by predicting contextual property and graph-level motifs. Different from above methods, our SimGRACE follows a contrastive framework that will be introduced in details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Contrastive Learning</head><p>Graph contrastive learning can be categorized into two groups. One group can encode useful information by contrasting local and global representations. Initially, DGI <ref type="bibr" target="#b44">[45]</ref> and InfoGraph <ref type="bibr" target="#b37">[38]</ref> are proposed to obtain expressive representations for graphs or nodes via maximizing the mutual information between graph-level representations and substructure-level representations of different granularity. More recently, MVGRL <ref type="bibr" target="#b13">[14]</ref> proposes to learn both node-level and graph-level representation by performing node diffusion and contrasting node representation to augmented graph representations. Another group is designed to learn representations that are tolerant to data transformation. Specifically, they first augment graph data and feed the augmented graphs into a shared encoder and projection head, after which their mutual information is maximized. Typically, for node-level tasks <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>, GCA <ref type="bibr" target="#b57">[58]</ref> argues that data augmentation schemes should preserve intrinsic structures and attributes of graphs and thus proposes to adopt adaptive augmentations that only perturb unimportant components. DGCL <ref type="bibr" target="#b47">[48]</ref> introduces a novel probabilistic method to alleviate the issue of false negatives in GCL. For graph-level tasks, GraphCL <ref type="bibr" target="#b53">[54]</ref> proposes four types of augmentations for general graphs and demonstrated that the learned representations can help downstream tasks. However, the success of GraphCL comes at the price of tedious manual trial-anderrors. To tackle this issue, JOAO <ref type="bibr" target="#b52">[53]</ref> proposes a unified bi-level optimization framework to automatically select data augmentations for GraphCL, which is time-consuming and inconvenient. More recently, MoCL <ref type="bibr" target="#b39">[40]</ref> proposes to incorporate domain knowledge into molecular graph augmentations in order to preserve the semantics. However, the domain knowledge is extremely expensive. Worse still, MoCL can only work on molecular graph data, which significantly limits their generality. Despite the fruitful progress, they still require tedious manual trial-and-errors, cumbersome search or expensive domain knowledge for augmentation selection. Instead, our SimGRACE breaks through state-of-the-arts GCL framework that takes semantic-preserved data augmentations as prerequisite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 SimGRACE</head><p>In this section, we will introduce SimGRACE framework in details. As sketched in Figure <ref type="figure">2</ref>, the framework consists of the following three major components:</p><p>(1) Encoder perturbation. A GNN encoder 𝑓 (•; 𝜽 ) and its its perturbed version 𝑓 (•; 𝜽 ′ ) first extract two graph-level representations h and h ′ for the same graph G, which can be formulated as,</p><formula xml:id="formula_0">h = 𝑓 (G; 𝜽 ), h ′ = 𝑓 (G; 𝜽 ′ ).<label>(1)</label></formula><p>The method we proposed to perturb the encoder 𝑓 (•; 𝜽 ) can be mathematically described as,</p><formula xml:id="formula_1">𝜽 ′ 𝑙 = 𝜽 𝑙 + 𝜂 • 𝚫𝜽 𝑙 ; 𝚫𝜽 𝑙 ∼ N 0, 𝜎 2 𝑙 ,<label>(2)</label></formula><p>where 𝜽 𝑙 and 𝜽 ′ 𝑙 are the weight tensors of the 𝑙-th layer of the GNN encoder and its perturbed version respectively. 𝜂 is the coefficient that scales the magnitude of the perturbation. 𝚫𝜽 𝑙 is the perturbation term which samples from Gaussian distribution with zero mean and variance 𝜎 2 𝑙 . Also, we show that the performance will deteriorate when we set 𝜂 = 0 in section 4.6.1. Note that BGRL <ref type="bibr" target="#b40">[41]</ref> and MERIT <ref type="bibr" target="#b17">[18]</ref> also update a target network with an online encoder during training. However, SimGRACE differs from them in three aspects: (1) SimGRACE perturbs the encoder with a random Guassian noise instead of momentum updating; (2) SimGRACE does not require data augmentation while BGRL and MERIT take it as prerequisite. (3) SimGRACE focuses on graph-level representation learning while BGRL and MERIT only work in node-level tasks.</p><p>(2) Projection head. As advocated in <ref type="bibr" target="#b41">[42]</ref>, a non-linear transformation 𝑔(•) named projection head maps the representations to another latent space can enhance the performance. In our Sim-GRACE framework, we also adopt a two-layer perceptron (MLP) to obtain 𝑧 and 𝑧 ′ ,</p><formula xml:id="formula_2">𝑧 = 𝑔(h), 𝑧 ′ = 𝑔(h ′ ).<label>(3)</label></formula><p>(3) Contrastive loss. In SimGRACE framework, we utilize the normalized temperature-scaled cross entropy loss (NT-Xent) as previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54]</ref> to enforce the agreement between positive pairs 𝑧 and 𝑧 ′ compared with negative pairs.</p><p>During SimGRACE training, a minibatch of 𝑁 graphs are randomly sampled and then they are fed into a GNN encoder 𝑓 (•; 𝜽 ) and its perturbed version 𝑓 (•; 𝜽 ′ ), resulting in two presentations for each graph and thus 2𝑁 representations in total. We re-denote 𝑧, 𝑧 ′ as 𝒛 𝑛 , 𝒛 ′ 𝑛 for 𝑛-th graph in the minibatch. Negative pairs are generated from the other 𝑁 − 1 perturbed representations within the same mini-batch as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref>. Denoting the cosine similarity function as sim (𝒛, 𝒛 ′ ) = 𝒛 ⊤ 𝒛 ′ /∥𝒛∥ ∥𝒛 ′ ∥, the contrastive loss for the 𝑛-th graph is defined as,</p><formula xml:id="formula_3">ℓ 𝑛 = − log exp sim 𝒛 𝑛 , 𝒛 ′ 𝑛 )/𝜏 𝑁 𝑛 ′ =1,𝑛 ′ ≠𝑛 exp (sim (𝒛 𝑛 , 𝒛 𝑛 ′ ) /𝜏) ,<label>(4)</label></formula><p>where 𝜏 is the temperature parameter. The final loss is computed across all positive pairs in the minibatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why can SimGRACE work well?</head><p>In order to understand why SimGRACE can work well, we first introduce the analysis tools from <ref type="bibr" target="#b42">[43]</ref>. Specifically, they identify two key properties related to contrastive learning: alignment and uniformity and then propose two metrics to measure the quality of representations obtained via contrastive learning. One is the alignment metric which is straightforwardly defined with the expected distance between positive pairs:</p><formula xml:id="formula_4">ℓ align (𝑓 ; 𝛼) ≜ E (𝑥,𝑦)∼𝑝 pos ∥𝑓 (𝑥) − 𝑓 (𝑦)∥ 𝛼 2 , 𝛼 &gt; 0 (5)</formula><p>where 𝑝 pos is the distribution of positive pairs (augmentations of the same sample). This metric is well aligned with the objective of contrastive learning: positive samples should stay close in the embedding space. Analogously, for our SimGRACE framework, we provide a modified metric for alignment,</p><formula xml:id="formula_5">ℓ align (𝑓 ; 𝛼) ≜ E 𝑥∼𝑝 data ∥𝑓 (𝑥; 𝜽 ) − 𝑓 (𝑥; 𝜽 ′ )∥ 𝛼 2 , 𝛼 &gt; 0 (6)</formula><p>where 𝑝 data is the data distribution. We set 𝛼 = 2 in our experiments. The other is the uniformity metric which is defined as the logarithm of the average pairwise Gaussian potential:</p><formula xml:id="formula_6">ℓ uniform (𝑓 ; 𝛼) ≜ log E 𝑥,𝑦 𝑖.𝑖.𝑑 .</formula><p>∼ 𝑝 data 𝑒 −𝑡 ∥ 𝑓 (𝑥;𝜽 )−𝑓 (𝑦;𝜽 ) ∥ 2 2 . 𝑡 &gt; 0 (7) In our experiments, we set 𝑡 = 2. The uniformity metric is also aligned with the objective of contrastive learning that the embeddings of random samples should scatter on the hypersphere. We take the checkpoints of SimGRACE, GraphCL and MoCL every 2 epochs during training and visualize the alignment ℓ 𝑎𝑙𝑖𝑔𝑛 and uniformity ℓ 𝑢𝑛𝑖 𝑓 𝑜𝑟𝑚 metrics in Figure <ref type="figure" target="#fig_0">3</ref>. As can be observed, all the three methods can improve the alignment and uniformity. However, GraphCL achieves a smaller gain on the alignment than Sim-GRACE and MoCL. In other words, the positive pairs can not stay close in GraphCL because general graph data augmentations (drop edges, drop nodes and etc.) destroy the semantics of original graph data, which degrades the quality of the representations learned by GraphCL. Instead, MoCL augments graph data with domain knowledge as guidance and thus can preserve semantics during augmentation. Eventually, MoCL dramatically improves the alignment. Compared with GraphCL, SimGRACE can achieve better alignment while improving uniformity because encoder perturbation can preserve data semantics well. On the other hand, although MoCL achieves better alignment than SimGRACE via introducing domain knowledge as guidance, it only achieves a small gain on the uniformity, and eventually underperforms SimGRACE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">AT-SimGRACE</head><p>Recently, GraphCL <ref type="bibr" target="#b53">[54]</ref> shows that GNNs can gain robustness using their proposed framework. However, they did not explain why GraphCL can enhance the robustness. Additionally, GraphCL seems to be immunized to random attacks well while being unsatisfactory against adversarial attacks. In this section, we aim to utilize Adversarial Training (AT) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> to improve the adversarial robustness of SimGRACE in a principled way. Generally, AT directly incorporates adversarial examples into the training process to solve the following optimization problem:</p><formula xml:id="formula_7">min 𝜽 L ′ (𝜽 ), where L ′ (𝜽 ) = 1 𝑛 𝑛 ∑︁ 𝑖=1 max ∥x ′ 𝑖 −x 𝑖 ∥ 𝑝 ≤𝜖 ℓ ′ 𝑖 𝑓 x ′ 𝑖 ; 𝜽 , 𝑦 𝑖 , (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where 𝑛 is the number of training examples, x ′ 𝑖 is the adversarial example within the 𝜖-ball (bounded by an 𝐿 𝑝 -norm) centered at natural example x 𝑖 , 𝑓 is the DNN with weight 𝜽, ℓ ′ (•) is the standard supervised classification loss (e.g., the cross-entropy loss), and L ′ (𝜽 ) is called the "adversarial loss". However, above general framework of AT can not directly be applied in graph contrastive learning because (1) AT requires labels as supervision while labels are not available in graph contrastive learning; (2) Perturbing each graph for the dataset in an adversarial way will introduce heavy computational overhead, which has been pointed out in GROC <ref type="bibr" target="#b18">[19]</ref>. To remedy the first issue, we substitute supervised classification loss in Eq. ( <ref type="formula" target="#formula_7">8</ref>) with contrastive loss in Eq. (4). To tackle the second issue, instead of conducting adversarial transformation of graph data, we perturb the encoder in an adversarial way, which is more computationally efficient. Assuming that 𝚯 is the weight space of GNNs, for any w and any positive 𝜖, we can define the norm ball in 𝜽 with radius 𝜖 centered at w as, </p><formula xml:id="formula_9">R(w; 𝜖) := {𝜽 ∈ 𝚯 : ∥𝜽 − w∥ ≤ 𝜖},<label>(9)</label></formula><formula xml:id="formula_10">ℓ 𝑖 (𝑓 (G 𝑖 ; 𝜽 + 𝚫) , 𝑓 (G 𝑖 ; 𝜽 )) ,<label>(10)</label></formula><p>where 𝑀 is the number of graphs in the dataset. We propose Algorithm 1 to solve this optimization problem. Specifically, for inner maximization, we forward 𝐼 steps to update 𝚫 in the direction of increasing the contrastive loss using gradient ascent algorithm. With the output perturbation 𝚫 of inner maximization, the outer loops update the weights 𝜽 of GNNs with mini-batched SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Encoder perturbation of AT-SimGRACE</head><p>Data: </p><formula xml:id="formula_11">Graph dataset D = {G 1 , G 2 , ..., G 𝑀 },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical Justification</head><p>In this section, we aim to explain the reasons why AT-SimGRACE can enhance the robustness of graph contrastive learning. To start, it is widely accepted that flatter loss landscape can bring robustness <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>. For example, as formulated in Eq. 8, adversarial training (AT) enhances robustness via restricting the change of loss when the input of models is perturbed indeed. Thus, we want to theoretically justify why AT-SimGRACE works via validating that AT-SimGRACE can flatten the loss landscape. Inspired by previous work <ref type="bibr" target="#b28">[29]</ref> that connects sharpness of loss landscape and PAC-Bayes theory <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, we utilize PAC-Bayes framework to derive guarantees on the expected error. Assuming that the prior distribution 𝑃 over the weights is a zero mean, 𝜎 2 variance Gaussian distribution, with probability at least 1 − 𝛿 over the draw of 𝑀 graphs, the expected error of the encoder can be bounded as:</p><formula xml:id="formula_12">E { G 𝑖 } 𝑀 𝑖=1 ,𝚫 [L (𝜽 +𝚫)] ≤ E 𝚫 [L (𝜽 +𝚫)] +4 √︄ 𝐾𝐿(𝜽 + 𝚫∥𝑃) + ln 2𝑀 𝛿 𝑀 . (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>We choose 𝚫 as a zero mean spherical Gaussian perturbation with variance 𝜎 2 in every direction, and set the variance of the perturbation to the weight with respect to its magnitude 𝜎 = 𝛼 ∥𝜽 ∥. Besides, we substitute</p><formula xml:id="formula_14">E 𝚫 [L (𝜽 + 𝚫)] with L (𝜽 ) + E 𝚫 [L (𝜽 + 𝚫)] − L (𝜽 ).</formula><p>Then, we can rewrite Eq. 11 as: </p><formula xml:id="formula_15">E { G 𝑖 } 𝑀 𝑖=1 ,𝚫 [L (𝜽 + 𝚫)] ≤ L (𝜽 ) + {E 𝚫 [L (𝜽 + 𝚫)] − L (𝜽 )} Expected sharpness + 4 √︄ 1 𝑀 1 2𝛼 + ln 2𝑀 𝛿 .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct experiments to evaluate SimGRACE and AT-SimGRACE through answering the following research questions.</p><p>• RQ1. (Generalizability) Does SimGRACE outperform competitors in unsupervised and semi-supervised settings?</p><p>• RQ2. (Transferability) Can GNNs pre-trained with Sim-GRACE show better transferability than competitors?</p><p>• RQ3. (Robustness) Can AT-SimGRACE perform better than existing competitors against various adversarial attacks?</p><p>• RQ4. (Efficiency) How about the efficiency (time and memory) of SimGRACE? Does it more efficient than competitors?</p><p>• RQ5. (Hyperparameters Sensitivity) Is the proposed Sim-GRACE sensitive to hyperparameters like the magnitude of the perturbation 𝜂, training epochs and batch size?  <ref type="bibr" target="#b26">[27]</ref>, including graph data for various social networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">52]</ref> and biochemical molecules <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>. For transfer learning, we perform pre-training on ZINC-2M and PPI-306K and finetune the model with various datasets including PPI, BBBP, ToxCast and SIDER. More details about above datasets can be seen in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Protocols.</head><p>Following previous works for graphlevel self-supervised representation learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>, we evaluate the generalizability of the learned representations on both unsupervised and semi-supervised settings. In unsupervised setting, we train SimGRACE using the whole dataset to learn graph representations and feed them into a downstream SVM classifier with 10-fold cross-validation. For semi-supervised setting, we pre-train GNNs with SimGRACE on all the data and did finetuning &amp; evaluation with 𝐾 (𝐾 = 1 label rate ) folds for datasets without the explicit training/validation/test split. For datasets with the train/validation/test split, we pre-train GNNs with the training data, finetuning on the partial training data and evaluation on the validation/test sets. More details about GNNs architectures and hyper-parameters can be seen in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Compared baselines.</head><p>We compare SimGRACE with stateof-the-arts graph kernel methods including GL <ref type="bibr" target="#b35">[36]</ref>, WL <ref type="bibr" target="#b34">[35]</ref> and DGK <ref type="bibr" target="#b51">[52]</ref>. Also, we compare SimGRACE with other graph selfsupervised learning methods: GAE <ref type="bibr" target="#b20">[21]</ref>, node2vec <ref type="bibr" target="#b11">[12]</ref>, sub2vec <ref type="bibr" target="#b0">[1]</ref>, graph2vec <ref type="bibr" target="#b27">[28]</ref>, EdgePred <ref type="bibr" target="#b15">[16]</ref>, AttrMasking <ref type="bibr" target="#b15">[16]</ref>, ContextPred <ref type="bibr" target="#b15">[16]</ref>, Infomax (DGI) <ref type="bibr" target="#b44">[45]</ref>, InfoGraph <ref type="bibr" target="#b38">[39]</ref> and instance-instance contrastive methods GraphCL <ref type="bibr" target="#b53">[54]</ref>, JOAO(v2) <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised and semi-supervised learning (RQ1)</head><p>For unsupervised representation learning, as can be observed in Table <ref type="table" target="#tab_4">2</ref>, SimGRACE outperforms other baselines and always ranks top three on all the datasets. Generally, SimGRACE performs better on biochemical molecules compared with data augmentation based methods. The reason is that the semantics of molecular graphs are more fragile compared with social networks. General augmentations (drop nodes, drop edges and etc.) adopted in other baselines will not alter the semantics of social networks dramatically. For semi-supervised task, as can be observed in Table <ref type="table" target="#tab_6">4</ref>, we report two semi-supervised tasks with 1 % and 10% label rate respectively. In 1% setting, SimGRACE outperforms previous baselines by a large margin or matching the performance of SOTA methods. For 10 % setting, SimGRACE performs comparably to SOTA methods including GraphCL and JOAO(v2) whose augmentations are derived via expensive trial-and-errors or cumbersome search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transferability (RQ2)</head><p>To evaluate the transferability of the pre-training scheme, we conduct experiments on transfer learning on molecular property prediction in chemistry and protein function prediction in biology following previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref>. Specifically, we pre-train and finetune the models with different datasets. For pre-training, learning rate is tuned in {0.01, 0.1, 1.0} and epoch number in {20, 40, 60, 80, 100} where grid serach is performed. As sketched in Table <ref type="table" target="#tab_5">3</ref>, there is no universally beneficial pre-training scheme especially for the outof-distribution scenario in transfer learning. However, SimGRACE  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Adversarial robustness (RQ3)</head><p>Following previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b53">54]</ref>, we perform on synthetic data to classify the component number in graphs, facing the RandSampling, GradArgmax and RL-S2V attacks, to evaluate the robustness of AT-SimGRACE. To keep fair, we adopt Structure2vec <ref type="bibr" target="#b5">[6]</ref> as the GNN encoder as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b53">54]</ref>. Besides, we pretrain the GNN encoder for 150 epochs because it takes longer time for the convergence of adversarial training. We set the inner learning rate 𝜁 = 0.001 and the radius of perturbation ball 𝜖 = 0.01. As demonstrated in Table <ref type="table" target="#tab_7">5</ref>, AT-SimGRACE boosts the robustness of GNNs dramatically compared with training from scratch and GraphCL under three typical evasion attacks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficiency (Training time and memory cost) (RQ4)</head><p>In Table <ref type="table" target="#tab_8">6</ref>, we compare the performance of SimGRACE with the state-of-the-arts methods including GraphCL and JOAOv2 in terms of their training time and the memory overhead. Here, the training time refers to the time for pre-training stage of the semi-supervised task and the memory overhead refers to total memory costs of model parameters and all hidden representations of a batch. As can be observed, SimGRACE runs near 40-90 times faster than JOAOv2 and 2.5-4 times faster than GraphCL. If we take the time for manual trial-and-errors in GraphCL into consideration, the superiority of SimGRACE will be more pronounced. Also, SimGRACE requires less computational memory than GraphCL and JOAOv2. In particular, the efficiency of SimGRACE can be more prominent on large-scale social graphs, such as COLLAB and RDT-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Hyper-parameters sensitivity analysis (RQ5)</head><p>4.6.1 Magnitude of the perturbation. As can be observed in Figure <ref type="figure" target="#fig_2">4</ref>, weight perturbation is crucial in SimGRACE. If we set the magnitude of the perturbation as zero (𝜂 = 0), the performance is usually the lowest compared with other setting of perturbation across these four datasets. This observation aligns with our intuition. Without perturbation, SimGRACE simply compares two original samples as a negative pair while the positive pair loss becomes zero, leading to homogeneously pushes all graph representations away from each other, which is non-intuitive to justify. Instead, appropriate perturbations enforce the model to learn representations invariant to the perturbations through maximizing the agreement between a graph and its perturbation. Besides, well aligned with previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref> that claim "hard" positive pairs and negative pairs can boost the performance of contrastive learning, we can observe that larger magnitude (within an appropriate range) of the perturbation can bring consistent improvement of the performance. However, over-large perturbations will lead to performance degradation because the semantics of graph data are not preserved.  more negative samples for contrasting. Similarly, training longer also provides more new negative samples for each sample because the split of total datasets is more various with more training epochs.</p><p>In our experiments, to keep fair, we follow the same settings of other competitors <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> via training the GNN encoder with batch size as 128 and number of epochs as 20. In fact, we can further improve the performance of SimGRACE with larger batch size and longer training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we propose a simple framework (SimGRACE) for graph contrastive learning. Although it may appear simple, we demonstrate that SimGRACE can outperform or match the stateof-the-art competitors on multiple graph datasets of various scales and types, while enjoying unprecedented degree of flexibility, high efficiency and ease of use. We emancipate graph contrastive learning from tedious manual tuning, cumbersome search or expensive domain knowledge. Furthermore, we devise adversarial training schemes to enhance the robustness of SimGRACE in a principled way and theoretically explain the reasons. There are two promising avenues for future work: (1) exploring if encoder perturbation can work well in other domains like computer vision and natural language processing. (2) applying the pre-trained GNNs to more real-world tasks including social analysis and biochemistry.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ℓ 𝑎𝑙𝑖𝑔𝑛 -ℓ 𝑢𝑛𝑖 𝑓 𝑜𝑟𝑚 plot for SimGRACE, GraphCL and MoCL on MUTAG dataset. The numbers around the points are the indexes of epochs. For both ℓ 𝑎𝑙𝑖𝑔𝑛 and ℓ 𝑢𝑛𝑖 𝑓 𝑜𝑟𝑚 , lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4 √︂ 1 𝑀 1 2𝛼</head><label>411</label><figDesc>It is obvious that E 𝚫 [L (𝜽 + 𝚫)] ≤ max 𝚫 [L (𝜽 + 𝚫)] and the third term + ln 2𝑀 𝛿 is a constant. Thus, AT-SimGRACE optimizes the worst-case of sharpness of loss landscape max 𝚫 [L (𝜽 + 𝚫)] − L (𝜽 ) to the bound of the expected error, which explains why AT-SimGRACE can enhance the robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance versus magnitude of the perturbation (𝜂) in unsupervised representation learning task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of SimGRACE trained with different batch size and epochs on NCI1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Illustration of SimGRACE, a simple framework of graph contrastive learning. Instead of augmenting the graph data, we feed the original graph G into a GNN encoder 𝑓 (•; 𝜽 ) and its perturbed version 𝑓 (•; 𝜽 ′ ). After passing a shared projection head 𝑔(•), we maximize the agreement between representations 𝒛 𝑖 and 𝒛 𝑗 via a contrastive loss.</figDesc><table><row><cell>GNN Encoder</cell><cell></cell></row><row><cell>Perturbed GNN encoder</cell><cell>Projection</cell></row><row><cell>Input Graph</cell><cell>Head</cell></row><row><cell>Perturb</cell><cell>Maximize Agreement</cell></row><row><cell></cell><cell>Projection</cell></row><row><cell></cell><cell>Head</cell></row><row><cell>Embeddings</cell><cell>Stop gradient</cell></row><row><cell>Figure 2:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparing classification accuracy with baselines under the same experiment setting. The top three accuracy or rank for each dataset are emphasized in bold. A.R. denotes average rank. -indicates that results are not available in published papers. ± 1.06 74.44 ± 0.31 72.85 ± 1.78 89.01 ± 1.13 70.65 ± 1.13 82.50 ± 1.42 53.46 ± 1.03 73.03 ± 0.87 3.8 GraphCL 77.87 ± 0.41 74.39 ± 0.45 78.62 ± 0.40 86.80 ± 1.34 71.36 ± 1.15 89.53 ± 0.84 55.99 ± 0.28 71.14 ± 0.44 3.1 JOAO 78.07 ± 0.47 74.55 ± 0.41 77.32 ± 0.54 87.35 ± 1.02 69.50 ± 0.36 85.29 ± 1.35 55.74 ± 0.63 70.21 ± 3.08 4.3 JOAOv2 78.36 ± 0.53 74.07 ± 1.10 77.40 ± 1.15 87.67 ± 0.79 69.33 ± 0.34 86.42 ± 1.45 56.03 ± 0.27 70.83 ± 0.25 3.6 SimGRACE 79.12 ± 0.44 75.35 ± 0.09 77.44 ± 1.11 89.01 ± 1.31 71.72 ± 0.82 89.51 ± 0.89 55.91 ± 0.34 71.30 ± 0.77</figDesc><table><row><cell>Methods</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>DD</cell><cell>MUTAG</cell><cell>COLLAB</cell><cell>RDT-B</cell><cell>RDT-M5K</cell><cell>IMDB-B</cell><cell>A.R. ↓</cell></row><row><cell>GL</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>81.66 ± 2.11</cell><cell>−</cell><cell cols="3">77.34 ± 0.18 41.01 ± 0.17 65.87 ± 0.98</cell><cell>8.3</cell></row><row><cell>WL</cell><cell cols="2">80.01 ± 0.50 72.92 ± 0.56</cell><cell>−</cell><cell>80.72 ± 3.00</cell><cell>−</cell><cell cols="3">68.82 ± 0.41 46.06 ± 0.21 72.30 ± 3.44</cell><cell>6.2</cell></row><row><cell>DGK</cell><cell cols="2">80.31 ± 0.46 73.30 ± 0.82</cell><cell>−</cell><cell>87.44 ± 2.72</cell><cell>−</cell><cell cols="3">78.04 ± 0.39 41.27 ± 0.18 66.96 ± 0.56</cell><cell>5.5</cell></row><row><cell>node2vec</cell><cell cols="2">54.89 ± 1.61 57.49 ± 3.57</cell><cell>−</cell><cell>72.63 ± 10.20</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>9.0</cell></row><row><cell>sub2vec</cell><cell cols="2">52.84 ± 1.47 53.03 ± 5.55</cell><cell>−</cell><cell>61.05 ± 15.80</cell><cell>−</cell><cell cols="3">71.48 ± 0.41 36.68 ± 0.42 55.26 ± 1.54</cell><cell>10.2</cell></row><row><cell>graph2vec</cell><cell cols="2">73.22 ± 1.81 73.30 ± 2.05</cell><cell>−</cell><cell>83.15 ± 9.25</cell><cell>−</cell><cell cols="3">75.78 ± 1.03 47.86 ± 0.26 71.10 ± 0.54</cell><cell>6.7</cell></row><row><cell>MVGRL</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>75.40 ± 7.80</cell><cell>−</cell><cell>82.00 ± 1.10</cell><cell>−</cell><cell>63.60 ± 4.20</cell><cell>8.3</cell></row><row><cell>InfoGraph</cell><cell cols="9">76.20 2.0</cell></row><row><cell cols="3">4.1 Experimental Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4.1.1 Datasets. For unsupervised and semi-supervised learning, we use datasets from the benchmark TUDataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Transfer learning comparison with other pretraining schemes. The top-3 accuracy for each dataset are emphasized in bold.</figDesc><table><row><cell>Pre-Train dataset</cell><cell>PPI-306K</cell><cell></cell><cell>ZINC 2M</cell><cell></cell></row><row><cell>Fine-Tune dataset</cell><cell>PPI</cell><cell>BBBP</cell><cell>ToxCast</cell><cell>SIDER</cell></row><row><cell>No Pre-Train</cell><cell>64.8 ± 1.0</cell><cell>65.8 ± 4.5</cell><cell>63.4 ± 0.6</cell><cell>57.3 ± 1.6</cell></row><row><cell>EdgePred</cell><cell>65.7 ± 1.3</cell><cell>68.8 ± 0.8</cell><cell>62.7 ± 0.4</cell><cell>58.4 ± 0.8</cell></row><row><cell>AttrMasking</cell><cell>65.2 ± 1.6</cell><cell>67.3 ± 2.4</cell><cell>64.1 ± 0.6</cell><cell>60.4 ± 0.7</cell></row><row><cell>ContextPred</cell><cell>64.4 ± 1.3</cell><cell>64.3 ± 2.8</cell><cell>64.2 ± 0.5</cell><cell>61.0± 0.7</cell></row><row><cell>GraphCL</cell><cell>67.88 ± 0.85</cell><cell>68.0 ± 2.0</cell><cell>63.9 ± 0.6</cell><cell>60.9 ± 0.6</cell></row><row><cell>JOAO</cell><cell cols="4">64.43 ± 1.38 69.68 ± 0.67 62.40 ± 0.57 60.53 ± 0.88</cell></row><row><cell>JOAOv2</cell><cell cols="4">63.94 ± 1.59 70.22± 0.98 62.94 ± 0.48 59.97 ± 0.79</cell></row><row><cell>SimGRACE</cell><cell cols="4">70.25 ± 1.22 71.25 ± 0.86 63.36 ± 0.52 60.59 ± 0.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparing classification accuracy with baselines under the same semi-supervised setting. The top three accuracy or rank are emphasized in bold. − indicates that label rate is too low for a given dataset size. L.R. and A.R. are short for label rate and average rank, respectively.</figDesc><table><row><cell>L.R.</cell><cell>Methods</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>DD</cell><cell>COLLAB</cell><cell>RDT-B</cell><cell>RDT-M5K</cell><cell>A.R. ↓</cell></row><row><cell></cell><cell>No pre-train.</cell><cell>60.72 ± 0.45</cell><cell>−</cell><cell>−</cell><cell>57.46 ± 0.25</cell><cell>−</cell><cell>−</cell><cell>8.5</cell></row><row><cell></cell><cell cols="2">Augmentations 60.49 ± 0.46</cell><cell>−</cell><cell>−</cell><cell>58.40 ± 0.97</cell><cell>−</cell><cell>−</cell><cell>8.0</cell></row><row><cell></cell><cell>GAE</cell><cell>61.63 ± 0.84</cell><cell>−</cell><cell>−</cell><cell>63.20 ± 0.67</cell><cell>−</cell><cell>−</cell><cell>5.5</cell></row><row><cell></cell><cell>Infomax</cell><cell>62.72 ± 0.65</cell><cell>−</cell><cell>−</cell><cell>61.70 ± 0.77</cell><cell>−</cell><cell>−</cell><cell>4.0</cell></row><row><cell>1%</cell><cell>ContextPred</cell><cell>61.21 ± 0.77</cell><cell>−</cell><cell>−</cell><cell>57.60 ± 2.07</cell><cell>−</cell><cell>−</cell><cell>7.5</cell></row><row><cell></cell><cell>GraphCL</cell><cell>62.55 ± 0.86</cell><cell>−</cell><cell>−</cell><cell>64.57 ± 1.15</cell><cell>−</cell><cell>−</cell><cell>2.0</cell></row><row><cell></cell><cell>JOAO</cell><cell>61.97 ± 0.72</cell><cell>−</cell><cell>−</cell><cell>63.71 ± 0.84</cell><cell>−</cell><cell>−</cell><cell>4.5</cell></row><row><cell></cell><cell>JOAOv2</cell><cell>62.52 ± 1.16</cell><cell>−</cell><cell>−</cell><cell>64.51 ± 2.21</cell><cell>−</cell><cell>−</cell><cell>3.0</cell></row><row><cell></cell><cell>SimGRACE</cell><cell>64.21 ± 0.65</cell><cell>−</cell><cell>−</cell><cell>64.28 ± 0.98</cell><cell>−</cell><cell>−</cell><cell>2.0</cell></row><row><cell></cell><cell>No pre-train.</cell><cell cols="6">73.72 ± 0.24 70.40 ± 1.54 73.56 ± 0.41 73.71 ± 0.27 86.63 ± 0.27 51.33 ± 0.44</cell><cell>7.7</cell></row><row><cell></cell><cell cols="7">Augmentations 73.59 ± 0.32 70.29 ± 0.64 74.30 ± 0.81 74.19 ± 0.13 87.74 ± 0.39 52.01 ± 0.20</cell><cell>7.0</cell></row><row><cell></cell><cell>GAE</cell><cell cols="6">74.36 ± 0.24 70.51 ± 0.17 74.54 ± 0.68 75.09 ± 0.19 87.69 ± 0.40 33.58 ± 0.13</cell><cell>6.3</cell></row><row><cell></cell><cell>Infomax</cell><cell cols="6">74.86± 0.26 72.27 ± 0.40 75.78 ± 0.34 73.76 ± 0.29 88.66 ± 0.95 53.61 ± 0.31</cell><cell>3.7</cell></row><row><cell>10%</cell><cell>ContextPred</cell><cell cols="6">73.00 ± 0.30 70.23 ± 0.63 74.66 ± 0.51 73.69 ± 0.37 84.76 ± 0.52 51.23 ± 0.84</cell><cell>8.3</cell></row><row><cell></cell><cell>GraphCL</cell><cell cols="6">74.63± 0.25 74.17± 0.34 76.17± 1.37 74.23 ± 0.21 89.11± 0.19 52.55 ± 0.45</cell><cell>2.8</cell></row><row><cell></cell><cell>JOAO</cell><cell cols="6">74.48 ± 0.27 72.13 ± 0.92 75.69 ± 0.67 75.30 ± 0.32 88.14 ± 0.25 52.83± 0.54</cell><cell>4.2</cell></row><row><cell></cell><cell>JOAOv2</cell><cell cols="6">74.86± 0.39 73.31± 0.48 75.81± 0.73 75.53± 0.18 88.79± 0.65 52.71 ± 0.28</cell><cell>2.5</cell></row><row><cell></cell><cell>SimGRACE</cell><cell cols="6">74.60 ± 0.41 74.03 ± 0.51 76.48 ± 0.52 74.74 ± 0.28 88.86 ± 0.62 53.97 ± 0.64</cell><cell>2.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance under three adversarial attacks for GNN with different depth following the protocols in<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="9">Two-Layer No Pre-Train GraphCL AT-SimGRACE No Pre-Train GraphCL AT-SimGRACE No Pre-Train GraphCL AT-SimGRACE Three-Layer Four-Layer</cell></row><row><cell>Unattack</cell><cell>93.20</cell><cell>94.73</cell><cell>94.24</cell><cell>98.20</cell><cell>98.33</cell><cell>99.32</cell><cell>98.87</cell><cell>99.00</cell><cell>99.13</cell></row><row><cell>RandSampling</cell><cell>78.73</cell><cell>80.68</cell><cell>81.73</cell><cell>92.27</cell><cell>92.60</cell><cell>94.27</cell><cell>95.13</cell><cell>97.40</cell><cell>97.67</cell></row><row><cell>GradArgmax</cell><cell>69.47</cell><cell>69.26</cell><cell>75.13</cell><cell>64.60</cell><cell>89.33</cell><cell>93.00</cell><cell>95.80</cell><cell>97.00</cell><cell>96.60</cell></row><row><cell>RL-S2V</cell><cell>42.93</cell><cell>42.20</cell><cell>44.86</cell><cell>41.93</cell><cell>61.66</cell><cell>66.00</cell><cell>70.20</cell><cell>84.86</cell><cell>85.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of efficiency on three graph datasets. Note that we do not take the time for manual trial-anderrors of GraphCL into consideration. In fact, picking the suitable augmentations manually for GraphCL is much more time-consuming. All the three methods are evaluated on a 32GB V100 GPU.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>shows competitive or better transferability than other pre-training</cell></row><row><cell></cell><cell></cell><cell></cell><cell>schemes, especially on PPI dataset.</cell></row><row><cell>Dataset</cell><cell>Algorithm</cell><cell>Training Time</cell><cell>Memory</cell></row><row><cell></cell><cell>GraphCL</cell><cell>111𝑠</cell><cell>1231𝑀𝐵</cell></row><row><cell>PROTEINS</cell><cell>JOAOv2</cell><cell>4088𝑠</cell><cell>1403𝑀𝐵</cell></row><row><cell></cell><cell>SimGRACE</cell><cell>46 s</cell><cell>1175 MB</cell></row><row><cell></cell><cell>GraphCL</cell><cell>1033𝑠</cell><cell>10199𝑀𝐵</cell></row><row><cell>COLLAB</cell><cell>JOAOv2</cell><cell>10742𝑠</cell><cell>7303𝑀𝐵</cell></row><row><cell></cell><cell>SimGRACE</cell><cell>378 s</cell><cell>6547 MB</cell></row><row><cell></cell><cell>GraphCL</cell><cell>917𝑠</cell><cell>4135𝑀𝐵</cell></row><row><cell>RDT-B</cell><cell>JOAOv2</cell><cell>10278𝑠</cell><cell>3935𝑀𝐵</cell></row><row><cell></cell><cell>SimGRACE</cell><cell>280 s</cell><cell>2729 MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>4.6.2 Batch-size and training epochs.Figure 5 demonstrates the performance of SimGRACE trained with various batch size and epochs. Generally, larger batch size or training epochs can bring better performance. The reason is that larger batch size will provide</figDesc><table><row><cell>Accuracy (%)</cell><cell>78.75 79.00 79.25 79.50 79.75 80.00</cell><cell>Batch size 64 128 256 512 1024</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>78.50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>78.25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>78.00</cell><cell>20</cell><cell>40</cell><cell>60 Epochs</cell><cell>80</cell><cell>100</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by the Science and Technology Innovation 2030 -Major Project (No. 2021ZD0150100) and National Natural Science Foundation of China (No. U21A20427).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For unsupervised setting, experiments are performed for 5 times each of which corresponds to a 10-fold evaluation, with mean and standard deviation of accuracies (%) reported. For semi-supervised learning, we perform experiments with 1% (if there are over 10 samples for each class) and 10% label rate for 5 times, each of which corresponds to a 10-fold evaluation, with mean and standard deviation of accuracies (%) reported. For pre-training, learning rate is tuned in {0.1, 1.0, 5.0, 10.0} and epoch number in {20, 40, 60, 80, 100} where grid search is performed. All datasets used in both unsupervised and semi-supervised experiments can be seen in Table <ref type="table">7</ref>. The datasets utilized in transfer learning can be seen in Table <ref type="table">8</ref>. ZINC-2M and PPI-306K are used for pre-training and the left ones are for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Transfer learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GNN ARCHITECTURES IN VARIOUS SETTINGS</head><p>To keep fair, we adopt the same GNNs architectures with previous competitors. Specifically, for unsupervised task, GIN <ref type="bibr" target="#b50">[51]</ref> with 3 layers and 32 hidden dimensions is adopted as the encoder. For semi-supervised task, we utilize ResGCN <ref type="bibr" target="#b3">[4]</ref> with 5 layers and 128 hidden dimensions. For transfer learning, we adopt GIN with the default setting in <ref type="bibr" target="#b15">[16]</ref> as the GNN-based encoder. For experiments on adversarial robustness, Structure2vec is adopted as the GNNbased encoder as in <ref type="bibr" target="#b6">[7]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sub2Vec: Feature Learning for Subgraphs. ADVANCES IN KNOWLEDGE DIS-COVERY AND DATA MINING, PAKDD</title>
		<author>
			<persName><forename type="first">Bijaya</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naren</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">B</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PT II</title>
		<imprint>
			<biblScope unit="page" from="170" to="182" />
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Rozemberczki</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiss</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarkar</forename><surname>Rik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the Loss Landscape of Adversarial Training: Identifying Challenges and How to Overcome Them</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salzmann</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomioka</forename><surname>Ryota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Süsstrunk</forename><surname>Sabine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are Powerful Graph Neural Nets Necessary?</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Dissection on Graph Classification. arXiv: Learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On Sampling Strategies for Neural Network-based Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="767" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative Embeddings of Latent Variable Models for Structured Data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Adversarial Attack on Graph Structured Data. international conference on machine learning</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1123" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>the association for computational linguistics</publisher>
		</imprint>
	</monogr>
	<note>north american chapter</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinguishing Enzyme Structures from Non-enzymes Without Alignments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul Dobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">GraphGT: Machine Learning Datasets for Deep Graph Generation and Transformation</title>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengning</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junji</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Varala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Angirekula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<title level="m">Explaining and Harnessing Adversarial Examples. international conference on learning representations</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">node2vec: Scalable Feature Learning for Networks</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Generative Models for Spatial Networks</title>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467394</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467394" />
	</analytic>
	<monogr>
		<title level="m">KDD. 505-515</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive Learning with Adversarial Examples</title>
		<author>
			<persName><forename type="first">Chih-Hui</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Nvasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining Virtual Event CA USA</title>
				<imprint>
			<date type="published" when="2020-07">2020. July, 2020. 2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1477" to="1483" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Jovanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13085</idno>
		<title level="m">Towards robust graph contrastive learning</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">He</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Haoqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Wu Yuxin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girshick</forename><surname>Saining</surname></persName>
		</author>
		<author>
			<persName><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="9726" to="9735" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<title level="m">Towards Deep Learning Models Resistant to Adversarial Attacks. international conference on learning representations</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">David</forename><surname>Mcallester</surname></persName>
		</author>
		<title level="m">PAC-Bayesian model averaging. COLT</title>
				<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="164" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Some PAC-Bayesian Theorems</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="355" to="363" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Nils</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajasekar</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jaiswal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">graph2vec: Learning Distributed Representations of Graphs</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring Generalization in Deep Learning</title>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5947" to="5956" />
			<date type="published" when="2017">2017. 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Pin-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Das</forename><surname>Payel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Ramamurthy Natesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning</title>
		<author>
			<persName><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<publisher>SSPR/SPR</publisher>
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrastive Learning with Hard Negative Samples</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>David Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=CR1XOQ0UTh-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Erik Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ecient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N S</forename><surname>Nino Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="page" from="488" to="495" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1849" to="1857" />
			<date type="published" when="2016">2016. 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MoCL: Contrastive Learning on Molecular Graphs with Multi-level Domain Knowledge</title>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bootstrapped Representation Learning on Graphs</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=QrzVRAA49Ud" />
	</analytic>
	<monogr>
		<title level="m">ICLR 2021 Workshop on Geometrical and Topological Representation Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornblith</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norouzi</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinton</forename><surname>Geoffrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Tongzhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isola</forename><surname>Phillip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">ICML</biblScope>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding Adversarial Robustness Through Loss Landscape Geometries</title>
		<author>
			<persName><forename type="first">Dian</forename><forename type="middle">Yap</forename><surname>Vinay Prabhu Uday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whaley</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><surname>John</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><forename type="middle">R</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax. ICLR</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H38f_9b90BO" />
		<title level="m">Towards Robust Graph Neural Networks against Label Noise</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Debiased Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02027</idno>
		<ptr target="https://arxiv.org/abs/2110.02027" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards Effective and Generalizable Fine-tuning for Pre-trained Molecular Graph Models</title>
		<author>
			<persName><forename type="first">Jiangbin</forename><surname>Jun Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1101/2022.02.03.479055</idno>
		<ptr target="https://www.biorxiv.org/content/early/2022/02/06/2022.02.03.479055.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">How Powerful are Graph Neural Networks? international conference on learning representations</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<title level="m">Deep Graph Kernels. ACM Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07594</idno>
		<title level="m">Graph Contrastive Learning Automated</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Contrastive self-supervised learning for graph classification</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An Empirical Study of Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</title>
				<editor>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</editor>
		<meeting>the Neural Information Processing Systems Track on Datasets and Benchmarks</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.04131" />
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Graph Contrastive Learning with Adaptive Augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
