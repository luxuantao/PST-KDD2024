<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Curriculum Learning for Data-Efficient Vision-Language Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tejas</forename><surname>Srinivasan</surname></persName>
							<email>tejas.srinivasan@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Curriculum Learning for Data-Efficient Vision-Language Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aligning image and text encoders from scratch using contrastive learning requires large amounts of paired image-text data. We alleviate this need by aligning individually pre-trained language and vision representation models using a much smaller amount of paired data, augmented with a curriculum learning algorithm to learn fine-grained visionlanguage alignments. TOnICS (Training with Ontology-Informed Contrastive Sampling) initially samples minibatches whose imagetext pairs contain a wide variety of objects to learn object-level alignment, and progressively samples minibatches where all imagetext pairs contain the same object to learn finergrained contextual alignment. Aligning pretrained BERT and VinVL models to each other using TOnICS outperforms CLIP on downstream zero-shot image retrieval while using less than 1% as much training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aligned representations for language and visionwhich encode texts and corresponding images in a common latent space-are necessary to perform effective cross-modal retrieval. CLIP <ref type="bibr" target="#b6">(Radford et al., 2021)</ref> and ALIGN <ref type="bibr" target="#b3">(Jia et al., 2021)</ref> train individual text and image encoders from scratch to produce aligned image-text representations. Their encoders demonstrate strong cross-modal alignment, evidenced by strong performance on zero-shot retrieval tasks. However, these models were trained on proprietary datasets of 400M and 1B image-text pairs respectively, on hundreds of GPUs and TPUs, which is infeasible for non-industry practitioners.</p><p>CLIP and ALIGN align their encoders using the contrastive InfoNCE objective <ref type="bibr" target="#b4">(Oord et al., 2018)</ref>, which seeks to maximize the mutual information between image and text representations. In the In-foNCE objective, the model must correctly identify the positive image-text pair from among a set of negatives formed by the other minibatch pairs.  Since samples within a minibatch act as negative samples for each other in the InfoNCE objective, the minibatch determines the granularity of alignment that is learned. Minibatches constructed by random sampling contain a large variety of objects in the images and texts (Figure <ref type="figure" target="#fig_1">1</ref>, top). To correctly match a dog-related caption to its image, it is sufficient to identify that the retrieved image must contain a dog, since the vast majority of randomly sampled negative images will not contain a dog. Thus, random minibatch sampling reduces the contrastive task to object-matching, for which object-level vision-language alignment suffices.</p><p>When minibatches are sampled such that the images contain the same objects, object-level alignments no longer suffice (Figure <ref type="figure" target="#fig_1">1</ref>, bottom). The contrastive task can no longer be solved by identifying that the retrieved image must contain a dog, since all the negative images will also have a dog. The model must produce language and vision representations that encode shared context-level information, resulting in a finer-grained alignment.</p><p>In this work, rather than training our image and text encoders from scratch, we leverage rich singlemodality pre-trained models-BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> for language, VinVL <ref type="bibr" target="#b9">(Zhang et al., 2021)</ref> 1 for vision-and align them to each other using the InfoNCE contrastive objective. We perform the vision-language alignment using TOnICS, a novel ontology-based curriculum learning algorithm. TOnICS initiates training with an easy contrastive task by sampling minibatches randomly and progressively makes the contrastive task harder by constructing minibatches containing the same object class in the image and text inputs. We show that our learned representations have strong crossmodal alignment-outperforming CLIP on zeroshot Flickr30K image retrieval-while using less than 1% as much paired image-text training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contrastive Vision-Language Alignment</head><p>We align language representations from BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> and visual representations from a VinVL object detector <ref type="bibr" target="#b9">(Zhang et al., 2021)</ref>.</p><p>Our BERT-VinVL Aligner model is similar to the phrase grounding model from <ref type="bibr" target="#b2">Gupta et al. (2020)</ref>. At every training step, the input to the model is a minibatch of N B triplets, where each triplet X i = {t i , v i , w} comes from an image-text pair. Each image caption t i is encoded using BERT. The caption contains a noun w, whose word representation is denoted as h i . For the corresponding image, v i is a set of region features extracted from a frozen pre-trained VinVL object detector. 2 We add a learnable linear projection atop these region features.</p><p>In the cross-modal interaction, we employ a single Transformer <ref type="bibr" target="#b8">(Vaswani et al., 2017)</ref> layer that uses i-th noun representation h i as the query and j-th image features v i as the keys and values. This layer outputs a visual representation v att (i, j), which is an attended representation of the j-th image, conditioned on the noun from the i-th caption. We then compute a dot product between the i-th noun representation h i and the attended representation of j-th image v att (i, j) to get an image-text score s(i, j) = ?(h i , v att (i, j)) (Figure <ref type="figure" target="#fig_2">2</ref>).</p><p>To align the noun representation h i to its corresponding image v i , we use the InfoNCE loss <ref type="bibr" target="#b4">(Oord et al., 2018)</ref> which maximizes a lower bound of 1 We use VinVL to refer to their pre-trained object detector. 2 Region features provided at https://github.com/ pzzhang/VinVL/blob/main/DOWNLOAD.md</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score s(i, j)</head><p>The dog has a collar . the mutual information between h i and v att (i, i). InfoNCE minimizes the cross-entropy of correctly retrieving an image v i from the set of all minibatch images, given the query noun representation h i , with other instances in the minibatch acting as negative samples. We refer to the objective in this setup as the image retrieval loss, L IR :</p><formula xml:id="formula_0">Text ( t i ) Image ( v j ) BERT Language</formula><formula xml:id="formula_1">L IR (i) = -log exp(s(i, i)) N B j=1 exp(s(i, j))</formula><p>The training loss L IR is the mean loss L IR (i) over all images i = {1...N B } in the minibatch B. We also similarly define a text retrieval loss, L T R , where the image v i is used to retrieve the correct noun representation h i :</p><formula xml:id="formula_2">L T R (i) = -log exp(s(i, i)) N B j=1 exp(s(j, i))</formula><p>We experiment with training our model using just the image retrieval loss L IR , as well as the sum of the two losses L IR + L T R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TOnICS: Training with Ontology Informed Contrastive Sampling</head><p>As noted above, negative samples for the contrastive learning objective come from other pairs in the minibatch. Therefore, the minibatch sampling itself influences the alignment learned by the model. We hypothesize that sampling minibatches randomly will yield object-level alignments, while sampling harder minibatches containing the same object in the image may result in fine-grained contextual alignments.</p><p>We introduce TOnICS, Training with Ontology-Informed Contrastive Sampling (Figure <ref type="figure" target="#fig_3">3</ref>), a curriculum learning algorithm that initially seeks to align vision and language representations at the object level, and later learns contextual alignments. We construct the ontology (Figure <ref type="figure" target="#fig_3">3</ref>, left), which contains an entity root node and its children object nodes ? o , each corresponding to an object class o. Every object node ? o has a corresponding set of triplet instances X(? o ), a subset of the full training dataset whose triplet instances all contain the same object class o in the image, and all containing a noun from the noun set w(o) in the caption.</p><p>TOnICS Minibatch Sampling At every training step, TOnICS proceeds in two stages. First, a node ? is sampled from the ontology, according to a sampling probability distribution P S (?). Second, we sample a minibatch according to the node that was just sampled. If we sample the entity node ? e , we sample the minibatch by sampling N B instances from the full training data at random. If we sample an object node ? o , we sample N B instances from the corresponding set X(? o ), ensuring the minibatch contains images with the same object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOnICS Curriculum Refresh</head><p>The curriculum is formed by varying the nodes' sampling probability distribution throughout training. We initialize training by setting P S (? e ) = 1 and P S (? o ) = 0 for all object nodes. After every fixed number of training steps, we evaluate the model's image retrieval performance on a set of 100 held-out instances. If the held-out retrieval accuracy is greater than a certain threshold, we say that the model has learned the object-level alignment task, and we can start introducing harder minibatches in the training by refreshing the curriculum. The refresh step is performed by multiplying the entity node's current sampling probability P S (? e ) by a factor ?; ? &lt; 1. The remaining probability mass (1 -?) ? P S (? e ) is distributed among the object nodes. For each object node ? o , we update its sampling probability:</p><formula xml:id="formula_3">P S (? o ) = P S (? o ) + (1 -?)P S (? e ) ? |X(? o )| |X(? o )| .</formula><p>Object classes that are more common in the training data have more sampling probability mass distributed to their object node ? o , by weighting mass according to the size of the node's instance set, |X(? o )|. With each curriculum refresh, sampling mass is pushed down from the entity node to the object nodes, as long as P S (? e ) does not fall below a fixed threshold ?. Thresholding P S (? e ) ensures the model still sees random minibatches and does not forget the initially learned object-level alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Details</head><p>We train our BERT-VinVL model on MS-COCO and Conceptual Captions. We compare our model against CLIP on downstream retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data and Ontology</head><p>We train our model on image-text pairs from a combination of MS-COCO <ref type="bibr" target="#b0">(Chen et al., 2015)</ref> and Conceptual Captions <ref type="bibr" target="#b7">(Sharma et al., 2018)</ref>. Our triplet instances only contain nouns which we wish </p><formula xml:id="formula_4">Minibatch Sampling Method Zero-Shot Flickr30K MS-COCO # Image- Text Pairs Image Retrieval Text Retrieval Image Retrieval Text Retrieval Model LT R R@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5 CLIP-ViT-B/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use pre-trained BERT-base as our text encoder. For our image encoder, we use VinVL, a pretrained object detector that detects regions of interest (ROIs) in the image and outputs pooled CNN features for all ROIs. We use pre-extracted ROI features and treat the VinVL encoder as frozen, as we cannot backpropagate through the object detector.</p><p>All our models are trained for 500K iterations with a batch size of N B = 256, yielding 255 negative pairs for every positive pair. Each model was trained on a single V100 GPU for 6 days, compared to CLIP which used 256 V100 GPUs for 12 days.</p><p>After every 5K iterations, we evaluate retrieval over a set of held-out instances and perform a curriculum refresh step if the held-out accuracy is at least 90%. When performing a refresh step, we retain ? = 90% of entity's sampling probability, so long as the probability does not fall below ? = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Evaluation</head><p>To compare the effect of using pre-trained unimodal encoders at the start of the alignment process, we compare our model against CLIP <ref type="bibr" target="#b6">(Radford et al., 2021)</ref>. CLIP also uses separate image and text encoders, aligned using a contrastive loss with image-text data. Unlike our BERT-VinVL Aligner model, CLIP trains the two encoders from scratch, and uses significantly more paired imagetext data-400M pairs, compared to our 2.84M pairs. Since we use the base variant of BERT, we compare against the CLIP-ViT-B/32 variant. <ref type="foot" target="#foot_0">3</ref> We do not compare against ALIGN as they have not released their base model checkpoint.</p><p>To evaluate the utility of our TOnICS algorithm, we also train our BERT-VinVL Aligner using a Random minibatch sampling baseline, where the minibatch instances are always randomly sampled throughout the training process.</p><p>We directly evaluate our trained Aligner model's (as well as pre-trained CLIP) on image and text retrieval. Specifically, we perform zero-shot retrieval on the test set of Flickr30K <ref type="bibr" target="#b5">(Plummer et al., 2015)</ref>, which contains 1,000 images. We also perform retrieval evaluation on the MS-COCO test set, which contains 5,000 images. This latter evaluation is not zero-shot since our training data contains images from the MS-COCO train set. We compare the Recall@1 and Recall@5 of all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we directly transfer both our trained BERT-VinVL Aligner model and pre-trained CLIP to the downstream task of image and text retrieval. Since our models are trained using retrieval objectives, we perform the retrieval evaluation using the same setup as training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We propose TOnICS, a curriculum learning algorithm for contrastive alignment of language and vision encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our BERT-VinVL Aligner model scores every image-text combination (t i , v j ) in the minibatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: TOnICS selects image-text pairs for the minibatch by first sampling a node ? from an ontology, according to a distribution P S (?). Sampling the root entity node yields easy minibatches containing pairs with a variety of objects, whereas sampling one of its children object nodes yields harder minibatches containing pairs sharing a common object, such as apple or dog, in a variety of contexts (left). TOnICS performs curriculum learning by moving node sampling mass away from the root entity node to the object nodes as training progresses (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of our BERT-VinVL Aligner model on image and text retrieval, compared to a CLIP model. Numbers in bold represent the best results among our model and CLIP.</figDesc><table><row><cell cols="2">32 400M</cell><cell>Random</cell><cell>-</cell><cell>58.66</cell><cell>83.38</cell><cell>79.2</cell><cell>95</cell><cell>30.45</cell><cell>56.02</cell><cell cols="2">50.12 75.02</cell></row><row><cell></cell><cell>2.84M</cell><cell>Random</cell><cell></cell><cell>58.18</cell><cell>84.24</cell><cell>22.2</cell><cell>47.9</cell><cell>42.67</cell><cell>74.43</cell><cell>15.5</cell><cell>37.7</cell></row><row><cell>BERT-VinVL</cell><cell>2.84M</cell><cell>TOnICS</cell><cell></cell><cell>60.32</cell><cell>85.14</cell><cell>24.4</cell><cell>49</cell><cell>47.94</cell><cell>77.38</cell><cell>16.1</cell><cell>35.1</cell></row><row><cell>Aligner</cell><cell>2.84M</cell><cell>Random</cell><cell></cell><cell>58.9</cell><cell>84.6</cell><cell>76.1</cell><cell>93.3</cell><cell>42.74</cell><cell>74.37</cell><cell cols="2">59.84 86.46</cell></row><row><cell></cell><cell>2.84M</cell><cell>TOnICS</cell><cell></cell><cell>59.7</cell><cell>85.24</cell><cell>76.6</cell><cell>94.1</cell><cell>48.26</cell><cell>77.87</cell><cell cols="2">65.44 89.36</cell></row><row><cell cols="5">to explicitly align with the visual modality. Each</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">noun in the training data is initially mapped to the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">object class with maximum noun-object PMI, cal-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">culated over training pairs with object detections,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">and then adjusted by hand to correct erroneous</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">mappings. Object classes containing fewer than</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">5000 instances in the training dataset are filtered</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">out. This finally results in a set of 406 nouns, each</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">noun corresponding to one of the 244 object cate-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">gories ?. For every image-text pair in the original</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">training dataset, we create one triplet for each noun</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">in our set of 406 nouns that the text contains.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Our final training data consists of 5.8M triplet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">instances corresponding to 2.84M image-text pairs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">(2.26M from Conceptual Captions, 580K from MS-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">COCO) from 2.4M unique images. The ontology</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">for TOnICS is constructed by creating an object</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">node for each of the 244 object categories, which</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">are children of the root entity node.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Checkpoint provided at https://huggingface. co/openai</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Flickr30K evaluation is zero-shot for both CLIP and our BERT-VinVL Aligner model since neither model's training data contains images from the Flickr30K train set. We see that even with the Random minibatch sampling and only the image retrieval loss, L IR , our BERT-VinVL Aligner achieves approximately the same image retrieval performance as CLIP. When the Aligner is trained with our TOnICS curriculum learning algorithm, we get a 1.5% improvement on R@1 over CLIP.</p><p>However, this model fails to do well at the text retrieval task. Adding the text retrieval loss L T R leads to substantial improvements in downstream text retrieval, with the Random baseline performing only 3% worse than CLIP. We further see that training with TOnICS leads to only slight improvements in Flickr30K text retrieval. Adding the text retrieval loss slightly hurts image retrieval performance, but still does better than CLIP by 1%.</p><p>Since our model, unlike CLIP, includes MS-COCO training images in the training data, it significantly outperforms CLIP on the MS-COCO retrieval evaluation. Hence, we compare our TOnICS algorithm to the Random baseline on the MS-COCO evaluation. We see that TOnICS leads to significant improvements in image retrieval (&gt; 5%), both when the text contrastive loss is and isn't used. We once again see that the text retrieval performance is very poor without the text retrieval objective during training, but improves significantly with it. TOnICS results in a 5% improvement over the Random baseline in text retrieval as well.</p><p>Minibatch sampling with TOnICS results in large gains in in-distribution retrieval evaluation (MS-COCO) as well as small improvements in zero-shot retrieval (Flickr30K). Training BERT-VinVL with TOnICS yields better zero-shot image retrieval performance than CLIP, even with substantially less training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this work, we align individually pre-trained language and vision encoders-BERT and VinVL, respectively-using a novel curriculum learning algorithm called TOnICS. Our aligned model is able to achieve better downstream zero-shot image retrieval performance than CLIP, in spite of being trained with less than 1% as many image-text training pairs. We further show that our TOnICS algorithm leads to gains in both in-domain and zero-shot retrieval tasks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft COCO Captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contrastive learning for weakly supervised phrase grounding</title>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flickr30k Entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Bryan A Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VinVL: Revisiting visual representations in vision-language models</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
