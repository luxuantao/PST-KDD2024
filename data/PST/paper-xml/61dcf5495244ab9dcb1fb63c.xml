<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncovering the Source of Machine Bias</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-09">9 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiyang</forename><surname>Hu</surname></persName>
							<email>xiyanghu@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
							<email>yanhuang@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Beibei</forename><surname>Li</surname></persName>
							<email>beibei.li@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tian</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Uncovering the Source of Machine Bias</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-09">9 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.03092v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Algorithmic Bias</term>
					<term>Fairness</term>
					<term>Transparency</term>
					<term>Discrimination</term>
					<term>Structural Modeling</term>
					<term>Dynamic Behavior</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a structural econometric model to capture the decision dynamics of human evaluators on an online micro-lending platform, and estimate the model parameters using a real-world dataset. We find two types of biases in gender, i.e. preference-based bias and belief-based bias, are present in human evaluators' decisions. Both types of biases are in favor of female applicants. Through counterfactual simulations, we quantify the effect of gender bias on loan granting outcomes and the welfare of the company and the borrowers. Our results imply that both the existence of the preference-based bias and that of the belief-based bias reduce the company's profits. When the preference-based bias is removed, the company earns more profits. When the belief-based bias is removed, the company's profits also increase. Both increases result from raising the approval probability for borrowers, especially male borrowers, who eventually pay back loans. For borrowers, the elimination of either bias decreases the gender gap of the true positive rates in the credit risk evaluation. We also train machine learning algorithms on both the real-world data and the data from the counterfactual simulations. We compare the decisions made by those algorithms to see how evaluators' biases are inherited by the algorithms and reflected in machine-based decisions. We find that machine learning algorithms can mitigate both the preferencebased bias and the belief-based bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Computing methodologies â†’ Machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Humans and organizations often need to make decisions under imperfect information, and they generally rely on certain statistics that quantify the likelihood of different outcomes <ref type="bibr" target="#b15">[16]</ref>. However, in practice these statistics generally cannot perfectly predict the outcome of the event, and often involve human inputs, which may contain bias against certain demographics (often referred to as protected groups) such as minorities or females. As a result, members in these demographic groups are often treated unfairly, which leads to various social and economic problems.</p><p>A typical context of decision making under imperfect information is the loan approval decision in micro-lending. The emerging micro-lending business provides faster and convenient access to financial resources to more people with a streamlined loan application process <ref type="bibr" target="#b46">[47]</ref>. The application process does not demand visits to a physical institution, and the credit assessment process involves human decisions <ref type="bibr" target="#b44">[45]</ref>. That is, human evaluators (i.e. platform staffs or lenders) make credit risk assessment with individual information (e.g., demographics) which is collected from loan applicants. Based on the evaluated credit risk, evaluators make the final loan approval decisions. However, this practice may contain bias due to limited human cognitive capacity to process complex computation for credit risk evaluation <ref type="bibr" target="#b35">[36]</ref>. Theoretically, bias in credit risk evaluation would heavily attenuate the evaluation accuracy, profitability of micro-lending platform or lenders, and cause social unfairness <ref type="bibr" target="#b26">[27]</ref>. Unfortunately, the influence of human bias is largely neglected in literature. Since human evaluations are inevitable in many contexts such as micro-lending, the first goal of this study is to investigate: Does bias exist in human evaluators' loan evaluation; if so, how does this bias affect their decisions and the market outcome?</p><p>Particularly, over the past decades, economists and social scientists have proposed different models to explain and quantify human bias <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48]</ref>. They classify human bias into two broad categories: preference-based bias (or tasted-based bias) and belief-based bias <ref type="bibr" target="#b7">[8]</ref>. Preference-based bias arises when evaluators have animus towards a particular group, while belief-based bias arises when evaluators' subjective beliefs about a group lead them to less favorably treat individuals from the group than members from other groups with the same observed performance. The classification of the two types of human bias is critical. It allows us to dynamically trace the evolution of bias during long-term (i.e. repeated) decisions rather than to regard bias as static. This static assumption of bias may overshadow the potentials and value of learning behaviors in decision making <ref type="bibr" target="#b57">[58]</ref>. It also enables us to unravel the influence of different types of human bias on decision outcomes.</p><p>Therefore, following such a classification of the source of the decision bias, in this study, we disentangle and quantify these two types of human bias in observable data from a micro-lending platform. In specific, we develop a structural econometric model of human evaluators' loan approval decisions, which captures the underlying economic processes that drive human evaluators' decisions. We then estimate the structural model based on the real-world data from a micro-lending platform that involves sufficient samples of repeated loan applications and approval decisions by the platform staff (i.e., human evaluators). Some of the parameters in the model capture the two possible types of human biases, and therefore, their parameter estimates can reveal whether human evaluators' decisions exhibit those two types of bias. We compare a number of alternative model specifications and identify the one that best explain the observed human decisions in the data. We also conduct a survey of human evaluators to confirm our chosen model specification. Using the estimated structural model, we conduct policy simulations to quantify the effects of these human bias and their welfare implications.</p><p>Specifically, to measure the extent of bias, we examine the gender gaps for the approval true positive rate (TPR). We adopt the concept of equal opportunity, one of the most popular fairness notions. Equal opportunity suggests that qualified individuals, no matter what their sensitive attributes are, have equal opportunity to receive favorable outcomes <ref type="bibr" target="#b32">[33]</ref>. In our loan application setting, this means two gender group should have the same true positive rates if no gender bias exists.</p><p>Furthermore, with the recent boom of AI technologies, many organizations have increasingly adopted machine learning algorithms to replace human in decision making for better efficiency and decision quality. <ref type="foot" target="#foot_0">1</ref> The original thought was algorithms are "objective" and therefore have the potential to eliminate the discrimination against members in certain disadvantaged groups. However, numerous studies have cautioned that machine learning algorithms may also present non-negligible bias against those groups.</p><p>One potential source of algorithm bias lies in some objective and inherent characteristics of data themselves. This type of cause has been well studied <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref>. For example, the correlations between the same features and the outcome may be different for different groups, and/or the data records of the disadvantaged group may be inferior in both quantity and quality than the regular group (the group that are not discriminated against historically). Another cause of algorithmic bias is the biased or flawed training data generated by humans. That is, human in the first place generates bias in the training data, which then translates into machine-learning assisted decision making <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>Therefore, in the second part of this paper, we intend to address the following question: To what extent a machine learning algorithm trained on historical data inherit human bias? To this end, we train various machines based on (a) data from human evaluators' decisions (the approved loans observed in the data) and (b) data from our counterfactual simulations with human biases removed. In the counterfactual simulation, we take advantage of an experiment the platform conducted: during a period of time, all applicants are approved without any selection. This novel experiment allows us to observe the credit behaviors of all the applicants on the micro-lending platform which are usually unobservable.This allows us to infer the outcomes of the loans that are not approved by human evaluators, which usually go unobserved and are impossible to evaluate. By leveraging these "de-biased" data sets, we are able to compare machines trained based on data with and without human bias and demonstrate how different types of human bias, as well as other data characteristics, affect decisions made by machines.</p><p>We find that the evaluators exhibit both types of biases discussed above, with both preference-based bias and belief-based bias in favor of females. This is due to females generally behave better than males in repaying loans, and study has shown that females are more risk averse and ethically sensitive <ref type="bibr" target="#b18">[19]</ref>. Although preference-based bias persists, belief-based bias gradually reduces as human evaluators learn from the repayment behaviors of each specific borrower. Our policy simulations suggest that the elimination of either the preference-based bias or the belief-based bias from human evaluators' decisions can increase the platform's profits. The mechanisms behind the two scenarios are the same. The extra profits result from lowering the approval probability of defaulters, especially female defaulters. On the borrower end, the elimination of the two types of bias can mitigate the gender gap in the credit evaluation measured by the true positive rate . And when both types of bias are removed at the same time, the gender gap is the minimized. When we further feed the real-world data and the counterfactual data into machine learning models, we find that eliminating upstream human decision biases help combat algorithmic biases.</p><p>The theoretical contribution of this study is multi-fold. First, while a great body of machine bias literature has focused on the bias generated in algorithm process (Suresh and Guttag 2020), our study is among the first to investigate the influence of human bias on AI-assisted decisions. Second, we introduce two types of human biases into a real-world decision-making context, and we are among the first to empirically identify and quantify these two human biases with a large-scale dataset and a structural econometric model. Furthermore, we also add to the micro-lending and FinTech literature by revealing how human bias could influence platforms' profitability and service equality. We also characterize how human evaluators make credit risk evaluations and loan approval decisions in practice. Methodologically, we design a novel and comprehensive empirical framework to uncover the behavioral sources of bias, which are usually implicit and difficult to identify accurately. The framework works on secondary datasets, and enables to conduct counterfactual simulations, and AI-based analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELEVANT LITERATURE</head><p>Our work is closely related to the literature on the bias and discrimination in human decision making, especially bias in financial evaluations. Two categories of human decision biases have been extensively studied; one is ethnic or racial discrimination, another is gender discrimination. It has been documented that in financial loan market, both non-Fintech and Fintech lenders tend to discriminate against ethnic-minority borrowers (higher interest rates and lower probability of being funded) through liability document and facial bias <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b54">55]</ref>. It has also been that women are more credit constrained than men by microfinance institutions <ref type="bibr" target="#b6">[7]</ref>. In P2P lending, female borrowers need pay higher interest rates <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. And female founders are less successful attracting male investors compared to observably similar male founders <ref type="bibr" target="#b24">[25]</ref>.</p><p>In addition to race bias and gender bias, other kinds of bias or discrimination common to see in society include immigration and age related bias <ref type="bibr" target="#b21">[22]</ref>, occupational related bias <ref type="bibr" target="#b17">[18]</ref>, home bias <ref type="bibr" target="#b42">[43]</ref>, etc. The major reasons behind these human decision bias lie in the minority applicants' relative quality compared to the majority <ref type="bibr" target="#b25">[26]</ref>, the decision makers' improper task objectives and incentives <ref type="bibr" target="#b21">[22]</ref>, and the inherent bias formation and evolution process of preference-based and belief-based evaluation biases <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Economic theories classify inherent human bias into two types: preference-based bias and belief-based bias <ref type="bibr" target="#b7">[8]</ref>. Preference-based bias arise when evaluators have animus towards a particular group, while belief-based bias arise when evaluators' subjective beliefs about a group lead them to less favorably treat individuals from the group than members from the regular group with the same observed performance. Belief-based bias can be further classified into two subcategories: belief-based bias with misspecified/incorrect beliefs and belief-based bias with correct beliefs (sometimes referred to as statistical bias). The former occurs when the evaluators' subjective beliefs about the group-level statistics of the protected group are not the same as the reality, and the latter occurs when the subjective beliefs match the reality. In a static setting, preference and beliefbased biases mix up with each other, and it is hard to disentangle their effects on human decisions. However, in a dynamic setting, we are able to distinguish between the two biases. This is because across periods, preference-based bias persists, while belief-based bias can be mitigated or even reversed when the evaluator observes new signals about each individual they are evaluating. In this paper, we follow the definitions of the two types of bias. Our context of multi-period microloan borrowing and lending provides a great setting to identify these two types of bias.</p><p>Our paper also builds on the growing literature on algorithmic discrimination and machine learning bias. One important source of machine bias is the human decision bias encoded in the training dataset. <ref type="bibr" target="#b28">[29]</ref> incorporate the prediction of machine learning models into a simple equilibrium model of finance credit, and find that algorithms increase rate disparity among and within different racial groups. <ref type="bibr" target="#b53">[54]</ref> conduct simulations to evaluate the race and age disparities in finance risk assessment, and demonstrate that human and machine interaction can lead to bias in both race and age. <ref type="bibr" target="#b41">[42]</ref> find that economic forces in the market can distort neutral algorithms into discriminating females in terms of their exposure to advertisements of STEM (science, technology, engineering and mathematics) jobs. Major reasons for algorithmic bias include lack of necessary data control (statistical bias) and unintended correlation with sensitive factors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>, training-sample bias <ref type="bibr" target="#b16">[17]</ref>, market mechanism <ref type="bibr" target="#b41">[42]</ref>, etc. Even though algorithms could lead to various bias issues, implementing appropriate designs and, regulations can make them positive forces for equity <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>To deal with bias problems in human and machine decision making. researchers have come up with diverse methods. The most direct way is to obtain and include more useful data. <ref type="bibr" target="#b39">[40]</ref> show direct inclusion of a protected variable (e.g., race) is useful for mitigating unfairness. <ref type="bibr" target="#b43">[44]</ref> find that using proper alternative data could improve both financial profitability and equality. Aside from enriching the data, learning from repeated events can also mitigate bias. <ref type="bibr" target="#b8">[9]</ref> conclude that based on signaling theory, evaluators/investors will leverage information from repeated borrowing of the same borrow in her lending history. <ref type="bibr" target="#b38">[39]</ref> argues that borrowers' past track record within the platform have the most important impact (than other demographic factors) on predicting the repayment performance of their current loans.</p><p>Another flourishing track to combat decision bias is to invent more transparent, delicate and de-biasing algorithms. It has been widely shown that enhanced algorithmic transparency and interpretability can help eliminate bias, and sometimes simple and transparent models are able to outperform complex black-box models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref>. <ref type="bibr" target="#b56">[57]</ref> demonstrate that Bayesian investment model can significantly improve investors' investment decisions based on other investment models. <ref type="bibr" target="#b13">[14]</ref> argue that human capital and machine learning can complement each other through combining algorithms and domain expertise or knowledge. Many other de-biasing methods draw from the perspective of statistics, optimizations, and behaviors, etc <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>In terms of decision quality, human predictions often tend to be less accurate, which can negatively affect the quality of their decisions. This is because on the one hand, people may have resourcelimited brain to process complex computation in evaluation <ref type="bibr" target="#b35">[36]</ref>; on the other hand, people may use a simple updating rule, which, for example, linearly combines their personal experience and accumulated knowledge for repeated tasks <ref type="bibr" target="#b36">[37]</ref>.</p><p>Compared with human decision making, algorithm-based decision making has demonstrated superior ability to achieve better accuracy and handle more complex information. Human v.s. machine decision making has been widely studied in healthcare area. In most cases, machine learning models outperformed or tied the judgment accuracy of an average clinician <ref type="bibr" target="#b9">[10]</ref>, and only a small fraction of clinicians were more accurate than machine learning models <ref type="bibr">[31, et al.]</ref>. Mechanical-prediction techniques were about 10% more accurate than clinical predictions <ref type="bibr" target="#b31">[32]</ref>. Very simple actuarial method (i.e., linear combination of criterion variables) has been shown to consistently perform than clinical judgment <ref type="bibr" target="#b19">[20]</ref>.</p><p>However, there are also scenarios in which human experts can outperform machines. Some examples are tasks that heavily require theory-driven judgement that are not suitable for statistical models; rare events or outliers that have never been seen by algorithms; complex configural relationships between the features and the dependent variable <ref type="bibr" target="#b20">[21]</ref>. In our context, micro-loan approval decisions do not face these problems that make humans better than machines. And in finance industry, algorithms are widely used to identify credit risks, with XGBoost <ref type="bibr" target="#b11">[12]</ref> as the most popular one. Using XGBoost as our focus machine learning model, we examine its bias inherited from human decisions.</p><p>Methodologically, our paper also builds upon the abundant work on modeling human decision dynamics through structural models. <ref type="bibr" target="#b23">[24]</ref> use Bayesian learning framework to model consumers' brand choices under quality signals from advertisement, price and past consumption experiences. <ref type="bibr" target="#b34">[35]</ref> investigate the dynamic of users' idea posting behavior on a crowd sourcing platform. <ref type="bibr" target="#b58">[59]</ref> study participants' learning behavior from superstars in crowdsourcing contests. <ref type="bibr" target="#b59">[60]</ref> examine taxi drivers' learning behavior based on fine-grained GPS observations. In this paper, we model the learning dynamics in loan application evaluators' decisions, and disentangle and estimate preference-based bias and belief-based bias in their behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESEARCH CONTEXT AND DATA 3.1 Context</head><p>We obtained our data from a leading Asian micro-lending platform. The platform was founded in 2011 and offers microloans at an average size of approximately $450 USD. Loan applicants on the platform use the loans primarily to fulfill temporary financial needs including supplementary working capital for small businesses, irregular shopping needs, education spending, and medical expenses. To apply for a loan, applicants must provide their personal information such as name, gender, age, income level, and a copy of their national identity cards. They must also provide their contact persons. People under the age of 18 and all students at school or university are not qualified to apply as they usually have no independent income sources. The loan term ranges from one to eight months. The annual interest rate charged by the platform is approximately 18% (plus or minus 1%, depending on the credit line of the borrowers).</p><p>During our sample period, the platform evaluates applicants' credit risk manually by its employees (i.e., evaluators). All the evaluators are trained regularly to maintain consistent evaluation criteria, which are derived from their collective daily work experience. Besides, no gender bias training has been conducted by the platform. They do not use any AI technologies (e.g., machine learning) as automatic or auxiliary tools for evaluation. In specific, after an applicant fills in her personal information and submits the application in the system, he will be randomly assigned to an evaluator. Then, the evaluator decides whether to issue the loan by assessing whether the applicant can bring positive economic benefit (i.e., profit) to the platform. Loan profit is roughly calculated based on the predicted probabilities of delinquency and default. If a borrower fails to repay an installment, he will be regarded as delinquent. If a loan is unpaid 90 days or more after the due date, default is confirmed by the platform. <ref type="foot" target="#foot_1">2</ref>The estimations of the chances of delinquency and default are based on the collected personal information for all new applicants. For repeated applicants, evaluators will additionally leverage information from the applicants' repayment performance on previous loans, such as the final overdue days (denoted as ğ·), the proportion of overdue installments (denoted as ğ‘€), the proportion of installments with positive attitude from the borrower (denoted as ğ´), which is measured from the records of whether a borrower has presentedshown a positive attitude fortowards their financial obligation during his or her communication with the platform, and the proportion of installments with financial help from family or friends (denoted as ğ» ).</p><p>When a borrower becomes delinquent, the platform will impose financial penalty on them. Simultaneously, debt collection methods such as sending reminder notifications to them and their contact persons will be implemented. Borrowers in default are prohibited from applying for loans again on the focal platform. Default records are also submitted to the personal credit record system maintained by the central government and a shared blacklist system maintained by a symposium of micro-finance institutions. The platform may take legal actions against defaulters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data and Description</head><p>Our data set contains fine-grained information of both the applicants whose submitted applications were approved and those whose applications were rejected by the platform between January 2015 and September 2017 (i.e., <ref type="bibr" target="#b32">33</ref>  For all the applicants, we obtain their demographic and socioeconomic data including gender, education level, monthly income level, disposable personal income per capita (DPI) of their home city, and house ownership, and their loan information including loan amount, loan term, and annual interest rate. We likewise have detailed per-installment repayment information of the approved loans. Table <ref type="table" target="#tab_2">1</ref> summaries information of the approved and rejected applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model-Free Analyses</head><p>On the platform, female applicants are generally more likely to be approved. The gaps between female and male approval rates do not shrink over time(Figure <ref type="figure" target="#fig_1">2a</ref>), implying that the platform evaluators may have a persistent impression of credit risks between males and females.</p><p>When we consider only borrowers who have applied repeatedly (Figure <ref type="figure" target="#fig_2">3</ref>), the approval gender gap shrinks, suggesting that learning helps amend the platform evaluators' prior bias in gender. Figure <ref type="figure" target="#fig_3">4</ref> shows the trend of the default rate as borrowers as the number of applications or the number of their previously approved loans increases. Consistently, the gender gap in the default rate is smaller for repeated borrowers; and it keeps decreasing as the borrowers' number of previous applications increases, and as number of previous approvals increases. This is consistent with the decreasing gender gap in the approval rate, which indicates that evaluators can learn effectively from users' previous repayment behaviors and adjust their prior belief-based bias in gender.</p><p>As noted earlier, the evaluator relies on four signals from the users' past repayment behaviors to make approval decisions on loan applications. Figure <ref type="figure" target="#fig_4">5</ref> shows the values of the four signals against the number of previous applications by genders. Generally, we find that, given the number of previous applications, all the four signals show similar values between females and males, indicating    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL</head><p>We consider a model in which the loan platform decides whether or not to approve a loan application by applicant ğ‘– at time ğ‘¡ (here ğ‘¡ indicates the ğ‘¡-th application rather than a natural time unit). We model the evaluator behavior in a dynamic environment where the evaluator is uncertain about the true credit quality of applicants.</p><p>When a new borrower comes to the micro-lending platform, the evaluator only observes her demographics and form a prior belief based on the demographic data. For every borrower, without any previous repayment behavior being observed, her first application is preprocessed by the evaluator using the prior belief. If a borrower's loan application is approved at ğ‘¡, then at the time of her next loan application, i.e., ğ‘¡ + 1, the evaluator will use the previous repayment behaviors as additional signals of the borrower's credit quality. At time ğ‘¡ = 0, without any observation of borrower ğ‘–'s repayment behaviors, given ğ‘–'s demographics X ğ‘– , the evaluator forms a belief of her credit quality with mean ğ›½X ğ‘– and variance ğœ 2 ğ‘„ 0 . X ğ‘– includes gender, age, education level, marriage status, number of children, house ownership, monthly income, the disposable personal income (DPI) of borrowers' living cities (in 2017). We also incorporate a constant 1 and the time of ğ‘–'s first application into X ğ‘– , because the overall borrowers' qualities may keep changing over time. We assume that the prior belief follows a normal distribution:</p><formula xml:id="formula_0">ğ‘„ ğ‘–0 âˆ¼ ğ‘ (ğœ·X ğ‘– , ğœ 2 ğ‘„ 0 ) for ğ‘– = 1, ..., ğ‘ .<label>(1)</label></formula><p>where ğœ·X ğ‘– is the evaluator's subject prior belief about the mean credit quality of a borrower with demographics X ğ‘– . The coefficient for gender is ğ›½ ğ‘” , where gender ğ‘” âˆˆ {ğ‘€, ğ¹ }, ğ‘€ stands for males and ğ¹ stands for females. We normalize ğ›½ ğ¹ to be zero. Therefore ğ›½ ğ‘€ captures the belief-based bias. For every borrower, we assume that the evaluator believes that their credit qualities are of identical uncertainty, i.e., ğœ 2 ğ‘„ 0 is identical for all ğ‘–. When the evaluator processes the loan application of borrower ğ‘– at time ğ‘¡, she has an information set ğ¼ ğ‘–ğ‘¡ , which contains all loan repayment histories up to the loan of time ğ‘¡ âˆ’ 1. Given the information set, the evaluator form an posterior belief of ğ‘„ ğ‘– . Let ğ‘„ ğ‘–ğ‘¡ be the expectation of the evaluator's belief for ğ‘–'s credit quality at time ğ‘¡, i.e.</p><formula xml:id="formula_1">ğ‘„ ğ‘–ğ‘¡ = E[ğ‘„ ğ‘– |ğ¼ ğ‘–ğ‘¡ ]<label>(2)</label></formula><p>At time t, when the evaluator is reviewing the loan application from borrower ğ‘–, she observes four signals from ğ‘–'s repayment behavior on previous loans. Consider when an applicant ğ‘– applies for a loan at time ğ‘¡, the evaluator observes four signals of the repayment behavior of her last loan, the final overdue days ğ· ğ‘–ğ‘¡ , the proportion of overdue installments ğ‘€ ğ‘–ğ‘¡ , the proportion of installments with positive attitude from the borrower ğ´ ğ‘–ğ‘¡ , and the proportion of installments with financial help from family or friends ğ» ğ‘–ğ‘¡ . Some of these signals may be more informative than others. Therefore, we tried several different combinations of these signals, and drop ğ‘€ ğ‘–ğ‘¡ from our signal list in our final model as ğ‘€ ğ‘–ğ‘¡ turns out to be uninformative and does not affect evaluators' loan approval decisions significantly..</p><p>In previous literature <ref type="bibr">[24, 35, 60, et al.]</ref>, it is common to model such behaviors using Bayesian updates of a normal-normal conjugate prior-posterior. We also start with such a conventional normalnormal conjugate Bayesian model, i.e., each time when additional signals become available, the evaluator updates her belief of a borrower's credit quality in a Bayesian fashion. However, the estimated parameters of such a Bayesian learning model show that all variances of these signals are extremely small. This indicates that evaluators weight the most recent signals heavily and are not updating their beliefs in a Bayesian fashion. Therefore, we use a simplified model to capture the evaluators' updating behaviors, where the posterior belief is a weighed sum of the prior belief and the new signals. We tried different combinations of the four signals.</p><p>The weighted sum model with signals ğ· ğ‘–ğ‘¡ , ğ´ ğ‘–ğ‘¡ , and ğ» ğ‘–ğ‘¡ performs the best and achieves the highest likelihood. We use this mode as our main model. In this section, we introduce this model in detail; and in the appendix (Section ??), we summarize the detailed of the Bayesian one.</p><p>Because the final overdue days ğ· ğ‘–ğ‘¡ has a long-tail distribution, we take the logarithm of ğ· ğ‘–ğ‘¡ and use log ğ· ğ‘–ğ‘¡ for subsequent calculations. We assume the evaluators believe that all these signals are linearly related to the credit quality in the following way:</p><formula xml:id="formula_2">log ğ· ğ‘€ ğ‘– = ğ· 0 + ğœ™ğ‘„ ğ‘– , ğ´ ğ‘€ ğ‘– = ğ´ 0 + ğœ“ğ‘„ ğ‘– , ğ» ğ‘€ ğ‘– = ğ» 0 + ğœŒğ‘„ ğ‘– ,<label>(3)</label></formula><p>where ğœ™, ğœ“ , ğœŒ are parameters of slopes, ğ· 0 , ğ´ 0 , and ğ» 0 are corresponding intercepts, and log ğ· ğ‘€ ğ‘– , ğ´ ğ‘€ ğ‘– , and ğ» ğ‘€ ğ‘– are the means of ğ· ğ‘–ğ‘¡ , ğ´ ğ‘–ğ‘¡ , ğ» ğ‘–ğ‘¡ . We assume each borrower's signals are distributed surrounding their means. At each time ğ‘¡ when the evaluator observes these repayment behaviors, she updates her belief about the credit quality based on a weighted sum of the prior and the signals:</p><formula xml:id="formula_3">ğ‘„ ğ‘–ğ‘¡ =(1 âˆ’ ğ›¼ ğ· âˆ’ ğ›¼ ğ´ âˆ’ ğ›¼ ğ» )ğ‘„ ğ‘–,ğ‘¡ âˆ’1 + ğ›¼ ğ· log ğ· ğ‘–ğ‘¡ âˆ’ ğ· 0 ğœ™ + ğ›¼ ğ´ ğ´ ğ‘–ğ‘¡ âˆ’ ğ´ 0 ğœ“ + ğ›¼ ğ» ğ» ğ‘–ğ‘¡ âˆ’ ğ» 0 ğœŒ<label>(4)</label></formula><p>where ğ›¼ ğ· , ğ›¼ ğ´ , ğ‘ğ‘›ğ‘‘ğ›¼ ğ» are the wights assigned to the three signals.</p><p>At time ğ‘¡, the evaluator decides whether to approve ğ‘–'s loan application based on her updated belief of the applicant's quality ğ‘„ ğ‘–ğ‘¡ . The evaluator first calculates the probability of non-default through a sigmoid function:</p><formula xml:id="formula_4">ğ‘ ğ‘–ğ‘¡ = â„(ğ‘„ ğ‘–ğ‘¡ ) = 1 1 + exp(âˆ’ğ‘„ ğ‘–ğ‘¡ ) .<label>(5)</label></formula><p>Then, with the probability of non-default ğ‘ ğ‘–ğ‘¡ , the evaluator decides to approve or reject ğ‘–'s loan application of time ğ‘¡ through a utility function. There are two key components in the utility function (Equation <ref type="formula" target="#formula_5">6</ref>). The first component captures the expected profit of approving this loan. The second component contains the evaluator's preference based bias. And we also assume there is a random shock within the utility function. The utility function is as follows: </p><formula xml:id="formula_5">ğ‘¢ ğ‘–ğ‘¡ = ğ‘§ * (ğ‘ ğ‘–ğ‘¡ ğ‘ ğ‘–ğ‘¡ âˆ’ (1 âˆ’ ğ‘ ğ‘–ğ‘¡ )ğ‘ ğ‘–ğ‘¡ ) âˆ’ ğ‘ ğ‘–ğ‘” + ğœ– ğ‘–ğ‘¡ ,<label>(6)</label></formula><p>where ğ‘” âˆˆ {ğ‘€, ğ¹ }, and ğ‘€ stands for males and ğ¹ stands for females, ğ‘ ğ‘–ğ‘” is the preference-based bias with ğ‘ ğ‘–ğ¹ normalized to be zero. Therefore, ğ‘ ğ‘–ğ‘€ captures the preference-based bias, which persists and is not affected by observing new signals. ğ‘ ğ‘–ğ‘¡ is the profit earned by the platform if the loan is paid back, and ğ‘ ğ‘–ğ‘¡ is the loss the platform incurs if the loan defaults. Both ğ‘ ğ‘–ğ‘¡ and ğ‘ ğ‘–ğ‘¡ are observed values in our dataset. ğ‘§ is the price parameter (or marginal utility of money). ğ‘ ğ‘–ğ‘¡ is the non-default probability of ğ‘– at time ğ‘¡, which is related to its quality ğ‘„ ğ‘–ğ‘¡ . All these parameters are estimated through maximizing the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Estimation Results</head><p>We report parameter estimates for our model in Table <ref type="table" target="#tab_3">2</ref>. Both the preference-based bias ğ‘ ğ‘–ğ‘€ (we normalize ğ‘ ğ‘–ğ¹ to be zero) and the belief-based bias ğ›½ ğ‘€ have expected signs (ğ‘ ğ‘–ğ‘€ = 0.2309, ğ›½ ğ‘€ = âˆ’0.1042), implying that the evaluator has a preference for female loan applicants. The estimate of the belief based bias ğ›½ ğ‘€ is significantly negative, suggesting that evaluators have a lower prior belief for males' credit qualities. The estimate of ğ‘ ğ‘–ğ‘€ is significantly positive. This implies that there is a significant preference-based bias in gender that favors female applicants, which cannot be corrected by observing repayment behaviors. Apart from ğ›½ ğ¹ , all other ğ›½s also have expected signs. This is consistent with the evaluator's preference for applicants with a better socioeconomic status as we observe in the data.</p><p>Our estimates of the slopes in the signal D-quality, signal Aquality and signal H-quality relationships are negative (ğœ™ = âˆ’0.0138), positive (ğœ“ = 0.3492) and positive (ğœŒ = 0.9803) respectively. These results suggest that in the evaluator's decision-making process, larger final overdue days ğ· ğ‘–ğ‘¡ are associated with poor credit quality; while the proportion of installments with a positive attitude ğ´ ğ‘–ğ‘¡ is positively associated with loan approvals. Getting financial help from family and friends ğ» ğ‘–ğ‘¡ is also viewed by evaluators as a positive signal for credit quality and is associated with a lower default probability. As can be seen in Equation <ref type="formula" target="#formula_3">4</ref>, the evaluator updates her belief based on a weighted sum of the prior belief and the signals. The estimates of the weights ğ›¼ ğ· = 0.0097, ğ›¼ ğ´ = 0.9780 and ğ›¼ ğ» = 0.0121 indicate that the evaluator gives most weight to the signal ğ´ ğ‘–ğ‘¡ , with a wight as high as 0.9780.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paramteters</head><p>In Table <ref type="table" target="#tab_4">3</ref>, we compare the characteristics of the actual observed approved users and the expected values of the characteristics of the approved users our structural model predicts. All these statistics are very similar between the actual observations and our model's </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">POLICY SIMULATIONS</head><p>We conduct several sets of counterfactual simulations to evaluate the effects of eliminating the biases found in the data on the outcome of loan applications across different gender groups. Our counterfactual analyses are done on a second dataset. It covers all the loan records from a one-month experimental period ("full sample" hereafter). During this period, all applicants are approved without screening. As a result, we have true label of all users. This ensures our results are based on the entire user distribution, rather than just the approved users, which have a different distribution from the whole user pool.</p><p>Specifically, we calculate the expected profits of the platform based on the predicted approval probability by a number of variants of the estimated structural model. We also examine the gender gap in the approval true positive rate (TPR). We adopt the concept of equal opportunity, one of the most popular fairness notions, to measure the extent of bias. Equal opportunity requires that qualified individuals, no matter what their sensitive attributes are, have an equal opportunity to receive favorable outcomes. In our loan application setting, this means two gender group should have the same true positive rates. We find that the elimination of either the preference-based bias or the belief-based bias can simultaneously increase the platform's profit and reduce the gender gap in loan approval decisions (TPR). In Section 6, We also feed the counterfactual datasets into machine learning algorithms to see how machine captures these different behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Does eliminating preference-based bias</head><p>help with the decision making? How does it affect the platform's payoff?</p><p>As noted above, the preference-based bias refers to people's animus towards a particular group. No matter in reality how well this certain group behaves, people with preference-based bias always make judgements and decisions by prejudice. In our setting, we focus on the preference-based bias in gender. In our model, ğ‘ ğ‘–ğ‘” captures the preference-based bias. For applicant ğ‘– of gender ğ‘” âˆˆ {ğ‘€, ğ¹ } (ğ‘€ for males and ğ¹ for females), we normalize ğ‘ ğ‘–ğ¹ for females to be zero. And the estimation result for preference-based bias is ğ‘ ğ‘–ğ‘€ = 0.2390. Note that in Equation <ref type="formula" target="#formula_5">6</ref>, we have the profit minus sign in front of ğ‘ ğ‘–ğ‘” , therefore ğ‘ ğ‘–ğ‘€ = 0.2390 suggests the evaluator has a preference for female borrowers and a prejudice against male borrowers. In an ideal setting, all the evaluators are trained and the prejudice to gender is completely removed. The elimination of the preference-based bias can be operationalized by setting ğ‘ ğ‘–ğ‘€ to be zero. We simulate the evaluators' decisions with ğ‘ ğ‘–ğ‘€ = 0 but all other parameters unchanged. We then compare the true positive rate (TPR) and the payoff under the original decision process and the one with preference-based bias removed on our full sample.</p><p>When the evaluators make approval decisions without the preferencebased bias in gender, the TPR for male users are all higher than their corresponding value from the original decision process.   that for female users, ğ‘ ğ‘–ğ¹ = 0, thus their TPR stays the same under two different decision-making process. In sum, eliminating the preference-based bias can generally decrease the TPR gap between the two gender groups in our setting. But this decrease is relatively small (from 9.69% to 6.94%, Table <ref type="table" target="#tab_6">5</ref>).</p><p>We also compare the platform's expected welfare (profit) under the current decision-making process and the one with preferencebased bias removed. We observe that by eliminating the preferencebased bias (ğ‘ ğ‘–ğ‘€ ) in the loan approval process, the platform obtains a higher profit (Table <ref type="table" target="#tab_5">4</ref>). The increase in the profit results from better decisions made on male applicants. Specifically, the increase driven by the gain from lowering the approval probability for male borrowers who eventually default on loans, which exceed the loss of lowering the approval probability for nondefault male borrowers. These findings suggest that although the current decision-making process incorporates gender information to identify high quality users, the evaluators over underrate male users and disdain male borrowers too much. Therefore, the preference-based bias results in suboptimal decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Does removing gender from the prior belief</head><p>formation help with the decision making? How does it affect the platform's payoff?</p><p>In this subsection, we examine the effects of removing the gender information in the prior belief, i.e. set ğ›½ ğ‘€ to be zero. ğ›½ ğ‘” , ğ‘” âˆˆ {ğ‘€, ğ¹ } (ğ‘€ for males and ğ¹ for females) captures the belief-based bias.</p><p>Belief-based bias refers to evaluators' subjective beliefs about certain groups. This kind of belief can be updated through observing additional behavioral signals, i.e. repayment behaviors in this paper.</p><p>In our setting, we normalize female users' ğ›½ ğ¹ to be zero, therefore ğ›½ ğ‘€ captures the relative belief-based bias for females compared with males. The estimated value of ğ›½ ğ‘€ is âˆ’0.1042, which indicates the evaluators have a subjective prior belief in favor of female borrowers. We compare the TPR and the platform's payoff under the original decision process and the one with belief-based bias removed on our full sample. Under the current decision making process, we observe a higher TPR for females than males. Since we normalized ğ›½ ğ¹ to be zero, the TPR of females does not change between the two decision processes. Gender information in the prior decreases the male TPR. With access to the gender information, the evaluators form a prior belief that underrates female borrowers. These additional rejected males are generally of good enough credit qualities.   The gender gap in the credit evaluation TPR (female's TPR minus male's TPR). New users only.</p><formula xml:id="formula_6">ğ›½ ğ‘€ = âˆ’0.1042 ğ›½ ğ‘€ = 0 ğ‘ ğ‘–ğ‘€ = 0.</formula><p>ğ›½ ğ‘€ = âˆ’0.1042 ğ›½ ğ‘€ = 0 ğ‘ ğ‘–ğ‘€ = 0.2390 5.13% 4.10% ğ‘ ğ‘–ğ‘€ = 0 3.85% 2.70%</p><p>Table <ref type="table">7</ref>: The gender gap in the credit evaluation TPR (female's TPR minus male's TPR). Repeated users only.</p><p>We further investigate the effect of ğ›½ ğ‘€ on the expected profit of the platform. With the belief-based bias removed, the platform obtains a larger profit. This suggests that setting ğ›½ ğ‘€ to zero leads to an increase in the probability of approval for all males, and the gain from lowering the approval probability for default male borrowers dominates the loss from lowering the approval probability for nondefault male users.</p><p>On the borrower side, when the belief-based bias is removed, the gender gap in TPR becomes smaller (Table <ref type="table" target="#tab_6">5</ref>). It decreases from 9.69% to 5.14%. This decrease is larger than the one resulting from eliminating the preference-based bias. When the both biases are ruled out, we can achieve the smallest gender gap in the TPR (2.48%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Are these effects different for new users and repeated users?</head><p>To further investigate the underlying mechanism, we look into the new users and the repeated users separately. Table <ref type="table" target="#tab_7">6</ref> shows the TPR gaps in the original and counterfactual settings of the new users, and Table <ref type="table">7</ref> shows the TPR gaps of the repeated users. Consistent with previous observation, eliminating either bias can help mitigate the bias for both new users and repeated users. We observe that overall the repeated users have smaller bias than new users. This is because of the different characteristic distributions between original new and repeated applicants, and the signals used for learning repeated applicants' credit quality. When removing the preference-based bias but keeping the belief-based bias, human evaluators' decision bias toward gender decrease from 12.31% to 8.62% for new applicants and from 5.13% to 3.85% for repeated applicants. If we keep preference-based bias but remove belief-based bias, compared with the former case, human evaluators' decision bias toward gender has a larger decrease to 5.51% for new applicants, but a smaller decrease to 4.10%. This is because for repeated users, human make use of signals to update the prior belief. The belief-based bias becomes less influential for repeated users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MACHINE LEARNING BIAS INHERITED FROM HUMAN BIAS</head><p>In this section, we examine how ML algorithms inherit human bias, by feeding the real-word dataset and the counterfactual datasets into ML algorithms. Specially, for the counterfactual setting, we run our structural model with counterfactual parameters to simulate the approval decision; we use the approved users' loan records as the input for our machine learning model. For users who were not approved in reality, we do matching to fill their loan repayment behavior. Here we use XGBoost <ref type="bibr" target="#b11">[12]</ref> as our machine learning model. XGBoost is widely used in loan evaluations and related literature <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b43">44]</ref>. As in Section 5, we report the the expected profits (Table <ref type="table" target="#tab_8">8</ref>) and the gender gap of the TPR (Table <ref type="table" target="#tab_9">9</ref>). For the XGBoost model, when the preference-based bias is removed, the expected profit increases from 167260.2 to 172125.8 (increases by 4865.6, compared with the structural model's corresponding increase of 3,989.9), and the TPR gap decreases from 7.49% to 6.20% (decreases by 1.29%, compared with the structural model's decrease of 2.75%). This indicates that machine learning models can mitigate the effects of the preference-based bias. When the beliefbased bias is removed, the expected profit increases from 167260.2 to 171352.0 (by 4091.8, compared with the structural model's increase of 5911), and the TPR gap decreases from 7.49% to 7.43% (decreases by 0.06%, compared with the structural model's decrease of 4.55%). This indicates that machine learning models can mitigate the effects of the belief-based bias.  The gender gap in the credit evaluation TPR (female's TPR minus male's TPR).</p><p>ğ›½ ğ‘€ = âˆ’0.1042 ğ›½ ğ‘€ = 0 ğ‘ ğ‘–ğ‘€ = 0.2390 6.86% 6.63% ğ‘ ğ‘–ğ‘€ = 0 4.66% 3.98%</p><p>Table <ref type="table" target="#tab_2">10</ref>: The gender gap in the credit evaluation TPR (female's TPR minus male's TPR). New users only.</p><formula xml:id="formula_7">ğ›½ ğ‘€ = âˆ’0.1042 ğ›½ ğ‘€ = 0 ğ‘ ğ‘–ğ‘€ = 0.2390 2.05% 2.31% ğ‘ ğ‘–ğ‘€ = 0 2.30% 1.94%</formula><p>Table <ref type="table" target="#tab_2">11</ref>: The gender gap in the credit evaluation TPR (female's TPR minus male's TPR). Repeated users only.</p><p>Similarly, we separate the new users and the repeated users to check the lower level effects (Table <ref type="table" target="#tab_2">10 and Table 11</ref>). Consistent with our observation in Section 5, repeated users have smaller bias than new users. This indicates that signals can help mitigate bias in machine learning models as well. Although for repeated users, there is a slight increase when removing any one of the two bias (2.05% â†’ 2.30%, 2.05% â†’ 2.31%), the increase is not significant (p-value = 0.5312; p-value = 0.3047). This support that for machine learning models, removing bias can help mitigate the bias for both new users and repeated users. When we compare the de-biased outcome of human decision making and algorithm decision making, we find that human can achieve smaller TPR gap (2.04%) than the algorithm (3.98%) for new applicants, but lager TPR gap (2.70%) than the algorithm (1.94%) for repeated applicants. This finding suggests that the optimal strategy is to provide enough training to human evaluators to eliminate their decision bias, and then use human evaluators on new users while use machine learning algorithm to evaluate repeated users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSIONS</head><p>We have proposed a framework to use structural modeling to distinguish and estimate two types of human bias, i.e., belief-based bias and preference-based bias, based on observational data. In our micro-lending context, the evaluators hold a persistent preferencebased bias but learn from three distinct signals (the final overdue days ğ· ğ‘–ğ‘¡ , the proportion of installments with positive attitude from the borrower ğ´ ğ‘–ğ‘¡ , the financial help from family and friends ğ» ğ‘–ğ‘¡ ), which updates the evaluators' belief-based bias. The model was estimated on real-world data, and our model explains the data well.</p><p>The estimation results by the structural model imply that the evaluators possess a preference-based bias in favor of female applicants and against male ones; they also hold a belief-based bias with a higher prior belief of females' credit qualities. By observing the repayment behaviors, the evaluators can quickly update their belief of the borrowers' credit qualities. And all the three signals play significant roles in the evaluators' learning.</p><p>The results from our policy simulations suggest that both the eliminations of the preference-based bias and the belief-based bias can increase the platform's profits. The underlying mechanisms of the two counterfactual settings are the same. Because the loss from lowering the approval probability for nondefault users is smaller than the gain from lowering the approval probability for default users, the platform achieves higher profits. One the borrower side, the eliminations of both types of bias can reduce the gender gap in the credit evaluation true positive rate.</p><p>We also feed the real-word dataset and the counterfactual datasets into XGBoost model, to examine how ML algorithms inherit human bias. We find that machine learning algorithm can mitigate both the preference-based bias and the belief-based bias.</p><p>Our paper also has certain limitations that can be addressed in future work. First, the microloan users are generally not stable in their financial condition, which may be one plausible reason why the evaluators heavily rely on the latest repayment behaviors to form a belief of borrowers' credit qualities. In a more stable setting like credit card or mortgage, evaluators may gradually update their beliefs of borrowers' credit qualities. Second, in our policy simulations, we only consider the changes on the evaluator side. In reality, the changes in previous evaluator approval behaviors can also lead to changes in subsequent application behaviors of borrowers. Future work may take both sides into consideration. Third, as a pioneer work on quantifying different types of bias, we do not consider the interaction of gender and other attributes due to model complexity and identification issues. Future work may explore those interaction effects. Despite these limitations, to our best knowledge, this paper is the first to use structural modeling to uncover and distinguish the different types of bias in decision-making processes based on observational data. As machine learning and AI models are increasingly deployed across many decision-making scenarios, it is more and more important to understand the source of biases and propose well-targeted solutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Statistics of Loan Applications</figDesc><graphic url="image-1.png" coords="5,57.72,83.69,247.16,159.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Time Trends of Approval and Default Rate</figDesc><graphic url="image-3.png" coords="5,56.60,451.82,161.41,98.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Approval Rate by Gender</figDesc><graphic url="image-6.png" coords="6,53.80,83.69,252.21,167.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average Default Rate by Gender</figDesc><graphic url="image-7.png" coords="7,57.72,84.55,247.17,148.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Signals Across Number of Applications</figDesc><graphic url="image-12.png" coords="8,73.04,528.59,201.76,121.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .</head><label>1</label><figDesc>Use 2/3 experimental dataset to match the default of non-approved users of the observational dataset 2. Use structural model approved users in the observational dataset to train the XGBoost model, and test on the rest 1/3 experimental dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Two datasets used in the training and evaluation</figDesc><graphic url="image-13.png" coords="9,346.65,355.58,84.43,66.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>ğ›½</head><label></label><figDesc>ğ‘€ = âˆ’0.1042 ğ›½ ğ‘€ = 0 ğ‘ ğ‘–ğ‘€ = 0.2390 167260.2 171352.0 ğ‘ ğ‘–ğ‘€ = 0 172125.8 174534.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>months). During the sample period, there are 311,200 loan applications in total, among which 135,938 loan applications (taking up or approval rate 43.68%) were approved, whereas 175,263 were rejected by the platform. Our sample covers 139,454 borrowers; that is, the average number (frequency) of loan applications per borrower is 2.23 (= 311,200/139,454). In our sample, 53,503 (38.37%) borrowers applied more than once, and they contributed 225,248 applications in total (i.e., 4.21 on average per borrower). For these multiple-time borrowers, the average approval rate is 47.24%. The average approval rate is only 34.34% for the 85,951 borrowers who applied just once. This indicates the platform's preference towards repeated borrowers, which is reasonable as these borrowers have performed well in historical loans. Figures 1a and 1b display the distributions of the frequency of loan applications and number of approved loans respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Information of the Approved and Rejected Applications</figDesc><table /><note>(a) Average approval rate for all applicants (b) Average default rate for new applicants (c) Average default rate for repeated applicants</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Structural Model Estimation Results</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Estimate Std. error</cell></row><row><cell>ğ‘ ğ‘–ğ‘€</cell><cell></cell><cell>0.2390  *  *  *</cell><cell>0.0220</cell></row><row><cell cols="2">Signal D</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ğœ™</cell><cell cols="2">-0.0138  *  *  *  0.0008</cell></row><row><cell></cell><cell>ğ· 0</cell><cell>0.5219  *  *  *</cell><cell>0.0207</cell></row><row><cell cols="2">Signal A</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ğœ“</cell><cell>0.3492  *  *  *</cell><cell>0.0034</cell></row><row><cell></cell><cell>ğ´ 0</cell><cell>0.3788  *  *  *</cell><cell>0.0063</cell></row><row><cell cols="2">Signal H</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ğœŒ</cell><cell>0.9803  *  *  *</cell><cell>0.0800</cell></row><row><cell>z</cell><cell>ğ» 0</cell><cell>0.1252  *  *  *  0.0154  *  *  *</cell><cell>0.0399 0.0001</cell></row><row><cell cols="3">Coefficients of the prior ğœ·</cell><cell></cell></row><row><cell></cell><cell cols="2">ğ›½ 0 -0.8961  ğ›½ â„ğ‘œğ‘¢ğ‘ ğ‘–ğ‘›ğ‘” 0.1458  *  *  *</cell><cell>0.0058</cell></row><row><cell cols="2">ğ›½ ğ‘’ğ‘‘ğ‘¢ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›</cell><cell>0.2443  *  *  *</cell><cell>0.0038</cell></row><row><cell></cell><cell>ğ›½ ğ‘–ğ‘›ğ‘ğ‘œğ‘šğ‘’</cell><cell>0.0936  *  *  *</cell><cell>0.0013</cell></row><row><cell></cell><cell>ğ›½ ğ·ğ‘ƒğ¼</cell><cell>0.1176  *  *  *</cell><cell>0.0018</cell></row><row><cell>ğ›¼ ğ·</cell><cell></cell><cell>0.0097  *  *  *</cell><cell>0.0007</cell></row><row><cell>ğ›¼ ğ´</cell><cell></cell><cell>0.9780  *  *  *</cell><cell>0.0058</cell></row><row><cell>ğ›¼ ğ»</cell><cell></cell><cell>0.0121  *  *  *</cell><cell>0.0009</cell></row><row><cell cols="3">Note: *ğ‘ &lt; 0.1; **ğ‘ &lt; 0.05; ***ğ‘ &lt; 0.01</cell><cell></cell></row></table><note>* * * 0.0119 ğ›½ ğ‘€ -0.1042 * * * 0.0088 ğ›½ ğ‘“ ğ‘–ğ‘Ÿğ‘ ğ‘¡ğ´ğ‘ğ‘ğ‘€ğ‘œğ‘›ğ‘¡â„ -0.0201 * * * 0.0003</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Simulated and Actual Characteristics of Approved Users</figDesc><table><row><cell>Note</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The expected profits of different decision process.</figDesc><table><row><cell></cell><cell>2390 151046.8</cell><cell>156957.8</cell></row><row><cell>ğ‘ ğ‘–ğ‘€ = 0</cell><cell>155036.7</cell><cell>160190.4</cell></row><row><cell></cell><cell cols="2">ğ›½ ğ‘€ = âˆ’0.1042 ğ›½ ğ‘€ = 0</cell></row><row><cell cols="2">ğ‘ ğ‘–ğ‘€ = 0.2390 9.69%</cell><cell>5.14%</cell></row><row><cell>ğ‘ ğ‘–ğ‘€ = 0</cell><cell>6.94%</cell><cell>2.48%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The gender gap in the credit evaluation TPR (female's TPR minus male's TPR).</figDesc><table><row><cell></cell><cell cols="2">ğ›½ ğ‘€ = âˆ’0.1042 ğ›½ ğ‘€ = 0</cell></row><row><cell cols="2">ğ‘ ğ‘–ğ‘€ = 0.2390 12.31%</cell><cell>5.51%</cell></row><row><cell>ğ‘ ğ‘–ğ‘€ = 0</cell><cell>8.62%</cell><cell>2.04%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The expected profits of different decision process.</figDesc><table><row><cell></cell><cell cols="2">ğ›½ ğ‘€ = âˆ’0.1042 ğ›½ ğ‘€ = 0</cell></row><row><cell cols="2">ğ‘ ğ‘–ğ‘€ = 0.2390 7.49%</cell><cell>7.43%</cell></row><row><cell>ğ‘ ğ‘–ğ‘€ = 0</cell><cell>6.20%</cell><cell>5.57%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The micro-lending platform we work with is also considering to use a machine learning algorithm to evaluate loan applications.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">These definitions and operations are similar to those in other literature such as<ref type="bibr" target="#b22">[23]</ref> and<ref type="bibr" target="#b48">[49]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do women pay more for credit? Evidence from Italy</title>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Alberto F Alesina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><forename type="middle">Emilio</forename><surname>Lotti</surname></persName>
		</author>
		<author>
			<persName><surname>Mistrulli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the European Economic Association</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Racial bias in bail decisions</title>
		<author>
			<persName><forename type="first">David</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Dobbie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="1885" to="1932" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Consumer-lending discrimination in the FinTech era</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adair</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>National Bureau of Economic Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A convex framework for fair regression</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02409</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Implicit discrimination</title>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dolly</forename><surname>Chugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="94" to="98" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shifting standards and stereotype-based judgments</title>
		<author>
			<persName><forename type="first">Monica</forename><surname>Biernat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Manis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discrimination in the small-business credit market</title>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>David G Blanchflower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="930" to="943" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The dynamics of discrimination: Theory and evidence</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>J Aislinn Bohren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Imas</surname></persName>
		</author>
		<author>
			<persName><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American economic review</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="3395" to="3436" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Judging online peer-to-peer lending behavior: A comparison of first-time and repeated borrowing requests</title>
		<author>
			<persName><forename type="first">Shun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information &amp; Management</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="857" to="867" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Artificial Intelligence and Behavioral Economics</title>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">F</forename><surname>Camerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Economics of Artificial Intelligence</title>
				<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="587" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gender discrimination in online peer-to-peer credit lending: evidence from a lending platform in China</title>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fujun</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Commerce Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="553" to="583" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gender gap in peer-to-peer lending: Evidence from China</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dezhu</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Banking &amp; Finance</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">105633</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine learning and human capital complementarities: Experimental evidence on bias mitigation</title>
		<author>
			<persName><forename type="first">Prithwiraj</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Starr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajshree</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Strategic Management Journal</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1381" to="1411" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The measure and mismeasure of fairness: A critical review of fair machine learning</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00023</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Biased programmers? or biased data? a field experiment in operationalizing ai ethics</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Cowgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Dell'acqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nakul</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Chaintreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM Conference on Economics and Computation</title>
				<meeting>the 21st ACM Conference on Economics and Computation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="679" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Occupational Identity Discrimination in Peer-to-Peer Lending</title>
		<author>
			<persName><forename type="first">Xiangbo</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Finance and Banking</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="278" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gender diversity and securities fraud</title>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Cumming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tak</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academy of management Journal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1572" to="1593" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A case study of graduate admissions: Application of three principles of human decision making</title>
		<author>
			<persName><forename type="first">Robyn</forename><forename type="middle">M</forename><surname>Dawes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">180</biblScope>
			<date type="published" when="1971">1971. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clinical versus actuarial judgment</title>
		<author>
			<persName><forename type="first">Robyn</forename><forename type="middle">M</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Meehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">243</biblScope>
			<biblScope unit="page" from="1668" to="1674" />
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Measuring bias in consumer lending</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Dobbie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Paravisini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Pathania</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the revolving revolution: the debt collection channel</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lukasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName><surname>Serrano-Padial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="897" to="930" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dynamic model of brand choice when price and advertising signal product quality</title>
		<author>
			<persName><forename type="first">TÃ¼lin</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1111" to="1125" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Are early stage investors biased against women</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ewens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">R</forename><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Financial Economics</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="653" to="677" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What constitutes evidence of discrimination in lending?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">R</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="739" to="748" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Runshan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manmohan</forename><surname>Aseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Param</forename><surname>Vir Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Algorithms. Available at SSRN</title>
		<imprint>
			<biblScope unit="page">3408275</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowds, lending, machine, and bias</title>
		<author>
			<persName><forename type="first">Runshan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Param</forename><surname>Vir</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="92" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predictably unequal? the effects of machine learning on credit markets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Goldsmith-Pinkham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarun</forename><surname>Ramadorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansgar</forename><surname>Walther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Effects of Machine Learning on Credit Markets</title>
				<imprint>
			<date type="published" when="2020-10-01">2020. October 1, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Toward an understanding of why people discriminate: Evidence from a series of natural field experiments</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Gneezy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>List</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Price</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Man versus model of man: A rationale, plus some evidence, for a method of improving on clinical inferences</title>
		<author>
			<persName><surname>Lewis R Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">422</biblScope>
			<date type="published" when="1970">1970. 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clinical versus mechanical prediction: a meta-analysis</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>William M Grove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyd</forename><forename type="middle">S</forename><surname>Zald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beth</forename><forename type="middle">E</forename><surname>Lebow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chad</forename><surname>Snitz</surname></persName>
		</author>
		<author>
			<persName><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological assessment</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02413</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimal Sparse Decision Trees</title>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margo</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Crowdsourcing new product ideas under consumer learning</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Param</forename><surname>Vir Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management science</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="2138" to="2159" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayes, bounds, and rational analysis</title>
		<author>
			<persName><surname>Thomas F Icard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="79" to="101" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-Bayesian social learning</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Jadbabaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooya</forename><surname>Molavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sandroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Tahbaz-Salehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and Economic Behavior</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="210" to="225" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discrimination aware decision tree learning</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="869" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Importance of a Borrower&apos;s Track Record on Repayment Performance: Evidence in P2P Lending Market</title>
		<author>
			<persName><forename type="first">Kim</forename><surname>Dongwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Asian Finance</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="85" to="93" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Economics, and Business</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Algorithmic fairness</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashesh</forename><surname>Rambachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aea papers and proceedings</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="22" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discrimination in the Age of Algorithms</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><surname>Cass R Sunstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Legal Analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="113" to="174" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads</title>
		<author>
			<persName><forename type="first">Anja</forename><surname>Lambrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="2966" to="2981" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Home bias in online investments: An empirical study of an online crowdfunding market</title>
		<author>
			<persName><forename type="first">Mingfeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1393" to="1414" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Value of Alternative Data in Credit Risk Prediction</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beibei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evidence from a Large Field Experiment. International Conference on Information Systems</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Social influence and defaults in peer-to-peer lending networks</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhexiang</forename><surname>Sheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A statistical framework for fair predictive algorithms</title>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Johndrow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08077</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Peer-to-Peer lending</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Mateescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Society Research Institute</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Strike three: Discrimination, incentives, and evaluation</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Christopher A Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Sulaeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><surname>Hamermesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1410" to="1435" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">What&apos;s in a Picture? Evidence of Discrimination from Prosper. com</title>
		<author>
			<persName><forename type="first">G</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">R</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><surname>Sydnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Human resources</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="53" to="92" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Globally-Consistent Rule-Based Summary-Explanations for Machine Learning Models: Application to Credit-Risk Evaluation</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Shaposhnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available at SSRN</title>
		<imprint>
			<biblScope unit="volume">3395422</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Broader Issues Surrounding Model Transparency in Criminal Justice Risk Scoring</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beau</forename><surname>Coker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Data Science Review</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Risk, race, and recidivism: Predictive bias and disparate impact</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">T</forename><surname>Skeem</surname></persName>
		</author>
		<author>
			<persName><surname>Lowenkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Criminology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="680" to="712" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Algorithmic risk assessment in the hands of humans</title>
		<author>
			<persName><forename type="first">Megan</forename><forename type="middle">T</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Doleac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available at SSRN</title>
		<imprint>
			<biblScope unit="volume">3489440</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">What&apos;sa picture? Evidence of discriminations of loan fundability in the prosper. com marketplace</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sydnor</surname></persName>
		</author>
		<author>
			<persName><surname>Pope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Human Resources</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="53" to="92" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Why machine learning may lead to unfairness: Evidence from risk assessment for juvenile justice in catalonia</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>SongÃ¼l Tolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>GÃ³mez</surname></persName>
		</author>
		<author>
			<persName><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law</title>
				<meeting>the Seventeenth International Conference on Artificial Intelligence and Law</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A Bayesian investment model for online P2P lending</title>
		<author>
			<persName><forename type="first">Xubo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Internet Technologies</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</title>
		<author>
			<persName><forename type="first">Shunan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2607" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A structural analysis of the role of superstars in crowdsourcing contests</title>
		<author>
			<persName><forename type="first">Shunyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Param</forename><surname>Vir Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anindya</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning Individual Behavior Using Sensor Data: The Case of Global Positioning System Traces and Taxi Drivers</title>
		<author>
			<persName><forename type="first">Yingjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beibei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramayya</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1301" to="1321" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
