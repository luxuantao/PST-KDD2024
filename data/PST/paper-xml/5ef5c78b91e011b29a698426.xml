<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Approach to Case-Based Reasoning in Knowledge Bases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-25">25 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
							<email>rajarshi@cs.umass.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
							<email>agodbole@cs.umass.edu</email>
						</author>
						<author>
							<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
							<email>shehzaad.dhuliawala@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<email>mccallum@cs.umass.edu</email>
						</author>
						<author>
							<persName><forename type="first">Melinda</forename><surname>Gates</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Gates Foundation Seattle Dallas Duke University bornin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Approach to Case-Based Reasoning in Knowledge Bases</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-25">25 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.14198v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires no training, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). Consider the task of finding a target entity given a source entity and a binary relation. Our non-parametric approach derives crisp logical rules for each query by finding multiple graph path patterns that connect similar source entities through the given relation. Using our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on  We also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given a new problem, humans possess the innate ability to 'retrieve' and 'adapt' solutions to similar problems from the past. For example, an automobile mechanic might fix a car engine by recalling previous experiences where cars exhibited similar symptoms of damage. This model of reasoning has been widely studied and verified in cognitive psychology for various applications such as mathematical problem solving <ref type="bibr" target="#b32">[Ross, 1984]</ref>, diagnosis by physicians <ref type="bibr" target="#b34">[Schmidt et al., 1990]</ref>, automobile mechanics <ref type="bibr" target="#b19">[Lancaster and Kolodner, 1987]</ref> etc. A lot of classical work in artificial intelligence (AI), particularly in the field of case-based reasoning (CBR) has focused on incorporating such kind of reasoning in AI systems <ref type="bibr" target="#b33">[Schank, 1982</ref><ref type="bibr" target="#b18">, Kolodner, 1983</ref><ref type="bibr">, Rissland, 1983</ref><ref type="bibr" target="#b0">, Aamodt and Plaza, 1994</ref><ref type="bibr">, Leake, 1996, inter-alia]</ref>.</p><p>At a high level, a case-based reasoning system is comprised of four steps <ref type="bibr" target="#b0">[Aamodt and Plaza, 1994</ref>] -(a) 'retrieve', in which given a new problem, 'cases' that are similar to the given problem are retrieved. A 'case' is usually associated with a problem description (used for matching it to a new problem) and its corresponding solution. After the initial retrieval step, the previous solutions are (b) 'reused' for the problem in hand. Often times, however, the retrieved solutions cannot be directly used, and hence the solutions needs to be (c) 'revised'. Lastly, if the revised solution is useful for solving the given problem, they are (d) 'retained' in a memory so that they can be used in the future.</p><p>Knowledge graphs (KGs) <ref type="bibr" target="#b36">[Suchanek et al., 2007</ref><ref type="bibr" target="#b2">, Bollacker et al., 2008</ref><ref type="bibr" target="#b6">, Carlson et al., 2010</ref>] contain rich facts about entities and capture relations with diverse semantics between those entities. However, KGs can be highly incomplete <ref type="bibr" target="#b24">[Min et al., 2013]</ref>, missing important edges (relations) Figure <ref type="figure">1</ref>: Overview of our approach. Given a query (Melinda, works in city, ?), our method first retrieves similar entities to the query entity. Then it gathers the reasoning paths that lead to the answer for the respective retrieved entities. The reasoning paths are applied for the query entity (Melinda) to retrieve the answer. between entities. For example, consider the small fragment of the KG around the entity MELINDA GATES in figure <ref type="figure">1</ref>. Even though a lot of facts about MELINDA is captured, it is missing the edge corresponding to works in city. A recent series of work address this problem by modeling multi-hop paths between entities in the KG, thereby able to reason along the path MELINDA → ceo → GATES FOUNDATION → headquartered → SEATTLE. However, the number of paths starting from an entity increases exponentially w.r.t the path length and therefore past work used parametric models to do approximate search using reinforcement learning (RL) <ref type="bibr" target="#b44">[Xiong et al., 2017</ref><ref type="bibr" target="#b9">, Das et al., 2018</ref><ref type="bibr" target="#b21">, Lin et al., 2018]</ref>. Using RL-based methods have their own shortcomings, like hard to train and high computational requirements. Moreover, these models try to encode all the rules for reasoning into the parameters of the model which makes learning even harder.</p><p>In this paper, we propose a simple non-parametric approach for reasoning in KGs (Figure <ref type="figure">1</ref>). Given an entity and a query relation (e q , r q ), we first retrieve k entities in the KG that are similar to e q and for which we observe the query relation edge r q . The retrieved entities could be present anywhere in the KG and are not just restricted in the immediate neighborhood of e q . Similarity between entities is measured based on the observed relations that the entities participate in. This ensures that the retrieved entities have similar observed properties as e q (e.g., if e q is a CEO, then the retrieved entities are also CEO's or business person). Next, for each of the retrieved entities, our method finds a set of reasoning paths that connect the retrieved entities to the entities that they are connected with via the query relation r q . In this way, our method removes the burden of storing the reasoning rules in the parameters of the model and rather extract it from entities similar to the query entity. Next, our method checks if similar reasoning paths exists starting from the query entity e q . If similar paths exists, then the answer to the original query is found by starting from e q and traversing the KG by following the reasoning path. In practice, we find multiple reasoning paths which end at different entities and we rank the entities based on the number of reasoning paths that lead to them, with the intuition that an entity supported by multiple reasoning paths is likely to be a better answer to the given query.</p><p>Apart from being non-parametric, our proposed method has many other desirable properties. Our method is generic and even though we find very simple and symbolic methods to work very well for many KG datasets, every component of our model can be augmented by plugging in sophisticated neural models. For example, currently we use simple symbolic string matching to find if a relation path exists for the query entity. However, this component can be replaced with a neural model that matches paths which are semantically similar to each other <ref type="bibr" target="#b8">[Das et al., 2017]</ref>. Similarly, we currently use a very simple inner product to compute similarities between entity embeddings. But that can be replaced with more sophisticated maximum inner product search <ref type="bibr" target="#b28">[Mussmann and Ermon, 2016]</ref>.</p><p>The contributions of the paper are as follows -(a) We present a non-parametric approach for reasoning over KGs, that uses multiple paths of evidence to derive an answer. These paths are gathered from entities in different parts of the KG that are similar to the query entity. (b) Our approach requires no training and can be readily applied to any new KGs. (c) Our method achieves state-of-the-art performance on NELL-995 and the harder subset of FB-122 outperforming sophisticated neural approaches and other tensor factorization methods. We also perform competitively on the WN18RR dataset. (d) Lastly, we provide detailed analysis about why our method outperforms parametric rule learning approaches like MINERVA <ref type="bibr" target="#b9">[Das et al., 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation and Task Description</head><p>Let E denote the set of entities and R denote the set of binary relations. A knowledge base (KB) is a collection of facts stored as triplets (e 1 , r, e 2 ) where e 1 , e 2 ∈ E and r ∈ R . From the KB, a knowledge graph G can be constructed where the entities e 1 , e 2 are represented as the nodes and relation r as labeled edge between them. Formally, a KG is a directed labeled multigraph G = (V, E, R ), where V and E denote the vertices and edges of the graph respectively. Note that V = E and E ⊆ V × R ×V . Also, following previous approaches <ref type="bibr" target="#b3">[Bordes et al., 2013</ref><ref type="bibr" target="#b44">, Xiong et al., 2017]</ref>, we add the inverse relation of every edge, i.e. for an edge (e 1 , r, e 2 ) ∈ E, we add the edge (e 2 , r −1 , e 1 ) to the graph. (If the set of binary relations R does not contain the inverse relation r −1 , it is added to R as well). A path in a KG between two entities e s , e t is defined as a sequence of alternating entity and relations that connect e s and e t . A length of a path is the number of relation (edges) in the path. Formally, let a path p = (e 1 , r 1 , e 2 , . . . , r n , e n+1 ) with st(p) = e 1 , en(p) = e n+1 and len(p) = n. Let P denote the set of all paths and P n denote the set of all paths with length up to n, i.e. P n ⊆ P = {p ∈ P | len(p) ≤n}.</p><p>We consider the task of query answering on KGs. Query answering seeks to answer questions of the form e 1q , r q , ? (e.g. Melinda, works in city, ?), where the answer is an entity in the KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Case-based Reasoning on Knowledge Graphs</head><p>This section describes how we apply case-based reasoning (CBR) in a KG. In CBR, given a new problem, similar cases are retrieved from memory <ref type="bibr" target="#b0">[Aamodt and Plaza, 1994]</ref>. In our setting, a problem is a query e 1q , r q , ? for a missing edge in the KG. A case is defined as an observed fact (e 1 , r, e 2 ) in a KG along with a set of paths up to length n that connect e 1 and e 2 . Formally, a case c = (e 1 , r, e 2 , P) <ref type="table" target="#tab_3">a 4 tuple where (e 1 , r, e 2 ) ∈ G and P ⊆ P (e 1 ,e 2</ref> </p><formula xml:id="formula_0">⊆ V × R × V × P n is</formula><formula xml:id="formula_1">) = {p ∈ P n | st(p) = e 1 , en(p) = e 2 }.</formula><p>In practice, it is intractable to store all paths between e 1 and e 2 (P (e 1 ,e 2 ) ) and therefore we only store a random sample. Note, that we have a case for every observed triple in the KG and we denote the set of all such cases as C . We also assume access to a pre-computed similarity matrix S ∈ R V ×V which stores the similarity between any two given entities in a KG. Intuitively, two entities which exhibit the same relations should have a high similarity (e.g., an athlete should have a high similarity score with another athlete). Lastly, we define a memory M that serves as the store for all the cases present in a KG and also the similarity matrix, i.e. M = (C , S).</p><p>As explained earlier, CBR broadly comprises of four steps, which we explain briefly for our setup on a KG.</p><p>• Retrieve: In this step, given a query e 1q , r q , ? , our method first retrieves a set of k similar entities w.r.t e 1q using the pre-computed similarity matrix S, such that for each entity e in the retrieved set, we observe at least one fact of the form (e , r q , e ) in G. In other words, each retrieved entity should have at least one outgoing edge in G with the edge label as r q . Next, we gather all such facts (e , r q , e ) in G for each of the retrieved e . Note, that there could be more than one fact for a given entity and query relation (e.g. USA, has city, New York City and USA, has city, Boston, for the entity 'USA' and query relation 'has city'). Finally for all the gathered facts from the k-nearest neighbors, we retrieve all the cases from the memory store M . As noted before, in our formulation, a case is a fact augmented with a sample of KG paths that connect the entities of the fact. As we will see later ( § 3), the KG paths often represent reasoning rules that determine why the fact (e , r q , e ) hold true. The goal of CBR is to reuse these rules for the new query.</p><p>• Reuse: The reasoning paths that were gathered in the retrieve step are re-used for the query entity. As described before, a path in a KG is an alternating sequence of entities and relations. The path gathered from the nearest neighbors have entities which are in the immediate neighborhood of the nearest neighbor entities. To reuse these paths, we first replace the entities from the paths with un-instantiated variables and only extract the sequence of relations in them <ref type="bibr" target="#b35">[Schoenmackers et al., 2010]</ref>. For example, if we have a retrieved case (a fact with a set of paths) such as ((USA, has city, Boston) , {(USA, has state, Massachusetts, city in state, Boston)}), we remove the entities from the path and extract rules such as: has state(x, y), city in state(y, z) =⇒ has city(x, z). We gather all such paths from all the cases retrieved for a query. Since the same path type can occur in different cases, we maintain a list of paths sorted w.r.t the counts (in descending order).</p><p>• Revise: After the paths have been gathered, we look if those paths exists for the query entity e 1q . In our approach we find that simple symbolic exact-string matching for the relations works quite well. However, neural relation extraction systems <ref type="bibr" target="#b48">[Zeng et al., 2014</ref><ref type="bibr" target="#b40">, Verga et al., 2016]</ref> can be incorporated to map a relation to other similar relations to improve recall. We keep this direction as a part of our future work. Instead if we find an exact match for the sequence of relations, we revise the rules by instantiating the variables with the entities which lie along the path in the neighborhood of e 1q .</p><p>• Retain: Finally, a case including the query fact and the paths that lead to the correct answer for the query can be added to the memory store M .</p><p>Computing the similarity matrix: CBR approaches need access to a similarity matrix S to retrieve similar cases to the query entity. Intuitively, the similarity between entities that have similar relations should be higher (e.g. similarity between two athletes should be higher than the similarity between an athlete and a country). To model this, we parameterize each entity with an m-hot vector e ∈ R R . That is, each entity is a m-hot vector with the dimension equal to the size of the number of binary relations in the KB. An entry in the vector is set to 1, if an entity has at least one edge with that relation type, otherwise is set to 0. Even though, this is a really simple way of parameterizing an entity, we found this to work extremely well in practice. However, as previously mentioned we propose a generic method and one could replace the entity embeddings with any pre-trained vectors from any model. As an example, we present experiments by replacing our m-hot representation with pre-trained embeddings obtained from the state-of-the-art RotatE model <ref type="bibr" target="#b37">[Sun et al., 2019]</ref>. Lastly, the similarity between two entities is calculated by a simple inner product between the normalized embeddings.</p><p>Caching 'cases' in the memory store: CBR also needs access to store containing cases. As mentioned before, in our setup a 'case' is a KG triple, along with a sample of paths that connect the two entities of the triple. Since the number of paths between entities grow exponentially w.r.t path length, it is intractable to store all paths between an entity pair. We instead consider a small subgraph around each entity in the KG spanned by 1000 randomly sampled paths of length up to 3. Next, for each triple (e 1 , r, e 2 ) in the KG, we exhaustively search the subgraph around e 1 , collected in the previous step to find paths up to length 3 which lead to e 2 . These paths along with the fact form a case. This process is repeated for all the triples and each case is added to the case store C .</p><p>Both the similarity matrix S and the case store C are pre-computed offline and is stored in the memory M . Once that is done, our method requires no further training and can be readily used for any query in the KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Evaluation Protocols</head><p>We test our CBR based approach on three datasets that are often used in the community for benchmarking models -FB122 <ref type="bibr" target="#b12">[Guo et al., 2016]</ref>, NELL-995 <ref type="bibr" target="#b44">[Xiong et al., 2017]</ref>, and WN18RR <ref type="bibr" target="#b10">[Dettmers et al., 2018]</ref>. FB122 comes with a set of KB rules that can be used to infer missing triples in the dataset. We do not use the rules in the dataset and even show that our model is able to recover the rules from similar entities to the query. WN18RR was created by <ref type="bibr" target="#b10">Dettmers et al. [2018]</ref> from the original WN18 dataset by removing various sources of test leakage, making the datasets more realistic and challenging. We compare our CBR based approach with various state-of-the-art models using standard ranking metrics such as HITS@N and mean reciprocal rank (MRR). For fair comparison to baselines, after a fact is predicted we do not add the new case (inferred fact and paths) in the memory (retain step in § 2.2), as that would mean we would use more information than our baselines to predict the followup queries. Hyper-parameters: The various hyper-parameters for our method are the number of nearest neighbor retrieved for a query entity (k), the number of paths that are gathered from the retrieved entity (l) and the maximum path length considered (n). For all our experiments, we set n = 3. We tune k and l for each dataset w.r.t the given validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on Query Answering</head><p>We first present results on query answering (link prediction) on the three datasets and compare to various state-of-the-art baseline models. It is quite common in literature <ref type="bibr" target="#b3">[Bordes et al., 2013</ref><ref type="bibr" target="#b46">, Yang et al., 2015</ref><ref type="bibr" target="#b10">, Dettmers et al., 2018</ref><ref type="bibr" target="#b37">, Sun et al., 2019]</ref> to report aggregate results on both tail prediction (e 1 , r, ?) and head prediction (?, r −1 , e 2 ). To be exactly comparable to baselines, we report results on tail prediction for NELL-995 and for other datasets, we report average of head and tail predictions. NELL-995: Table <ref type="table" target="#tab_1">2</ref> reports the query answering performance on the NELL-995 dataset. We compare to several strong baselines. In particular, we compare to various embedding based models such as DistMult <ref type="bibr" target="#b46">[Yang et al., 2015]</ref>, ComplEx <ref type="bibr" target="#b39">[Trouillon et al., 2016]</ref> and ConvE <ref type="bibr" target="#b10">[Dettmers et al., 2018]</ref>. We also wanted to compare to various neural models for learning logical rules such as neuraltheorem-provers <ref type="bibr" target="#b25">[Rocktäschel and Riedel, 2017]</ref>, NeuralLP <ref type="bibr" target="#b47">[Yang et al., 2017]</ref> and MINERVA <ref type="bibr" target="#b9">[Das et al., 2018]</ref>. However, only MINERVA scaled to the size of NELL-995 dataset and others did not. As it is clear from table 2, CBR outperforms all the baselines by a large margin with gains of over 4% on the strict HITS@1 metric. We further discuss and analyze the results in sec (3.4).</p><p>FB122: Next we consider the FB122 dataset by <ref type="bibr" target="#b12">Guo et al. [2016]</ref>. Comparing results on FB122 is attractive for a couple of reasons -(a) Firstly, this dataset comes with a set of logical rules hand coded by the authors that can be used for logical inference. It would be interesting to see if our CBR approach is able to automatically uncover the rules from the data. (b) Secondly, there is a recent work on a neural model for logical inference (GNTPs) <ref type="bibr" target="#b26">[Minervini et al., 2020]</ref> that scales neural-theorem-provers <ref type="bibr" target="#b25">[Rocktäschel and Riedel, 2017]</ref> to this dataset and hence we can directly compare with them. Table <ref type="table" target="#tab_0">1</ref> reports the results. We compare with several baselines. CBR significantly outperforms GNTPs and even outperforms most models which have access to the hand-coded rules during training. We also find that CBR is able to uncover correct rules for 27 out of 31 (87%) query relations.</p><p>WN18RR: Finally, we report results of WN18RR in table <ref type="table" target="#tab_2">3</ref>. CBR performs competitively with GNTPs and most embedding based methods except RotatE <ref type="bibr" target="#b37">[Sun et al., 2019]</ref>. Upon further analysis, we find that for 210 triples in the test set, the entity was not present in the graph and hence no answers were returned for those query entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments with limited data</head><p>As mentioned before, our CBR based approach needs no training and gathers reasoning patterns from few similar entities. Therefore, it should ideally perform well for query relations for which we do not have a lot of data. Recently, <ref type="bibr" target="#b22">Lv et al. [2019]</ref> studied this problem and propose a meta-learning <ref type="bibr" target="#b11">[Finn et al., 2017]</ref> based solution (Meta-KGR) for few-shot relations. We compare with their model to see if CBR based approach will be able to generalize for such few-shot relations. Table <ref type="table" target="#tab_3">4</ref> reports the results and quite encouragingly we find that we outperform all sophisticated meta-learning approaches by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis: CBR is capable of doing contextualized reasoning</head><p>In this section, we analyze the performance of our non-parametric approach and try to understand why it works better than existing parametric rule learning models like MINERVA. Lets consider the relation 'agent belongs to organization' in the NELL-995 dataset. Here the query entity can belong to a wide varity of types. For example, these are all triples for the query relation in NELL-995-(George Bush, agent belongs to organization, House of Republicans), (Vancouver Canucks, agent belongs to organization, NHL), (Chevrolet, agent belongs to organization, General Motors). As can be seen, the query entity for a relation can belong to many different types and hence the logical rules that needs to be learned would be different. An advantage of CBR based approach is  that, for each query entity it retrieves similar contexts and then gather from them. One the other hand, models like MINERVA has to encode all rules into its parameters which makes learning harder. In other words, CBR is capable of doing better fine-grained contextual reasoning for a given query. To further confirm this hypothesis, we count the number of paths that CBR finds that lead to the correct answer and compare it with MINERVA. CBR learns a total of 306.4 unique paths that lead to answer compared to 176.83 of MINERVA. Figure <ref type="figure" target="#fig_2">2</ref> plots the counts for each query relation which further shows that CBR finds more varied paths than MINERVA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results with RotatE embeddings</head><p>As mentioned before, the CBR approach is generic and one can incorporate sophisticated models into each of the step. We run experiments where the m-hot representation of entities is replaced with pretrained embeddings obtained from a trained RotatE <ref type="bibr" target="#b37">[Sun et al., 2019]</ref> model for building the similarity matrix. On the WN18RR dataset, we get a MRR of 0.425 as compared to 0.423 with our original approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Limitations of our Current Work</head><p>A key limitation of our current work is the symbolic matching of reasoning paths. Different symbolic sequence of relation can have similar semantics (e.g. works in org, org located in and studies in, college located in are similar paths for inferring lives in relation), but in our current work, these paths would be treated differently. This limitation can be alleviated by learning a similarity kernel for paths using distributed representation of relations.</p><p>On error analysis, we found that a major source of error occurs in our ranking of entities. Currently, ranking of predicted entities is done via number of paths that lead to it. Even though, this simple technique works well, we noticed that, if we had access to an oracle ranker, our performance would improve significantly. For example, in FB122 dataset, CBR retrieves a total of 241 entities out of more than 9.5K entities. An oracle ranking of these entities would increase the accuracy (HITS@1) from 0.28 to 0.74. This indicates that a substantial gain could be obtained if we train a model to rank the retrieved entities, a direction we leave as future work.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Inference time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Bayesian non-parametric approaches for link prediction: There is a rich body of work which employs bayesian non-parametric approaches to automatically learn the latent dimension of entities. The infinite relational model (IRM) <ref type="bibr" target="#b15">[Kemp et al., 2006</ref>] and its extensions <ref type="bibr" target="#b45">[Xu et al., 2006</ref>] learns a possibly unbounded number of latent clusters from the graph and an entity is represented by its cluster membership. Later Airoldi et al. <ref type="bibr">[2008]</ref> proposed the mixed membership stochastic block models that allows entities to have mixed membership over the clusters. Instead of cluster membership, <ref type="bibr" target="#b23">Miller et al. [2009]</ref> propose a model that learn features of entities and the non-parametric approach allows learning unbounded number of dimensions. <ref type="bibr" target="#b38">Sutskever et al. [2009]</ref> combine bayesian nonparametric and tensor factorization approaches and <ref type="bibr" target="#b49">Zhu et al. [2012]</ref> allows non-parametric learning in a max-margin framework. Our method does not learn latent dimension and features of entities using a bayesian approach. Instead we propose a framework for doing non-parametric reasoning by learning patterns from k-nearest neighbors of the query entity. Also, the bayesian models have only been applied to very small knowledge graphs (containing few hundred entities and few relations).</p><p>Rule induction in knowledge graphs is a very rich field with lots of seminal works. Inductive Logic Programming (ILP) <ref type="bibr" target="#b27">[Muggleton et al., 1992]</ref> learns general purpose predicate rules from examples and background knowledge. Early work in ILP such as FOIL <ref type="bibr" target="#b31">[Quinlan, 1990]</ref>, PROGOL <ref type="bibr">[Muggleton, 1995]</ref> are either rule-based or require negative examples which is often hard to find in KBs (by design, KBs store true facts). Statistical relational learning methods <ref type="bibr" target="#b12">[Getoor and Taskar, 2007</ref><ref type="bibr" target="#b17">, Kok and Domingos, 2007</ref><ref type="bibr" target="#b35">, Schoenmackers et al., 2010]</ref> along with probabilistic logic <ref type="bibr">[Richardson and Domingos, 2006</ref><ref type="bibr" target="#b4">, Broecheler et al., 2010</ref><ref type="bibr">, Wang et al., 2013]</ref> combine machine learning and logic but these approaches operate on symbols rather than vectors and hence do not enjoy the generalization properties of embedding based approaches. Moreover, unlike our approach, these methods do not learn rules from entities similar to the query entity. Recent work in rule induction, as discussed before <ref type="bibr" target="#b47">[Yang et al., 2017</ref><ref type="bibr" target="#b25">, Rocktäschel and Riedel, 2017</ref><ref type="bibr" target="#b9">, Das et al., 2018</ref><ref type="bibr" target="#b26">, Minervini et al., 2020]</ref> try to encode rules in the parameters of the model. In contrast, we propose a non-parametric approach for doing so. Moreover our model outperforms them on several datasets. K-NN based approach in other NLP applications: Nearest neighbor models have been applied to a number of NLP applications in the past such as parts-of-speech tagging <ref type="bibr" target="#b7">[Daelemans et al., 1996]</ref> and morphological analysis <ref type="bibr">[Bosch et al., 2007]</ref>. There has also been several recent work which leverages k-nearest neighbors for various NLP tasks, which is a step towards case based reasoning. Retrieve-and-edit based approaches are gaining popularity for various structured prediction tasks <ref type="bibr" target="#b13">[Guu et al., 2018</ref><ref type="bibr">, Hashimoto et al., 2018]</ref>. Accurate sequence labeling by explicitly and only copying labels from retrieved neighbors have been achieved by <ref type="bibr" target="#b43">Wiseman and Stratos [2019]</ref>. Another recent line of work use training examples at test time to improve language generation <ref type="bibr" target="#b42">[Weston et al., 2018</ref><ref type="bibr" target="#b29">, Pandey et al., 2018</ref><ref type="bibr" target="#b5">, Cao et al., 2018</ref><ref type="bibr" target="#b30">, Peng et al., 2019]</ref>. Improvements in language model have been also observed by <ref type="bibr" target="#b16">Khandelwal et al. [2020]</ref> by utilizing explicit examples from past training data obtained from nearest neighbor search in the encoded space. However, unlike us these work do not extract explicit reasoning patterns (or solutions in cases) from nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a very simple non-parametric approach for reasoning in KGs that is similar to case-based reasoning approaches in classical AI. Our proposed model requires no training and can be readily applied to any knowledge graphs. It achieves new state-of-the-art performance in NELL-995 and FB122 datasets. Also we show that, our approach is robust in low-data settings. Overall, our nonparametric approach is capable of deriving crisp logical rules for each query by extracting reasoning patterns from other entities and entirely removes the burden of storing logical rules in the model parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(x, z) ⇐ ceo(x,y) ^ headquartered(y,z) ii) works_in_city(x, z) ⇐ chair(x,y) ^ headquartered(y,z) the reasoning patterns that work works_in_city(x, z) ⇐ ceo(x,y) ^ headquartered(y,z) works_in_city(x, z) ⇐ chair(x,y) ^ headquartered(y,z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The number of unique correct paths CBR and MINERVA find for each query relation in NELL-995</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Link prediction results on the FB122 dataset.</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell>HITS@3</cell><cell>HITS@5</cell><cell>HITS@10</cell><cell>MRR</cell></row><row><cell></cell><cell cols="3">KALE-Pre [Guo et al., 2016]</cell><cell></cell><cell>0.358</cell><cell>0.419</cell><cell>0.498</cell><cell>0.291</cell></row><row><cell></cell><cell cols="3">KALE-Joint [Guo et al., 2016]</cell><cell></cell><cell>0.384</cell><cell>0.447</cell><cell>0.522</cell><cell>0.325</cell></row><row><cell>WITH RULES</cell><cell cols="4">ASR-DistMult [Minervini et al., 2017]</cell><cell>0.363</cell><cell>0.403</cell><cell>0.449</cell><cell>0.330</cell></row><row><cell></cell><cell cols="4">ASR-ComplEx [Minervini et al., 2017]</cell><cell>0.373</cell><cell>0.410</cell><cell>0.459</cell><cell>0.338</cell></row><row><cell></cell><cell cols="3">TransE [Bordes et al., 2013]</cell><cell></cell><cell>0.360</cell><cell>0.415</cell><cell>0.481</cell><cell>0.296</cell></row><row><cell></cell><cell cols="3">DistMult [Yang et al., 2017]</cell><cell></cell><cell>0.360</cell><cell>0.403</cell><cell>0.453</cell><cell>0.313</cell></row><row><cell>WITHOUT RULES</cell><cell cols="3">ComplEx [Trouillon et al., 2016]</cell><cell></cell><cell>0.370</cell><cell>0.413</cell><cell>0.462</cell><cell>0.329</cell></row><row><cell></cell><cell cols="3">GNTPs [Minervini et al., 2020]</cell><cell></cell><cell>0.337</cell><cell>0.369</cell><cell>0.412</cell><cell>0.313</cell></row><row><cell></cell><cell cols="2">CBR (Ours)</cell><cell></cell><cell></cell><cell>0.400</cell><cell>0.445</cell><cell>0.488</cell><cell>0.359</cell></row><row><cell cols="2">Metric</cell><cell cols="5">ComplEx ConvE DistMult MINERVA CBR</cell></row><row><cell cols="2">HITS@1</cell><cell>0.612</cell><cell>0.672</cell><cell>0.610</cell><cell>0.663</cell><cell>0.705</cell></row><row><cell cols="2">HITS@3</cell><cell>0.761</cell><cell>0.808</cell><cell>0.733</cell><cell>0.773</cell><cell>0.828</cell></row><row><cell cols="2">HITS@10</cell><cell>0.827</cell><cell>0.864</cell><cell>0.795</cell><cell>0.831</cell><cell>0.875</cell></row><row><cell cols="2">MRR</cell><cell>0.694</cell><cell>0.747</cell><cell>0.680</cell><cell>0.725</cell><cell>0.772</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Query-answering results on NELL-995 dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Link prediction results on WN18RR dataset.</figDesc><table><row><cell>Metric</cell><cell cols="7">TransE DistMult ComplEx ConvE RotatE GNTP CBR</cell></row><row><cell>HITS@1</cell><cell>-</cell><cell>0.39</cell><cell>0.41</cell><cell>0.40</cell><cell>0.43</cell><cell>0.41</cell><cell>0.38</cell></row><row><cell>HITS@3</cell><cell>-</cell><cell>0.44</cell><cell>0.46</cell><cell>0.44</cell><cell>0.49</cell><cell>0.44</cell><cell>0.46</cell></row><row><cell>HITS@10</cell><cell>0.50</cell><cell>0.49</cell><cell>0.51</cell><cell>0.52</cell><cell>0.57</cell><cell>0.48</cell><cell>0.51</cell></row><row><cell>MRR</cell><cell>0.23</cell><cell>0.43</cell><cell>0.44</cell><cell>0.43</cell><cell>0.48</cell><cell>0.43</cell><cell>0.43</cell></row><row><cell cols="2">Model</cell><cell></cell><cell></cell><cell>HITS@1</cell><cell>HITS@10</cell><cell>MRR</cell><cell></cell></row><row><cell cols="3">NeuralLP [Yang et al., 2017]</cell><cell></cell><cell>0.048</cell><cell>0.351</cell><cell>0.179</cell><cell></cell></row><row><cell cols="4">NTP-λ [Rocktäschel and Riedel, 2017]</cell><cell>0.102</cell><cell>0.334</cell><cell>0.155</cell><cell></cell></row><row><cell cols="3">MINERVA [Das et al., 2018]</cell><cell></cell><cell>0.162</cell><cell>0.283</cell><cell>0.201</cell><cell></cell></row><row><cell cols="4">MultiHop(DistMult) [Lin et al., 2018]</cell><cell>0.145</cell><cell>0.306</cell><cell>0.200</cell><cell></cell></row><row><cell cols="4">MultiHop(ConvE) [Lin et al., 2018]</cell><cell>0.178</cell><cell>0.329</cell><cell>0.231</cell><cell></cell></row><row><cell cols="4">Meta-KGR(DistMult) [Lv et al., 2019]</cell><cell>0.197</cell><cell>0.345</cell><cell>0.248</cell><cell></cell></row><row><cell cols="4">Meta-KGR(ConvE) [Lv et al., 2019]</cell><cell>0.197</cell><cell>0.347</cell><cell>0.253</cell><cell></cell></row><row><cell cols="2">CBR (ours)</cell><cell></cell><cell></cell><cell>0.234</cell><cell>0.403</cell><cell>0.293</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Link prediction results on NELL-995 for few shot relations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Inference time (in seconds) on two datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>report the inference times of our model on the entire evaluation set of WN18RR (6268 queries) and NELL-995 (2825 queries) and compares it with MINERVA. Since our approach first retrieves similar entities and then gathers reasoning paths from them, it is slower than MINERVA. However, given the empirical improvements in accuracy, we believe this is not a significant tradeoff.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code available at https://github.com/rajarshd/CBR-AKBC</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is funded in part by the Center for Data Science and the Center for Intelligent Information Retrieval, and in part by the National Science Foundation under Grant No. IIS-1514053 and in part by the International Business Machines Corporation Cognitive Horizons Network agreement number W1668553 and in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Case-based reasoning: Foundational issues, methodological variations, and system approaches</title>
		<author>
			<persName><forename type="first">Agnar</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Plaza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>AI communications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Edoardo M Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
		<title level="m">Mixed membership stochastic blockmodels. JMLR</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Antal van den Bosch, Bertjan Busser, Sander Canisius, and Walter Daelemans. An efficient memorybased morphosyntactic tagger and parser for dutch</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<title level="s">LOT Occasional Series</title>
		<imprint>
			<date type="published" when="2007">2013. 2007</date>
		</imprint>
	</monogr>
	<note>Translating embeddings for modeling multi-relational data</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic similarity logic</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilyana</forename><surname>Mihalkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retrieve, rerank and rewrite: Soft template based neural summarization</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward an Architecture for Never-ending Language Learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mbt: A memory-based part of speech tagger-generator</title>
		<author>
			<persName><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Zavrel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Berck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Gillis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WVLC</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Shu Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2007">2007. 2016</date>
		</imprint>
	</monogr>
	<note>Introduction to statistical relational learning</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<editor>TACL</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A retrieve-and-edit framework for predicting structured outputs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning systems of concepts with an infinite relational model</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical predicate invention</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maintaining organization in a dynamic long-term memory</title>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">L</forename><surname>Kolodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Problem solving in a natural task as a function of experience</title>
		<author>
			<persName><forename type="first">Juliana</forename><forename type="middle">S</forename><surname>Lancaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">L</forename><surname>Kolodner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Georgia Tech CS Department</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cbr in context: The present and future. Case-based reasoning: Experiences, lessons, and future directions</title>
		<author>
			<persName><surname>David B Leake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-hop knowledge graph reasoning with reward shaping</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adapting meta knowledge graph information for multi-hop reasoning over few-shot relations</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nonparametric latent feature models for link prediction</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<editor>Neurips</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adversarial sets for regularising neural link predictors</title>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07596</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Differentiable reasoning on large knowledge bases and natural language</title>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matko</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Inverse entailment and progol. New generation computing, 1995. Stephen Muggleton, Ramon Otero, and Alireza Tamaddoni-Nezhad. Inductive logic programming</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Muggleton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning and inference via maximum inner product search</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exemplar encoder-decoder for neural conversation generation</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachindra</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Text generation with exemplar-based adaptive decoding</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning</title>
		<author>
			<persName><forename type="first">Quinlan</forename><forename type="middle">;</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Edwina</surname></persName>
		</author>
		<author>
			<persName><surname>Rissland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI, 1983. Tim Rocktäschel and Sebastian Riedel</title>
				<imprint>
			<date type="published" when="1990">1990. 2006. 2017</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Remindings and their effects in learning a cognitive skill</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">H</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dynamic memory: A theory of reminding and learning in computers and people</title>
		<author>
			<persName><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><surname>Schank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A cognitive perspective on medical expertise: theory and implications</title>
		<author>
			<persName><forename type="first">Henk</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henny</forename><surname>Boshuizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic medicine</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning first-order horn clauses from web text</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Yago: A core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<editor>Neurips</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multilingual relation extraction using compositional universal schema</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Programming with personalized pagerank: a locally groundable first-order probabilistic logic</title>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Retrieve and refine: Improved sequence generation models for dialogue</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ConvAI Workshop EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Label-agnostic sequence labeling by copying nearest neighbors</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Infinite hidden relational models</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Max-margin nonparametric latent feature models for link prediction</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
