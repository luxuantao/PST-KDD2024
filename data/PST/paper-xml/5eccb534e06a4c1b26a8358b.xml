<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Residual Feature Aggregation Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
							<email>jieliu@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuting</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>tangjie@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
							<email>gswu@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Residual Feature Aggregation Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, very deep convolutional neural networks (CNNs) have shown great power in single image superresolution (SISR) and achieved significant improvements against traditional methods. Among these CNN-based methods, the residual connections play a critical role in boosting the network performance. As the network depth grows, the residual features gradually focused on different aspects of the input image, which is very useful for reconstructing the spatial details. However, existing methods neglect to fully utilize the hierarchical features on the residual branches. To address this issue, we propose a novel residual feature aggregation (RFA) framework for more efficient feature extraction. The RFA framework groups several residual modules together and directly forwards the features on each local residual branch by adding skip connections. Therefore, the RFA framework is capable of aggregating these informative residual features to produce more representative features. To maximize the power of the RFA framework, we further propose an enhanced spatial attention (ESA) block to make the residual features to be more focused on critical spatial contents. The ESA block is designed to be lightweight and efficient. Our final RFANet is constructed by applying the proposed RFA framework with the ESA blocks. Comprehensive experiments demonstrate the necessity of our RFA framework and the superiority of our RFANet over state-of-the-art SISR methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of single image super-resolution (SISR) is to map a degraded low-resolution (LR) image to a visually high-resolution (HR) image, which is a highly ill-posed procedure since multiple HR solutions can map to one LR input. Many image SR methods have been proposed to tackle this inverse problem, including early interpolationbased <ref type="bibr" target="#b36">[37]</ref>, reconstruction-based <ref type="bibr" target="#b33">[34]</ref>, and recent learning based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3]</ref>. * Corresponding author. Recent deep convolutional neural network based methods have made great progress in reconstructing the HR images. The first successful attempt was done by Dong et al. <ref type="bibr" target="#b3">[4]</ref>, who proposed the three-layer SRCNN for SISR and achieved superior performance against conventional methods. Kim et al. further increased the depth to 20 in VDSR <ref type="bibr" target="#b12">[13]</ref> and DRCN <ref type="bibr" target="#b13">[14]</ref> by introducing residual learning to ease the training difficulty. Following these pioneering works, many CNN-based methods have been proposed and achieved state-of-the-art results in SISR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Although considerable improvements have been achieved in SISR, existing CNN-based models are still faced with some limitations. As the network depth grows, the features in each convolutional layer would be hierarchical with different receptive fields. Most existing CNN-based models do not make fully use of the information from the intermediate layers. Especially, residual learning is widely used in CNN-based models to extract the residual information of input features, while almost all the existing SR models only use the residual learning as a strategy to ease the training difficulty. For clarity, we call the entire residual construct as a residual module and the residual branch as a residual block. Usually, a SR model is made by stacking a bunch of residual modules, where the residual features are fused with the identity features before propagating to the next module (Fig. <ref type="figure" target="#fig_0">1(a)</ref>). As a result, later residual blocks can only see the complex fused features. These methods neglect to make fully use of the cleaner residual features, thereby leading to performance degradation. The residual features, however, are extremely helpful for reconstructing the HR images.</p><p>To address these problems, we propose a residual feature aggregation (RFA) framework, which aggregates the local residual features for more powerful feature representation. Fig. <ref type="figure" target="#fig_0">1</ref>(a) shows a common network design where multiple residual modules are stacked together to build a deep network. Under this design, the residual features of preceding blocks must go through a long path to propagate to subsequent blocks. After a series of addition and convolutional operations, these features are quickly merged with the identity features to form more complex features. Therefore, these highly representative residual features are used very locally, which limits the representational power of the network. As depicted in Fig. <ref type="figure" target="#fig_0">1</ref>(b), the proposed RFA framework reorganizes the stacked residual modules, where the last residual module is extended to cover the fist three residual modules to ease the training difficulty. Then the residual features of the first three blocks are sent directly to the output of the last residual block. Finally, these hierarchical features are concatenated together and sent to a 1×1 convolutional layer to generate a more representative feature. The only overhead is a 1 × 1 convolution every four residual blocks, which is negligible compared with the whole very deep networks.</p><p>As shown in Fig. <ref type="figure">8</ref>, the residual features of different residual blocks can reflect different aspects of the spatial contents. But these residual features are not highlighted enough. It is necessary to enhance the spatial distribution of residual features with spatial attention mechanism so that the performance of our RFA framework could be further improved. However, existing spatial attention mechanisms in image SR are either less powerful or computationally intensive. For example, the plain spatial attention in <ref type="bibr" target="#b9">[10]</ref> lacks of a large receptive field which is essential for image SR and the Non-Local mechanisms in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref> consume a lot of computational resource. To solve this issue, we propose a lightweight and efficient enhanced spatial attention (ESA) block. The ESA block enables a large receptive field by the joint use of a strided convolution and a max-pooling with large window size. To keep the body of the ESA block lightweight enough, we apply a 1 × 1 convolution at the beginning of the ESA block for channel dimension reduction.</p><p>To verify the effectiveness of the proposed methods, we build a very deep network RFANet by combining the RFA framework with the ESA block. The RFANet achieves comparable or superior results compared with RCAN <ref type="bibr" target="#b37">[38]</ref> (16M) and SAN <ref type="bibr" target="#b2">[3]</ref> (15.7M) by using much fewer parameters (11M). In summary, the main contributions of this paper are as follows:</p><p>• We propose a general residual feature aggregation (RFA) framework for more accurate image SR. Comprehensive ablation study shows that the performance of residual networks as well as dense networks can get a substantial improvement.</p><p>• We propose an enhanced spatial attention (ESA) block to adaptively rescale features according to the spatial context. The ESA block allows the network to learn more discriminative features. Besides, it is lightweight and has better performance than the plain spatial attention block.</p><p>• We propose a residual feature aggregation network (RFANet) which is constructed by incorporating the proposed RFA framework with the powerful ESA block. Thanks to the enhanced spatial attention mechanism, the RFA framework can aggregate more representative features, thus generating more accurate SR results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Super-resolution can be broadly divided into two main categories: traditional and deep learning based methods. Due to the powerful learning ability, the classical methods have been outperformed by their deep learning based counterparts. In this section, we briefly review the works related to deep neural networks for single image super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">CNN-based Networks</head><p>Dong et al. <ref type="bibr" target="#b3">[4]</ref> first proposed a shallow three-layer convolutional neural network (SRCNN) for image SR and achieved superior performance against previous works. Inspired by this pioneering work, Kim et al. designed deeper VDSR <ref type="bibr" target="#b12">[13]</ref> and DRCN <ref type="bibr" target="#b13">[14]</ref> with 20 layers based on residual learning. Later, Tai et al. introduced recursive blocks in DRRN <ref type="bibr" target="#b23">[24]</ref> and memory blocks in MemNet <ref type="bibr" target="#b24">[25]</ref>. These methods extract features from the interpolated LR images, which consumes a lot of memory and computation time. To address this problem, Shi et al. proposed an efficient subpixel convolutional layer in ESPCN <ref type="bibr" target="#b22">[23]</ref>, where LR feature maps are upscaled into HR output at the end of the network. Thanks to the efficient sub-pixel layer, many very deep networks have been proposed for a better performance. Lim et al. proposed a very deep and wide network EDSR <ref type="bibr" target="#b17">[18]</ref> by stacking modified residual blocks in which the batch normalization (BN) layers are removed. Ledig et al. introduced the SRResNet in <ref type="bibr" target="#b15">[16]</ref> and are further improved in <ref type="bibr" target="#b30">[31]</ref> by introducing the dense connections. Zhang et al. also used dense connections in RDN <ref type="bibr" target="#b39">[40]</ref> to utilize all the hierarchical features from all the convolutional layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention-based Networks</head><p>Attention mechanism are widely used in recent computer vision tasks, such as image captioning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2]</ref>, image and video classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>. It can be interpreted as a way to bias the allocation of available resources towards the most informative parts of an input signal <ref type="bibr" target="#b7">[8]</ref>. Wang et al. <ref type="bibr" target="#b28">[29]</ref> proposed a powerful trunk-and-mask attention mechanism inserted between the intermediate stages of deep residual networks. Hu et al. <ref type="bibr" target="#b7">[8]</ref> proposed the squeeze-and-excitation network (SENet) to exploit channel-wise relationships and achieved a significant improvement for image classification.</p><p>Recently, some attention-based models are also proposed to further improve the SR performance. Zhang et al. <ref type="bibr" target="#b37">[38]</ref> proposed the residual channel attention network (RCAN) by introducing the channel attention mechanism into a modified residual block for image SR. The channel attention mechanism uses global average pooling to extract channel statistics which are called first-order statistics. On the contrary, Dai et al. <ref type="bibr" target="#b2">[3]</ref> proposed the second-order attention network (SAN) to explore more powerful feature expression by using second-order feature statistics. RCAN and SAN are the two best performing methods among all currently published methods in terms of PSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Network Architecture for Image SR</head><p>Many recent SR networks have similar network architectures. Here we introduce one of the basic architecture used by some state-of-the-art methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, a basic image SR network usually consists of three parts: the head part, the trunk part and the reconstruction part. The head part is responsible for initial feature extraction with only one convolutional layer. Given the LR input I LR , we can get the shallow feature F 0 through this layer</p><formula xml:id="formula_0">F 0 = H(I LR )<label>(1)</label></formula><p>where H stands for the shallow feature extraction function of the head part. Then the extracted feature F 0 is sent to the trunk part for deep feature learning. The trunk part is made up of T base modules (BM), which can be formulated as</p><formula xml:id="formula_1">F t = B t (F t−1 ) = B t (B t−1 (. . . (B 0 (F 0 )) . . . )) (2)</formula><p>where B t denotes the t-th base module function. F t−1 is the input of the t-th module and F t is the corresponding output. Finally, the extracted deep feature F t is upscaled through the reconstruction part</p><formula xml:id="formula_2">I SR = R(F t + F 0 ) = G(I LR )<label>(3)</label></formula><p>where I SR is the super-resolved image, R denotes the reconstruction function and G denotes the function of the SR network. Here, global residual learning is used to ease the training difficulty, so the input to R is the element-wise addition of F t and F 0 . The key module of the reconstruction part is the upscale module, where appropriate number of sub-pixel <ref type="bibr" target="#b22">[23]</ref> convolutions are applied.</p><p>The SR network will be optimized with L 1 loss function. Given a training set of N LR image patches I LR and their HR counterparts I HR , the loss function of the basic network with the parameter set Θ is</p><formula xml:id="formula_3">L(Θ) = 1 N N i=1 ||G(I i LR ) − I i HR || 1<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual Feature Aggregation Framework</head><p>Residual learning has demonstrated its significance for the image classification problem. Recently, residual learning is also introduced in image SR to further boost the performance. Fig. <ref type="figure">3</ref>(Left) depicts a basic residual module used in EDSR <ref type="bibr" target="#b17">[18]</ref> and ESRGAN <ref type="bibr" target="#b30">[31]</ref>. The residual modules are often stacked together to form the trunk part of the SR network (Fig. <ref type="figure" target="#fig_1">2</ref>). Each residual module consists of two branches:the residual branch (i.e. residual block) and the identity branch. In the task of image SR, the residual block can produce some useful hierarchical features focusing on different aspects of the original LR image. Consider the scenario of several consecutive residual modules (e.g. addition and convolution operations. As a result, the residual feature is hard to be fully utilized and plays a very local role in the learning process of the entire network.</p><p>To solve this issue, we propose a residual feature aggregation (RFA) framework to make a better use of the local residual features. Fig. <ref type="figure">3</ref>(Right) shows the details of an RFA module which contains four residual blocks. As we can see, the residual features of the first three blocks are sent directly to the end of the RFA module and then concatenated together with the output of the last residual block. Finally, a 1 × 1 convolution is applied to fuse these features before the element-wise addition with the identity feature. Compared with the way of simply stacking multiple residual modules, our RFA framework enables non-local use of the residual features. The useful hierarchical information that preceding residual blocks contain can be propagated to the end of the RFA module without any loss or interference, thus leading to a more discriminative feature representation.</p><p>The proposed residual feature aggregation methodology is a general framework that can be easily applied with existing SR blocks (e.g. dense block <ref type="bibr" target="#b39">[40]</ref>). We will investigate the effects in detail when our RFA framework is used in conjunction with the state-of-the-art blocks. In order to maximize the effectiveness of our RFA framework, it is best to be used in conjunction with the spatial attention mechanism, since we need the residual features to be focused on spatial contents of key importance. To this end, we design an enhanced spatial attention (ESA) block that is more powerful than the plain one in <ref type="bibr" target="#b9">[10]</ref>. The ESA mechanism works at the end of the residual block (Fig. <ref type="figure" target="#fig_3">4(Left)</ref>) to force the features to be more focused on the regions of interest. We can get a more representative feature when aggregating these highlighted features together. In the design of an attention block, several elements have to be carefully considered. First, the attention block must be lightweight enough since it will be inserted into every residual module of the network. Second, a large receptive field is required for the attention block to work well for the task of image SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Enhanced Spatial Attention Block</head><p>As shown in Fig. <ref type="figure" target="#fig_3">4</ref>(Right), the proposed ESA mechanism starts with a 1 × 1 convolutional layer to reduce channel dimensions, so that the whole block can be extremely lightweight. Then to enlarge the receptive field we use one strided convolution (with stride 2) followed by a max-pooling layer. The combination of strided convolution and max-pooling is widely used in image classification to quickly reduce the spatial dimensions at the beginning of the network. However, the receptive field enlargement brought by a regular 2 × 2 max-pooling layer is still very limited. So we choose to apply the max-pooling operation with a larger window (e.g. 7 × 7) and stride (e.g. stride 3). Corresponding to the front, an up-sampling layer is added to recover the spatial dimensions and a 1 × 1 convolutional layer is used to recover the channel dimensions. Finally, the attention mask is generated via a sigmoid layer. We also use a skip connection to forward the high-resolution features before spatial dimension reduction directly to the end of the block.</p><p>Put aside the amount of calculation, a potentially better way to implement the spatial attention block is to use the Non-Local block. Actually, there are works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref> that have attempted to use the Non-Local block to model pixel-wise similarities in image SR. Though it brings performance boost, the huge computation overhead is unacceptable which violates the first element of our design principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>We apply the RFA framework with the ESA block to build our final SR network (RFANet). RFANet uses 30 RFA modules and each RFA module contains 4 ESA blocks. In the ESA block, the reduction ratio of the 1×1 convolutional layer is set to 4 and we use three 3 × 3 convolutions in the convolutional groups. For other convolution filters outside the ESA block, the number of filters are set to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussions</head><p>Difference to MemNet. MemNet stands for the very deep persistent memory network proposed by Tai et al. <ref type="bibr" target="#b24">[25]</ref>. The most crucial part of MemNet is the stacked memory blocks. A memory block consists of a recursive unit and a gate unit to explicitly mine persistent memory though an adaptive learning process. The recursive unit is implemented by a residual building block and this residual building block is executed in each recursion to generate multi-level representations. The gate unit is responsible for adaptively learning these representations. The unfolded memory block has a similar connection pattern with our RFA framework. The key difference is that memory block aggregates the output features of a whole residual module while our RFA framework concentrates on the feature of the residual branch. Moreover, the memory block operates in a recursive manner very locally. In RFA framework, the basic building blocks are organized in a chain way so that each residual branch can focus on different aspects of the LR image, so the aggregated residual features would be more diverse and discriminative. Difference to RDN. The main building block of RDN <ref type="bibr" target="#b39">[40]</ref> is called residual dense block (RDB). RDB combines residual skip connections with dense connections. The motivation of RDB is that the hierarchical feature representations should be fully used to learn local patterns. In a dense block, each layer can have direct access to its subsequent layers. Before merging with the identity branch, a 1 × 1 convolutional layer is also used to fuse features coming from all the intermediate layers. Though shares a similar motivation behind the block design, our RFA module operates in a quite different way. A RFA module contains several residual modules and mainly aggregates features from the residual branches. In contrast, the RDB collects intermediate features between plain convolutional layers. The dense block is very computationally intensive because of the dense feature fusion strategy. Our RFA module is much more lightweight since the feature aggregation only happens at the end of the module. In general, the proposed RFA module works at a higher level than the dense block and the performance can be further boosted when applying our RFA framework to the dense block (Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Following previous works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref>, we use 800 highresolution training images from DIV2K <ref type="bibr" target="#b25">[26]</ref> dataset as training set. During training, data augmentation is performed by randomly rotating 90 • , 180 • , 270 • and horizontally flipping. In each training mini-batch, 16 LR color patches with size 48 × 48 are used. For testing, we use five standard benchmark datasets: Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b32">[33]</ref>, B100 <ref type="bibr" target="#b19">[20]</ref>, Urban100 <ref type="bibr" target="#b11">[12]</ref>, and Manga109 <ref type="bibr" target="#b20">[21]</ref>. Bicubic (BI) and blur-downscale (BD) degradation models <ref type="bibr" target="#b35">[36]</ref> are used when conducting experiments. The SR results are evaluated by PSNR and SSIM metrics on Y channel of transformed YCbCr space. Our model is trained by ADAM optimizer with β 1 = 0.9, β 2 = 0.99, and ǫ = 10 −8 . The learning rate is initialized as 5 × 10 −4 and then decreases to half every 2 × 10 5 iterations. We use PyTorch framework to implement our models with a Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Combination with Residual Block</head><p>In this section, we investigate the combination of our RFA framework with the basic residual block used in EDSR <ref type="bibr" target="#b17">[18]</ref>. Different from the original residual block used in image classification, EDSR removes the Batch Normalization layers and achieved substantial improvements. The baseline model contains 120 residual modules and we refer to this model as "EDSR-Baseline". Our RFA model adopts 30 RFA modules to keep the number of residual blocks the same as EDSR-Baseline for a fair comparison. We refer to this model as "RFA-EDSR" for short. As shown in the second column of Table <ref type="table" target="#tab_0">1</ref>, the PSNR of EDSR-Baseline is 32.40 dB which demonstrates a strong baseline for image SR. When deploying our RFA framework with the residual block (RFA-EDSR), the PSNR reaches 32.50 dB. Compared with the EDSR-Baseline, the RFA-EDSR has only one more 1×1 convolution every four residual blocks while boosting the PSNR by 0.1 dB. We attribute this considerable improvement to the effective design of our RFA framework where the residual feature in each residual block can be better utilized by the network. These comparisons demonstrate that the proposed RFA framework is essential to very deep networks for Image SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Combination with Dense Block</head><p>The motivation behind dense block <ref type="bibr" target="#b39">[40]</ref> is also to combine hierarchical cues available along the network depth to get richer feature representations. But the combination happens inside a single residual module. In contrast, our RFA framework aims to combining the residual features at a higher level. It is reasonable to apply the RFA framework in conjunction with the dense block to further improve the performance. In this ablation study, we use 42 dense blocks to maintain similar number of parameters with EDSR-Baseline and RFA-EDSR. We refer to the dense block baseline model as "Dense-Baseline". When applying RFA framework with dense blocks (RFA-Dense), we use 14 RFA modules to make these two models comparable. As shown in the third column of Table <ref type="table" target="#tab_0">1</ref>, RFA-Dense improves the performance of Dense-Baseline from 32.42 dB to 32.51 dB. This indicates that the proposed RFA framework can further combine the hierarchical information against the dense block. Note that this semi-trained RRA-Dense model  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Combination with Attention Block</head><p>By using attention mechanism, the performance of image SR has achieved significant improvements. Here, we will comprehensively investigate the effects of applying our RFA framework to the attention blocks. Table <ref type="table" target="#tab_0">1</ref> shows the ablation results including channel attention (CA) <ref type="bibr" target="#b37">[38]</ref>, spatial attention (SA) <ref type="bibr" target="#b9">[10]</ref>, enhanced spatial attention (ESA) and their combinations (i.e. RFA-CA, RFA-SA and RFA-ESA) with the RFA framework. As we can see, by using channel attention block alone, the PSNR already achieves 32.56 dB, which demonstrates the excellent performance of channel attention mechanism. The plain SA has a much lower PSNR than CA, but when equipped with our RFA framework, the RFA-SA achieves a comparable PSNR with CA. On the contrary, RFA-CA does not show any consid-erable improvement compared with CA. This indicates that the RFA framework is best to be used with spatial attention mechanism. To this end, we design an enhanced spatial attention block and it achieves the same PSNR as CA, which indicates its effectiveness for image SR. Furthermore, The RFA-ESA solution improved the ESA from 32.56 dB to 32.65 dB. This shows that the proposed RFA framework can further boost the performance of spatial attention mechanism by a large margin. Among all the investigated methods, the proposed RFA-ESA method achieves the best performance and we will use it to compare with the state-ofthe-art methods. From now on, we use the name "RFANet" to represent the RFA-ESA network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results with Bicubic Degradation (BI)</head><p>It is widely used to simulate LR images with BI degradation model in image SR settings. To verify the effective-  <ref type="bibr" target="#b3">[4]</ref>, FSRCNN <ref type="bibr" target="#b4">[5]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, EDSR <ref type="bibr" target="#b17">[18]</ref>, SRMD <ref type="bibr" target="#b35">[36]</ref>, NLRN <ref type="bibr" target="#b18">[19]</ref>, DBPN <ref type="bibr" target="#b5">[6]</ref>, RDN <ref type="bibr" target="#b39">[40]</ref>, RCAN <ref type="bibr" target="#b37">[38]</ref> and SAN <ref type="bibr" target="#b2">[3]</ref>.  our RFANet behaves particularly well on Urban100 and Manga109 datasets. This is mainly because both datasets contain rich structured contents and our RFANet can gradually aggregate these hierarchical information to form more representative features. This property can be further verified from the SSIM scores of our RFANet. The SSIM score is focused on the visible structures in the image. For example, on Urban100 (×2) dataset, our PSNR is the second best but we achieve the best SSIM, which indicates our RFANet can recover better visible structures. Similar phenomena can also be found on Set14 (×4) dataset. The visual comparisons of Fig. <ref type="figure" target="#fig_4">5</ref> can also prove that our RFANet reconstructs better structural details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results with Blur-downscale Degradation (BD)</head><p>Following <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref>, we also provide the results with blur-downscale degradation (BD) model. We compare our RFANet with 10 state-of-the-art methods: SPMSR <ref type="bibr" target="#b21">[22]</ref>, SRCNN <ref type="bibr" target="#b3">[4]</ref>, FSRCNN <ref type="bibr" target="#b4">[5]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, IRCNN <ref type="bibr" target="#b34">[35]</ref>, SRMD <ref type="bibr" target="#b35">[36]</ref>, RDN <ref type="bibr" target="#b39">[40]</ref>, SRFBN <ref type="bibr" target="#b16">[17]</ref>, RCAN <ref type="bibr" target="#b37">[38]</ref>, and SAN <ref type="bibr" target="#b2">[3]</ref>. As shown in Table <ref type="table" target="#tab_2">3</ref>, our RFANet outperforms other methods on all the datasets. Specifically, we achieve 0.06dB PSNR gain over SAN on Urban100 dataset. Compared with SAN, the PSNR gain on Set14 dataset is marginal but we can still achieve considerable improvement in terms of SSIM. The consistently better results of RFANet indicate that our method can adapt well to scenarios with multiple degradation models. Fig. <ref type="figure" target="#fig_3">4</ref>.4 shows the visual superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Effects of Residual Feature Aggregation (RFA)</head><p>We now illustrate how our residual feature aggregation design affects the output features in different stages of the network. Inspired by <ref type="bibr" target="#b10">[11]</ref>, we adopt the weight norm as an approximate for the dependency of a convolutional layer  on its preceding layers. The weight norm is calculated by the corresponding weights from all filters w.r.t. each residual feature map in the aggregation 1 × 1 convolutional layer (see Fig. <ref type="figure">3</ref>). In general, the larger the norm is, the stronger dependency it has on this particular feature map. For clarity, we choose to visualize every two modules in a total of 30 RFA modules. Fig. <ref type="figure">7</ref> presents the norm of the filter weights vs. feature map index. The legend of Fig. <ref type="figure">7</ref> shows the index of residual blocks in each RFA module. Several observations can be made from the plot: (1) The aggregation layers spread their weights over all the residual blocks which indicates that all the residual features are directly used to produce the output features of the RFA module. (2) The variance of weight norms in latter modules are larger than that of the previous modules. This indicates that the network gradually learns to distinguish the residual features and assign more weights to the features of critical importance. (3) At the beginning, the last block contributes most than the other three blocks. With the depth increases, the other three blocks also play an important role in feature learning, indicating the necessity of residual feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Effects of Enhanced Spatial Attention</head><p>Fig. <ref type="figure">8</ref> visualizes the average feature maps of residual blocks within a RFA module. The top row is the feature maps before attention mechanism and the bottom row is the feature maps after attention mechanism. We can get some intuitive clues from this visualization: (1) The attention mechanism has the effect of modulating the activation values. We can see that the activation ranges of the bottom row are smaller than the top row, which can ease the training difficulty to some extent (e.g. residual scaling in EDSR <ref type="bibr" target="#b17">[18]</ref>).</p><p>(2) Feature maps after the attention mechanism tend to contain more negative values, showing a stronger effect of suppressing the smooth area of the input image, which further leads to a more accurate residual image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Model Complexity Analysis</head><p>Fig. <ref type="figure" target="#fig_7">9</ref> shows the comparisons about model size and performance with 11 stae-of-the-art SR methods:SRCNN <ref type="bibr" target="#b3">[4]</ref>, FSRCNN <ref type="bibr" target="#b4">[5]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, NLRN <ref type="bibr" target="#b18">[19]</ref>, SRMD <ref type="bibr" target="#b35">[36]</ref>, DBPN <ref type="bibr" target="#b5">[6]</ref>, RDN <ref type="bibr" target="#b39">[40]</ref>, RCAN <ref type="bibr" target="#b37">[38]</ref> and SAN <ref type="bibr" target="#b2">[3]</ref>. Our RFANet has much fewer parameters than RDN, RCAN and SAN, but obtains better performance, which verifies the effectiveness of our method. Compared with DBPN, our RFANet achieves much higher PSNR with a slightly larger model, indicating that we have a good trade-off between performance and model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a general residual feature aggregation (RFA) framework for image SR. The RFA framework effectively groups the residual blocks together, where the features of local residual blocks are sent directly to the end of the RFA framework for fully utilizing these useful hierarchical features. To maximize the power of the proposed RFA framework, we further design an enhanced spatial attention (ESA) block to make the residual features to be more focused on spatial contents of key importance. To compare with state-of-the-art methods, we propose the RFANet by applying the RFA framework in conjunction with the ESA block. Comprehensive benchmark evaluations with BI and BD degradation models well demonstrate the effectiveness of our RFANet in terms of both quantitative and visual results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) A chain of residual modules. A residual module consists of a residual block (RB) and an identity connection. (b) The residual feature aggregation (RFA) framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Basic architecture of a SR network. The red dotted rectangle represents the trunk part of the network, which consists of T base modules (BM).</figDesc><graphic url="image-2.png" coords="3,465.39,76.67,53.94,53.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 (Figure 3 .</head><label>13</label><figDesc>Figure 3. Left: A basic residual module. Right: Details of the RFA module, which contains 4 residual blocks (RB) and a 1×1 convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Left: The enhanced spatial attention (ESA) block. Right: Details of the ESA mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visual comparisons for ×4 SR with BI degradation model.</figDesc><graphic url="image-10.png" coords="6,288.20,379.85,52.03,52.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Visual comparisons for ×4 SR with BD degradation model.</figDesc><graphic url="image-60.png" coords="7,324.39,158.24,64.39,64.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. Average norms of filter weights. Each set of histograms corresponds to one RFA module. There are four blocks inside a RFA module. The histogram represents the norm of filter weights in the aggregation convolutional layer w.r.t. the feature map of each block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. PSNR vs. Parameters on Urban100 (×4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation results of different blocks combined with the RFA framework. We report the best PSNR (dB) values on Set5 (×4) in 4 × 10 5 iterations.</figDesc><table><row><cell>Residual Block Dense Block Channel Attention Block Spatial Attention Block Enhanced Spatial Attention Block Residual Feature Aggregation</cell><cell>√</cell><cell>√ √</cell><cell>√</cell><cell>√ √</cell><cell>√</cell><cell>√</cell><cell>√</cell><cell>√ √</cell><cell>√ √</cell><cell>√ √</cell></row><row><cell>PSNR</cell><cell>32.40</cell><cell>32.50</cell><cell>32.42</cell><cell>32.51</cell><cell cols="3">32.56 32.48 32.56</cell><cell>32.56</cell><cell>32.54</cell><cell>32.65</cell></row></table><note>Name EDSR-Baseline RFA-EDSR Dense-Baseline RFA-Dense CA SA ESA RFA-CA RFA-SA RFA-ESA (RFANet)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results with BI degradation model. Best and second best results are highlighted and underlined. SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM Bicubic ×2 33.66/0.9299 30.24/0.8688 29.56/0.8431 26.88/0.8403 30.80/0.9399 SRCNN [4] ×2 36.66/0.9542 32.45/0.9067 31.36/0.8879 29.50/0.8946 35.60/0.9663 FSRCNN [5] ×2 37.05/0.9560 32.66/0.9090 31.53/0.8920 29.88/0.9020 36.67/0.9710 VDSR [13] ×2 37.53/0.9590 33.05/0.9130 31.90/0.8960 30.77/0.9140 37.22/0.9750 ×2 38.27/0.9614 34.12/0.9216 32.41/0.9027 33.34/0.9384 39.44/0.9786 SAN [3] ×2 38.31/0.9620 34.07/0.9213 32.42/0.9028 33.10/0.9370 39.32/0.9792 RFANet (Ours) ×2 38.26/0.9615 34.16/0.9220 32.41/0.9026 33.33/0.9389 39.44/0.9783 Bicubic ×3 30.39/0.8682 27.55/0.7742 27.21/0.7385 24.46/0.7349 26.95/0.8556 SRCNN [4] ×3 32.75/0.9090 29.30/0.8215 28.41/0.7863 26.24/0.7989 30.48/0.9117 FSRCNN [5] ×3 33.18/0.9140 29.37/0.8240 28.53/0.7910 26.43/0.8080 31.10/0.9210 VDSR [13] ×3 33.67/0.9210 29.78/0.8320 28.83/0.7990 27.14/0.8290 32.01/0.9340 LapSRN [15] ×3 33.82/0.9227 29.87/0.8320 28.82/0.7980 27.07/0.8280 32.21/0.9350 MemNet [25] ×3 34.09/0.9248 30.01/0.8350 28.96/0.8001 27.56/0.8376 32.51/0.9369 EDSR [18] ×3 34.65/0.9280 30.52/0.8462 29.25/0.8093 28.80/0.8653 34.17/0.9476 SRMD [36] ×3 34.12/0.9254 30.04/0.8382 28.97/0.8025 27.57/0.8398 33.00/0.9403 NLRN [19] ×3 34.27/0.9266 30.16/0.8374 29.06/0.8026 27.93/0.8453 -/-RDN [40] ×3 34.71/0.9296 30.57/0.8468 29.26/0.8093 28.80/0.8653 34.13/0.9484 RCAN [38] ×3 34.74/0.9299 30.65/0.8482 29.32/0.8111 29.09/0.8702 34.44/0.9499 SAN [3] ×3 34.75/0.9300 30.59/0.8476 29.33/0.8112 28.93/0.8671 34.30/0.9494 RFANet (Ours) ×3 34.79/0.9300 30.67/0.8487 29.34/0.8115 29.15/0.8720 34.59/0.9506 Bicubic ×4 28.42/0.8104 26.00/0.7027 25.96/0.6675 23.14/0.6577 24.89/0.7866 SRCNN [4] ×4 30.48/0.8628 27.50/0.7513 26.90/0.7101 24.52/0.7221 27.58/0.8555 FSRCNN [5] ×4 30.72/0.8660 27.61/0.7550 26.98/0.7150 24.62/0.7280 27.90/0.8610 VDSR [13] ×4 31.35/0.8830 28.02/0.7680 27.29/0.7260 25.18/0.7540 28.83/0.8870 LapSRN [15] ×4 31.54/0.8850 28.19/0.7720 27.32/0.7270 25.21/0.7560 29.09/0.8900 MemNet [25] ×4 31.74/0.8893 28.26/0.7723 27.40/0.7281 25.50/0.7630 29.42/0.8942 EDSR [18] ×4 32.46/0.8968 28.80/0.7876 27.71/0.7420 26.64/0.8033 31.02/0.9148 SRMD [36] ×4 31.96/0.8925 28.35/0.7787 27.49/0.7337 25.68/0.7731 30.09/0.9024 NLRN [19] ×4 31.92/0.8916 28.36/0.7745 27.48/0.7346 25.79/0.7729 -/-DBPN [6] ×4 32.47/0.8980 28.82/0.7860 27.72/0.7400 26.38/0.7946 30.91/0.9137 RDN [40] ×4 32.47/0.8990 28.81/0.7871 27.72/0.7419 26.61/0.8028 31.00/0.9151 RCAN [38] ×4 32.63/0.9002 28.87/0.7889 27.77/0.7436 26.82/0.8087 31.22/0.9173 SAN [3] ×4 32.64/0.9003 28.92/0.7888 27.78/0.7436 26.79/0.8068 31.18/0.9169 RFANet (Ours) ×4 32.66/0.9004 28.88/0.7894 27.79/0.7442 26.92/0.8112 31.41/0.9187</figDesc><table><row><cell>Method PSNR/LapSRN [15] Set5 Scale 37.52/0.9591 33.08/0.9130 31.08/0.8950 30.41/0.9101 37.27/0.9740 Set14 BSD100 Urban100 Manga109 ×2 MemNet [25] 37.78/0.9597 33.28/0.9142 32.08/0.8978 31.31/0.9195 37.72/0.9740 ×2 EDSR [18] 38.11/0.9602 33.92/0.9195 32.32/0.9013 32.93/0.9351 39.10/0.9773 ×2 SRMD [36] 37.79/0.9601 33.32/0.9159 32.05/0.8985 31.33/0.9204 38.07/0.9761 ×2 NLRN [19] 38.00/0.9603 33.46/0.9159 32.19/0.8992 31.81/0.9246 -/-×2 DBPN [6] 38.09/0.9600 33.85/0.9190 32.27/0.9000 32.55/0.9324 38.89/0.9775 ×2 RDN [40] 38.24/0.9614 34.01/0.9212 32.34/0.9017 32.89/0.9353 39.18/0.9780 ×2 RCAN [38]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results with BD degradation model. Best and second best results are highlighted and underlined.</figDesc><table><row><cell>Method</cell><cell>Scale</cell><cell>Set5 PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM Set14 BSD100 Urban100 Manga109</cell></row><row><cell>Bicubic SPMSR [22] SRCNN [4] FSRCNN [5] VDSR [13] IRCNN [35] SRMD [36] RDN [40] SRFBN [17] RCAN [38] SAN [3] RFANet (Ours)</cell><cell>×3 ×3 ×3 ×3 ×3 ×3 ×3 ×3 ×3 ×3 ×3 ×3</cell><cell>28.78/0.8308 26.38/0.7271 26.33/0.6918 23.52/0.6862 25.46/0.8149 32.21/0.9001 28.89/0.8105 28.13/0.7740 25.84/0.7856 29.64/0.9003 32.05/0.8944 28.80/0.8074 28.13/0.7736 25.70/0.7770 29.47/0.8924 26.23/0.8124 24.44/0.7106 24.86/0.6832 22.04/0.6745 23.04/0.7927 33.25/0.9150 29.46/0.8244 28.57/0.7893 26.61/0.8136 31.06/0.9234 33.38/0.9182 29.63/0.8281 28.65/0.7922 26.77/0.8154 31.15/0.9245 34.01/0.9242 30.11/0.8364 28.98/0.8009 27.50/0.8370 32.97/0.9391 34.58/0.9280 30.53/0.8447 29.23/0.8079 28.46/0.8582 33.97/0.9465 34.66/0.9283 30.48/0.8439 29.21/0.8069 28.48/0.8581 34.07/0.9466 34.70/0.9288 30.63/0.8462 29.32/0.8093 28.81/0.8647 34.38/0.9483 34.75/0.9290 30.68/0.8466 29.33/0.8101 28.83/0.8646 34.46/0.9487 34.77/0.9292 30.68/0.8473 29.34/0.8104 28.89/0.8661 34.49/0.9492</cell></row><row><cell cols="3">ness of our RFANet, we compare RFANet with 12 state-</cell></row><row><cell cols="3">of-the-art image SR methods: SRCNN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows all the quantitative results with</cell></row><row><cell>BI model. In general, our RFANet can achieve compara-</cell></row><row><cell>ble or superior results compared with all the other meth-</cell></row><row><cell>ods including the extremely competitive RCAN and SAN.</cell></row><row><cell>Most quantitative results of RFANet are either the best or</cell></row><row><cell>the second best. For scale ×2, RFANet achieves the best reuslts on Set14, the best SSIM on Urban100 and the high-</cell></row><row><cell>est PSNR on Manga109. For scale ×3, RFANet outper-forms the other methods on all the datasets. Our RFANet</cell></row><row><cell>also has excellent performance with scale ×4, the best re-sults are achieved on Set5, B100, Urban100 and Manga109,</cell></row><row><cell>respectively. Compared with other methods, we found that</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Line</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SCA-CNN: spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6298" to="6306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8692</biblScope>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modulating image restoration with continual levels via adaptive feature modification layers</title>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11056" to="11064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta-sr: A magnification-arbitrary network for super-resolution</title>
		<author>
			<persName><forename type="first">Xuecai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1575" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Channel-wise and spatial feature modulation network for single image super-resolution</title>
		<author>
			<persName><forename type="first">Yanting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<idno>CoRR, abs/1809.11130</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5835" to="5843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feedback network for image superresolution</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1680" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="416" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A statistical prediction model based on sparse representations for single image super-resolution</title>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2569" to="2582" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4549" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">NTIRE 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A+: adjusted anchored neighborhood regression for fast superresolution</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">9006</biblScope>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6450" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ESRGAN: enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11133</biblScope>
			<biblScope unit="page" from="63" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6920</biblScope>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means and steering kernel regression</title>
		<author>
			<persName><forename type="first">Kaibing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4544" to="4556" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2808" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2226" to="2238" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Residual non-local attention networks for image restoration</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
