<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Neural News Recommendation with Unsupervised Preference Disentanglement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
							<email>hulinmei@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siyong</forename><surname>Xu</surname></persName>
							<email>xusiyong@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
							<email>leechen@bupt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
							<email>yangcheng@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<email>nanduan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xing.xie@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Neural News Recommendation with Unsupervised Preference Disentanglement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. Most existing methods usually learn the representations of users and news from news contents for recommendation. However, they seldom consider highorder connectivity underlying the user-news interactions.</p><p>Moreover, existing methods failed to disentangle a user's latent preference factors which cause her clicks on different news. In this paper, we model the usernews interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD. Our model can encode high-order relationships into user and news representations by information propagation along the graph. Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability. A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations. Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The amount of news and articles on many news platforms, such as Google News 1 , has been growing constantly at an explosive rate, making it difficult for users to seek for news that they are interested in.</p><p>In order to tackle the problem of information overload and meet the needs of users, news recommendation has been playing an increasingly important role for mining users' reading interest and providing personalized contents <ref type="bibr" target="#b11">(IJntema et al., 2010;</ref><ref type="bibr" target="#b17">Liu et al., 2010)</ref>.</p><p>A core problem in news recommendation is how to learn better representations of users and news. Recently, many deep learning based methods have been proposed to automatically learn informative user and news representations <ref type="bibr" target="#b20">(Okura et al., 2017;</ref><ref type="bibr" target="#b24">Wang et al., 2018)</ref>. For instance, DKN <ref type="bibr" target="#b24">(Wang et al., 2018)</ref> learns knowledge-aware news representation via multi-channel CNN and gets a representation of a user by aggregating her clicked news history with different weights. However, these methods <ref type="bibr" target="#b27">(Wu et al., 2019b;</ref><ref type="bibr" target="#b31">Zhu et al., 2019;</ref><ref type="bibr" target="#b0">An et al., 2019)</ref> usually focus on news contents, and seldom consider the collaborative signal in the form of high-order connectivity underlying the user-news interactions. Capturing high-order connectivity among users and news could deeply exploit structure characteristics and alleviate the sparsity, thus improving the rec-ommendation performance <ref type="bibr" target="#b25">(Wang et al., 2019)</ref>. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, the high-order relationship u 1 -d 1 -u 2 indicates the behavior similarity between u 1 and u 2 so that we may recommend d 3 to u 2 since u 1 clicked d 3 , while d 1 -u 2 -d 4 implies d 1 and d 4 may have similar target users.</p><p>Moreover, users may click different news due to their great diversity of preferences. The real-world user-news interactions arise from highly complex latent preference factors. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, u 2 might click d 1 under her preference to entertainment news, while chooses d 4 due to her interest in politics. When aggregating neighborhood information along the graph, different importance of neighbors under different latent preference factors should be considered. Learning representations that uncover and disentangle these latent preference factors can bring enhanced expressiveness and interpretability, which nevertheless remains largely unexplored by the existing literatures on news recommendation.</p><p>In this work, to address the above issues, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation Model with Unsupervised preference Disentanglement (GNUD). Our model is able to capture the high-order connectivities underlying the user-news interactions by propagating the user and news representations along the graph. Furthermore, the learned representations are disentangled by a neighborhood routing mechanism, which dynamically identifies the latent preference factors that may have caused the click between a user and news, and accordingly assigning the news to a subspace that extracts and convolutes features specific to that factor. To force each disentangled subspace to independently reflect an isolated preference, a novel preference regularizer is also designed to maximize the mutual information measuring dependency between two random variables in information theory to strengthen the relationship between the preference factors and the disentangled embeddings. It further improves the disentangled representations of users and news. To summarize, this work makes the following three contributions:</p><p>(1) In this work, we model the user-news interactions as a bipartite graph and propose a novel graph neural news recommendation model GNUD with unsupervised preference disentanglement. Our model improves the recommendation performance by fully considering the high-order connectivities and latent preference factors underlying the usernews interactions.</p><p>(2) In our model GNUD, a preference regularizer is designed to enforce each disentangled embedding space to independently reflect an isolated preference, further improving the quality of disentangled representations for users and news.</p><p>(3) Experimental results on real-world datasets demonstrate that our proposed model significantly outperforms state-of-the-art news recommendation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will review the related studies in three aspects, namely news recommendation, graph neural networks and disentangled representation learning.</p><p>News recommendation. Personalized news recommendation is an important task in natural language processing field, which has been widely explored in recent years. Learning better user and news representations is a central task for news recommendation. Traditional collaborative filtering (CF) based methods <ref type="bibr" target="#b23">(Wang and Blei, 2011)</ref> often utilize historical interactions between users and news to define the objective function for model training, aiming to predict a personalized ranking over a set of candidates for each user. They usually suffer from cold-start problem since news are often substituted frequently. Many works attempt to take advantage of rich content information, effectively improving the recommendation performance. For example, DSSM <ref type="bibr" target="#b10">(Huang et al., 2013</ref>) is a content-based deep neural network to rank a set of documents given a query. Some works <ref type="bibr" target="#b24">(Wang et al., 2018;</ref><ref type="bibr" target="#b31">Zhu et al., 2019)</ref> propose to improve news representations via external knowledge, and learn representations of users from their browsed news using an attention module. <ref type="bibr" target="#b27">Wu et al. (2019b)</ref> applied attention mechanism at both word-and news-level to model different informativeness on news content for different users. <ref type="bibr" target="#b26">Wu et al. (2019a)</ref> exploited different types of news information with an attentive multi-view learning framework. <ref type="bibr" target="#b0">An et al. (2019)</ref> considered both titles and topic categories of news, and learned both long-and shortterm user representations, while <ref type="bibr" target="#b28">Wu et al. (2019c)</ref> represented them by multi-head attention mechanism. However, these works seldom mine highorder structure information.</p><p>Graph neural networks. Recently, graph neu-ral networks (GNN) <ref type="bibr" target="#b16">(Kipf and Welling, 2016;</ref><ref type="bibr" target="#b6">Hamilton et al., 2017;</ref><ref type="bibr" target="#b22">Veličković et al., 2017)</ref> have received growing attentions in graph embedding because of its powerful representation learning based on node features and graph structure. <ref type="bibr" target="#b25">Wang et al. (2019)</ref> explored the GNN to capture high-order connectivity information in user-item graph by propagating embeddings on it, which achieves better performance on recommendation. However, existing news recommendation methods focus on, and rely heavily on news contents. Few news recommendation models consider the user-news interaction graph structure which encodes useful highorder connectivity information. <ref type="bibr" target="#b9">Hu et al. (2020)</ref> modeled the user-news interactions as a graph and proposed a graph convolution based model combining long-term and short-term interests, which demonstrates the effectiveness of exploiting the user-news interaction graph structure. Different from all these methods, in this work, we consider both the high-order connectivity information and latent preference factor underlying the user-news interactions. We propose a novel graph neural news recommendation model with unsupervised preference disentanglement. Disentangled representation learning. Disentangled representation learning aims to identify and disentangle different latent explanatory factors hidden in the observed data <ref type="bibr" target="#b1">(Bengio et al., 2013)</ref>, which has been successfully applied in the field of computer vision <ref type="bibr" target="#b12">(Kim and Mnih, 2018;</ref><ref type="bibr" target="#b3">Gidaris et al., 2018;</ref><ref type="bibr" target="#b8">Hsieh et al., 2018)</ref>. β−VAE <ref type="bibr" target="#b7">(Higgins et al., 2017)</ref> is a deep unsupervised generative approach that can automatically discover the independent latent factors of variation in unsupervised data, which is based on the VAE framework <ref type="bibr" target="#b15">(Kingma and Welling, 2013)</ref>. Recently, disentangled representation learning has been investigated on graph-structured data <ref type="bibr">(Ma et al., 2019a,b)</ref>. To the best of our knowledge, this is the first work to explore disentanglement in news recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>The news recommendation problem can be formalized as follows. Given the user-news historical interactions {(u, d)}, we aim to predict whether a user u i will click a candidate news d j that she has not seen before.</p><p>In this paper, for a news article d, we consider the title T and profile P (a given set of entities E and their corresponding entity types C from the news content) as features. The entities E and their corresponding entity types C are already given in the datasets. Each news title T consists of a word sequence T = {w 1 , w 2 , • • • , w m }. Each profile P contains a sequence of entities defined as</p><formula xml:id="formula_0">E = {e 1 , e 2 , • • • , e p } and corresponding entity types C = {c 1 , c 2 , • • • , c p }. We denote the title embedding as T = [w 1 , w 2 , • • • , w m ] T ∈ R m×n 1 , entity set embedding as E = [e 1 , e 2 , • • • , e p ] T ∈ R p×n 1 , and the entity-type set embedding as C = [c 1 , c 2 , • • • , c p ] T ∈ R p×n 2 .</formula><p>w, e and c are respectively the embedding vectors of word w, entity e, and entity type c. n 1 and n 2 are the dimension of word (entity) and entity-type embeddings. These embeddings can be pre-trained from a large corpus or randomly initialized. Following <ref type="bibr" target="#b31">(Zhu et al., 2019)</ref>, we define the profile embedding</p><formula xml:id="formula_1">P = [e 1 , g(c 1 ), e 2 , g(c 2 ), • • • , e p , g(c p )] T where P ∈ R 2p×n 1 . g(c) is the transformation function as g(c) = M c c, where M c ∈ R n 1 ×n 2 is a trainable transformation matrix.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Proposed Method</head><p>In this section, we first introduce the news content information extractor which learns a news representation h d from news content. Then we detail our proposed graph neural model GNUD with unsupervised preference disentanglement for news recommendation. Our model not only exploits the high-order structure information underlying the user-news interaction graph but also considers the different latent preference factors causing the clicks between users and news. A novel preference regularizer is also introduced to force each disentangled subspace independently reflect an isolated preference factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">News Content Information Extractor</head><p>We first describe how to obtain a news representation h d from news content including news title T and profile P . The content-based news representations would be taken as initial input embeddings of our model GNUD. Following DAN <ref type="bibr" target="#b31">(Zhu et al., 2019)</ref>, we use two parallel convolutional neural networks (PCNN) taking the title T and profile P of news as input to learn the title-level and profilelevel representation T and P for news. Finally we concatenate T and P, and get the final news representation h d through a fully connected layer f :</p><formula xml:id="formula_2">h d = f ([ T; P]).</formula><p>(1) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GNUD</head><p>To capture the high-order connectivity underlying the user-news interactions, we model the user-news interactions as a bipartite graph G = {U, D, E}, where U and D are the sets of users and news, E is the set of edges and each edge e = (u, d) ∈ E indicates that user u explicitly clicks news d. Our model GNUD enables information propagation among users and news along the graph, thus capturing the high-order relationships among users and news. Additionally, GNUD learns disentangled embeddings that uncover the latent preference factors behind user-news interactions, enhancing expressiveness and interpretability. In the following, we present one single graph covolution layer with preference disentanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Graph Convolution Layer with Preference Disentanglement</head><p>Given the user-news bipartite graph G where the user embedding h u is randomly initialized and news embedding h d is obtained with the news content information extractor (Section 4.1), a graph convolutional layer aims to learn the representation y u of a node u by aggregating its neighbors' features:</p><formula xml:id="formula_3">y u = Conv(h u , {h d : (u, d) ∈ E}).<label>(2)</label></formula><p>Considering that users' click behaviors could be caused by different latent preference factors, we propose to derive a layer Conv(•) such that the output y u and y d are disentangled representations.</p><p>Each disentangled component reflect one preference factor related to the user or news. The learned disentangled user and news embeddings can bring enhanced expressiveness and interpretability. Assuming that there are K factors, we would like to let y u and y d be composed of K independent components:</p><formula xml:id="formula_4">y u = [z u,1 , z u,2 , • • • , z u,K ], y d = [z d,1 , z d,2 , • • • , z d,K ], where z u,k and z d,k ∈ R l out K</formula><p>(1 ≤ k ≤ K) ( l out is the dimension of y u and y d ), respectively characterizing the k-th aspect of user u and news d related to the k-th preference factor. Note that in the following of this paper, we focus on user u and describe the learning process of its representation y u . The news d can be learned similarly, which is omitted. Formally, given a u-related node i ∈ {u} {d : (u, d) ∈ E}, we use a subspace-specific projection matrix W k to map the feature vector h i ∈ R l in into the k-th preference related subspace:</p><formula xml:id="formula_5">s i,k = ReLU(W k h i + b k ) ReLU(W k h i + b k ) 2 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">W k ∈ R l in × l out K , and b k ∈ R l out</formula><p>K . Note that s u,k is not equal to the final representation of the k-th component of u: z u,k , since it has not mined any information from neighboring news yet. To construct z u,k , we need to mine the information from both s u,k and the neighborhood features</p><formula xml:id="formula_7">{s d,k : (u, d) ∈ E}.</formula><p>The main intuition is that when constructing z u,k characterizing the k-th aspect of u, we should only use the neighboring news articles d which connect with user u due to the preference factor k instead of all the neighbors. In this work, we apply a neighborhood routing algorithm <ref type="bibr" target="#b18">(Ma et al., 2019a)</ref> to identify the subset of neighboring news that actually connect to u due to the preference factor k.</p><p>Neighborhood routing algorithm. The neighborhood routing algorithm infers the latent preference factors behind user-news interactions by iteratively analyzing the potential subspace formed by a user and her clicked news. The detail is illustrated in Algorithm 1. Formally, let r d,k be the probability that the user u clicks the news d due to the factor k. Then it's also the probability that we should use the news d to construct z u,k . r d,k is an unobserved latent variable which can be inferred in an iterative process. The motivation of the iterative process is as follows. Given z u,k , the value of the latent variables {r d,k : 1 ≤ k ≤ K, (u, d) ∈ E} can be obtained by measuring the similarity between user u and her clicked news d under the k-th subspace, which is computed as Eq. 4. Initially, we set z u,k = s u,k . On the other hand, after obtaining the latent variables {r d,k }, we can find an estimate of z u,k by aggregating information from the clicked news, which is computed as Eq. 5:</p><formula xml:id="formula_8">Algorithm 1 Neighborhood Routing Algorithm Require: s i,k , i ∈ {u} {d : (u, d) ∈ E}, 1 ≤ k ≤ K; Ensure: z u,k , 1 ≤ k ≤ K; 1: ∀k = 1, ...K, z u,k ← s u,k 2: for T iterations do 3: for d that satisfies (u, d) ∈ E do 4: ∀k = 1, • • • , K : r d,k ← z u,k s d,k 5: ∀k = 1, • • • , K : r d,k ← softmax(r d,k ) 6:</formula><p>end for 7:</p><formula xml:id="formula_9">for factor k = 1, 2, ...K do 8: z u,k ← s u,k + d:(u,d)∈E r d,k s d,k 9: z u,k ← z u,k / z u,k 2 10:</formula><p>end for 11: end for 12: return z u,k</p><formula xml:id="formula_10">r (t) d,k = exp(z u,k (t) s d,k ) K k =1 exp(z u,k (t) s d,k ) ,<label>(4)</label></formula><formula xml:id="formula_11">z (t+1) u,k = s u,k + d:(u,d)∈G r (t) d,k s d,k s u,k + d:(u,d)∈G r (t) d,k s d,k 2 ,<label>(5)</label></formula><p>where iteration t = 0, • • • , T − 1. After T iterations, the output z (T ) u,k is the final embedding of user u in the k-th latent subspace and we obtain</p><formula xml:id="formula_12">y u = [z u,1 , z u,2 , • • • , z u,K ].</formula><p>The above shows a single graph convolutional layer with preference disentanglement, which aggregates information from the first-order neighbors. In order to capture information from high-order neighborhood and learn high-level features, we stack multiple layers. Specially, we use L layers and get the final disentangled representation y (L) u ∈ R K∆n (K∆n = l out ) for user u and y (L) d for news d, where ∆n is the dimension of a disentangled subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Preference Regularizer</head><p>Naturally, we hope each disentangled subspace can reflect an isolated latent preference factor independently. Since there are no explicit labels indicating the user preferences in the training data, a novel preference regularizer is also designed to maximize the mutual information measuring dependency between two random variables in information theory to strengthen the relationship between the preference factors and the disentangled embeddings.</p><p>According to <ref type="bibr" target="#b30">(Yang et al., 2018)</ref>, the mutual information maximization can be converted to the following form.</p><p>Given the representation of a user u in k-th (1 ≤ k ≤ K) latent subspace, the preference regularizer P (k|z u,k ) estimates the probability of the k-th subspace (w.r.t. the k-th preference) that z u,k belongs to:</p><formula xml:id="formula_13">P (k|z u,k ) = softmax(W p • z u,k + b p ),<label>(6)</label></formula><p>where W p ∈ R K×∆n , and parameters in the regularizer P (•) are shared with all the users and news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>Finally, we add a fully-connected layer, i.e., L+1) , where W (L+1) ∈ R K∆n×K∆n , b (L+1) ∈ R K∆n . We use the simple dot product to compute the news click probability score, which is computed as ŝ u, d = y u y d .</p><formula xml:id="formula_14">y u = W (L+1) y (L) u + b (</formula><p>Once obtaining the click probability scores ŝ u, d , we define the following base loss function for training sample (u, d) with the ground truth y u,d :</p><formula xml:id="formula_15">L 1 = −[y u,d ln(ŷ u,d ) + (1 − y u,d ) ln(1 − ŷu,d )],</formula><p>(7) where ŷu,d = σ(ŝ u, d ).</p><p>Then we add the preference regularization term of both u and d, which can be written as:</p><formula xml:id="formula_16">L 2 = − 1 K K k=1 i∈{u,d} ln P (k|z i,k )[k].<label>(8)</label></formula><p>The overall training loss can be rewritten as:</p><formula xml:id="formula_17">L = (u,d)∈T train ((1 − λ)L 1 + λL 2 ) + η Θ ,<label>(9)</label></formula><p>where T train is training set. For each positive sample (u, d), we sample a negative sample from unobserved reading history of u for training. λ is a balance coefficient. η is the regularization coefficient and Θ denotes all the trainable parameters. Note that during training and testing, the news that have not been read by any users are taken as isolated nodes in the graph. Their representations are based on only content feature h d without neighbor aggregation, and can also be disentangled via Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Experimental Settings</head><p>Datasets. We conduct experiments on the realworld online news datasets Adressa <ref type="bibr" target="#b4">(Gulla et al., 2017)</ref> 2 from a Norwegian news portal to evaluate our model. We use two datasets named Adressa-1week and Adressa-10week, which respectively collect news click logs as long as 1 week and 10 weeks. Following DAN <ref type="bibr" target="#b31">(Zhu et al., 2019)</ref>, we just select user id, news id, time-stamp, the title and profile of news to build our datasets, and preprocess the data by removing the stopwords in the news content. The statistics of our final datasets are shown in Table <ref type="table">1</ref>.</p><p>For the Adressa-1week dataset, we use the first 5 days' historical data for the construction of usernews bipartite graph. The 6-th day's is used to build training samples: {(u, d)}. 20% randomly sampled from the last day's are for validation and the remaining are regarded as test set. Note that during testing, we reconstruct the graph with all the previous 6 days' historical data. Similarly, for the Adressa-10week dataset, we construct the graph with the first 50 days' data, the following 10 days are served to generate training pairs, 20% of the last 10 days' for validation data and 80% for test. Note that, for the baselines, we also use the data from the first 5 (50) days for constructing user's historical data, the following 1 (10) days is used to generate training pairs. The validation and test set constructed with the last 1 (10) days are also the same for all the models.</p><p>Experimental settings. In our experiments, the dimension of word/entity embeddings and entity type embeddings is set as n 1 = n 2 = 50, and the dimension of input user and news embeddings l in is set as 128. The embeddings of words, entities, entity types and users are randomly initialized with a Gaussian distribution N (0, 0.1). In our methods, due to the large scale of the datasets, we sample a fixed-size set of neighbors (size = 10) for a user, and we set size = 30 for a news, according to the average degree of users and news respectively. The number of latent preference factors is K = 7, and the dimension of each disentangled subspace is ∆n = 16. The number of graph convolution layers is set to 2. The dropout rate is 0.5. The balance coefficient λ is set as 0.004. We test our model with different value of λ ranging from 0.001 ) is applied for model optimization, and the learning rate is 0.0005. The batch size is set to 128. These hyper-parameters were all selected according to the results on validation set.</p><p>It is worth noting that our model can deal with new coming news documents that have not previously existed in the user-news interaction graph G during training or testing. Our model takes these news documents as isolated nodes in the graph G. Their representations are based on only content feature h d without neighbor aggregation, and can also be disentangled via Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Evaluation</head><p>We evaluate the performance of our model GNUD by comparing it with the following state-of-the-art baseline methods:</p><p>LibFM (Rendle, 2012), a feature-based matrix factorization method, with the concatenation of TF-IDF vectors of news title and profile as input.</p><p>CNN <ref type="bibr" target="#b13">(Kim, 2014)</ref>, applying two parallel CNNs to word sequences in news titles and profiles respectively and concatenate them as news features. The user representation is learned from the user's news history.</p><p>DSSM <ref type="bibr" target="#b10">(Huang et al., 2013)</ref>, a deep structured semantic model. In our experiments, we model the user's clicked news as the query and the candidate news as the documents.</p><p>Wide &amp; Deep <ref type="bibr" target="#b2">(Cheng et al., 2016)</ref>, a deep model for recommendation which combines a (Wide) linear model and (Deep) feed-forward neural network. We also use the concatenation of news title and profile embeddings as features.</p><p>DeepFM <ref type="bibr" target="#b5">(Guo et al., 2017)</ref>, a general model that combines factorization machines and deep neural networks that share the input. We use the same input as Wide &amp; Deep for DeepFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Adressa DMF <ref type="bibr" target="#b29">(Xue et al., 2017)</ref>, a CF based deep matrix factorization model without considering the news content.</p><p>DKN <ref type="bibr" target="#b24">(Wang et al., 2018)</ref>, a deep content based news recommendation framework fusing semanticlevel and knowledge-level representations. We model the news title and profile as semantic-level and knowledge-level representations, respectively.</p><p>DAN <ref type="bibr" target="#b31">(Zhu et al., 2019)</ref>, a deep attention neural network for news recommendation which can capture the dynamic diversity of news and user's interests, and consider the users' click sequence information.</p><p>GNewsRec <ref type="bibr" target="#b9">(Hu et al., 2020)</ref>, a graph neural network based method combining long-term and short term interest modeling for news recommendation.</p><p>All the baselines are initialized as the corresponding papers, and in terms of neural network models we use the same word embedding dimension for fair comparison. Then they are carefully tuned to achieve their optimal performance. We independently repeat each experiment for 10 times and report the average performance.</p><p>Result analysis. The comparisons between different methods are summarized in Table <ref type="table" target="#tab_1">2</ref>. We can observe that our proposed model GNUD consistently outperforms all the state-of-the-art baseline methods on both datasets. GNUD improves the best deep neural models DKN and DAN more than 6.45% on AUC and 7.79% on F1 on both datasets. The main reason is that our model fully exploits the high-order structure information in the user-news interaction graph, learning better representations of users and news. Compared to the best-performed baseline method GNewsRec, our model GNUD achieves better performance on both datasets in terms of both AUC (+2.85% and +4.59% on the two datasets, respectively) and F1 (+1.05% and +0.08%, respectively). This is because that our model considers the latent preference factors that cause the user-news interactions and learns representations that uncover and disentangle these latent preference factors, which enhance expressiveness.</p><p>From Table <ref type="table" target="#tab_1">2</ref>, we can also see that all the contentbased methods outperform the CF based model DMF. This is because CF based methods suffer a lot from cold-start problem since most news are new coming. Except for DMF, all the deep neural network based baselines (e.g., CNN, DSSM Wide&amp;Deep, DeepFM, etc.) significantly outperform LibFM, which shows that deep neural models can capture more implicit but informative features for user and news representations. DKN and DAN further improve other deep neural models by incorporating external knowledge and applying a dynamic attention mechanism.</p><p>Comparison of GNUD variants. To further demonstrate the efficacy of the design of our model GNUD, we compare among the variants of our model. As we can see from the last three lines in Table <ref type="table" target="#tab_1">2</ref>, when the preference disentanglement is removed, the performance of the model GNUD w/o Disen (GNUD without preference disentanglement) drops largely by 5.68% and 4.97% in terms of AUC on the two datasets (4.81% and 0.51% on F1), respectively. This observation demonstrates the effectiveness and necessity of preference disentangled representations of users and news. Com-  pared to GNUD w/o PR (GNUD without preference regularizer), we can see that introducing the preference regularizer which enforces each disentangled embedding subspace independently reflect an isolated preference, can bring performance gains on both AUC (+0.89% and +2.6%, respectively) and F1 (+2.23% and +0.17%, respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study</head><p>To intuitively demonstrate the efficacy of our model, we randomly sample a user u and extract her logs from the test set. The representation of user u is disentangled into K = 7 subspaces and we randomly sample 2 subspaces. For each one, we visualize the top news that user u pay most attention to (with the probability r d,k larger than a threshold). As shown in Figure <ref type="figure" target="#fig_2">3</ref>, different subspaces relect different preference factors. For example, one subspace (shown in blue) is related to "energy" as the top two news contain the keywords such as "oil industry", "hygen" and "wind power". The other subspace (shown in green) may indicate the latent preference factor about "healthy diet" as the related news contain the keywords such as "health", "vitamin" and "vegetables".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parameter Analysis</head><p>In this section, we examine how different choices of some hyper-parameters affect the performance of GNUD.</p><p>Analysis of layer numbers. We investigate whether GNUD can benefit from multiple embedding propagation layers. We vary the layer numbers in the range of {1, 2, 3} on both datasets. As we can see in Table <ref type="table" target="#tab_3">3</ref>, GNUD-2 (2 layers) is superior to others. The reason is that GNUD-1 considers the first-order neighbors only, while using over 2 layers may lead to overfitting, which indicates that applying a too deep architecture might bring noise to the representations in news recommendation task. Therefore, GNUD-2 is regarded as the most suitable choice.</p><p>Number of latent preference factors. We fix the dimension of each latent preference subspace as 16 and check the impact of the number K of latent preference factors. As shown in Figure <ref type="figure" target="#fig_3">4</ref> (a), we can find that with the increase of K, the performance first grows, reaching the best at K=7, and then begins to drop. Thus we set K=7 in our experiments.</p><p>Number of routing iterations. We study the performance with different number of routing iterations. As shown in Figure <ref type="figure" target="#fig_3">4</ref> (b), we can see that our model generally gets better performance with more routing iterations and finally achieves convergence after 7 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we consider the high-order connectivity as well as the latent preference factors underlying the user-news interactions, and propose a novel graph neural news recommendation model GNUD with unsupervised preference disentanglement. Our model regards the user-news interactions as a bipartite graph and encode high-order relationships among users and news by graph convolution. Furthermore, the learned representations are disentangled with different latent preference factors by a neighborhood routing mechanism, enhancing expressiveness and interpretability. A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, further improving the quality of user and news embeddings. Experimental results on realworld news datasets demonstrate that our model achieves significant performance gains compared to state-of-the-art methods, supporting the importance of exploiting the high-order connectivity and disentangling the latent preference factors in user and news representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of user-news interaction graph and high-order connectivity. The representations of user and news are disentangled with latent preference factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our proposed model GNUD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of a user's clicked news which belong to different disentangled subspaces w.r.t. different preference factors. We use six keywords (translated into English) to illustrate a news.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Influence of different number of preference factors and routing iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The performance of different methods on news recommendation.</figDesc><table><row><cell></cell><cell></cell><cell>-1week</cell><cell cols="2">Adressa-10week</cell></row><row><cell></cell><cell>AUC</cell><cell>F1</cell><cell>AUC</cell><cell>F1</cell></row><row><cell>LibFM</cell><cell cols="4">61.20±1.29 59.87±0.98 63.76±1.05 62.41±0.72</cell></row><row><cell>CNN</cell><cell cols="4">67.59±0.94 66.33±1.44 69.07±0.95 67.78±0.69</cell></row><row><cell>DSSM</cell><cell cols="4">68.61±1.02 69.92±1.13 70.11±1.35 70.96±1.56</cell></row><row><cell>Wide&amp;Deep</cell><cell cols="4">68.25±1.12 69.32±1.28 73.28±1.26 69.52±0.83</cell></row><row><cell>DeepFM</cell><cell cols="4">69.09±1.45 61.48±1,31 74.04±1.69 65.82±1.18</cell></row><row><cell>DMF</cell><cell cols="4">55.66±0.84 56.46±0.97 53.20±0.89 54.15±0.47</cell></row><row><cell>DKN</cell><cell cols="4">75.57±1.13 76.11±0.74 74.32±0.94 72.29±0.41</cell></row><row><cell>DAN</cell><cell cols="4">75.93±1.25 74.01±0.83 76.76±1.06 71.65±0.57</cell></row><row><cell>GNewsRec</cell><cell cols="4">81.16±1.19 82.85±1.15 78.62±1.38 81.01±0.64</cell></row><row><cell cols="5">GNUD w/o Disen 78.33±1.29 79.09±1.22 78.24±0.13 80.58±0.45</cell></row><row><cell>GNUD w/o PR</cell><cell cols="4">83.12±1.53 81.67±1.56 80.61±1.07 80.92±0.31</cell></row><row><cell>GNUD</cell><cell cols="4">84.01±1.16 83.90±0.58 83.21±1.91 81.09±0.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The news d 3 about home has low probability in the both subspaces. It does not belong to any of the two preferences.</figDesc><table><row><cell>Methods</cell><cell cols="2">Adressa-1week Adressa-10week AUC F1 AUC F1</cell></row><row><cell cols="2">GNUD-1 80.96 79.86 82.22</cell><cell>80.61</cell></row><row><cell cols="2">GNUD-2 84.01 83.90 83.21</cell><cell>81.09</cell></row><row><cell cols="2">GNUD-3 84.03 82.18 83.05</cell><cell>80.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The performance of GNUD with different layer numbers.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Natural Science Foundation of China (No. U1936220, 61806020, 61772082, 61972047, 61702296), the National Key Research and Development Program of China (2018YFB1402600), the CCF-Tencent Open Fund, and the Fundamental Research Funds for the Central Universities. We also acknowledge the valuable comments from Jianxun Lian at Microsoft Research Asia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural news recommendation with long-and short-term user representations</title>
		<author>
			<persName><forename type="first">Mingxiao</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DLRS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The adressa dataset for news recommendation</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Atle Gulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Özlem</forename><surname>Özgöbek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomeng</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1042" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepfm: a factorizationmachine based neural network for ctr prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph neural news recommendation with long-term and short-term interest modeling</title>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">102142</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ontology-based news recommendation</title>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Ijntema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Goossen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Hogenboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT/ICDT Workshops</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2654" to="2663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Personalized news recommendation based on click behavior</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elin</forename><forename type="middle">Rønby</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IUI</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for recommendation</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="5712" to="5723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Embedding-based news recommendation for millions of users</title>
		<author>
			<persName><forename type="first">Shumpei</forename><surname>Okura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukihiro</forename><surname>Tagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Tajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1933" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dkn: Deep knowledge-aware network for news recommendation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1835" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural news recommendation with attentive multiview learning</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxiao</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="3863" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Npa: Neural news recommendation with personalized attention</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxiao</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="2576" to="2584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural news recommendation with multi-head selfattention</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019c</date>
			<biblScope unit="page" from="6390" to="6395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep matrix factorization models for recommender systems</title>
		<author>
			<persName><forename type="first">Hong-Jian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stylistic chinese poetry generation via unsupervised style disentanglement</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dan: Deep attention neural network for news recommendation</title>
		<author>
			<persName><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeliang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5973" to="5980" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
