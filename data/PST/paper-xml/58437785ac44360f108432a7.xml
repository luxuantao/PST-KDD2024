<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video-Based Emotion Recognition using CNN-RNN and C3D Hybrid Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yin</forename><surname>Fan</surname></persName>
							<email>fanyin@qiyi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">iQIYI Co. Ltd</orgName>
								<address>
									<postCode>10080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangju</forename><surname>Lu</surname></persName>
							<email>luxiangju@qiyi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">iQIYI Co. Ltd</orgName>
								<address>
									<postCode>10080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dian</forename><surname>Li</surname></persName>
							<email>lidian@qiyi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">iQIYI Co. Ltd</orgName>
								<address>
									<postCode>10080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanliu</forename><surname>Liu</surname></persName>
							<email>liuyuanliu@qiyi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">iQIYI Co. Ltd</orgName>
								<address>
									<postCode>10080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video-Based Emotion Recognition using CNN-RNN and C3D Hybrid Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2993148.2997632</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts Computing methodologies ~ Activity recognition and understanding Emotion Recognition</term>
					<term>Recurrent Neural Network</term>
					<term>Long Short Term Memory network</term>
					<term>3D convolutional Network</term>
					<term>Model Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a video-based emotion recognition system submitted to the EmotiW 2016 Challenge. The core module of this system is a hybrid network that combines recurrent neural network (RNN) and 3D convolutional networks (C3D) in a late-fusion fashion. RNN and C3D encode appearance and motion information in different ways. Specifically, RNN takes appearance features extracted by convolutional neural network (CNN) over individual video frames as input and encodes motion later, while C3D models appearance and motion of video simultaneously. Combined with an audio module, our system achieved a recognition accuracy of 59.02% without using any additional emotion-labeled video clips in training set, compared to 53.8% of the winner of EmotiW 2015. Extensive experiments show that combining RNN and C3D together can improve video-based emotion recognition noticeably.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The EmotiW challenge has been successfully held for four years since 2013 and has made great influences in the emotion recognition area. Previous winners usually focus on facial graph analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> or designing specific CNN-RNN networks <ref type="bibr" target="#b4">[5]</ref>. Such classifier that takes video sequences as input is becoming more and more important for video event detection or prediction.</p><p>Traditional convolutional neural networks have a major limitation that they just handle spatial information. For example, in the EmotiW 2014 winner's work <ref type="bibr" target="#b3">[4]</ref>, all video frames are extracted from videos and regarded as the static images for further process. The aggregating image features of each video form a feature vector which neglects the important temporal video information.</p><p>Recently, deep 3-dimensional convolutional networks (C3D) made great progresses in dealing with various video analysis tasks. C3D can model appearance and motion information simultaneously and the C3D features with a linear classifier can achieve good performance on different video analysis benchmarks <ref type="bibr" target="#b5">[6]</ref>.</p><p>As for task of video-based emotion recognition, few work based on C3D structures is given in existing papers. We first take experiments to classify emotions based on C3D structures and we find that such C3D networks have a good performance in emotion recognition. The promising results inspire us to explore further experiments, including the experiments of hybridizing to LSTM models. As a result, such hybrid networks give the competitive results for emotion classification. The main contribution of the paper is the proposed hybrid CNN-RNN and C3D network 1 .</p><p>A particular type of recurrent neural networks, the Long Short-Term Memory (LSTM) recurrent neural network is widely adopted <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>. LSTM has memory ability and suits for processing sequences with contexts well. An encoder LSTM can be used to map an input sequence into a fixed length vector representation. This vector representation is decoded using single decoder LSTM or multiple decoder LSTMs to handle different tasks such as emotion classification. Recently, LSTMs are also well developed and shown to be efficient to deal with various sequences versus sequences problems, such as audio analysis <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, video captioning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, video action recognition <ref type="bibr" target="#b21">[22]</ref>, and so on.</p><p>Unlike C3D networks, a few works are given for video-based emotion recognition using CNN or RNN structures in recent papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref>. Such deep networks reach top competitive results in the history of EmotiW challenges <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref>. Therefore we take similar LSTM models in our work while applying C3D structures at the same time. Without using any additional videos with emotion labels in our training set, the hybrid approach outperforms the state-of-the-art results by a significant margin. We achieved a recognition accuracy of 59.02%, compared to an accuracy of 53.8% as the winner of EmotiW 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE PROPOSED METHOD</head><p>The overview of the system is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The hybrid network has two core parts: CNN-RNN and C3D. We also add an audio classifier into the system. In this section we first introduce LSTM, which is a CNN-features-based spatio-temporal RNN model. Then we describe the properties of C3D structure, which can be directly used as a spatio-temporal model. At last, we give the details of building a hybrid classifier on LSTMs and C3Ds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LSTM -A Special RNN</head><formula xml:id="formula_0">t xh t hh t h h g W x W h b     () t hz t z z g W h b  ,</formula><p>where g is the hidden activation function, such as a sigmoid or hyperbolic tangent, and t h is the hidden state.</p><p>Although RNNs have been widely used in many tasks such as handwriting recognition <ref type="bibr" target="#b16">[17]</ref> or speech recognition <ref type="bibr" target="#b17">[18]</ref>, they have difficulties in learning long-term dependencies due to the vanishing and exploding gradient problem. A Long Short Term Memory (LSTM) network is a special kind of RNN which is capable of addressing this long-term dependencies.</p><p>The core of LSTM networks is that it can remember a value for an arbitrary length of time. A LSTM unit is equipped with gates that determine when the input is significant enough to remember, when the unit should continue to remember or forget the value, and when the unit should output the value. We use LSTM unit described in <ref type="bibr" target="#b7">[8]</ref> in our method (Figure <ref type="figure" target="#fig_3">2</ref>), which iterates between the following operations: </p><formula xml:id="formula_1">t xi t hi t i i W x W h b      1 () t xf t hf t f f W x W h b      1 () t xo t ho t o o W x W h b      1 () t xc t hc t c g W x W h b      tt cf  1 t ci   t g tt ho  () t c  ,</formula><p>where  is the sigmoid function, and  is the hyperbolic tangent function.</p><p>We use a similar framework with the one described in <ref type="bibr" target="#b7">[8]</ref>, which combines LSTMs with deep convolutional networks to train a model spatially and temporally for video sequences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">C3D -A Direct Spatio-Temporal Model</head><p>C3D can be understood as a 3D convolution on three channels. In such way, the results of C3D transform can be used as features for many tasks.</p><p>In our work, unlike CNN-RNN framework, C3D networks have a useful attribute that models both appearance and motion simultaneously.</p><p>In 2D ConvNets, convolution and pooling operations are only spatially applied to 2D static images. While in our 3D ConvNets, the operations are performed spatio-temporally by adding an additional time dimension <ref type="bibr" target="#b5">[6]</ref>. Hence such C3D networks preserve the temporal information of the input signals, resulting in a more distinctive result. The 2D and 3D convolutional frameworks are shown in Figure <ref type="figure">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hybrid CNN-RNN and C3D Networks</head><p>Previous work shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. And we found that the CNN-RNN and C3D hybrid network can further improve the performance.</p><p>Considering that audio modality can bring ~3% gain in recognition accuracy <ref type="bibr" target="#b2">[3]</ref>, we also trained a SVM with the linear kernel using audio features extracted with the OpenSmile toolkit <ref type="bibr" target="#b6">[7]</ref>.</p><p>The CNN-RNN, C3D and audio SVM model were trained separately and their predication scores were combined into the final score. For each category, the prediction score obtained from different models are fused by a weighted summation. We select the parameters</p><formula xml:id="formula_2">CNN RNN W  , 3 CD</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W and</head><p>Audio W according to the performance on the validation set. The category with the highest score is taken to be the final recognition result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We participate in the EmotiW 2016 challenge and mainly evaluate the proposed method on the AFEW 6.0 database <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In this section, the EmotiW 2016 challenge is briefly introduced and the implementation details of our method are given. Then the performance of our method is compared with the state-of-the-art. And we also test the sub-modules of our system, including different CNN features as inputs of LSTMs, and different parameters of LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The EmotiW 2016 Challenge</head><p>The fourth Emotion Recognition in the Wild (EmotiW) 2016 challenge <ref type="bibr" target="#b26">[27]</ref> has two sub-challenges: (1) video based emotion recognition on the AFEW 6.0 database <ref type="bibr" target="#b0">[1]</ref>; (2) group level emotion recognition on the HAPPEI database <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>We only participate in the video-based emotion recognition sub-challenge. For this dataset, each video is labeled with one of seven emotions, that is, anger, disgust, fear, happiness, sad, surprise, and neutral. The task of the challenge is to assign an emotional label to each video in the test set. The challenge succeeds EmotiW 2013-15, and the major change in this year is the newly added reality TV videos in the test set.</p><p>In total, the AFEW 6.0 database contains 1750 video clips which are divided into three parts: 774 for training, 383 for validation, and 593 for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>As for data pre-processing, all faces of video frames are extracted and aligned using a similarity transform according to the facial key points. Additionally, for non-faces frames falsely detected by the face detector, a CNN-based face filter is applied for filtering out the mistakes.</p><p>For the CNN-RNN network used in the paper, the CNN features of the faces are taken from fc6 layer of VGG16-Face model fine-tuned with FER2013 face emotion database <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>. For each iteration of RNN training, 16 face features are randomly selected as inputs. One layer LSTM network is applied with 128 embedding outputs.</p><p>For the C3D network, a series of a length of 16 sequent faces for each video clip is chosen as the inputs. The C3D net has 8 convolutions, 5 max-poolings, and 2 fully connected layers, followed by a softmax output layer. Other parameters are similar to <ref type="bibr" target="#b5">[6]</ref>. The specific C3D structure used in our implementation is shown in Figure <ref type="figure" target="#fig_5">4</ref>. In the test stage, a series of 16 sequences is taken for each video by a stride of 8 frames. If a video clip doesn't have enough frames, we pad the last frame to get only one segment. The average score of all series of the video is the final reference of the prediction label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results of Submissions</head><p>The challenge allows 8 times submissions in total. For all the submissions, we use one audio model all the same as stated in section 2.3. And a different number of CNN-RNN models and C3D models are merged together. The results of our submissions are shown in Table <ref type="table" target="#tab_0">1</ref>.  <ref type="table" target="#tab_0">1</ref>, we can find that the more C3D models are taken to merge, a higher accuracy can be obtained. Due to the (c) Submission 1 on validation set Without audio information, a best single CNN-RNN model achieves an accuracy of 45.43%, while a single C3D can reach only 39.69%. An accuracy of 48.30% can be reached if the two models are fused. Also, we can find that the recognition accuracy is increased by adding more validation data in training phase. The final recognition accuracy is 59.02%, outperforming the baseline 40.47% and the 2015 winner team's final accuracy 53.80%. The confusion matrices of our submissions on the validation and testing sets are given in Figure <ref type="figure" target="#fig_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CNN-RNN Classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Pre-processing of Video Frames</head><p>If all the original video frames are taken as input directly, a poor distinctive ability is obtained for all kinds of emotions, with an average accuracy around 20%.</p><p>Optical flow is claimed to be a crucial component of video classification approaches. We additionally train our temporal models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The performance of the system is around 25% for emotion classification, better than only using frames as inputs.</p><p>A face detector and face similarity transform is applied to preprocess video frames. Faces of video frames are detected by Viola-Jones cascades <ref type="bibr" target="#b27">[28]</ref> and falsely detected non-face frames are filtered out by a face model trained based on CNN networks. Then face alignments are realized by a similarity transform based on facial point landmarks. The system takes aligned faces as inputs, give a competitive result. Also, optical flow of faces are tested following <ref type="bibr" target="#b20">[21]</ref>, but no improvements are obtained for emotion classification.</p><p>Note that, the visual frames that are inputs of the hybrid network refer to aligned faces that are already analyzed and extracted from original video frame sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Pre-training model / CNN Feature Extractor</head><p>As fine-tuning is widely adopted and proved to be very efficient to re-train a pre-training model using limited data for a special To select a basis for the fine-tuned models, several mainstream CNN architectures trained with ImageNet are examined, including CaffeNet <ref type="bibr" target="#b9">[10]</ref>, GoogLeNet <ref type="bibr" target="#b10">[11]</ref>, VGG <ref type="bibr" target="#b12">[13]</ref>, and Residual Network <ref type="bibr" target="#b11">[12]</ref>, which perform well for general objects recognition. Deep-face VGG model (VGG-FACE) <ref type="bibr" target="#b13">[14]</ref> is tested for comparisons with VGG model trained on ImageNet. The VGG-FACE model is trained on face images only, which is originally used for a face verification task. We also evaluate DCN network architecture described in <ref type="bibr" target="#b18">[19]</ref> without any pre-training. The faces are resized to a fixed size of 256*256, which is the same as the size of the pre-trained ImageNet and VGG-FACE models. All the results of emotion fine-tuning performances from pre-training CNNs are shown in Table <ref type="table" target="#tab_1">2</ref>. In Table <ref type="table" target="#tab_1">2</ref>, it's surprising to see that GoogLeNet works worse than all other network architectures. Probably the inception style network is not suitable for emotion recognition. Most of the models pre-trained using ImageNet database outperform the model without any pre-training due to the per-trained models providing a good initialization. Meanwhile, the model after pretraining using face database (VGG-FACE) achieves the best recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">LSTM Parameters</head><p>In this subsection, CNN features of video faces based on the fine-tuned VGG16-FACE model are adopted. Such features are aggregated as LSTMs' inputs. LSTMs' hidden units, and so on. The comparison on performance can be seen in Table <ref type="table" target="#tab_2">3</ref>.</p><p>From Table <ref type="table" target="#tab_2">3</ref>, we can find that with less number of embedded outputs, the LSTM net reaches a much better accuracy. That is, a simpler structure has a better classification ability. The reason may be that the training data is too limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">C3D Network</head><p>Considering that in <ref type="bibr" target="#b2">[3]</ref>, the images with two face scales are used to further improve the performance, we adopt two kinds of face images, the face described in 3.4.1 (Face-ours) and the face provided in AFEW6.0 (Face-AFEW). We fine-tuned the pretrained sport1m <ref type="bibr" target="#b19">[20]</ref> C3D model with these two kinds of face images, respectively. The faces are resized to a fixed size of 128*171, which is the same as the size of the pre-trained sport1m C3D model. Similar to LSTM classifier, the input of C3D was fixed to be a sequence of 16 frames. Table <ref type="table" target="#tab_3">4</ref> shows the comparative results. We can find that the performances of C3D network are similar when different scales of faces are used for training, and the better accuracies can be reached when C3D and CNN-RNN models are merged. Moreover, when combining the models of two kinds of face images, the accuracy is further increased. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we proposed a method for video-based emotion recognition in the wild. We used CNN-LSTM and C3D networks to simultaneously model video appearances and motions. Especially, we found that the combination of the two kinds of networks can give impressive results, which demonstrated the effectiveness of the method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The overview of the system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>general Recurrent Neural Network (RNN) can model temporal information by transforming a sequence of inputs to a sequence of outputs. Given an input sequence 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A simple LSTM block with only input, output, and forget gates</figDesc><graphic url="image-11.png" coords="2,341.30,475.55,193.30,111.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) 2D convolution on multiple frames (b) 3D convolution on multiple frames Figure 3. 2D and 3D convolutions. The input size of the frames is {H*W*L}. For the convolution layers, the kernel size is {K*K}, and the kernel temporal depth is d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. C3D architecture.</figDesc><graphic url="image-13.png" coords="3,80.15,201.50,187.70,70.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Confusion matrices on AFEW6.0 dataset. computational complexities and the limited time for training, only a maximal number of three C3D models are tested. Interestingly, things are different for CNN-RNN network models. There is almost no difference in accuracy when taking one or more CNN-RNN models. The details of different CNN-RNN and C3D models are given in section 3.4 and 3.5.</figDesc><graphic url="image-16.png" coords="4,54.00,267.50,238.30,174.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) A Single VGG-LSTM model on validation set (b)A Single C3D model on validation set (d) Submission 6 on testing set task, we use the emotional data to fine-tune pre-training models for face CNN features. Here the Facial Expression Recognition database (FER2013) [16] was used for CNN pre-training. The FER2013 database consists of 35889 images: 28709 for training, 3589 for public test, and 3589 for private test. We use training data and public test data for training, and evaluate the model performance with private test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Submissions on the validation and testing sets</head><label>1</label><figDesc></figDesc><table><row><cell>Sub</cell><cell>Val</cell><cell>Test</cell><cell>Models 1Audio+</cell><cell>TrainingData</cell></row><row><cell>1</cell><cell>51.96</cell><cell>56.16</cell><cell>2 CNN-RNNs + 1 C3D</cell><cell>TrainingSet Only</cell></row><row><cell>4</cell><cell>-</cell><cell>58.01</cell><cell>2 CNN-RNNs + 2 C3Ds</cell><cell>Training+Valid</cell></row><row><cell>5</cell><cell>-</cell><cell>58.85</cell><cell>1 CNN-RNN + 2 C3Ds</cell><cell>Training+Valid</cell></row><row><cell>6</cell><cell>-</cell><cell>59.02</cell><cell>1 CNN-RNN + 3 C3Ds</cell><cell>Training+Valid</cell></row><row><cell cols="2">From Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Testing Accuracies for All CNN architectures Pre-Trained Accuracy</head><label>2</label><figDesc></figDesc><table><row><cell>DCN</cell><cell>None</cell><cell>64.29</cell></row><row><cell>GoogLeNet</cell><cell>With ImageNet</cell><cell>62.96</cell></row><row><cell>CaffeNet</cell><cell>With ImageNet</cell><cell>68.05</cell></row><row><cell>VGG16</cell><cell>With ImageNet</cell><cell>68.24</cell></row><row><cell>Residual Network</cell><cell>With ImageNet</cell><cell>69.65</cell></row><row><cell>VGG16-FACE</cell><cell>With Faces</cell><cell>70.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . Validation Accuracies for Different CNN-RNNs</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Fc6</cell><cell>Fc7</cell></row><row><cell>VGG-LSTM-</cell><cell>128</cell><cell>45.43</cell><cell>42.30</cell></row><row><cell>OneLayer</cell><cell>256</cell><cell>44.39</cell><cell>-</cell></row><row><cell>VGG-LSTM-Two-Layers</cell><cell>256-256</cell><cell>41.26</cell><cell>-</cell></row><row><cell>CaffeNet-LSTM-OneLayer</cell><cell>128</cell><cell>40.99</cell><cell>-</cell></row><row><cell cols="4">We explore different LSTM parameter settings, including the</cell></row><row><cell cols="4">different number of LSTM net's layers, different number of</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 . Accuracies on Validation for C3D Using Different Faces Accuracy</head><label>4</label><figDesc></figDesc><table><row><cell>Face-AFEW(1 C3D)</cell><cell>39.69</cell></row><row><cell>Face-ours(1 C3D)</cell><cell>38.97</cell></row><row><cell>Face-ours(1 CNN-RNN + 1 C3D)</cell><cell>46.49</cell></row><row><cell>Face-AFEW(1 CNN-RNN + 1 C3D)</cell><cell>42.82</cell></row><row><cell>Face-ours + Face-AFEW(2 CNN-RNNs + 2 C3Ds)</cell><cell>48.30</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
		<title level="m">Automatic Group Happiness Intensity Analysis. IEEE Transaction on Affective Computing</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Capturing AU-Aware Facial Features and Their Latent Relations for Emotion Recognition in the Wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ACM ICMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Combining Multiple Kernel Methods on Riemannian Manifold for Emotion Recognition in the Wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM ICMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ebrahimi Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
				<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InProceedings of the 18th ACM international conference on Multimedia</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-10">2010, October</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Longterm recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Gülç Ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
				<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition. CVPR</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">FER-2013 face database</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1365">2013. 1365</date>
		</imprint>
		<respStmt>
			<orgName>Université de Montré al.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ferná Ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="855" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Long shortterm memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<idno>INTERSPEECH. 338-342</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fusing Aligned and Non-Aligned Face Information for Automatic Affect Recognition in the Wild: A Deep Learning Approach</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond Short Snippets: Deep Networks for Video Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition. CVPR</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Action Recognition using Visual Attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Workshop track -ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrasting and Combining Least Squares Based Learners for Emotion Recognition in the Wild</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gürpinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
				<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequencevideo to text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03476</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">EmotiW 2016: Video and Group-level Emotion Recognition Challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>ACM ICMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ICCV: Face Detection using SURF Cascade</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jianguo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yimin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An application of recurrent neural networks to discriminative keyword spotting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ferná Ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
				<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
