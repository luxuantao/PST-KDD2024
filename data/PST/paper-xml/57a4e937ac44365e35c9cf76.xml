<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Learning Framework for Character Motion Synthesis and Editing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Holden</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Saito</surname></persName>
							<email>saito@marza.com</email>
						</author>
						<author>
							<persName><forename type="first">Marza</forename><forename type="middle">Animation</forename><surname>Planet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taku</forename><surname>Komura</surname></persName>
							<email>tkomura@ed.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Learning Framework for Character Motion Synthesis and Editing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2897824.2925975</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>convolutional neural networks</term>
					<term>autoencoder</term>
					<term>human motion</term>
					<term>character animation</term>
					<term>manifold learning Concepts:</term>
					<term>Computing methodologies → Motion capture;</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Our framework allows the animator to synthesize character movements automatically from given trajectories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-driven motion synthesis allows animators to produce convincing character movements from high level parameters. Such approaches greatly help animation production as animators only need to provide high level instructions rather than low level details through keyframes. Various techniques that make use of large motion capture datasets and machine learning to parameterize motion have been proposed in computer animation.</p><p>Most data-driven approaches currently available require a significant amount of manual data preprocessing, including motion segmentation, alignment, and labeling. A mistake at any stage can easily result in a failure of the final animation. Such preprocessing is therefore usually carefully performed through a significant amount of human intervention, making sure the output movements appear smooth and natural. This makes full automation difficult and so often these systems require dedicated technical developers to maintain.</p><p>In this paper, we propose a model of animation synthesis and editing based on a deep learning framework, which can automatically learn an embedding of motion data in a non-linear manifold using a large set of human motion data with no manual data preprocessing or human intervention. We train a convolutional autoencoder on a large motion database such that it can reproduce the motion data given as input, as well as synthesize novel motion via interpolation. This unsupervised non-linear manifold learning process does not require any motion segmentation or alignment which makes the process significantly easier than previous approaches. On top of this autoencoder we stack another feedforward neural network that maps high level parameters to low level human motion, as represented by the hidden units of the autoencoder. With this, users can easily produce realistic human motion sequences from intuitive inputs such as a curve over some terrain that the character should follow, or the trajectory of the end effectors for punching and kicking. As the feedforward control network and the motion manifold are trained independently, users can easily swap and re-train the feedforward network according to the desired interface. Our approach is also inherently parallel, which makes it very fast to compute and a good fit for mainstream animation packages.</p><p>We also propose techniques to edit the motion data in the space of the motion manifold. The hidden units of the convolutional autoencoder represent the motion in a sparse and continuous fashion, such that adjusting the data in this space preserves the naturalness and smoothness of the motion, while still allowing complex movements of the body to be reproduced. One demonstrative example of this editing is to combine the style of one motion with the timing of another by minimizing the difference in the Gram matrices of the hidden units of the synthesized motion and that of the reference ACM Trans. Graph., <ref type="bibr">Vol. 35,</ref><ref type="bibr">No. 4</ref>, Article 138, Publication Date: July 2016 style motion <ref type="bibr" target="#b6">[Gatys et al. 2015</ref>].</p><p>In summary, our contribution is the following:</p><p>• A fast, parallel deep learning framework for synthesizing character animation from high level parameters.</p><p>• A method of motion editing on the motion manifold for satisfying user constraints and transforming motion style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We first review kernel-based methods for motion synthesis, which have been the main technique for synthesizing motions via blending motion capture data. We next review methods of interactive character control, where user instructions are applied for synthesizing novel motion using a motion database. Finally, we review methods in deep learning and how they have been applied to character animation.</p><p>Kernel-based Methods for Motion Blending Radial-basis functions (RBFs) are effective for blending multiple motions of the same class. <ref type="bibr" target="#b33">Rose et al. [1998]</ref> call the motions of the same class "verbs" and interpolate them using RBFs according to the direction that the character needs to move toward, or reach out. <ref type="bibr" target="#b34">Rose et al. [2001]</ref> show an inverse kinematics usage of RBFs by mapping the joint positions to the posture of the character. To make the blended motion appear plausible, motions need to be categorized and aligned along the timeline. <ref type="bibr" target="#b16">Kovar and Gleicher [2004]</ref> automate this process by computing the similarity of the movements and aligning them using dynamic time warping. However, RBF methods can easily overfit to the data due to the lack of mechanism for handling noise and variance. <ref type="bibr" target="#b29">Mukai and Kuriyama [2005]</ref> overcome this issue by using Gaussian Processes (GP), where the metaparameters are optimized to fit the model to the data. <ref type="bibr" target="#b9">Grochow et al. [2004]</ref> apply Gaussian Process Latent Variable Model (GPLVM) to map the motion data to a low dimensional space such that animators can intuitively control the characters. <ref type="bibr" target="#b42">Wang et al. [2005]</ref> apply GPLVM for time-series motion data -learning the posture in the next frame given the previous frames. <ref type="bibr" target="#b24">Levine et al. [2012]</ref> apply reinforcement learning in the space reduced by GPLVM to compute the optimal motion for tasks such as locomotion, kicking and punching.</p><p>Kernel-based methods such as RBF and GP suffer from large memory cost. Therefore, the number of motions that can be blended is limited. Our approach does not have such a limitation and can scale to huge sets of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactive Character Control</head><p>For interactive character control, a mechanism to produce a continuous motion based on the high level commands is required. Motion graphs <ref type="bibr" target="#b1">[Arikan and Forsyth 2002;</ref><ref type="bibr" target="#b21">Lee et al. 2002;</ref><ref type="bibr" target="#b17">Kovar et al. 2002]</ref> are an effective data structure for such a purpose. Motion graphs are automatically computed from a large set of motion capture data. As motion graphs only replay captured motion data, techniques to blend motions in the same class are proposed to enrich the dataset <ref type="bibr" target="#b25">[Min and Chai 2012;</ref><ref type="bibr" target="#b11">Heck and Gleicher 2007;</ref><ref type="bibr" target="#b35">Safonova and Hodgins 2007;</ref><ref type="bibr" target="#b36">Shin and Oh 2006;</ref><ref type="bibr" target="#b24">Levine et al. 2012</ref>].</p><p>Many of the methods that apply motion blending for motion synthesis during interactive character control require the motions to be classified, segmented and aligned to produce a rich model of each motion class. Poses from different times or different classes must not be mixed. Although <ref type="bibr" target="#b16">Kovar and Gleicher [2004]</ref> try to automate this process, the choice of the distance metric between motions and segmentation criteria of the motion sequence can significantly affect the performance and the accuracy. The requirement of explicitly conducting these steps can be a bottleneck of their performance. On the contrary, our unsupervised framework automatically blends nearby motions for synthesizing realistic movements, without any requirement of motion segmentation and classification.</p><p>For finding the best action at each time step following user instruction, techniques based on reinforcement learning are applied <ref type="bibr" target="#b19">[Lee and Lee 2004;</ref><ref type="bibr" target="#b35">Safonova and Hodgins 2007;</ref><ref type="bibr" target="#b22">Lee et al. 2010;</ref><ref type="bibr" target="#b24">Levine et al. 2012]</ref>. Reinforcement learning requires a huge amount of precomputation which increases exponentially with respect to the number of actions available to the character. For this reason, methods such as motion fields <ref type="bibr" target="#b22">[Lee et al. 2010]</ref> quickly become intractable as the amount of data and the number of control parameters grow, and various additional techniques need to be used to reduce the number of states. For example, <ref type="bibr" target="#b24">Levine et al. [2012]</ref> classify the motion dataset into different classes in advance and reduce the dimensionality within each motion class. We avoid using reinforcement learning and directly regress the high level commands given by the user to the low level motion features. Such a mapping is computed by stochastic gradient descent and does not grow with respect to the number of control parameters such as in reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning for Motion Data</head><p>Techniques based on deep learning are currently the state-of-the-art in the area of image and speech recognition <ref type="bibr" target="#b18">[Krizhevsky et al. 2012;</ref><ref type="bibr" target="#b8">Graves et al. 2013]</ref>. Recently, there is a huge interest in applying deep learning techniques for synthesizing novel data from the learned model <ref type="bibr" target="#b41">[Vincent et al. 2010;</ref><ref type="bibr" target="#b7">Goodfellow et al. 2014]</ref>. One important strength of frameworks based on deep learning is that they automatically learn the features from the dataset. For example, when convolutional neural networks (CNN) are applied to image recognition, filters similar to Gabor filters appear at the bottom layers, and more complex filters that correspond to different objects appear in the higher level layers <ref type="bibr" target="#b45">[Zeiler and Fergus 2014]</ref>. One of our main interests is to make use of such a strength for character animation.</p><p>Deep learning and neural networks are also attracting the interests of the control community in applications such as robotics and physically-based animation. <ref type="bibr" target="#b0">Allen and Faloutsos [2009]</ref> use the NEAT algorithm, that evolves the topology of the neural network for controlling bipedal characters. <ref type="bibr" target="#b38">Tan et al. [2014]</ref> apply this technique for control of bike stunts. <ref type="bibr">Levine and Vladlen [2014]</ref> learn the optimal control policies using neural networks and apply it to bipedal gait control. <ref type="bibr" target="#b28">Mordatch et al. [2015]</ref> apply a recurrent neural network (RNN) to learn a near-optimal feedback controller for articulated characters. While these approaches learn the dynamics for controlling characters in a physical environment, our focus is on learning features from captured human motion data and applying them for animation production.</p><p>There have been a few approaches to apply deep learning to human motion capture data. <ref type="bibr" target="#b4">Du et al. [2015]</ref> construct a hierarchical RNN using a large motion dataset from various sources and show their method achieves a state-of-the-art recognition rate. <ref type="bibr" target="#b13">Holden et al. [2015]</ref> apply a convolutional autoencoder to the CMU motion capture database and show the learned representation achieves good performance in tasks such as motion retrieval.</p><p>In terms of motion synthesis, <ref type="bibr" target="#b39">Taylor et al. [2009;</ref><ref type="bibr" target="#b40">2011]</ref>   The outline of our method. High level parameterizations are disambiguated and used as input to feed forward neural networks. These networks produce motion in the space of the hidden units of a convolutional autoencoder which can be further used to edit the generated motion.</p><p>interpolated movements while reducing drifting. These are timeseries approaches, where computing the entire motion requires integration from the first frame. We find this framework is not very suitable for the purpose of animation production as animators wish to see the entire motion at once and then sequentially apply minor edits while revising the motion. They do not wish to see edits happening at early frames being propagated into the future, which will be the case when time-series approaches are adopted. For this reason, in this paper we adopt and improve on the convolutional autoencoder representation <ref type="bibr" target="#b13">[Holden et al. 2015</ref>] which can produce motion at once, in parallel, without performing any integration process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview</head><p>The outline of the system is shown in Fig. <ref type="figure" target="#fig_0">2</ref>. Using data from a large human motion database (see Section 4), a convolutional autoencoder is trained and thus a general motion manifold is found (green box in Fig. <ref type="figure" target="#fig_0">2</ref>, see Section 5). After this training, motion can be represented by the hidden units of the network. Given this representation, a mapping is produced between high level control parameters and the hidden units via a feedforward neural network stacked on top of the convolutional autoencoder (orange box in Fig. <ref type="figure" target="#fig_0">2</ref>, see Section 6). The high level control parameters shown in this work are the trajectory of the character over the terrain and the movement of the end effectors. As these parameterizations can contain ambiguities, another small network is used to compute parameters which disambiguate the input (red box in Fig. <ref type="figure" target="#fig_0">2</ref>, see Section 6.3). Just the subset of the motion capture data relevant to the task is used to train these feedforward networks. Using this framework, the user can produce an animation of the character walking and running by drawing a curve on the terrain, or the user can let the character punch and kick by specifying the trajectories of the hands and feet. Once a motion has been generated, it can be edited in the space of hidden units, such that the resulting motion satisfies constraints such as positional constraints for foot-skate cleanup (see Section 7.1). Using this technique we describe a method to transform the style of the character motion using a short stylized clip as a reference (Section 7.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Acquisition</head><p>In this section we describe our construction of a large motion database suitable for deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Motion Dataset for Deep Learning</head><p>We construct a motion dataset for deep learning by collecting many freely available large online databases of motion capture [CMU ; <ref type="bibr">Müller et al. 2007;</ref><ref type="bibr" target="#b32">Ofli et al. 2013;</ref><ref type="bibr" target="#b43">Xia et al. 2015]</ref>, as well as adding data from our internal captures, and retargeting them to a uniform skeleton structure with a single scale and the same bone lengths. The retargeting is done by first copying any corresponding joint angles in the source skeleton structure to the target skeleton structure, then scaling the source skeleton to the same size as the target skeleton, and finally performing a full-body inverse kinematics scheme <ref type="bibr" target="#b44">[Yamane and Nakamura 2003]</ref> to move the joints of the target skeleton to match any corresponding joint positions in the source skeleton. Once constructed, the final dataset is about twice the size of the CMU motion capture database and contains around six million frames of high quality motion capture data for a single character sampled at 120 frames per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Format for Training</head><p>We convert the dataset into a format that is suitable for training. We subsample all of the motion in the database to 60 frames per second and convert the data into the 3D joint position format from the joint angle representation in the original dataset. The joint positions are defined in the body's local coordinate system whose origin is on the ground where root position is projected onto. The forward direction of the body (Z-axis) is computed using the vectors across the left and right shoulders and hips, averaging them and computing the cross product with the vertical axis (Y-axis). This is smoothed using a Gaussian filter to remove any high frequency movements. The global velocity in the XZ-plane, and rotational velocity of the body around the vertical axis (Y-axis) in every frame is appended to the input representation. These can be integrated over time to recover the global translation and rotation of the motion. Foot contact labels are found by detecting when either the toe or heel of the character goes below a certain height and velocity <ref type="bibr" target="#b21">[Lee et al. 2002]</ref>, and are also appended to input representation. The mean pose is subtracted from the data and the joint positions are divided by the standard deviation to normalize the scale of the character. The velocities, and contact labels are also divided by their own standard deviations.</p><p>We find the joint position representation effective for several reasons: the Euclidean distance between two poses in this representation closely matches visual dissimilarity of the poses, multiple poses can be interpolated with simple linear operators, and many constraints are naturally formulated in this representation.</p><p>In general, our model does not require motion clips to have a fixed length, but having a fixed window size during training can improve the speed, so for this purpose we separate the motion database into overlapping windows of n frames (overlapped by n/2 frames), where n = 240 in our experiments. This results in a final input vector, representing a single sample from the database, as X ∈ R n×d with n being the window size and d the degrees of freedom of the body model, which is 70 in our experiments. After training the window size n is not fixed in our framework, thus it can handle motions of arbitrary lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Building the Motion Manifold</head><p>To construct a manifold over the space of human motion we build an autoencoding convolutional neural network and train it on the complete motion database. We follow the approach by <ref type="bibr" target="#b13">Holden et al. [2015]</ref>, but adopt a different setup for optimizing the network for motion synthesis. First, we only use a single layer for encoding the motion, as multiple layers of pooling/de-pooling can result in blurred motion after reconstruction due to the pooling layers of the 138:3 • A Deep Learning Framework for Character Motion Synthesis and Editing network reducing the temporal resolution of the data. Using only one layer lacks the power to further abstract the low level features so this task is performed by the deep feedforward network described in Section 6. We also change several components to improve the performance of the network training and bases quality. The details are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Network Structure</head><p>In our framework, the convolutional autoencoder performs a onedimensional convolution over the temporal domain, independently for each filter. The network provides a forward operation Φ (encoding) and a backward operation Φ † (decoding). The forward operation receives the input vector X in the visible unit space and outputs the encoded values H in the hidden unit space.</p><p>The forward operation:</p><formula xml:id="formula_0">Φ(X) = ReLU (Ψ(X * W0 + b0)),<label>(1)</label></formula><p>consists of a convolution (denoted * ) using weights matrix W0 ∈ R m×d×w 0 , addition of a bias b0 ∈ R m , a max pooling operation Ψ, and the nonlinear operation ReLU (x) = max(x, 0), where w0 is the temporal filter width and m is the number of hidden units in the autoencoding layer, each set to 25 and 256 in this work; a temporal filter width of 25 ensures each filter roughly corresponds to half a second of motion, while 256 hidden units is experimentally found to produce good reconstruction.</p><p>The max pooling operation Ψ returns the maximum value of each pair of consecutive hidden units on the temporal axis. This reduces the temporal resolution, ensures that the learned bases focus on representative features, and also allows the bases to express a degree of temporal invariance. We use the rectified linear operation ReLU instead of the common tanh operation, since its performance as an activation function was demonstrated by <ref type="bibr" target="#b31">Nair and Hinton [2010]</ref>.</p><p>The backward operation:</p><formula xml:id="formula_1">Φ † (H) = (Ψ † (H) − b0) * W0.<label>(2)</label></formula><p>takes hidden units H ∈ R n 2 ×m as input, and consists of an inversepooling operation Ψ † , a subtraction of a bias b0 and convolution using the weights matrix W0. W0 ∈ R d×m×w 0 is simply the weights matrix W0 reflected on the temporal axis, and transposed on the first two axes, used to invert the convolution operation.</p><p>When performing the inverse pooling operation, each unit in the hidden layer must produce two units in the visible layer (those which were pooled during the forward operation). This operation is therefore non-invertible and an approximation must be used. During training Ψ † randomly picks between the two corresponding visible units and assigns the complete value to one of those units, leaving the other unit set to zero. This represents a good approximation of the inverse of the maximum operation but introduces noise into the result. Therefore when performing synthesis Ψ † acts like an average pooling operation and spreads the hidden unit value evenly across both visible units.</p><p>The filter values W0 are initialized to some small random values found using the "fan-in" and "fan-out" criteria <ref type="bibr" target="#b12">[Hinton 2012</ref>], while the bias b0 are initialized to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training the Autoencoder</head><p>We train the network to reproduce some input X following both the forward and backward operations. Training is therefore given as  an optimization problem. We minimize the following cost function with respect to the parameters of the network, θ = {W0, b0} :</p><formula xml:id="formula_2">Cost(X, θ) = X − Φ † (Φ(X)) 2 2 + α θ 1<label>(3)</label></formula><p>In this equation, the first term measures the squared reproduction error and the second term represents an additional sparsity term that ensures the minimum number of network parameters are used to reproduce the input. This is scaled by some small constant α which in our case is set to 0.1.</p><p>To minimize this function we perform stochastic gradient descent. We input random elements X from the database, and using automatic derivatives calculated via Theano <ref type="bibr" target="#b2">[Bergstra et al. 2010]</ref>, we update the network parameters θ. We make use of the adaptive gradient descent algorithm Adam <ref type="bibr" target="#b15">[Kingma and Ba 2014]</ref> to improve the training speed and quality of the final bases. To avoid overfitting, we use a Dropout <ref type="bibr" target="#b37">[Srivastava et al. 2014</ref>] of 0.2. Training is performed for 15 epochs and takes around 6 hours on a NVIDIA GeForce GTX 660 GPU.</p><p>Once training is complete the filters express strong temporal and inter-joint correspondences. A visualisation of the resulting weights can be seen in Fig. <ref type="figure" target="#fig_1">3</ref>. Here it is shown that each filter expresses the movement of several joints over a period of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Mapping High Level Parameters to Human Motions</head><p>We learn a regression between high level parameters and the character motion using a feedforward convolutional neural network. The high level parameters are abstract parameters for describing the motion, such as the root trajectory projected onto the terrain or the trajectories of the end effectors such as the hands and feet. This is a general framework that can be applied for various types of high level parameters and animation outputs. Producing a mapping between the low dimensional, high level parameters to the full body motion is a difficult task due to the huge amount of ambiguity and multi-modality in the output. There can be many different valid motions that could be performed to follow the high level parameters. For example, when the character is instructed to walk along a line, the timing of the motion is completely invariant: a character may walk with different step sizes or walk out of sync with another In the rest of this section, we first describe about the structure of the feedforward network and how it can be trained, and then about the details of the high level parameters for the locomotion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Structure of the Feedforward Network</head><p>We now describe the feedforward convolutional network which maps the high level parameters T to the hidden layer of the autoencoding network constructed in the previous section, such that eventually the system outputs a motion of the character X ∈ R n×d .</p><p>The feedforward convolutional network uses a similar forward operation as defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>) but contains three layers, and an additional operation Υ, which is a task-specific operation to resolve the ambiguity problem. We will discuss more about it in Section 6.3. The feedforward operation is given by the following:</p><formula xml:id="formula_3">Π(T) = ReLU (Ψ(ReLU (ReLU (Υ(T) * W1 + b1) * W2 + b2) * W3 + b3)),<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">W1 ∈ R h 1 ×l×w 1 , b1 ∈ R h 1 , W2 ∈ R h 2 ×h 1 ×w 2 , b2 ∈ R h 2 , W3 ∈ R m×h 2 ×w 3 , b3 ∈ R m , h1, h2</formula><p>are the number of hidden units in the two hidden layers of the feedforward network, w1, w2, w3 are the filter widths of the three convolutional operators and l is the DOF of the high level parameters, which are set to 64, 128, 45, 25, 15 and 7, respectively. These filter widths ensure each frame of motion is generated using roughly one second of trajectory information. The parameters of this feedforward network used for regression are therefore given by φ = {W1, W2, W3, b1, b2, b3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training the Feedforward Network</head><p>To train the regression between the high level parameters and the output motion, we minimize a cost function using stochastic gradient descent in the same way as explained in Section 5.2, but this time with respect to the parameters of the feedforward network, keeping the parameters of the autoencoding network fixed. The cost function is defined as the following and consists of two terms:</p><formula xml:id="formula_5">Cost(T, X, φ) = X − Φ † (Π(T)) 2 2 + α φ 1 (5)</formula><p>The first term computes the mean squared error of the regression and the second term is a sparsity term to ensure the minimum number of hidden units are used to perform the regression. As before, α is set to 0.1.</p><p>When training this network, we only use data relevant to the task. For example, during the locomotion task we only use the locomotion-specific data. Training therefore takes significantly less time than the autoencoder. For the locomotion task, training is performed for 200 epochs and takes approximately 1 hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Disambiguation for Locomotion</head><p>In this section, we describe about our solution to disambiguate the locomotion, given a curve drawn on the terrain. A curve on a terrain alone does not give enough information to fully describe the motion that should be produced due to the ambiguity problem mentioned above. We examined various types of inputs, and discovered that providing the timing when the feet is in contact with the ground can greatly disambiguate the locomotion. Indeed, the contact timing even distinguishes walking and running as there is always a double support phase in walking, and there is a flying phase in running. We therefore train a model which can be used to automatically compute foot contacts from a given trajectory. We include this in the input to the feedforward network to resolve the ambiguity.</p><p>The input to this network is the trajectory in the form of translational velocities on the XZ plane and rotational velocity around the Y axis, given for each timestep and relative to the forward facing direction of the path T ∈ R n×k , where n is the number of frames in the trajectory and k is the dimensionality of the trajectory input, which is 3. The character height is considered constant. This input is passed to the function Υ in Eq. ( <ref type="formula" target="#formula_3">4</ref>), which adds foot contact information to the trajectory input:</p><formula xml:id="formula_6">Υ(T) = [T F] ,<label>(6)</label></formula><p>where F ∈ {−1, 1} n×4 is a matrix that represents the contact states of left heel, left toe, right heel, and right toe at each frame, and whose values are 1 when in contact with the ground, and -1 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Contact States by Square Waves:</head><p>We model the states of the four contacts using four square waves and learn the parameters of these waves from the data in a way that allows us to compute them from the trajectory T. The four square waves that model the four contacts, and produce F, are defined as follows:</p><formula xml:id="formula_7">F(ω, τ ) =     sign(sin(c ω + a h ) − b h − τ lh ) sign(sin(c ω + a t ) − b t − τ lt ) sign(sin(c ω + a h + π) − b h − τ rh ) sign(sin(c ω + a t + π) − b t − τ rt )     , (7)</formula><p>where ω and τ control the frequency and step duration at each frame of motion (see Fig. <ref type="figure" target="#fig_4">5</ref>), and are the parameters we are interested in computing from the trajectory.</p><p>These parameters alone are enough to produce contact information, but to allow for more artistic control some user parameters are also provided. a h , a t are constants that adjust the phases of the heels and toes, b h , b t are constants that can be used to adjust the contact duration of the heels and toes (therefore forcing the character to walk or run), and c is a constant that can scale the frequency of the stepping. In our experiments, we set a h , a t to −0.1 and zero, b h , b t to zero, and c to one. Below, we describe how to extract ω and τ from the locomotion data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>138:5 • A Deep Learning Framework for Character Motion Synthesis and Editing</head><p>Extracting Wave Parameters from Data: To produce a regression between the input curve T and parameters ω, τ , we need to first compute these parameter values from the foot contact information in the dataset. For each frame i the angle ωi can be computed by summing the differential ωi = ∆ωi + ∆ωi−1 + ... + ∆ω0. We therefore calculate ∆ωi for each frame i in the data instead. From the dataset, ∆ωi can be computed by ∆ωi = π L i where Li is the wavelength of the steps, and is computed by subtracting the timings of adjacent off-to-on frames, and averaging them for the four contact points (left/right, heel/toe). Learning ∆ω from the data instead of ω also allows for the footstep frequency to change during the locomotion, for example to allow the character to take more steps during a turn. We extract τi by looking at the foot contact information in the gait cycle that includes frame i and taking the ratio of the number of frames with the foot up ui over the number of frames with the foot down di. This ratio can be converted to the square wave threshold value τi using the following:</p><formula xml:id="formula_8">τi = cos πdi ui + di . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>For each heel and toe we learn separate τ variables, while ∆ω is the same between all contacts. This avoids feet going out of sync. These parameters are packed into a matrix Γ = {τ lh , τ lt , τ rh , τ rt , ∆ω}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regressing the Locomotion Path and Contact Information:</head><p>Now we describe how we produce a regression between the input curve T and the contact square wave parameters Γ. Using the locomotion data from the motion capture dataset, we compute the locomotion path T by projecting the motion of the root joint onto the ground. We also extract the corresponding Γ and the foot contact parameters of the square waves, by the method described above.</p><p>We then regress T to Γ using a small two layer convolutional neural network.</p><formula xml:id="formula_10">Γ(T) = ReLU (T * W4 + b4) * W5 + b5<label>(9)</label></formula><p>Here W4 ∈ R h 4 ×k×w 4 , b4 ∈ R h 4 , W5 ∈ R l×h 4 ×w 5 , b5 ∈ R l are the parameters of this network, where w4, w5 are the filter widths, h4 is the number of hidden units, and k, l are the DOF of T, Γ at each frame, respectively, which are 3 and 5. This network is trained using stochastic gradient descent as explained in Section 5.2.</p><p>Once this network is trained, given some trajectory T, it computes the values for τ, ∆ω, which can be used to calculate F using Eq. ( <ref type="formula">7</ref>), therefore producing foot contact information for the trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Motion Editing in Hidden Unit Space</head><p>In this section, we describe how to edit or transform the style of the motion in the space of hidden units, which is the abstract representation of the motion data learned by the autoencoder. Because the motion is edited in the space of the hidden units, which parameterize the manifold over valid motion, it ensures that even after editing, the motion remains smooth and natural. We represent constraints as costs with respect to the values of hidden units. This formulation of motion editing as a minimization problem is often convenient and powerful as it specifies the desired result of the edit without inferring any technique of achieving it. We first describe an approach to apply kinematic constraints (see Section 7.1) and then about adjusting the style of the motion in the space of hidden units (see Section 7.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Applying Constraints in Hidden Unit Space</head><p>Because the scope of motion editing is very large we start by describing how to apply constraints in the hidden space using three common constraints often found in character animation as examples: positional constraints, bone length constraints, and trajectory constraints, but our approach is applicable to other types of constraints providing the constraint can be described using a cost function. Note that all these costs compute the error for all frames simultaneously, summed over the temporal domain.</p><p>Positional Constraints: Constraining the joint positions is essential for fixing foot sliding artifacts or guiding the hand of the character to to grasp objects. Given an initial input motion in the hidden unit space H, its cost in terms of penalty for violating the positional constraints is computed as follows:</p><formula xml:id="formula_11">P os(H) = j v H r + ω H × p H j + v H j − v j 2 2 . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where v j ∈ R n×3 is the target velocity of joint j in the body coordinate system, and</p><formula xml:id="formula_13">v H r , p H j , v H j ∈ R n×3</formula><p>, ω H ∈ R n are the root velocity, joint j's local position and velocity, and the body's angular velocity around the Y axis, respectively, computed from the hidden unit values H by the decoding operation Φ † (H) in Eq. ( <ref type="formula" target="#formula_1">2</ref>), for all the frames. For example, in order to avoid foot sliding, the heel and toe velocity must be zero when they are in contact with the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bone Length Constraints:</head><p>As we use the joint positions as the representation in our framework, we need to impose the bone length constraint between adjacent joints to preserve the rigidity of the body. The cost for such a constraint can be written as follows:</p><formula xml:id="formula_14">Bone(H) = i b | p Hi b j 1 − p Hi b j 2 − l b | 2 (11)</formula><p>where b is the index for the set of bones in the body, p Hi Trajectory Constraints: As a result of motion editing or error in the synthesis motion may not exactly follow the desired trajectory. We may therefore need to additionally constrain the motion to some trajectory precisely. The cost for such a constraint can be written as follows:</p><formula xml:id="formula_15">T raj(H) = ω H − ω 2 2 + v H r − v r 2 2 (12)</formula><p>Projection to Null Space of Constraints: The motion generated by the autoencoder is adjusted in the space of hidden units via gradient descent until the total cost converges within a threshold:</p><formula xml:id="formula_16">H = arg min H P os(H) + Bone(H) + T raj(H).<label>(13)</label></formula><p>By minimizing Eq. ( <ref type="formula" target="#formula_16">13</ref>) and projecting the found H back into the visible unit space using Eq. ( <ref type="formula" target="#formula_1">2</ref>), we can constrain the joints to the desired position while keeping the rigidity of each bone.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Motion Stylization in Hidden Unit Space</head><p>Our framework for editing the motion in the hidden space can also be applied to transform the style of the motion using an example motion clip as a reference. <ref type="bibr" target="#b6">Gatys et al. [2015]</ref> describe that the artistic style of an image is encoded in the Gram matrix of the hidden layers of a neural network and presents examples of combining the content of a photograph and the style of a painting. By finding hidden unit values which produce a Gram matrix similar to the reference data, the input image can be adjusted to some different style, while retaining the original content. We can use our framework to apply this technique to motion data and produce a motion that has the timing and content of one input, with the style of another.</p><p>The cost function in this case is defined by two terms relating to the content and style of the output. Given some motion data C which defines the content of the produced output, and another S which defines the style of the produced output, the cost function over hidden units H is given as the following:</p><formula xml:id="formula_17">Style(H) = s G(Φ(S)) − G(H) 2 2 + c Φ(C) − H 2 2 (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>where c and s dictate the relative importance given to content and style, which are set to 1.0 and 0.01, respectively in our experiments, and the function G computes the Gram matrix, which is the mean of the inner product of the hidden unit values across the temporal domain i and can be thought of as the average similarity or coactivation of the hidden units:</p><formula xml:id="formula_19">G(H) = n i HiH i n .<label>(15)</label></formula><p>Unlike in Section 7.1, where H is found via motion synthesis or the forward operation of the autoencoder, to avoid a bias toward either content or style, H is initialized from white noise and a stylized motion is found by optimizing Eq. ( <ref type="formula" target="#formula_17">14</ref>) until convergence using adaptive gradient descent with automatic derivatives calculated via Theano. The computed motion is then edited using Eq. ( <ref type="formula" target="#formula_16">13</ref>) to satisfy kinematic constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experimental Results</head><p>We now show some experimental results of training and synthesizing character movements. We first show examples of animating character movements using high level parameters and the framework described in Section 6, with projection to the null space of constraints as described in Section 7.1. We next show examples of applying stylization using the framework described in Section 7.2.</p><p>As our system has a fast execution at runtime it is suitable for creating animation of large crowds. We therefore show such an example of this. We then evaluate the autoencoder representation by comparing its performance with comparable network structures. Finally  we present a breakdown of the computation at the end of this section. The readers are referred to the supplementary video for the details of the produced animation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locomotion on the Terrain</head><p>The feedforward network is trained such that a curve drawn on the terrain is used to generate the actual locomotion of the character. Among the data in the database, various types of locomotion data with different speed and stepping patterns are used to train the system. Using the training data, the trajectory of the root is projected onto the ground to produce a terrain curve to be used as an input.</p><p>During runtime, curves drawn by Maya are first used to produce walking and running animation. In the first two examples, the speed of the character is constant. The timing that the heels and toes are in contact with the ground is automatically generated from the curve and used as the input to the feedforward network. The character walks when the velocity is low and runs when the velocity is high (see Fig. <ref type="figure">1</ref>). In the next example, we take the body velocity profile from some test set not used in the training. We also add some turning motion by drawing the angular velocity profile by Maya (see Fig. <ref type="figure" target="#fig_7">6</ref>). In the final example, we use a speed profile from a test data item where the character accelerates and decelerates. This is applied to a terrain curve drawn by Maya. A transition from walking to stopping and running appears as a result (see Fig. <ref type="figure" target="#fig_8">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Punching and Kicking Control</head><p>We show another example where the feedforward network is set up such that the character punches and kicks to follow end effector trajectories provided by the user. We tested the system using test data not included in the training set. The character generates full body movements that follow the trajectories of the end effectors. Some snapshots of the animation are shown in Fig. <ref type="figure" target="#fig_9">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Editing in the Hidden Unit Space</head><p>Here we show the motions before and after applying the constraints to the those generated by the feedforward network. We also compare the motion edited in the hidden unit space with those edited in the Cartesian space by inverse kinematics. The former produces much smoother results as the motion is edited on the leanred manifold. The results   <ref type="formula" target="#formula_3">4</ref>) injured style are given, and they are passed through the autoencoder to compute the Gram matrices. These Gram matrices are used for converting the style of the given locomotion data using the optimization method described in Section 7.2. Locomotion data is taken from the dataset, while style data is a combination of internal captures, and captures from <ref type="bibr" target="#b43">[Xia et al. 2015</ref>]. The snapshots of the animation are shown in Fig. <ref type="figure" target="#fig_10">9</ref>.</p><p>Crowd Animation Our approach allows for parallel computation across the timeline using the GPU. This allows us to create motion for many characters at once. We make use of this feature to apply our system for synthesizing an animation of a large set of characters using the terrain curve framework described in Section 6. Results of this are shown in Fig. <ref type="figure" target="#fig_11">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing the Autoencoder with Other Network Structures</head><p>Here we evaluate the representation found by the autoencoder. We compare its performance to a naive construction of a neural network without max pooling or dropout. If trained without dropout or max pooling the network does not learn strong temporal coherence, as can be seen in Fig. <ref type="figure" target="#fig_2">4</ref>. This motion manifold without temporal coherence is fairly similar to a per-pose PCA -when applied to the style transfer task, because there is no temporal smoothness encoded in this model, the output is extremely noisy as shown in Fig. <ref type="figure" target="#fig_12">11</ref>. In the motion synthesis task the neural network without max pooling or dropout actually has a slightly lower mean error, but similar noise was present in results. This can be seen in Fig. <ref type="figure" target="#fig_13">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breakdown of the Computation</head><p>In this section we give a breakdown of the various timings of the computation for each result presented. This is shown in Table <ref type="table" target="#tab_1">1</ref>. All examples of animation are generated at a sample rate of 60fps. For the crowd scene the frame rate is given by finding the total number of frames generated across all 200 characters. All results are produced using a NVIDIA GeForce GTX 660 and Theano. Our technique clearly scales well as the total time required to generate results remains similar, even with long sequences of animation or many characters. All times are given in seconds. In particular our technique works well on the crowd scene due to the fact it can run in parallel both across characters, and across the timeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussions</head><p>Many other approaches to motion synthesis are time-series approaches <ref type="bibr" target="#b39">[Taylor and Hinton 2009;</ref><ref type="bibr" target="#b40">Taylor et al. 2011;</ref><ref type="bibr" target="#b27">Mittelman et al. 2014;</ref><ref type="bibr" target="#b43">Xia et al. 2015</ref>], but our approach to motion synthesis is a procedural approach as it does not require step-by-step calculation, and individual frames at arbitrary times can be generated on demand. This makes it a good fit for animation production software such as Maya which allows animators to jump to arbitrary points in the timeline. Animators also do not need to worry about changes affecting the generated animation outside of the local convolution window. This also makes the system highly parallelizable, as motion for all frames can be computed independently. As a result the trajectories of many characters can be generated during runtime on the GPU at fast rates. Generating motion in parallel across the timeline requires continuity between frames. This is handled by our framework in two ways. The high level continuity (such as the timing) is provided by the generation of foot contact information, while the low level continuity (smoothness etc.) is ensured by the manifold.</p><p>Although procedural approaches are not new in character animation <ref type="bibr" target="#b20">[Lee and Shin 1999;</ref><ref type="bibr" target="#b14">Kim et al. 2009;</ref><ref type="bibr" target="#b26">Min et al. 2009]</ref>, previous methods require the motion to be segmented, aligned and labeled before the data can be included into the model. On the contrary, our model automatically learns the model from a large set of mo-  tion data without manual labeling or segmentation. This makes the system highly practical as the users can easily add the new motion data into the training set to enrich the model.</p><p>Our convolutional filters only shift along the temporal axis; a natural question to ask is if this convolution can also be used spatially, for example, over the graph structure of the character. The idea of using a temporal convolutional model is to ensure the learned bases of the autoencoder are local and invariant -that their influence is limited to a few frames, and that they can appear anywhere in the timeline. These assumptions of locality and invariance do not generalize well in the spatial domain. There are strong correlations between separate parts of the body according to the motion (for example, arms and legs synchronized during walking), and it is difficult to confine the influence along the graph structure. Also, the bases such as those for the arm are in general not applicable to other parts of the body, which shows the structure is not invariant. It is to be noted that our filters do capture the correlation of different joints; the signals of different DOFs are summed in the convolution operation in Eq. ( <ref type="formula" target="#formula_0">1</ref>), and thus the filters are optimized to discover correlated movements.</p><p>There are some important parameters of the system, which are determined carefully taking into account the nature of human movements and through experimental results. These include the filter widths of the human motion (w0 in Eq. ( <ref type="formula" target="#formula_0">1</ref>)) and those of the trajectories for the feedforward network (w1, w2, w3 in Eq. ( <ref type="formula" target="#formula_3">4</ref>)).</p><p>For the filter width of the human motion (w0 = 25), this covers about half-a-second. Setting this value too long results in the motions to be smoothed out excessively or even fail to train. Setting it too short will make the system work like per-pose training, where the smoothness of motion cannot be learned from the data.</p><p>For the feedforward network, the filter width is set a little longer (w1 = 45, w2 = 25, w3 = 15), such that the character prepares early enough for future events. Setting this too short will result in lack of variations, and too long will result in overfitting, where odd movements are produced for novel trajectories. These values are initially set through intuition and fine-tuned through visual analysis.</p><p>Limitations In our framework, the input parameters of the feedforward network need to be carefully selected such that there is little ambiguity between the high level parameters and the output motion. Ambiguity is a common issue in machine learning where the outputs of the regressor are averaged out when multiple outputs correspond to the same input in the training data. In some cases additional data that resolves the ambiguity may be required. This can either be supplied by the user directly, or must be found using an additional model such is done with our foot contact model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We propose a deep learning framework to map high level parameters to an output motion by first learning a motion manifold using a large motion database and then producing a mapping between the user input to the output motion. We also propose approaches to edit and transform the styles of the motions under the same framework.</p><p>Currently, our autoencoder has only a single layer as deep stacked autoencoders suffer from blurriness during the depooling process.</p><p>In our system, the role of combining and abstracting the low level features is covered by the feedforward network stacked on top of it. However, a more simple feedforward network, which is easier to train, can be used if a stacked deep autoencoder is used for learning the motion manifold. It will be interesting to look into newly emerging depooling techniques such as hypercolumns <ref type="bibr" target="#b10">[Hariharan et al. 2014</ref>] that cope with the blurring effect to produce a deep network for the motion manifold.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The outline of our method. High level parameterizations are disambiguated and used as input to feed forward neural networks. These networks produce motion in the space of the hidden units of a convolutional autoencoder which can be further used to edit the generated motion.</figDesc><graphic url="image-2.png" coords="3,55.20,54.00,237.63,116.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A selection of convolutional filters from the autoencoder. Here the horizontal axis represents time, while the vertical axis represents the degrees of freedom. Filters are sparse (mainly zero), and show strong temporal and inter-joint correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convolutional filters learned by a network without dropout or max pooling. Compared to the filters in Fig. 3 these filters are noisy and have not learned any strong temporal correlations as most of their activations are located around the central frame of the filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>138:4 • D. Holden et al. ACM Trans. Graph., Vol. 35, No. 4, Article 138, Publication Date: July 2016motion performed on the same trajectory. Naively mixing these outof-sync motions results in an averaging of the output, making the character appear to float along the path. Unfortunately, there is no universal solution for solving such an ambiguity problem, and each problem must be solved individually based on the nature of the high level parameters and the class of the output motion. Here we provide a solution for a locomotion task, which is a general problem with high demand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A square wave representing the foot contact computed from a sin wave. The foot is in contact with the ground when the value is 1.</figDesc><graphic url="image-5.png" coords="5,319.16,54.00,237.65,98.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>positions of the two end joints of b at frame i, computed by the decoding operation Φ † (H) in Eq. (2) and l b is the length of bone b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>138:6 • D. Holden et al. ACM Trans. Graph., Vol. 35, No. 4, Article 138, Publication Date: July 2016</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A side stepping motion produced from a velocity profile from some test data and an angular velocity profile drawn by Maya.</figDesc><graphic url="image-6.png" coords="7,55.20,54.00,237.64,95.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A locomotion including transition from walk to stop and run.</figDesc><graphic url="image-7.png" coords="7,319.16,54.00,237.64,107.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The character performs punching and kicking to attack the given targets.</figDesc><graphic url="image-8.png" coords="7,319.16,204.71,237.61,52.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Several animations are generated with the timing from one clip and the style of. Red: input style motions. Green: input timing motions. Blue: output motion. In a clockwise order the styles used are zombie, depressed, old man, injured.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Crowd motion for 200 characters is generated in parallel using the GPU.</figDesc><graphic url="image-10.png" coords="8,55.20,229.55,237.63,103.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Two graphs of the vertical movement of the hand joint in motion generated in the style transfer task. Top: movement when the neural network uses dropout and max pooling. Bottom: movement when the neural network does not use these operations -the movement is very noisy due to the lack of temporal correlation encoded in the motion manifold.</figDesc><graphic url="image-11.png" coords="8,319.16,54.00,237.65,154.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Two graphs of the vertical movement of the spine joint in motion generated in the motion synthesis task. Purple Line: Ground Truth. Blue Line: Generated Movement. Top: movement when the neural network uses dropout and max pooling -although the movement does not follow exactly, the signal is smooth. Bottom: movement when the neural network does not use these operations -the movement fits more closely to the ground truth but has visible high frequency noise.</figDesc><graphic url="image-12.png" coords="9,55.20,166.78,237.65,156.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance breakdown.</figDesc><table><row><cell>Task</cell><cell cols="4">Duration Foot Contacts Synthesis Editing</cell><cell>Total</cell><cell>FPS</cell></row><row><cell>Walking</cell><cell>60s</cell><cell>0.025s</cell><cell>0.067s</cell><cell cols="2">1.096s 1.188s</cell><cell>3030</cell></row><row><cell>Running</cell><cell>60s</cell><cell>0.031s</cell><cell>0.073s</cell><cell cols="2">1.110s 1.214s</cell><cell>2965</cell></row><row><cell>Punching</cell><cell>4s</cell><cell>-</cell><cell>0.019s</cell><cell cols="2">0.259s 0.278s</cell><cell>863</cell></row><row><cell>Kicking</cell><cell>4s</cell><cell>-</cell><cell>0.020s</cell><cell cols="2">0.302s 0.322s</cell><cell>745</cell></row><row><cell>Style Transfer</cell><cell>8s</cell><cell>-</cell><cell>-</cell><cell cols="2">2.234s 2.234s</cell><cell>214</cell></row><row><cell>Crowd Scene</cell><cell>10s</cell><cell>0.557s</cell><cell>1.335s</cell><cell cols="3">2.252s 4.144s 28957</cell></row></table><note>138:8 • D. Holden et al. ACM Trans. Graph., Vol. 35, No. 4, Article 138, Publication Date: July 2016</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">ACM Trans. Graph., Vol. 35, No. 4, Article 138, Publication Date: July 2016</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the reviewers for the fruitful suggestions. This research is supported by Marza Animation Planet.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evolved controllers for simulated locomotion</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Motion in games</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive motion generation from examples</title>
		<author>
			<persName><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Learning Framework for Character Motion Synthesis and Editing ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="483" to="490" />
			<date type="published" when="2002-07">2002. July 2016</date>
		</imprint>
	</monogr>
	<note>ACM Trans on Graph. Publication Date</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Python for Scientific Computing Conference (SciPy)</title>
				<meeting>of the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Oral Presentation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://mocap.cs.cmu.edu/" />
		<title level="m">Carnegie-Mellon Mocap Database</title>
				<imprint/>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
				<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno>CoRR abs/1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ben-Gio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
				<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Style-based inverse kinematics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grochow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovi Ć</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="522" to="531" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and finegrained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Áez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma-Lik</surname></persName>
		</author>
		<idno>CoRR abs/1411.5752</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parametric motion graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2007 Symposium on Interactive 3D Graphics and Games</title>
				<meeting>of the 2007 Symposium on Interactive 3D Graphics and Games</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Orr</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7700</biblScope>
			<biblScope unit="page" from="599" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning motion manifolds with convolutional autoencoders</title>
		<author>
			<persName><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2015 Technical Briefs, ACM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synchronized multi-character motion editing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated extraction and parameterization of motions in large data sets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="559" to="568" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="473" to="482" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
				<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Precomputing avatar behavior from human motion data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 2004 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</title>
				<meeting>of 2004 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hierarchical approach to interactive motion editing for human-like figures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;99</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Pol-Lard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Motion fields for interactive character locomotion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wampler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Popovi Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovi Ć</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning complex neural network policies with trajectory optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st International Conference on Machine Learning</title>
				<meeting>of the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="829" to="837" />
		</imprint>
	</monogr>
	<note>ICML-14</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Continuous character control with lowdimensional embeddings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haraux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovi Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Motion graphs++: a compact generative model for semantic motion analysis and synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactive generation of human animation with deformable motion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structured recurrent temporal restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mittelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st International Conference on Machine Learning</title>
				<meeting>of the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
	<note>ICML-14</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interactive control of diverse complex characters with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lowrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
				<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geostatistical motion interpolation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mukai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuriyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1062" to="1070" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Documentation mocap database hdm05</title>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Öder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kr Üger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<idno>CG-2007-2</idno>
		<imprint>
			<date type="published" when="2007-06">2007. June</date>
		</imprint>
		<respStmt>
			<orgName>Universität Bonn</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ICML-10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th International Conference on Machine Learning</title>
				<meeting>of the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Berkeley mhad: A comprehensive multimodal human action database</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ba-Jcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Verbs and adverbs: Multidimensional motion interpolation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bodenheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Artist-directed inverse-kinematics using radial basis function interpolation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Rose Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-P</forename><forename type="middle">J</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="239" to="250" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Construction and optimal search of interpolated motion graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Safonova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2007-07">2007. July 2016</date>
		</imprint>
	</monogr>
	<note>ACM Trans. Graph.. Publication Date</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fat graphs: constructing an interactive character with continuous controls</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics Association</title>
				<meeting>of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics Association</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning bicycle stunts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Factored conditional restricted boltzmann machines for modeling motion style</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 26th International Conference on Machine Learning</title>
				<meeting>of the 26th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1025" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two distributed-state models for generating high-dimensional time series</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1025" to="1068" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
				<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1441" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Realtime style transfer for unlabeled heterogeneous human motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans on Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Natural motion animation through constraining and deconstraining at will. Visualization and Computer Graphics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="352" to="360" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-07">2014. 2014. July 2016</date>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>Publication Date</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
