<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Relational Learning for Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<email>shiyu.chang@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
							<email>xiaoxiao.guo@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Relational Learning for Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs (KGs) are the key components of various natural language processing applications. To further expand KGs' coverage, previous studies on knowledge graph completion usually require a large number of training instances for each relation. However, we observe that long-tail relations are actually more common in KGs and those newly added relations often do not have many known triples for training. In this work, we aim at predicting new facts under a challenging setting where only one training instance is available. We propose a one-shot relational learning framework, which utilizes the knowledge extracted by embedding models and learns a matching metric by considering both the learned embeddings and one-hop graph structures. Empirically, our model yields considerable performance improvements over existing embedding models, and also eliminates the need of retraining the embedding models when dealing with newly added relations. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale knowledge graphs <ref type="bibr" target="#b29">(Suchanek et al., 2007;</ref><ref type="bibr" target="#b34">Vrandečić and Krötzsch, 2014;</ref><ref type="bibr" target="#b1">Bollacker et al., 2008;</ref><ref type="bibr" target="#b0">Auer et al., 2007;</ref><ref type="bibr" target="#b3">Carlson et al., 2010)</ref> represent every piece of information as binary relationships between entities, usually in the form of triples i.e. <ref type="bibr">(subject, predicate, object)</ref>. This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web.</p><p>Despite KGs' large scale, they are known to be highly incomplete <ref type="bibr" target="#b17">(Min et al., 2013)</ref>. To automatically complete KGs, extensive research efforts <ref type="bibr" target="#b21">(Nickel et al., 2011;</ref><ref type="bibr" target="#b2">Bordes et al., 2013</ref>  et <ref type="bibr">al., 2014;</ref><ref type="bibr" target="#b31">Trouillon et al., 2016;</ref><ref type="bibr" target="#b12">Lao and Cohen, 2010;</ref><ref type="bibr" target="#b20">Neelakantan et al., 2015;</ref><ref type="bibr" target="#b36">Xiong et al., 2017;</ref><ref type="bibr" target="#b5">Das et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018)</ref> have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new facts of existing relations; and have achieved considerable performance on various public datasets. However, those datasets (e.g. FB15k, WN18) used by previous models mostly only cover common relations in KGs. For more practical scenarios, we believe the desired KG completion models should handle two key properties of KGs. First, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, a large portion of KG relations are actually long-tail. In other words, they have very few instances. But intuitively, the fewer training triples that one relation has, the more KG completion techniques could be of use. Therefore, it is crucial for models to be able to complete relations with limited numbers of triples. However, existing research usually assumes the availability of sufficient training triples for all relations, which limits their usefulness on sparse long-tail relations.</p><p>Second, to capture up-to-date knowledge, realworld KGs are often dynamic and evolving at any given moment. New relations will be added whenever new knowledge is acquired. If a model can predict new triples given only a small number of examples, a large amount of human effort could be spared. However, to predict target relations, previous methods usually rely on well-learned representations of these relations. In the dynamic scenario, the representations of new relations cannot be sufficiently trained given limited training instances, thus the ability to adapt to new relations is also limited for current models.</p><p>In contrast to previous methods, we propose a model that depends only on the entity embeddings and local graph structures. Our model aims at learning a matching metric that can be used to discover more similar triples given one reference triple. The learnable metric model is based on a permutation-invariant network that effectively encodes the one-hop neighbors of entities, and also a recurrent neural network that allows multi-step matching. Once trained, the model will be able to make predictions about any relation while existing methods usually require fine-tuning to adapt to new relations. With two newly constructed datasets, we show that our model can achieve consistent improvement over various embedding models on the one-shot link prediction task.</p><p>In summary, our contributions are three-fold:</p><p>• We are the first to consider the long-tail relations in the link prediction task and formulate the problem as few-shot relational learning;</p><p>• We propose an effective one-shot learning framework for relational data, which achieves better performance than various embedding-based methods;</p><p>• We also present two newly constructed datasets for the task of one-shot knowledge graph completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Embedding Models for Relational Learning Various models have been developed to model relational KGs in continous vector space and to automatically infer missing links. RESCAL <ref type="bibr" target="#b21">(Nickel et al., 2011)</ref> is one of the earlier work that models the relationship using tensor operations. <ref type="bibr" target="#b2">Bordes et al. (2013)</ref> proposed to model relationships in the 1-D vector space. Following this line of research, more advanced models such as DistMult <ref type="bibr" target="#b38">(Yang et al., 2014)</ref>, ComplEx <ref type="bibr" target="#b31">(Trouillon et al., 2016)</ref> and <ref type="bibr">ConvE (Dettmers et al., 2017)</ref> have been proposed. These embedding-based models usually assume enough training instances for all relations and entities and do not pay attention to those sparse symbols. More recently, several models <ref type="bibr" target="#b25">(Shi and Weninger, 2017;</ref><ref type="bibr" target="#b35">Xie et al., 2016)</ref> have been proposed to handle unseen entities by leveraging text descriptions. In contrast to these approaches, our model deals with long-tail or newly added relations and focuses on one-shot relational learning without any external information, such as text descriptions of entities or relations.</p><p>Few-Shot Learning Recent deep learning based few-shot learning approaches fall into two main categories: (1) metric based approaches <ref type="bibr" target="#b10">(Koch, 2015;</ref><ref type="bibr" target="#b33">Vinyals et al., 2016;</ref><ref type="bibr" target="#b26">Snell et al., 2017;</ref><ref type="bibr" target="#b39">Yu et al., 2018)</ref>, which try to learn generalizable metrics and the corresponding matching functions from a set of training tasks. Most methods in this class adopt the general matching framework proposed in deep siamese network <ref type="bibr" target="#b10">(Koch, 2015)</ref>. One example is the Matching Networks <ref type="bibr" target="#b33">(Vinyals et al., 2016)</ref>, which make predictions by comparing the input example with a small labeled support set;</p><p>(2) meta-learner based approaches <ref type="bibr" target="#b23">(Ravi and Larochelle, 2017;</ref><ref type="bibr" target="#b19">Munkhdalai and Yu, 2017;</ref><ref type="bibr" target="#b7">Finn et al., 2017;</ref><ref type="bibr" target="#b13">Li et al., 2017)</ref>, which aim to learn the optimization of model parameters (by either outputting the parameter updates or directly predicting the model parameters) given the gradients on few-shot examples. One example is the LSTMbased meta-learner <ref type="bibr" target="#b23">(Ravi and Larochelle, 2017)</ref>, which learns the step size for each dimension of the stochastic gradients. Besides the above categories, there are also some other styles of few-shot learning algorithms, e.g. Bayesian Program Induction <ref type="bibr" target="#b11">(Lake et al., 2015)</ref>, which represents concepts as simple programs that best explain observed examples under a Bayesian criterion.</p><p>Previous few-shot learning research mainly focuses on vision and imitation learning <ref type="bibr" target="#b6">(Duan et al., 2017)</ref> domains. In the language domain, <ref type="bibr" target="#b39">Yu et al. (2018)</ref> proposed a multi-metric based approach for text classification. To the best of our knowledge, this work is the first research on few-shot learning for knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Knowledge graphs G are represented as a collection of triples {(h, r, t)} ⊆ E × R × E, where E and R are the entity set and relation set. The task of knowledge graph completion is to either predict unseen relations r between two existing entities: (h, ?, t) or predict the tail entity t given the head entity and the query relation: (h, r, ?). As our purpose is to infer unseen facts for newly added or existing long-tail relations, we focus on the latter case. In contrast to previous work that usually assumes enough triples for the query relation are available for training, this work studies the case where only one training triple is available. To be more specific, the goal is to rank the true tail entity t true higher than other candidate entities t ∈ C h,r , given only an example triple (h 0 , r, t 0 ). The candidates set is constructed using the entity type constraint <ref type="bibr" target="#b30">(Toutanova et al., 2015)</ref>. It is also worth noting that when we predict new facts of the relation r, we only consider a closed set of entities, i.e. no unseen entities during testing. For open-world settings where new entities might appear during testing, external information such as text descriptions about these entities are usually required and we leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">One-Shot Learning Settings</head><p>This section describes the settings for the training and evaluation of our one-shot learning model.</p><p>The goal of our work is to learn a metric that could be used to predict new facts with oneshot examples. Following the standard one-shot learning settings <ref type="bibr" target="#b33">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b23">Ravi and Larochelle, 2017)</ref>, we assume access to a set of training tasks. In our problem, each training task corresponds to a KG relations r ∈ R, and has its own training/testing triples: T r = {D train r , D test r }. This task set is often denoted as the meta-training set, T meta−train .</p><p>To imitate the one-shot prediction at evaluation time, there is only one triple (h 0 , r, t 0 ) in each</p><formula xml:id="formula_0">D train r . The D test r = {(h i , r, t i , C h i ,r</formula><p>)} consists of the testing triples of r with ground-truth tail entities t i for each query (h i , r), and the corresponding tail entity candidates C h i ,r = {t ij } where each t ij is an entity in G. The metric model can thus be tested on this set by ranking the candidate set C h i ,r given the test query (h i , r) and the labeled triple in D train r . We denote an arbitrary ranking-loss function as θ (h i , r, t i |C h i ,r , D train r ), where θ represents the parameters of our metric model. This loss function indicates how well the metric model works on tuple (h i , r, t i , C h i ,r ) while observing only one-shot data from D train r . The objective of training the metric model, i.e. the metatraining objective, thus becomes:</p><formula xml:id="formula_1">min θ ET r   (h i ,r,t i ,C h i ,r )∈D test r θ (hi, r, ti|C h i ,r , D train r ) |D test r |   , (1)</formula><p>where T r is sampled from the meta-training set T meta−train , and |D test r | denotes the number of tuples in D test r . Once trained, we can use the model to make predictions on new relations r ∈ R , which is called the meta-testing step in literature. These meta-testing relations are unseen from metatraining, i.e. R ∩ R = φ. Each meta-testing relation r also has its own one-shot training data D train r and testing data D test r , defined in the same way as in meta-training. These meta-testing relations form a meta-test set T meta−test .</p><p>Moreover, we leave out a subset of relations in T meta−train as the meta-validation set T meta−validation . Because of the assumption of one-shot learning, the meta-testing relations do not have validation sets like in the traditional machine learning setting. Otherwise, the metric model will actually see more than one-shot labeled data during meta-testing, thus the one-shot assumption is violated.</p><p>Finally, we assume that the method has access to a background knowledge graph G , which is a subset of G with all the relations from T meta−train , T meta−validation and T meta−test removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>In this section, we describe the proposed model for similarity metric learning and also the corresponding loss function we use to train our model.</p><p>The core of our proposed model is a similarity function M((h, t), (h , t )|G ). Thus for any query relation r, as long as there is one known fact (h 0 , r, t 0 ), the model could predict the likelihood of testing triples {(h i , r, t ij )|t ij ∈ C h i ,r }, based on the matching score between each (h i , t ij ) and (h 0 , t 0 ). The implementation of the above matching function involves two sub-problems: (1) the representations of entity pairs; and (2) the comparison function between two entity-pair representations. Our overall model, as shown in Figure <ref type="figure">2</ref>, deals with the above two problems with two major components respectively:</p><p>• Neighbor encoder (Figure <ref type="figure">2b</ref>), aims at utilizing the local graph structure to better represent entities. In this way, the model can leverage more information that KG provides for every entity within an entity pair.</p><p>• Matching processor (Figure <ref type="figure">2c</ref>), takes the vector representations of any two entity pairs from the neighbor encoder; then performs multi-step matching between two entity-pairs and outputs a scalar as the similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neighbor Encoder</head><p>This module is designed to enhance the representation of each entity with its local connections in knowledge graph.</p><p>Although the entity embeddings from KG embedding models <ref type="bibr" target="#b2">(Bordes et al., 2013;</ref><ref type="bibr" target="#b38">Yang et al., 2014)</ref> already have relational information encoded, previous work <ref type="bibr" target="#b20">(Neelakantan et al., 2015;</ref><ref type="bibr" target="#b14">Lin et al., 2015a;</ref><ref type="bibr" target="#b36">Xiong et al., 2017)</ref> showed that explicitly modeling the structural patterns, such as paths, is usually beneficial for relationship prediction. In view of this, we propose to use a neighbor encoder to incorporate graph structures into our metric-learning model. In order to benefit from the structural information while maintaining the efficiency to easily scale to real-world large-scale KGs, our neighbor encoder only considers entities' local connections, i.e. the one-hop neighbors.</p><p>For any given entity e, its local connections form a set of (relation, entity) tuples. As shown in Figure <ref type="figure">2a</ref>, for the entity Leonardo da Vinci, one of such tuples is (occupation, painter). We refer this neighbor set as as N e = {(r k , e k )|(e, r k , e k ) ∈ G }. The purpose of our neighbor encoder is to encode N e and output a vector as the latent representation of e. Because this is a problem of encoding sets with varying sizes, we hope the encoding function can be (1) invariant to permutations and also (2) insensitive to the size of the neighbor set. Inspired by the results from <ref type="bibr" target="#b40">(Zaheer et al., 2017)</ref>, we use the following function f that satisfies the above properties:</p><formula xml:id="formula_2">f (N e ) = σ( 1 |N e | (r k ,e k )∈Ne C r k ,e k ).</formula><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>where C r k ,e k is the feature representation of a relation-entity pair (r k , e k ) and σ is the activation function. In this paper we set σ = tanh which achieves the best performance on T meta−validation .</p><p>To encode every tuple (r k , e k ) ∈ N e into C r k ,e k , we first use an embedding layer emb with dimension d (which can be pre-trained using existing embedding-based models) to get the vector representations of r k and e k :</p><formula xml:id="formula_4">v r k = emb(r k ), v e k = emb(e k )</formula><p>Dropout <ref type="bibr" target="#b28">(Srivastava et al., 2014)</ref> is applied here to the vectors v r k , v e k to achieve better generalization. We then apply a feed-forward layer to encode the interaction within this tuple:</p><formula xml:id="formula_5">C r k ,e k = W c (v r k ⊕ v e k ) + b c ,<label>(3)</label></formula><p>where  To enable batching during training, we manually specify the maximum number of neighbors and use all-zero vectors as "dummy" neighbors. Although different entities have different degrees (number of neighbors), the degree distribution is usually very concentrated, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. We can easily find a proper bound as the maximum number of neighbors to batch groups of entities.</p><formula xml:id="formula_6">W c ∈ R d×2d , b c ∈ R d are</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity count</head><p>The neighbor encoder module we propose here is similar to the Relational Graph Convolutional Networks <ref type="bibr" target="#b24">(Schlichtkrull et al., 2017)</ref> in the sense that we also use the shared kernel {W c , b c } to encode the neighbors of different entities. But unlike their model that operates on the whole graph and performs multiple steps of information propagation, we only encode the local graphs of the entities and perform one-step propagation. This enables us to easily apply our model to large-scale KGs such as Wikidata. Besides, their model also does not operate on pre-trained graph embeddings. We leave the investigation of other graph encoding strategies, e.g. <ref type="bibr" target="#b37">(Xu et al., 2018;</ref><ref type="bibr" target="#b27">Song et al., 2018)</ref>, to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Matching Processor</head><p>Given the neighbor encoder module, now we discuss how we can do effective similarity matching based on our recurrent matching processor. By applying f (N e ) to the reference entity pair (h 0 , t 0 ) and any query entity pair (h i , t ij ), we get two Shuffle the tasks in T meta−learning 7:</p><p>for Tr in T meta−learning do 8:</p><p>Sample one triple as the reference 9:</p><p>Sample a batch B + of query triples 10:</p><p>Pollute the tail entity of query triples to get B − 11:</p><p>Calculate the matching scores for triple in B + and B − 12:</p><p>Calculate the batch loss L = B 13:</p><p>Update θ using gradient g ∝ ∇L 14:</p><p>end for 15: end for neighbor vectors for each:</p><formula xml:id="formula_7">[f (N h 0 ); f (N t 0 )] and [f (N h i ); f (N t ij )].</formula><p>To get a similarity score that can be used to rank (h i , t ij ) among other candidates, we can simply concatenate the f (N h ) and f (N t ) in each pair to form a single pair representation vector, and calculate the cosine similarity between pairs. However, this simple metric model turns out to be too shallow and does not give good performance. To enlarge our model's capacity, we leverage a LSTM-based (Hochreiter and Schmidhuber, 1997) recurrent "processing" block <ref type="bibr" target="#b32">(Vinyals et al., 2015</ref><ref type="bibr" target="#b33">(Vinyals et al., , 2016) )</ref> to perform multi-step matching. Every process step is defined as follows:</p><formula xml:id="formula_8">h k+1 , c k+1 = LST M (q, [h k ⊕ s, c k ]) h k+1 = h k+1 + q score k+1 = h k+1 s h k+1 s ,<label>(4)</label></formula><p>where LST M (x, [h, c]) is a standard LSTM cell with input x, hidden state h and cell state c, and</p><formula xml:id="formula_9">s = f (N h 0 ) ⊕ f (N t 0 ), q = f (N h i ) ⊕ f (N t ij ) are</formula><p>the concatenated neighbor vectors of the reference pair and query pair. After K processing steps<ref type="foot" target="#foot_0">2</ref> , we use score K as the final similarity score between the query and support entity pair. For every query (h i , r, ?), by comparing (h i , t ij ) with (h 0 , t 0 ), we can get the ranking scores for every t ij ∈ C h i ,r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss Function and Training</head><p>For a query relation r and its reference/training triple (h 0 , r, t 0 ), we collect a group of positive (true) query triples {(h i , r, t + i )|(h i , r, t + i ) ∈ G} and construct another group negative (false) query triples {(h i , r, t − i )|(h i , r, t − i ) ∈ G} by polluting the tail entities. Following previous embeddingbased models, we use a hinge loss function to optimize our model:</p><formula xml:id="formula_10">θ = max(0, γ + score − θ − score + θ ),<label>(5)</label></formula><p>where score + θ and score − θ are scalars calculated by comparing the query triple (h i , r, t + i /t − i ) with the reference triple (h 0 , r, t 0 ) using our metric model, and the margin γ is a hyperparameter to be tuned.  Existing benchmarks for knowledge graph completion, such as FB15k-237 <ref type="bibr" target="#b30">(Toutanova et al., 2015)</ref> and <ref type="bibr">YAGO3-10 (Mahdisoltani et al., 2013)</ref> are all small subsets of real-world KGs. These datasets consider the same set of relations during training and testing and often include sufficient training triples for every relation. To construct datasets for one-shot learning, we go back to the original KGs and select those relations that do not have too many triples as one-shot task relations. We refer the rest of the relations as background relations, since their triples provide important background knowledge for us to match entity pairs.</p><p>Our first dataset is based on NELL <ref type="bibr" target="#b18">(Mitchell et al., 2018)</ref>, a system that continuously collects structured knowledge by reading webs. We take the latest dump and remove those inverse relations. We select the relations with less than 500 but more than 50 triples 3 as one-shot tasks. To show that our model is able to operate on large-scale KGs, 3 We want to have enough triples for evaluation.</p><p>we follow the similar process to build another larger dataset based on Wikidata <ref type="bibr" target="#b34">(Vrandečić and Krötzsch, 2014)</ref>. The dataset statistics are shown in Table <ref type="table" target="#tab_2">1</ref>. Note that the Wiki-One dataset is an order of magnitude larger than any other benchmark datasets in terms of the numbers of entities and triples. For NELL-One, we use 51/5/11 task relations for training/validation/testing. For Wiki-One, the division ratio is 133:16:34.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>In our experiments, we consider the following embedding-based methods: RESCAL <ref type="bibr" target="#b21">(Nickel et al., 2011)</ref>, TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, Dist-Mult <ref type="bibr" target="#b38">(Yang et al., 2014)</ref> and ComplEx <ref type="bibr" target="#b31">(Trouillon et al., 2016)</ref>. For TransE, we use the code released by <ref type="bibr" target="#b15">Lin et al. (2015b)</ref>. For the other models, we have tried the code released by <ref type="bibr" target="#b31">Trouillon et al. (2016)</ref> but it gives much worse results than TransE on our datasets. Thus we use our own implementations based on PyTorch <ref type="bibr" target="#b22">(Paszke et al., 2017)</ref> for comparison. When evaluating existing embedding models, during training, we use not only the triples of background relations but also all the triples of the training relations and the one-shot training triple of those validation/test relations. However, since the proposed metric model does not require the embeddings of query relations, we only include the triples of the background relations for embedding training. As TransE and DistMult use 1-D vectors to represent entities and relations, they can be directly used in our natching model. While for RESCAL, since it uses matrices to represent relations, we employ mean-pooling over these matrices to get 1-D embeddings. For the ComplEx model, we use the concatenation of the real part and imaginary part. The hyperparameters of our model are tuned on the validation task set and can be found in the appendix.</p><p>Apart from the above embedding models, a more recent method <ref type="bibr">(Dettmers et al., 2017)</ref> applies convolution to model relationships and achieves the best performance on several benchmarks. For every query (h, r, ?), their model enumerates the whole entity set to get positive and negative triples for training. We find that this training paradigm takes lots of computational resources when dealing with large entity sets and cannot scale to realworld KGs such as Wikidata<ref type="foot" target="#foot_1">4</ref> that have millions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NELL-One</head><p>Wiki-One Model MRR Hits@10 Hits@5 Hits@1 MRR Hits@10 Hits@5 Hits@1 of entities. For the scalability concern, our experiments only consider models that use negative sampling for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The main results of our methods are shown in Table 2. We denote our method as "GMatching" since our model is trained to match local graph patterns. We use mean reciprocal rank (MRR) and Hits@K to evaluate different models. We can see that our method produces consistent improvements over various embedding models on these one-shot relations. The improvements are even more substantial on the larger Wiki-One dataset.</p><p>To investigate the learning power of our model, we also try to train our metric model with randomly initialized embeddings. Surprisingly, although the results are worse than the metric models with pre-trained embeddings, they are still superior to the baseline embedding models. This suggests that, by incorporating the neighbor entities into our model, the embeddings of many relations and entities actually get updated in an effective way and provide useful information for our model to make predictions on test data. It is worth noting that once trained, our model can be used to predict any newly added relations without fine-tuning, while existing models usually need to be re-trained to handle those newly added symbols. On a large real-world KG, this re-training process can be slow and highly computational expensive.</p><p>Remark on Model Selection Given the existence of various KG embedding models, one interesting experiment is to incorporate model selec-tion into hyper-parameter tuning and choose the best validation model for testing.</p><p>If we think about comparing KG embedding and metric learning as two approaches, the results from the model selection process can then be used as the "final" measurement for comparison. For example, the baseline KG embedding achieves best MRR on Wiki-One with RESCAL (11.9%), so we report the corresponding testing MRR (7.2%) as the final model selection result for KG embedding approach. In this way, at the top half of Table <ref type="table" target="#tab_3">2</ref>, we select the best KG embedding method according to the validation performance. The results are highlighted with underlines. Similarly, we select the best metric learning approach at the bottom.</p><p>Our metric-based method outperforms KG embedding by a large margin from this perspective as well. Taking MRR as an example, the selected metric model achieves 17.1% on NELL-One and 20.0% on Wiki-One; while the results of KG embedding are 9.3% and 7.2%. The improvement is 7.8% and 12.8% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis on Neighbor-Encoder</head><p>As our model leverages entities' local graph structures by encoding the neighbors, here we try to investigate the effect of the neighbor set by restricting the maximum number of neighbors. If the size of the true neighbor set is larger than the maximum limit, the neighbors are then selected by random sampling. Figure <ref type="figure">4</ref> shows the learning curves of different settings. These curves are based on the Hits@10 calculated on the validation set. We see that encoding more neighbors  for every entity generally leads to better performance. We also observe that the model that encodes 40 neighbors in maximum actually yields worse performance than the model that only encodes 30 neighbors. We think the potential reason is that for some entity pairs, there are some local connections that are irrelevant and provide noisy information to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head><p>We conduct ablation studies using the model that achieves the best Hits@10 on the NELL-One dataset. The results are shown in Table <ref type="table" target="#tab_6">4</ref>. We use Hits@10 on validation and test set for comparison, as the hyperparameters are selected us-ing this evaluation metric. We can see that both the matching processor<ref type="foot" target="#foot_2">5</ref> and the neighbor encoder play important roles in our model. Another important observation is that the scaling factor 1/N e turns out to be very essential for the neighbor encoder. Without scaling, the neighbor encoder actually gives worse results compared to the simple embedding-based matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Performance on Different Relations</head><p>When testing various models, we observe that the results on different relations are actually of high variance. Table <ref type="table" target="#tab_5">3</ref> shows the decomposed results on NELL-One generated by our best metric model (GMatching-ComplEx) and its corresponding embedding method. For reference, we also report the embedding model's performance under standard training settings where 75% of the triples (instead of only one) are used for training and the rest are used for testing. We can see that relations with smaller candidate sets are generally easier and our model could even perform better than the embedding model trained under standard settings. For some relations such as athleteInjured-HisBodypart, their involved entities have very few connections in KG. It is as expected that one-shot learning on these kinds of relations is quite challenging. Those relations with lots of (&gt;3000) candidates are challenging for all models. Even for embedding model with more training triples, the performance on some relations is still very limited. This suggests that the knowledge graph completion task is still far from being solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper introduces a one-shot relational learning framework that could be used to predict new facts of long-tail relations in KGs. Our model leverages the local graph structure of entities and learns a differentiable metric to match entity pairs. In contrast to existing methods that usually need finetuning to adapt to new relations, our trained model can be directly used to predict any unseen relation and also achieves much better performance in the one-shot setting. Our future work might consider incorporating external text data and also enhancing our model to make better use of multiple training examples in the few-shot learning case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The histogram of relation frequencies in Wikidata. There are a large portion of relations that only have a few triples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: a) and b): Our neighbor encoder operating on entity Leonardo da Vinci; c): The matching processor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The distribution of entities' degrees (numbers of neighbors) on our two datasets. Since we work on closed-set of entities, we draw the figure by considering the intersection between entities in our background knowledge G and the entities appearing in T meta−train , T meta−validation or T meta−test . Note that all triples in T meta−train , T meta−validation or T meta−test are removed from G . Upper: NELL; Lower: Wikidata.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>One-shot Training 1: Input: 2: a) Meta-training task set Tmeta−training; 3: b) Pre-trained KG embeddings (excluding relation in Tmeta−training); 4: c) Initial parameters θ of the metric model; 5: for epoch = 0:M-1 do 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For every training episode, we first sample one task/relation T r from the meta-training set T meta−training . Then from all the known triples in T r , we sample one triple as the reference/training triple D train r and a batch of other triples as the positive query/test triples D test r . The detail of the training process is shown in Algorithm 1. Our experiments are discussed in the next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Entity count Entity degree distribution on NELL and Wikidata</head><label></label><figDesc>parameters to be learned and ⊕ denotes concatenation.</figDesc><table><row><cell>1600 1400 1200 1000 800 600 400 200 0 0 5000 10000 15000 20000 25000 30000 35000</cell><cell>0 0</cell><cell>50 50</cell><cell>100 100 Number of neighbors 150 150</cell><cell>200 200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the Datasets. # Ent. denotes the number of unique entities and # R. denotes the number of all relations. # Tasks denotes the number of relations we use as one-shot tasks.</figDesc><table><row><cell cols="2">5 Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">5.1 Datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell># Ent.</cell><cell># R.</cell><cell># Triples</cell><cell># Tasks</cell></row><row><cell>NELL-One</cell><cell>68,545</cell><cell>358</cell><cell>181,109</cell><cell>67</cell></row><row><cell>Wiki-One</cell><cell cols="3">4,838,244 822 5,859,240</cell><cell>183</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>RESCAL .071/.140 .100/.229 .082/.186 .048/.089 .119/.072 .167/.082 .132/.062 .093/.051 TransE .082/.093 .177/.192 .126/.141 .032/.043 .023/.035 .036/.052 .029/.043 .015/.025 DistMult .075/.102 .128/.177 .093/.126 .045/.066 .042/.048 .086/.101 .055/.070 .017/.019 ComplEx .072/.131 .128/.223 .041/.086 .041/.086 .079/.069 .148/.121 .106/.092 .046/.040 Link prediction results on validation/test relations. KG embeddings baselines are shown at the top of the table and our one-shot learning (GMatching) results are shown at the bottom. Bold numbers denote the best results on meta-validation/meta-test. Underline numbers denote the model selection results from all KG embeddings baselines, or from all one-shot methods, i.e. selecting the method with the best validation score and reporting the corresponding test score.</figDesc><table><row><cell cols="2">GMatching (RESCAL) .144/.188 .277/.305 .216/.243 .087/.133 .113/.139 .330/.305 .180/.228 .033/.061</cell></row><row><cell>GMatching (TransE)</cell><cell>.168/.171 .293/.255 .239/.210 .103/.122 .167/.219 .349/.328 .289/.269 .083/.163</cell></row><row><cell>GMatching (DistMult)</cell><cell>.119/.171 .238/.301 .183/.221 .054/.114 .190/.222 .384/.340 .291/.271 .114/.164</cell></row><row><cell cols="2">GMatching (ComplEx) .132/.185 .308/.313 .232/.260 .049/.119 .201/.200 .350/.336 .231/.272 .141/.120</cell></row><row><cell>GMatching (Random)</cell><cell>.083/.151 .211/.252 .135/.186 .024/.103 .174/.198 .309/.299 .222/.260 .121/.133</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results decomposed over different relations. " " denotes the results with standard training settings and "# Candidates" denotes the size of candidate entity set.</figDesc><table><row><cell></cell><cell>0.30 0.35</cell><cell>N=10 N=20 N=30 N=40 N=50</cell><cell>Learning curves</cell></row><row><cell></cell><cell>0.25</cell><cell></cell></row><row><cell>Hits@10</cell><cell>0.15 0.20</cell><cell></cell></row><row><cell></cell><cell>0.10</cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell cols="2">Training steps 0 100000 200000 300000 400000 500000 600000</cell></row><row><cell cols="4">Figure 4: The learning curves on NELL-one. Every</cell></row><row><cell cols="4">run uses different number of neighbors. The y-axis is</cell></row><row><cell cols="4">Hits@10 calculated on all the validation relations.</cell></row><row><cell></cell><cell></cell><cell cols="2">Configuration</cell><cell>Hits@10</cell></row><row><cell></cell><cell></cell><cell cols="2">Full Model with ComplEx .308/.313</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Matching Processor</cell><cell>.266/.269</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Neighbor Encoder</cell><cell>.248/.296</cell></row><row><cell></cell><cell></cell><cell cols="2">w/o Scaling Factor</cell><cell>.229/.219</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation on different components.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">K is a hyperparameter to be tuned.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">On a GPU card with 12GB memory, we fail to run their ConvE model on Wiki-One with batch size 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">Matching without Matching Processor is equivalent to matching using simple cosine similarity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by an IBM Faculty Award. We also thank the anonymous reviewers for their useful feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>For the NELL dataset, we set embedding size as 100. For Wikidata, we set the embedding size as 50 for faster training with millions of triples. The embeddings are trained for 1,000 epochs. The other hyperparamters are tuned using the Hits@10 metric 6 on the validation tasks. For matching steps, the optimal setting is 2 for NELL-One and 4 for Wiki-One. For the number of neighbors, we find that the maximum limit 50 works the best for both datasets. For parameter updates, we use Adam (Kingma and Ba, 2014) with the initial learning rate 0.001 and we half the learning rate after 200k update steps. The margin used in our loss function is 5.0. The dimension of LSTM's hidden size is 200. Table <ref type="table">5</ref>: 5-shot experiments on NELL-One. 6 The percentage of correct answer ranks within top10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Few-Shot Experiments</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
				<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><surname>Estevam R Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06581</idno>
		<title level="m">Variational knowledge graph reasoning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05851</idno>
		<idno>arXiv:1707.01476</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
				<editor>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Oneshot imitation learning</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradly</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ope-Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Siamese neural networks for oneshot image recognition</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00379</idno>
		<imprint>
			<date type="published" when="2015">2015a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter</title>
				<meeting>the 2013 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neverending learning</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estevam</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><surname>Kisiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00837</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Meta networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base inference</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mc-Callum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 aaai spring symposium series</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Open-world knowledge graph completion</title>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03438</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A graph-to-sequence model for amr-to-text generation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02473</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<title level="m">Order matters: Sequence to sequence for sets</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06690</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Graph2seq: Graph to sequence learning with attention-based neural networks</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vadim</forename><surname>Sheinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00823</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07513</idno>
		<title level="m">Diverse few-shot text classification with multiple metrics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
