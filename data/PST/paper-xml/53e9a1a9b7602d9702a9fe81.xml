<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The ALIVE system: wireless, full-body interaction with autonomous agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pattie</forename><surname>Maes</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Media Laboratory</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Media Laboratory</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bruce</forename><surname>Blumberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Media Laboratory</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Pentland</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Media Laboratory</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The ALIVE system: wireless, full-body interaction with autonomous agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8AB841FD83F7C341179D35AFCE549C52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Interactive interface -Virtual reality -Autonomous agents -Computer vision</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The cumbersome nature of wired interfaces often limits the range of application of virtual environments. In this paper, we discuss the design and implementation of a novel system, called ALIVE, which allows unencumbered full-body interaction between a human participant and a rich graphical world inhabited by autonomous agents. Based on results obtained with thousands of users, the paper argues that this kind of system can provide more complex and very different experiences than traditional virtual reality systems. The ALIVE system significantly broadens the range of potential applications of virtual reality systems; in particular, the paper discusses novel applications in the area of training and teaching, entertainment, and digital assistants or interface agents. We give an overview of the methods used in the implementation of the existing ALIVE systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Virtual environments allow a user to be embedded into a computer-generated environment. Most interfaces to these environments require the use of gloves, goggles, and/or a helmet, and offer limited interactions including only hand gestures and the ability to change viewpoint. (Notable exceptions are <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref>, discussed below.) The cumbersome nature of the equipment and the limited nature of the interaction in these interfaces has proven to restrict the range of applications that have used this type of technology.</p><p>In this paper, we discuss the design and implementation of a novel system called ALIVE, an "Artificial Life Interactive Video Environment", that allows wireless full-body interaction between a human participant and a rich graphical world inhabited by autonomous agents. We use a single video camera to obtain a color image of a person, which we composite into a 3D graphical world. The resulting image is projected onto a large screen that faces the user and acts as a Correspondence to: P. Maes type of "magic mirror" (Fig. <ref type="figure">1</ref>): the user sees his/herself surrounded by objects and agents. No goggles, gloves, or wires are needed for interaction with the virtual world. Computer vision and audition techniques are used to extract information about the person, such as his/her 3D location, the position of various body parts as well as simple gestures and utterances. We combine active sensing and domain knowledge to achieve robust and realtime performance on these tasks.</p><p>The virtual world in our system is inhabited by inanimate objects as well as agents. Agents are modeled as autonomously behaving entities that have their own sensors and goals and that can interpret the actions of the participant and react to them in "interactive-time". Because of the presence of these semi-intelligent entities, the system does not only allow for the obvious direct-manipulation style of interaction, but also for a more powerful, indirect style of interaction in which gestures can have more complex meanings, which may vary according to the situation in which the agents and user find themselves.</p><p>Based on results obtained with real users in several public forums where the ALIVE system has been installed, this paper argues that (1) the "magic mirror" approach has several advantages over head-mounted display-based virtual reality systems for certain applications, and (2) virtual worlds including autonomous agents can provide more complex and very different experiences than traditional interactive virtual environments. The ALIVE system significantly broadens the range of potential applications of virtual reality systems; in particular, we discuss applications in the area of training and teaching, entertainment and participatory theater, and last but not least, telecommunication and interface agents.</p><p>The following sections discuss how autonomous agents are modeled in the ALIVE system, how vision algorithms are used to implement a wireless full-body interface, and how the agents we have devised interact with people. We conclude with a discussion of the experiences of users of our installations, issues for further study, and possible novel applications now possible with this new interface. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling agents</head><p>One important feature of the ALIVE system is that it allows the user to interact, in natural and believable ways, with autonomous semi-intelligent agents. Our agents have a set of internal needs and motivations, a set of sensors with which to perceive their environment, a repertoire of activities which they can perform, a motor system which allows them to move in and act on the virtual environment and a behavior system which chooses, in real time, the set of activities to perform given the internal needs of the agent and the opportunities presented by the environment. The agent's state and geometry is updated according to the motor activities associated with the chosen behaviors and is re-rendered on every time-step. The user's location and hand and body gestures affect the behavior of the agents, and the user receives visual as well as auditory feedback about the agents' internal state and reactions.</p><p>A number of researchers have taken an artificial intelligence approach to animation, in which the animated agents perform some actions in response to their perceived environment. Reynolds <ref type="bibr" target="#b27">[28]</ref> was among the first to apply behaviorbased animation to computer graphics (see also <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref>). This work, together with the work of Raibert and Hodgkins <ref type="bibr" target="#b25">[26]</ref>, Girard and Maciejewski <ref type="bibr" target="#b17">[18]</ref>, McKenna and Zeltzer <ref type="bibr" target="#b23">[24]</ref>, and Badler et al. <ref type="bibr" target="#b1">[2]</ref> has focused primarily on modeling single behaviors (e.g. flocking, walking, etc.), and on the problems of motion planning and motor control. Some researchers, including Badler et al. <ref type="bibr" target="#b1">[2]</ref>, Zeltzer <ref type="bibr" target="#b35">[36]</ref> and the Thalmanns <ref type="bibr" target="#b29">[30]</ref> have begun to propose more general architectures for behavior-based animated characters. Most recently, Tu and Terzopoulos <ref type="bibr" target="#b30">[31]</ref> have modeled autonomous animated fish. Their model incorporates a "physics-based" model of fish locomotion and motor control, a perception system utilizing synthetic vision, and a behavior system which models a number of fish behaviors. While very impressive, the system's design is closely tied to the specifics of fish locomotion and behavior.</p><p>A number of other researchers have focused on building systems in which animated agents interact, in realtime, with the user. Joe Bates' Woggle world combines behavior models with ideas from traditional animation to explore what he calls "believable agents" <ref type="bibr" target="#b3">[4]</ref>. Bates' agents can have fairly complex interactions, despite the user interaction being limited to moving a mouse. Fisher's <ref type="bibr" target="#b16">[17]</ref> Menagerie system allows a user to interact with animated agents in real time using goggles. In contrast with behaviors in the ALIVE system, these agents typically are engaged in a single high-level behavior, for example, flocking. A major difference between the ALIVE system and these others is the combination of both behavioral and motivational complexity in the creature model, and the use of a vision interface with which these behaviors can use the user's actual position, body pose, and hand gestures as sensory input.</p><p>The ALIVE system incorporates a behavior modeling tool kit for developing semi-intelligent autonomous agents and their interaction with one another and with the user <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. This tool kit is the result of our long-term research into architectures for adaptive autonomous agents <ref type="bibr" target="#b21">[22]</ref>; its goal is to produce agents that choose the set of activities on every time step that "make the most sense" given the agent's internal needs and motivations, past history, and the perceived environment with its attendant opportunities, challenges and changes.</p><p>Deciding on the right set of actions is complicated by a number of factors. For example, due to the problems inherent in sensing and perception, an agent's perception of its world is likely to be incomplete at best, and completely erroneous at worst. Moreover, there are typically competing goals which work at cross-purposes (e.g. moving toward food may move the agent away from water). Ideally, an agent will neither dither among competing activities nor pursue an unattainable goal to the exclusion of lower priority but attainable goals. External opportunities need to be weighed against internal needs in order to provide just the right level of opportunistic behavior. Actions may be unavailable or unreliable. To successfully produce competent autonomous action over extended periods of time, the agent's behavior system must provide solutions to these problems, as well as others. See <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref> for a detailed discussion of how our archictecture addresses these issues.</p><p>When using the behavior tool kit to build an agent, the designer specifies:</p><p>-The virtual sensors of the agent. Virtual sensors are used by the agents to sense other agents in the world including the agent which acts as the virtual world's representation of the user. Agents in ALIVE also use synthetic vision for low-level obstacle avoidance and navigation. See Reynolds et al. <ref type="bibr" target="#b27">[28]</ref>, Renault et al. <ref type="bibr" target="#b26">[27]</ref> and Tu and Terzopoulos <ref type="bibr" target="#b30">[31]</ref> for other examples of this approach. -The releasing mechanisms of the agent. Releasing mechanisms are entities which identify behaviorally significant stimuli from the agent's set of sensors and transduce values which represent the behavior-specific strength of the stimuli. For example, a dog may have a releasing mechanism for detecting when the user's hand is extended and down and its value may be inversely proportional to the distance of the dog's nose from the user's hand. -The motivations or internal needs of the agent. Internal needs are modeled as variables which may vary over time. For example, a "dog" agent may have an internal need to receive attention from the user. Whenever the user pats the dog, this variable will temporarily decrease in value and as a result the dog will be less motivated to seek human attention. -The behaviors of the agent. The behavior system of the agent is organized as a distributed collection of goaldirected, self-interested entities called behaviors or activities (e.g. "find-food", "chase-hamster"). Behaviors rely on input from relevant motivational variables and releasing mechanisms to arrive at a "value" which represents the importance or relevance of the behavior given the agent's state and environment. They compete on the basis of this value for control of the agent's body (i.e. for the prioritized privilege to issue motor commands to the agent's motor system). Behaviors may be organized into groups of competing behaviors called behavior groups, which, in turn, may be organized into loose hierarchies, with the top of the hierarchies representing more general behaviors (e.g. "find-food") and the leaves representing more specific behaviors (e.g. "chew"). The lowest level behaviors control the motor system, for example, making the dog move a little to the left or right, or making it bark in a certain way. -The motor skills of the agent. The primitive actions of the agent such as "walking", "sitting", "wagging tail" etc. are called motor skills. Motor skills rely on kinematic and inverse-kinematic modeling to modify the underlying geometry. Motor skills and the motor system in general are defined in such a way that only motor skills which are complementary may run concurrently. Thus, the dog may "walk", "look-at", "wag-tail" and "growl" all at the same time, but may not "walk" and "sit" concurrently. In addition, the motor system supports blending of motor preferences by the behavior system. For example, one behavior may express a preference to move forward, and another to use a "bound", and the result is the agent moves forward using a bounding gait.</p><p>The ALIVE system creates a "special" 3D agent in the environment to represent the user. The position and state of that agent are based on the information computed by the vision system on the basis of the camera image of the user, as well as auditory cues <ref type="bibr" target="#b8">[9]</ref>. Thus, the artificial agents can sense the user using the same virtual sensors that they use to detect objects and other agents. The person agent is rendered in the final image using the live video image of the actual user, or using a combination of graphics and texture-mapped video in situations where bandwidth is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A vision-based interface</head><p>We wish to create a non-intrusive interface to virtual worlds, while allowing a rich and intuitive set of gestures to be used in controlling and navigating that world. Passive computer vision techniques hold great promise as an interface tool, especially as available computational and video processing power increases. We are currently at the point where simple, realtime computer vision techniques can provide the types of information that had previously only been possible with wired sensors, as well as providing information that has heretofore been unavailable.</p><p>A wireless sensor, such as vision, has several additional advantages over tethered goggles-and-gloves interfaces. It provides a safer solution because the user can still see where he/she is moving, and thus can avoid bumping into things, or tripping over wires. Second, the user enjoys greater behavioral and expressive freedom. We observed that users of the ALIVE system feel very uninhibited (we have have seen users doing cartwheels, jumping jacks, etc). Finally, the user ends up concentrating more on the environment itself, rather than on the complex and unfamiliar equipment being used to interact with that environment.</p><p>Other systems, such as the Visual Portal <ref type="bibr" target="#b14">[15]</ref> and the CAVE <ref type="bibr" target="#b9">[10]</ref> system have solved many of the limitations of traditional goggle-based environments through the use of wireless batons and other sensors, thus avoiding both the problems of a tethered display and viewpoint estimation (head angle). Our system has the advantage that it is completely unencumbered, and works on users with no special tools or marks. We also adopt a mirror paradigm, where the user explicitly sees a representation of him/herself and his/her relationship to other objects in the world.</p><p>The novel vision-based interface presented here was inspired by the pioneering work of Myron Krueger's Videoplace system <ref type="bibr" target="#b20">[21]</ref>. ALIVE and Videoplace differ primarily in three respects. The first is that Videoplace focuses on 2D rather than 3D worlds and interaction. A second difference is our emphasis on modeling agents. Most of Krueger's worlds allow users to interact with other users, a notable exception being the "critter", a 2D animated sprite. Finally, the ALIVE vision system is able to recognize hand and body gestures as patterns in space and time.</p><p>Another system that bears similarities to ALIVE is the Mandala system <ref type="bibr" target="#b31">[32]</ref> which composites the user's color image with a virtual world that is sometimes video-based and sometimes computer-animated. Unlike ALIVE, the Mandala system only supports 2D and requires a chromakey background or specially colored manipulation objects; it does not attempt to recognize parts of the user's figure nor does it do any gesture recognition. Other systems have been developed for vision-based interactive graphics but have generally been restricted to off-line analysis of either face or limb motion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>. (However, see <ref type="bibr" target="#b12">[13]</ref> for a realtime facial analysis system.)</p><p>We have developed a set of vision routines for perceiving body actions and gestures performed by a human participant in an interactive system. Vision routines in ALIVE acquire the image of the user, compute a figure/ground segmentation, and find the location of head, hands, and other salient body features. We use only a single, calibrated, wide field-of-view camera to determine the 3D position of these features. We do assume that the background is fixed, although it can be arbitrarily complex, and that the person is normally facing the camera/screen. With the most recent version of our system, the integration of the person and and localization of his/her head or hand features in the world are performed using the following modules: figure-ground processing, scene projection, hand tracking, and gesture interpretation.</p><p>To detect appropriate hand/face features and composite the user's image onto the magic mirror, the vision system must isolate the figure of the user from the background (and from other users, if present). This is accomplished by use of spatially local pattern recognition techniques to characterize changes in the scene, followed by connected-components and morphological analysis to extract objects.</p><p>We assume the background to be an arbitrary, but static, pattern. Mean and variance information about the background pattern are computed, and these statistics are used to determine space-variant criteria for pixel class membership. We will omit mathematical details of our segmentation algorithm, for further reference see <ref type="bibr" target="#b34">[35]</ref>. In general, we use a hierarchical color classification to compute figure/ground segmentation, using a Gaussian model of each background pixel's color and an n-class adaptive model of foreground (person) colors. The classification takes care to identify possible shadow regions, and to normalize these region's brightness before the figure/ground classification. The classification also makes use of Markov neighborhood statistics in setting the priors for each pixel's classification.</p><p>Once each pixel has been identified as most likely belonging to the user, we use connected components and mor-phological analysis to delineate the foreground region. This analysis begins with a seed point at the centroid location of the person in the previous frame; if this fails to grow a sufficiently large region, random seed points are selected until a stable region is found. Finally, we compute the contour of the extracted region by chain-coding the connected foreground region.</p><p>When the figure of the user has been isolated from the background, we compute an estimate of its 3D location in the world. If we assume the user is indeed sitting or standing on the ground plane, and we know the calibration of the camera, then we can compute the location of the bounding box in 3D. Establishing the calibration of a camera is a well-studied problem, and several classical techniques are available to solve it in certain broad cases <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>. Typically, these methods model the camera optics as a pinhole perspective optical system, and establish its parameters by matching known 3D points with their 2D projection.</p><p>Knowledge of the camera geometry allows us to project a ray from the camera through the 2D projection of the bottom of the bounding box of the user. Since the user is on the ground plane, the intersection of the projected ray and the ground plane will establish the 3D location of the user's feet. The 2D dimensions of the user's bounding box and its base location in 3D constitute the low-level information about the user that is continuously computed and made available to all agents in the computer graphics world. The contour is projected from 2D screen coordinates into 3D world coordinates, based on the computed depth location of the person. This is then used to perform video compositing and depth clipping to combine the user's video image with computer graphics imagery.</p><p>One of the most salient cues used by the agents in our world is the location of the user's hands. We have implemented a feature localization algorithm that determines hand locations by searching within a window along the side of the contour for extremal horizontal and vertical points. If the highest point in the window is above the shoulder of the user, we label that the hand, otherwise the horizontal extremal point is used. The highest point within a window of the contour located above the centroid of the foreground region is labeled the head. These feature localization algorithms are not infallible, but we have found they work well in a wide range of conditions, especially if combined with color space classification to identify the location of flesh tones <ref type="bibr" target="#b34">[35]</ref>.</p><p>ALIVE improves on earlier systems in which only the 2D position of the user's hand was used to determine activation of objects such as virtual buttons. The improvements avoid inadvertent manipulation of objects, such as unintended activation of buttons. The system uses combination of clues, including 2D position of the hands, Z-position of the user's body, and gesture information to make sure that the user's intention is to actually manipulate an object. For example, in order for the button to be pushed, the user has to perform a "pointing gesture", have the hand over the button in 2D and be in the correct Z-plane.</p><p>Both the absolute position of hands, and whether they are performing characteristic gesture patterns, are relevant to the agents in the virtual world. We use pattern recognition strategies to detect and classify these characteristic gesture patterns. Static gestures, such as pointing, are computed directly from the hand feature location. To recognize dynamic gestures, we use a high-resolution, active camera to provide a foveated image of the hands (or face) of the user. The camera is guided by the computed feature location, and provides images which can be used successfully in a spatio-temporal gesture recognition method <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ALIVE environments</head><p>We have combined the behavior modeling and vision techniques described above to construct a system for videobased interaction with artificial agents. In our system the user moves around in a real-world space of approximately 16 by 16 feet. A video camera at the front of the space captures the user's image, which is then segmented from the background. The user's image is composited with the 3D animated world and the resulting image displayed on a large projection screen (10 by 10 feet) that faces the user.</p><p>Several virtual worlds have been implemented for this environment. The first ALIVE installation had two worlds, one inhabited by a puppet character and one by hamster/predator creatures. The behavior of these agents were fairly simple, and was constructed using the behavior toolkit described above.</p><p>In the puppet world, the puppet would follow the user around (in 3D) and try to hold the user's hand, and would imitate some of the actions of the user (sitting down, jumping, etc). It would be sent away when the user pointed away and come back when the user waved. The puppet employed facial expressions to convey its internal state. For example, it would pout when the user sent it away and smile when the user motioned it to come back. It giggled when the user would touch its belly.</p><p>In a typical experience with the hamster/predator world, the hamster avoided objects and would follow the user and beg for food, if none was available. The hamster would roll over to have its body scratched if the user bent over. If the user has been patting the hamster for a while, its need for attention would be fulfilled and some other activity would take over (e.g., looking for food). The user was able to feed the hamster by picking up food from a virtual table and putting it on the floor. The user was also able to let the predator out of its cage and into the hamster's world. The predator would then try to chase and eat the hamster. The predator viewed the person as a predator and would attempt to avoid and flee from the person. Both the predator and the hamster were successful at arbitrating among their multiple internal needs (avoiding the predator, finding food, not running into obstacles, etc.).</p><p>In the most recently implemented ALIVE world the user can interact with a virtual dog, as shown in Figs. <ref type="figure">3</ref><ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure">6</ref>. The dog has a more sophisticated repertoire of behaviors than the previous characters, including behaviors for playing, feeding, drinking, receiving attention and sleeping. The dog also uses auditory input, consisting of simple verbal commands that are recognized using a commercial speech recognition system, and produces auditory output, consisting of a wide variety of prerecorded samples which are played at appropriate times. The dog has both interactive behaviors and autonomous action; while it's primary goal is to play with the user, internal motivations (e.g., thirst) will occasionally override.</p><p>In the ALIVE worlds, the user can interact with the agents, using gestures that are interpreted by the agents depending on their current state. An important contribution of the agent system is that behavior and gestures of the user are interpreted differently by agents in the world depending on their current context and past history. Users can use a direct manipulation interaction style <ref type="bibr" target="#b19">[20]</ref> for objects which have no associated behaviors.</p><p>For example, in the current implementation, the waving gesture elicits a response from the puppet agent, but not from the dog agent. And the response given by the puppet is state-dependent; it will return to the user when waved at only when it has been sent away or otherwise ignored by the user. In addition, when the user performs a pointing gesture, and thereby sends the puppet away, the puppet will go to a different place depending on where the user is standing (as well as the direction in which he/she is pointing). If the user comes towards the puppet after it has been "sent away", this gesture is interpreted to mean that the user no longer wants the puppet to "go away", and so the puppet will smile and return to the user. In this manner, the gestures employed by the user can have rich meaning that varies on the previous history, the agents internal needs and the current situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">User experiences</head><p>The ALIVE system has been installed at several public exhibitions, <ref type="foot" target="#foot_0">1</ref> which has allowed us to gather experience with thousands of users in the system. The overwhelming majority report that they enjoy interacting with the system and consider the actions and reactions of objects and agents believable. We feel several interesting lessons can be learned from these users experiences:</p><p>-The magic mirror paradigm proves natural and easy to use. Unlike viewer-centered virtual reality systems, the user does not get easily disoriented. They know at all times where they are in the artificial world and can observe the actions of the other agents as well as their own. The user is able to see the whole world as in a thirdperson perspective; the mirror-like paradigm makes it easier for the user to map his/her actions onto those of his/her image than is the case in a normal third-person perspective, because the mirror mapping from user to user's image is a very simple one which people are familiar with. However, some users do report confusion when moving forwards or backwards with respect to the magic mirror. This may be due to the fact that movement front or back is displayed as movement down or up on the magic mirror. This appears counter-intuitive to some people (i.e., they appear to associate front with up, and back with down). A possible solution to this problem is to put the camera closer to eye level, thereby reducing the perspective distortion (in ALIVE the camera is located on top of the screen, pointing down in a 45 degree angle). -Successful gestures are intuitive with respect to the domain and provide immediate feedback. Users have less difficulty interacting with the agents when they can use simple gestures that are natural and appropriate given the domain, e.g., petting for creatures or pointing and waving for the virtual puppet. In addition, interaction is improved when the user receives feedback from the agents, either in terms of movement and/or facial and body expression (the hamster rolls over when being patted, the puppet smiles when being tickled, etc). In the ALIVE system, whenever an agent recognizes a gesture by the user, it provides some distinguishable visual feedback to the user. This helps the user get an understanding for the space of recognized gestures. -Users are more tolerant of imperfections in an agent's perception (such as lags and occasional incorrect or missed recognition) as opposed to that of objects in the virtual world. People expect virtual inanimate objects to "work" like an object, i.e., the reaction of the object has to be immediate, predictable and consistent. On the other hand, people assume that animal or human-like agents have perception and state, and thus are able to accept that the agent may not have sensed something. As a result, gestures that are hard to recognize, such as waving, can be used successfully in the context of agents (an agent might not have "seen" the user waving), but the same gesture would cause the user frustration if used in the context of some inanimate object, e.g., a switch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and conclusions</head><p>The "magic mirror" interface introduced by the ALIVE system offers several advantages over tethered goggle and glove interfaces. One direct benefit is that it constitutes a safer and more convenient solution, because the user is free from goggles, gloves, and wired sensors. More importantly, it constitutes a more universal solution to virtual reality since users of all shapes and sizes can use one and the same system. Because of the natural gesture-based interaction, users do not need to be given detailed instructions beforehand about how to use the system. All of these advantages make the magic-mirror-type system more accessible to a broad group of users, including children, the physically handicapped, etc. The most significant characteristic of the magic mirror approach is that the user can see him/herself in the virtual world. Typical goggles-and-gloves systems only allow the user a first-person point of view: the user only sees his/her hands. ALIVE allows a third-person point of view, so the user tends to be less disoriented about his/her own position and orientation in the space. This third-person perspective is essential to applications that require a user to see his/her own body. For example, when using a virtual environment to teach a user a physical skill such as dance or golf, the user can benefit from being able to see (and then correct) his/her own position and movements.</p><p>The presence of agents in the virtual environment opens up a whole range of novel applications for virtual envi-ronments. The ALIVE system demonstrated here represents only the beginning of a set of novel applications that could be explored. Work is currently underway at the MIT Media Laboratory to build prototypes for the following applications:</p><p>-Entertainment agents.</p><p>The ALIVE worlds described above are simple examples of entertaining agents. We are currently investigating ALIVE for interactive story-telling applications in which the user plays one of the characters in the story and all other characters are artificial agents that collaborate to make the story move forwards. Another obvious entertainment application of ALIVE is video games. We have hooked up the ALIVE vision-based interface to existing video game software, so as to let the user control a game with his/her full body. In addition, we are investigating ways in which autonomous video game characters can learn and improve their competence over time, so as to keep posing a challenge to a video game player. -Agents as personal teachers and trainers.</p><p>The magic mirror metaphor is ideally suited to teach a user physical skills, because the user can see him/herself performing the movements and actions to be mastered. Furthermore, the autonomous agents in the world can be modeled to act as a personal trainer, able to demonstrate to the user how to perform an action and to provide personalized and timely feedback to the user on the basis of the sensory information about the user's gestures and body positions. For example, one could build a virtual aerobics teacher that would give feedback to the user to tell him/her she has to lift his/her knees higher or 'move those feet a little faster'. -Interface agents or personal digital assistants.</p><p>The ALIVE system can be successfully combined with the concept of a personal digital assistant. These "interface agents" help a user with daily tasks such as remembering where things were put, filtering electronic news and mail, scheduling meetings, etc <ref type="bibr" target="#b22">[23]</ref>. A system such as ALIVE can be used to visualize the interface agent's states and activities. In addition, in an enhanced reality setup, the interface agent could point to real objects as well as virtual objects in its interactions with the user. For example, the agent could point at a drawer of a real file cabinet and remind the user that he/she put a particular document in that drawer.</p><p>In conclusion, the ALIVE system allows a user to interact in an unobtrusive, natural way with a virtual environment inhabited by autonomous agents. The "magic mirror" interface introduced by the system has proven practical and universal: users do not need any training nor do they need to be equipped in order to successfully interact with the world. They can use simple, natural gestures, that are interpreted by the autonomous agents inhabiting the environment. The use of unobtrusive, full-body sensing as well as the presence of agents in the environment opens up a range of novel applications for virtual environments. The technology developed in the ALIVE project introduces possibilities for modeling virtual environment systems containing agents that act as personal entertainers, teachers and assistants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .Fig. 3 .Fig. 4 .Fig. 5 .Fig. 6 .</head><label>123456</label><figDesc>Fig. 1. The ALIVE "Magic-Mirror": a user sees himself in a virtual world Fig. 2. Simple vision routines extract figure from background and label salient points of contour Fig. 3. Image of user is composited with computer graphics. Here the dog responds to pointing gesture by sitting Fig. 4. Another example of a recognized gesture. Dog walks in direction indicated by user Fig. 5. dog shakes hands with user. Dog responds to hand gestures differently, depending on stance of user Fig. 6. Image of dog standing on hind legs to mimic user's gesture</figDesc><graphic coords="2,301.45,442.05,236.25,177.24" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>including Tomorrow's Realities, SIGGRAPH 93, Anaheim, AAAI-94 Art Exhibition, Seattle, ARTEC-95, Nagoya, Japan, and Interactive Communities, SIGGRAPH 95, Los Angeles.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Graduate students Michael P. Johnson, Thad Starner and Leonard Foner as well as undergraduate students Johnny Yoon, Cecile Pham and Kenneth Russell contributed significantly to the implementation of ALIVE. Jeremy Brown was responsible for the initial implementation of the puppet character. This work was funded by SEGA of America, the Office of Naval Research, and the National Science Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pattie Maes is an Associate Professor at MIT's Media Laboratory. She holds the Sony Career Development Chair. Previously, she was a visiting Professor and Research scientist at the MIT Artificial Intelligence Laboratory. She holds a Bachelor's degree and PhD degree in Computer Science from the University of Brussels in Belgium. She is one of the pioneers of a new research area called Software Agents, that is, semi-intelligent computer programs which assist a user with the overload of information on the complexity of the on-line world. She is a frequently quted in the popular press and on television as an expert in this increasingly important application area. She is one of the project leaders for the ALIVE project (Artificial Life Interactive Video Environment), which was demonstrated at Siggraph '93 and '95 and at the AAAI '94 Art Show. This Project allows a user to inetract in real time with 3D animated autononous characters. Patti Maes has been a speaker at numerous conferences including the IJ-CAI conference, AAAI conference; SIGGRAPH conference, CHI conference, Apple develepors conference, etc. She is the editor of three books and is an editorial board member and reviewer for numerous professional journals and conferences. Her work has achieved several prices including the IBM best Bechelor's thesis award (1984) the Oopsla-1987 best paper award and so on. She is a consultant in the area of Software Agents for several major companies such as Apple Computer, Hughes Research, etc. Finally, she is a founder of Firefly Network, Inc., in Boston, Mass., one of the first companies to commercialize software-agent technology. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Active Perception</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Simulating humans. computer graphics animation and control</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brown</surname></persName>
		</author>
		<title level="m">Computer Vision</title>
		<meeting><address><addrLine>Englewood</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kantrowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Loyall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Olbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sengers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weyhrauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Culture</title>
		<imprint>
			<biblScope unit="page" from="113" to="114" />
			<date type="published" when="1993">1993</date>
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action-selection in Hamsterdam: lessons from ethology</title>
		<author>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on the Simulation of Adaptive Behavior</title>
		<meeting>the Third International Conference on the Simulation of Adaptive Behavior<address><addrLine>Brighton</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1994-08">1994. August 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-level direction of autonomous creatures for real-time virtual environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Galyean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-GRAPH 95</title>
		<meeting>SIG-GRAPH 95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Plasm: a fish sample, Siggraph 85 Art Show</title>
		<author>
			<persName><forename type="first">P</forename><surname>Broadwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schaufler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Debver Colo. Also available as &quot;gold</title>
		<meeting><address><addrLine>Summer</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1985">1985. 1994</date>
		</imprint>
	</monogr>
	<note>Silicon Graphics Inc</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Numerical Analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Faires</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Boston</pubPlace>
		</imprint>
		<respStmt>
			<orgName>PWS-Kent</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision-steered beam-forming and transaural rendering for the artificial life interactive video environment (ALIVE)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Audio Eng. Soc. Convention</title>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The CAVE, audio visual experience automatic virtual environment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cruz-Neira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sandin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Defanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun ACM</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="64" to="72" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Space-Time Gestures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Vision and Pattern Recognition</title>
		<meeting><address><addrLine>NY, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-06">1993. June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Attention-driven Expression and Gesture Analysis in an Interactive Environment, Int Workshop on Face and Gesture Recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-06-26">1995. June 26-28</date>
			<pubPlace>Zurich, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Correlation and interpolation networks for real-time expression analysis/synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems-7</title>
		<editor>
			<persName><forename type="first">Touretzky</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leen</forename></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition using a Dynamic Model and Motion Energy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Int. Conf. Computer Vision</title>
		<meeting>Fifth Int. Conf. Computer Vision</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High resolution virtual reality</title>
		<author>
			<persName><forename type="middle">M</forename><surname>Deering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="195" to="201" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Robot Dynamics Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Featherstone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Kluwer, Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tomorrow&apos;s Realities</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amkraut</surname></persName>
		</author>
		<author>
			<persName><surname>Menagerie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH-93 Visual Proceedings, ACM SIGGRAPH 1993</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="212" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computational modeling for the computer animation of legged figures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 85</title>
		<meeting>SIGGRAPH 85<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985-07-22">1985. July 22-26, 1985</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Robot Vision</title>
		<author>
			<persName><forename type="first">Bkp</forename><surname>Horn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>M.I.T. Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Direct Manipulation Interfaces</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hollan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">User-centered system design: new perspectives on human-computer interaction</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Draper</surname></persName>
		</editor>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="87" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Artificial reality II</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Krueger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Addison Wesley</publisher>
			<pubPlace>Reading Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Designing autonomous agents: theory and practice from biology to engineering and back</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Bradford Books/MIT Press</publisher>
			<pubPlace>Reading Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Agents that reduce work and information overload</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun ACM</title>
		<imprint>
			<date type="published" when="1994-07">1994. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic simulation of autonomous legged locomotion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAP H 90</title>
		<meeting>SIGGRAP H 90<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-08-06">1990. August 6-10, 1990</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Talking and listening to computers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mountford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The art of human-computer interface design</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Laurel</surname></persName>
		</editor>
		<meeting><address><addrLine>Reading Mass</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Animation of dynamic legged locomotion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics: Proceedings of SIGGRAPH &apos;91</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A vision-based approach to behavioral animation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Renault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Visualization Comput Animation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="18" to="21" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flocks, herds and schools: a distributed behavioral model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH &apos;87</title>
		<meeting>SIGGRAPH &apos;87</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of facial image sequences using physical and anatomical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="569" to="579" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Artificial Life and Virtual Reality</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester U.K</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Artificial fishes: physics, locomotion, perception, behavior</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 1994</title>
		<meeting>SIGGRAPH 1994</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mandala: virtual village</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH-93 Visual Proceedings, Tomorrow&apos;s Realities, ACM SIGGRAPH 1993</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page">207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A &apos;notion&apos; for interactive behavioral animation control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilhelms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skinner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput Graphics Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance-driven facial animation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 1990</title>
		<meeting>SIGGRAPH 1990</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">pfinder: a real-time system for tracking people</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Conference on Real-Time Vision</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Bove</surname></persName>
		</editor>
		<meeting><address><addrLine>Philadelpia, Penn</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-07">1995. July 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Making them move: mechanics, control and animation of articulated figures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeltzer</surname></persName>
		</author>
		<editor>Badler NI, Barsky BA, Zeltser D</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Morgan Kauffman</publisher>
			<biblScope unit="page" from="3" to="33" />
			<pubPlace>San Mateo, Calif</pubPlace>
		</imprint>
	</monogr>
	<note>Task-level graphical simulation: abstraction, representation and control</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
