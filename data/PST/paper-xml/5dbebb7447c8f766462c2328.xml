<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-27">27 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
							<email>r.qiu@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<email>huang@itee.uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-27">27 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3358010</idno>
					<idno type="arXiv">arXiv:1911.11942v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>recommender system</term>
					<term>session-based recommendation</term>
					<term>graph neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting a user's preference in a short anonymous interaction session instead of long-term history is a challenging problem in the real-life session-based recommendation, e.g., e-commerce and media stream. Recent research of the session-based recommender system mainly focuses on sequential patterns by utilizing the attention mechanism, which is straightforward for the session's natural sequence sorted by time. However, the user's preference is much more complicated than a solely consecutive time pattern in the transition of item choices. In this paper, therefore, we study the item transition pattern by constructing a session graph and propose a novel model which collaboratively considers the sequence order and the latent order in the session graph for a session-based recommender system. We formulate the next item recommendation within the session as a graph classification problem. Specifically, we propose a weighted attention graph layer and a Readout function to learn embeddings of items and sessions for the next item recommendation. Extensive experiments have been conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica, and the experimental results show that our model outperforms other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender system (RS) is an important tool to precisely advertise interested items to potential users in today's flood of web information. In recent years, the content-based RS <ref type="bibr" target="#b20">[21]</ref> and the collaborative filtering RS <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> are two widely used methods because they can effectively approximate the similarity between items while being simple and efficient. However, a clear drawback of these approaches is that a user's recent preference is ignored. In many scenarios, for example, e-commerce, customers may have their current prioritized choices of products over other products because of their recent need. Consequently, this shift of preference will only be shown in recent interactions between users and items <ref type="bibr" target="#b34">[35]</ref>. In this case, the content-based RS and the collaborative filtering RS would fail to capture the important change of the user's preference, which leads to useless or even negative recommendations.</p><p>In contrast, a session-based RS can deal with the shift of the user's preference by taking a recent session of user-item interactions (e.g., clicks of items within 24 hours) into consideration. However, it is very common that for modern commercial online systems, they do not record the user's long-term history. In order to make use of the user's preference, the session can be regarded as the representation of the recent preference of the current anonymous user <ref type="bibr" target="#b34">[35]</ref>. As a result, how to represent a user's preference by extracting representative information from interactions in the session is the essence of the session-based RS.</p><p>As a session is a slice of interactions divided by time, it can be naturally represented as a time series sequence. The sequence characteristic is considered as the most important information by recent methods. However, it has some challenging limitations:</p><p>• The user's preference does not completely depend on the chronology of the sequence. With the prevalence of RNN <ref type="bibr" target="#b9">[10]</ref> applied on the sequence data, for instance, GRU4REC <ref type="bibr" target="#b8">[9]</ref> and NARM <ref type="bibr" target="#b15">[16]</ref> mainly model the time order of items and encode these items using a RNN like neural networks. After encoding the item, the representation of the session is a combination of the item features. Such a paradigm of dealing with the session is natural with the original order of items inside the session. However, as mentioned above, the shift of the user's preference within the session indicates that items should not be simply considered as time series.</p><p>The item transition pattern is more complicated. • The recent approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>, which divide the user's preference into the long-term (global) and the short-term (local) preference, are too simple to capture the complicated item transition pattern. These methods choose the last item of the session to stand for the short-term (local) preference and the remaining items for the long-term (global) preference. This setting directly ignores the pattern of item choices, which introduces the bias to the model. NARM <ref type="bibr" target="#b15">[16]</ref> applies a self-attention on the last item after encoding with a RNN. To alleviate the influence of time order, STAMP <ref type="bibr" target="#b18">[19]</ref> only utilizes the self-attention mechanism without RNN. SR-GNN <ref type="bibr" target="#b37">[38]</ref> proposes to use a single layer gated graph neural network <ref type="bibr" target="#b17">[18]</ref> to learn the representation of items and again, a self-attention on the last item to extract a session level feature. Actually, the self-attention calculates the relative importance of the last single item, which ignores a specific item transition pattern within the session.</p><p>With the problems mentioned above, it is important to determine the intrinsic order of items within a session. This inherent order is neither the straightforward time order by RNN, nor the complete randomness by the self-attention. In this paper, a model named Full Graph Neural Network (FGNN) is proposed to learn the inherent order of the item transition pattern and compute a session level representation to generate recommendation. To utilize graph neural networks, we build a session graph for every session and formulate the recommendation as a graph classification problem. In order to capture the inherent order of the item transition pattern, which is vital for the item level feature representation, a multiple weighted graph attention layer (WGAT) network is proposed to compute the information flow between items within the session. After obtaining the item representations, the Readout function, which automatically learns to determine an appropriate order, is deployed to aggregate these features. Extensive experiments are conducted on two benchmark e-commerce datasets, the Yoochoose dataset from the RecSys Challenge 2015 and the Diginetica dataset from CIKM Cup 2016. The experimental results show the superiority of our method in the task of next item recommendation. Our main contributions are summarized as follows:</p><p>• To the best of our knowledge, we are the first to investigate the inherent order of item transition pattern in sessionbased recommendation. Specifically, we propose a novel FGNN model to perform the next item session-based recommendation based on the inherent order. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first review some related work about the general recommender system (RS) in Section 2.1 and the session-based recommender system (SBRS) in Section 2.2. At last, we will describe graph neural networks (GNN) for the node representation learning and graph classification problems in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Recommender System</head><p>The most popular method in recent years for the general recommender system is the collaborative filtering (CF), which represents the user interest based on the whole history. For example, the famous shallow method, Matrix Factorization (MF) <ref type="bibr" target="#b12">[13]</ref> factorizes the whole user-item interaction matrix with latent representation for every user and item. With the prevalence of deep learning, neural networks are widely used. Neural collaborative filtering (NCF) <ref type="bibr" target="#b6">[7]</ref> first proposes to use the multi layer perceptron to approximate the matrix factorization process. More subsequent work extends the incorporation of different deep learning tools, for instance, zeroshot learning and domain adaptation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. These methods all depend on the identification of users and the whole record of interactions for every user. However, the user information is anonymous for many modern commercial online systems, which leads to the failure of these CF based algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Session-based Recommender System</head><p>The research on the session-based recommender system (SBRS) is a sub-field of RS. Compared with RS, SBRS takes the user's recent user-item interactions into consideration rather than requiring all historical actions. SBRS is based on the assumption that the recent choice of items can be viewed as the recent preference of a user. Sequential recommendation is based on the Markov chain model <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref>, which learns the dependence of items of a sequence data to predict the next click. Using probabilistic decision-tree models, Zimdars et al. <ref type="bibr" target="#b41">[42]</ref> proposed to encode the state of the item transition pattern. Shani et al. <ref type="bibr" target="#b27">[28]</ref> made use of a Markov Decision Process (MDP) to compute the probability of recommendation with the transition probability between items.</p><p>Deep learning models are popular recently with the boom of recurrent neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>, which is naturally designed for processing sequential data. Hidasi et al. <ref type="bibr" target="#b8">[9]</ref> proposed the GRU4REC, which applies a multi layer GRU <ref type="bibr" target="#b1">[2]</ref> to simply treat the data as time series. Based on the RNN model, some work makes improvements on the architectural choice and the objective function design <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>. In addition to RNN, Jannach and Ludewig <ref type="bibr" target="#b10">[11]</ref> proposed to use the neighborhood-based method to capture cooccurrence signals. Incorporating content features of items, Tuan and Phuong <ref type="bibr" target="#b30">[31]</ref> utilized 3D convolutional neural networks to learn more accurate representations. Wu et al. <ref type="bibr" target="#b36">[37]</ref> proposed a list-wise deep neural network model to train a ranking model. Some recent work uses the attention mechanism to avoid the time order. NARM <ref type="bibr" target="#b15">[16]</ref> stacks GRU as the encoder to extract information and</p><formula xml:id="formula_0">! " # " $ WGAT Readout % &amp; ' × …… Item set ( Softmax = …… ) * …… + , -$ -! -! . -" -# -" . -$ . -# .</formula><p>Session graph / 0 then a self-attention layer to assign weight to each hidden state to sum up as the session embedding. To further alleviate the bias introduced by time series, STAMP <ref type="bibr" target="#b18">[19]</ref> entirely replaces the recurrent encoder with an attention layer. SR-GNN <ref type="bibr" target="#b37">[38]</ref> applies a gated graph network <ref type="bibr" target="#b17">[18]</ref> as the item feature encoder and a self-attention layer to aggregate the item features together as the session feature. SSRM <ref type="bibr" target="#b4">[5]</ref> considers a specific user's history sessions and applies the attention mechanism to combine them. Though the attention mechanism can proactively ignore the bias introduced by the time order of interaction, it considers a session as a totally random set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Neural Networks</head><p>In recent years, GNN has attracted much interest in the deep learning society. Initially, GNN is applied to the simple situation on directed graphs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. In recent years, many GNN methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref> work under the mechanism that is similar to message passing network <ref type="bibr" target="#b2">[3]</ref> to compute the information flow between nodes via edges. Additionally, the graph level feature representation learning is essential for graph level tasks, for example, graph classification and graph isomorphism <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref>. Set2Set <ref type="bibr" target="#b33">[34]</ref> assigns each node in the graph a descriptor as the order feature and uses this re-defined order to process all nodes. SortPool <ref type="bibr" target="#b40">[41]</ref> sorts the nodes based on their learned feature and uses a normal neural network layer to process the sorted nodes. DiffPool <ref type="bibr" target="#b39">[40]</ref> designs two sets of GNN for every layer to learn a new dense adjacent matrix for a smaller size of the new densely connected graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we introduce how GNN works on the graph data.</p><p>Let G(V , E) denote a graph, where ∈ V is the node set with node feature vectors X and e ∈ E is the edge set. There are two commonly popular tasks, e.g., node classification and graph classification. In this work, we focus on graph classification because our purpose is to learn a final embedding for the session rather than single items. For the graph classification, given a set of graphs {G 1 , . . . , G N } ⊆ G and the corresponding labels { 1 , . . . , N } ⊆ Y, we aim to learn a representation of the graph h G to predict the graph label, G = (h G ).</p><p>GNN makes use of the structure of the graph and the feature vectors of nodes to learn the representation of nodes or graphs. In recent years, most GNN work by aggregating information from neighboring nodes iteratively. After k iterations of update, the final representations of the nodes capture the structural information as well as the node information within k-hop neighbor. The procedure can be formed as</p><formula xml:id="formula_1">a (k) = Agg({h (k−1) u , u ∈ N ( )}), h (k) = Map(h (k−1) , a (k) ),<label>(1)</label></formula><p>where h (k) is the feature vector for node in the kth layer. For the input h 0 to the first layer, the feature vectors X are passed in. Agg and Map are two functions that can be defined in a different form. Agg serves as the aggregator to aggregate features of neighboring nodes. A typical characteristic of Agg is permutation invariant. Map is a mapping to transform the self information and the neighboring information to a new feature vector. For the graph classification, a Readout function aggregates all node features from the final layer of the graph to generate a graph level representation h G :</p><formula xml:id="formula_2">h G = Readout({h (k) , ∈ V }),<label>(2)</label></formula><p>where the Readout function needs to be permutation invariant as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>In this section, we describe our FGNN model in detail. Above all, in Section 4.1, we define the problem and give out the notations used in this paper. The complete pipeline of the calculation is demonstrated in Figure <ref type="figure">1</ref>. At first, an input session sequence is converted into a session graph (Section 4.2). After obtaining the session graph, an L weighted graph attentional layer (WGAT) model is applied to perform graph convolution among the nodes (Section 4.3). Once the node features are learned, a Readout function combines all these features to form a graph level representation q * (Section 4.4).</p><p>Based on the graph representation, FGNN makes a recommendation by comparing it with the whole item set V (Section 4.5). Finally, we describe the way we train our model in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Definition and Notation</head><p>The purpose of a session-based recommender system is to predict the next item that matches the user's preference based on the interactions within the session. In the following, we give out the definition of the SBRS problem.</p><formula xml:id="formula_3">! " # $ %! %# %% %! %# %% '' '' # % ! ! # % )) (a) A session graph. %! %# %% ! %! %# %% '' '' ' " ) " ! # " $ ) $ )) $ (b)</formula><p>The computation for the second-layer feature x ′′ 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>An example of how to compute a node representation of a two-layer GNN. The original session sequence is the same as the input in Figure <ref type="figure">1</ref>. (a) The session graph is added with self-loop to every node. x i is the input feature for the corresponding node i . (b) The computation of the second-layer feature x ′′ 6 is based on all the first and second order neighboring nodes of 6 . The first order neighbors are 7 and 6 itself. The second order neighbors of 6 are the first order neighbors of 7 and 6 , i.e., 3 , 7 and 6 for 7 , and 7 and 6 for 6 .</p><p>In a SBRS, there is an item set V = { 1 , 2 , 3 , . . . , m }, where all items are unique and m denotes the number of items. A session sequence from an anonymous user is defined as a sequential list S = [ s, 1 , s, 2 , s, 3 , . . . , s,n ], s, * ∈ V. n is the length of the session S, which may contain duplicated items, s,p = s,q , p, q &lt; n. The goal of our model is to predict the next item s,n that matches the user's preference the most.</p><p>During calculation, for every item ∈ V, our model learns a corresponding embedding vector x ∈ R d , where d is the dimension of x. For each training session, there is a label item l abel serving as the target to predict. In order to recommend items based on the given session and the whole item set, our model outputs a probability distribution ˆ over V, where the items with top-K values in ˆ will be the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Session Graph</head><p>As shown in Figure <ref type="figure">1</ref>, at the first stage, the session sequence is converted into a session graph for the purpose to process each session via GNN. Because of the natural order of the session sequence, we transform it into a weighted directed graph, G s = (V s , E s ), G s ∈ G, where G is the set of all session graphs. In the session graph G s , the node set V s represents all nodes, which are items s,n from S. For every node v, the input feature is the initial embedding vector x. The edge set E s represents all directed edges ( s,n−1 , s,n , w s, (n−1)n ), where s,n is the click of the item after s,n−1 in S, and w s, (n−1)n is the weight of the edge. The weight of the edge is defined as the frequency of the occurrence of the edge within the session. For the convenience, in the following, we use the nodes in the session graph to stand for the items in the session sequence. For the self-attention used in WGAT introduced in Section 4.3, if a node does not contain a self loop, it will be added with a self loop with a weight 1. Based on our observation of our daily life and the datasets, it is common for a user to click two consecutive items for a few times within the session. After converting the session into a graph, the final embedding of S is based on the calculation on this session graph G s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weighted Graph Attentional Layer</head><p>After obtaining the session graph, a GNN is needed to learn embeddings for nodes in a graph, which is the WGAT × L part in Figure <ref type="figure">1</ref>. In recent years, some baseline methods on GNN, for example, GCN <ref type="bibr" target="#b11">[12]</ref> and GAT <ref type="bibr" target="#b32">[33]</ref>, are demonstrated to be capable of extracting features of the graph. However, most of them are only well-suited for unweighted and undirected graphs. For the session graph, weighted and directed, these baseline methods cannot be directly applied without losing the information carried by the weighted directed edge. Therefore, a suitable graph convolutional layer is needed to effectively convey information between the nodes in the graph.</p><p>In this paper, we propose a weighted graph attentional layer (WGAT), which simultaneously incorporates the edge weight when performing the attention aggregation on neighboring nodes. We describe the forward propagation of WGAT in the following. The information propagation procedures are shown in Figure <ref type="figure">2</ref>. The input to a WGAT is a set of node initial features, the item embeddings,</p><formula xml:id="formula_4">x = {x 0 , x 1 , x 2 , . . . x n−1 }, x i ∈ R d ,</formula><p>where n is the number of nodes in the graph, and d is the dimension of the embedding x i . After applying the WGAT, a new set of node features,</p><formula xml:id="formula_5">x ′ = {x ′ 0 , x ′ 1 , x ′ 2 , . . . x ′ n−1 }, x ′ i ∈ R d ′</formula><p>, will be given out as the output. Specifically, the input feature vectors x 0 i of the first WGAT layer are generated from an embedding layer, whose input is the one-hot encoding of the item,</p><formula xml:id="formula_6">x 0 i = Embed( i ),<label>(3)</label></formula><p>where Embed is the embedding layer.</p><p>To learn the node representation via the higher order item transition pattern within the graph structure, a self-attention mechanism for every node i is used to aggregate information from its neighboring nodes N (i), which is defined as the nodes with edges towards the node i (may contain i itself if there is a self-loop edge). Because the size of the session graph is not huge, we can take the entire neighborhood of a node into consideration without any sampling. At the first stage, a self-attention coefficient e i j , which determines how importantly the node j will influence the node i, is calculated based on x i , x j and w i j ,</p><formula xml:id="formula_7">e i j = Att(W x i ,W x j , w i j ),<label>(4)</label></formula><p>where Att is a mapping Att :</p><formula xml:id="formula_8">R d × R d × R 1 → R 1</formula><p>and W is a shared parameter which performs linear mapping across all nodes. As a matter of fact, the attention of a node i can extend to every node, which is a special case the same as how STAMP makes the attention of the last node of the sequence. Here we restrict the range of the attention within the first order neighborhood of the node i to make use of the inherent structure of the session graph S.</p><p>To compare the importance of different nodes directly, a softmax function is applied to convert the coefficient into a probability form across the neighbors and itself,</p><formula xml:id="formula_9">α i j = softmax j (e i j ) = exp(e i j ) k ∈N(i ) exp(e ik ) .<label>(5)</label></formula><p>The choice of att can be diversified. In our experiments, we use an MLP layer with the parameter W att ∈ R 2d +1 , followed by a LeakyRelu non-linearity unit with negative input slope α = 0.2</p><formula xml:id="formula_10">α i j = exp(LeakyRelu(W att [W x i ||W x j ||w i j ])) k ∈N(i ) exp(LeakyRelu(W att [W x i ||W x k ||w ik ])) ,<label>(6)</label></formula><p>where || means concatenation of two vectors.</p><p>For every node i in G s , in a WGAT layer, all attention coefficients of their neighbors can be computed as <ref type="bibr" target="#b5">(6)</ref>. To utilize these attention coefficients, a linear combination for the corresponding neighbors is applied to update the features of the nodes.</p><formula xml:id="formula_11">x ′ i = σ ( j ∈N(i ) α i j W x j ),<label>(7)</label></formula><p>where σ is a non-linearity unit and in our experiments, we use the ReLU <ref type="bibr" target="#b19">[20]</ref>.</p><p>As suggested in previous work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, the multi-head attention can help to stabilize the training of the self-attention layers. Therefore, we apply the multi-head setting for our WGAT.</p><formula xml:id="formula_12">x ′ i = K k=1 σ ( j ∈N(i ) α k i j W k x j ),<label>(8)</label></formula><p>where K is the number of heads and for every head, there is a different set of parameters. in <ref type="bibr" target="#b7">(8)</ref> stands for the concatenation of all heads. As a result, after the calculation of (8), x ′ i ∈ R Kd ′ . Specifically, if we stack multiple WGAT layers, the final nodes feature will be shaped as R Kd ′ as well. However, what we expect is R d ′ . Consequently, we calculate the mean over all heads of the attention results.</p><formula xml:id="formula_13">x ′ i = σ ( 1 K K k=1 j ∈N(i ) α k i j W k x j ).<label>(9)</label></formula><p>Once the forward propagation of multiple WGAT layers has finished, we obtain the final feature vector of all nodes, which is the item level embeddings. These embeddings will serve as the input of the session embedding computation stage that we detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Readout Function</head><p>A Readout function aims to give out a representation of the whole graph based on the node features after the forward computation of the GNN layers. As we introduce above, the Readout function needs to learn the order of the item transition pattern to avoid the bias of the time order and the inaccuracy of the self-attention on the last input item. For the convenience, some algorithms use simple permutation invariant operations for example, Mean, Max or Sum over all node features. Though clearly, the methods mentioned above are simple and perfectly do not violate the constraints of the permutation invariance, they can not provide a sufficient model capacity for learning a representative session embedding for the session graph. In contrast, Set2Set <ref type="bibr" target="#b33">[34]</ref> is a graph level feature extractor which learns a query vector indicating the order of reading from the memory for an undirected graph. We modify this method to suit the setting of the session graphs. The computation procedures are as follows:</p><formula xml:id="formula_14">q t = GRU(q * t −1 ),<label>(10)</label></formula><formula xml:id="formula_15">e i,t = f (x i , q t ),<label>(11)</label></formula><p>a i,t = exp(e i,t ) j exp(e j,t )</p><formula xml:id="formula_16">, (<label>12</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">r t = i a i,t x i ,<label>(13)</label></formula><formula xml:id="formula_19">q * t = q t r t ,<label>(14)</label></formula><p>where i indexes node i in the session graph G s , q t , q t ∈ R d , is a query vector which can be seen as the order to read r t ∈ R d from the memory and GRU is the gated recurrent unit, which at the first step takes no input and at the following steps, takes the former output q * t −1 ∈ R 2d . f calculates the attention coefficient e i,t between the embedding of every node x i and the query vector q t . a i,t is the probability form of e i,t after applying a softmax function over e i,t , which is then used to a linear combination on the node embeddings x i . The final output q * t of one forward computation of the Readout function is the concatenation of q t and r t .</p><p>Based on all node embeddings for a session graph, we use Equation 10∼14 to obtain a graph level embedding which contains a query vector q t in addition to the semantic embedding vector r t . The query vector q t controls what to read from the node embeddings, which actually provides an order to process all nodes if we recursively apply the Readout function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Recommendation</head><p>Once the graph level embedding q * t is obtained, we can use it to make a recommendation by computing a score vector ẑ for every item over the whole item set V with the their initial embeddings in the matrix form,</p><formula xml:id="formula_20">ẑ = (W out q * t ) T X 0 ,<label>(15)</label></formula><p>where W out ∈ R d ×2d is a parameter that performs a linear mapping on the graph embedding q * t , the T means the transformation on a matrix, and X 0 is from Equation <ref type="formula" target="#formula_6">3</ref>.</p><p>For every item in the item set V, we can calculate a recommendation score and combine them together, we obtain a score vector ẑ. Furthermore, we apply a softmax function over ẑ to transform it into the probability distribution form ˆ ,</p><formula xml:id="formula_21">ˆ = softmax( ẑ). (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>For the top-K recommendation, it is simple to choose the highest K probabilities over all items based on ˆ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Objective Function</head><p>Since we already have the recommendation probability of a session, we can use the label item l abel to train our model with the supervised learning method.</p><p>As mentioned above, we formulate the recommendation as a graph level classification problem. Consequently, we apply the multiclass cross entropy loss between ˆ and the one-hot encoding of l abel as the objective function. For a batch of training sessions, we can have</p><formula xml:id="formula_23">L = − l i =1 one-hot( l abel,i )log( ˆ i ), (<label>17</label></formula><formula xml:id="formula_24">)</formula><p>where l is the batch size we use in the optimizer.</p><p>In the end, we use the Back-Propagation Through Time (BPTT) algorithm to train the whole FGNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct experiments with the purpose to prove the efficacy of our proposed FGNN model by answering the following research questions:</p><p>• RQ1 Does the FGNN outperform other state-of-the-art SBRS methods? (in Section 5.5) • RQ2 How does the WGAT work for the session-based recommendation problem? (in Section 5.6) • RQ3 How does the Readout function work differently from other graph level embedding methods? (in Section 5.7)</p><p>In the following, we first describe the details of the basic setting of the experiments and afterwards, we answer the questions above by showing the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We choose two representative benchmark e-commerce datasets, i.e., Yoochoose and Diginetica, to evaluate our model.</p><p>• Yoochoose is used as a challenge dataset for RecSys Challenge 2015 <ref type="foot" target="#foot_0">1</ref> . It is obtained by recording click-streams from an E-commerce website within 6 months. • Diginetica is used as a challenge dataset for CIKM cup 2016 <ref type="foot" target="#foot_1">2</ref> .</p><p>It contains the transaction data which is suitable for sessionbased recommendation.</p><p>For the fairness and the convenience of comparison, we follow <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref> to filter out sessions of length 1 and items which occur less than 5 times in each dataset respectively. After the preprocessing step, there are 7,981,580 sessions and 37,483 items remaining in Yoochoose dataset, while 204,771 sessions and 43097 items in Diginetica dataset. Similar to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>, we split a session of length n into n − 1 partial sessions of length ranging from 2 to n to augment the datasets. For the partial session of length i in the session S, it is defined as [ s, 0 , . . . , s,i −1 ] with the last item s,i −1 as l abel . Following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>, for the Yoochoose dataset, the most recent portions 1/64 and 1/4 of the training sequence are used as two split datasets respectively. Statistical details of all datasets are shown in Table <ref type="table" target="#tab_2">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>In order to prove the advantage of our proposed FGNN model, we compare FGNN with the following representative methods:</p><p>• POP always recommends the most popular items in the whole training set, which serves as a strong baseline in some situations although it is simple.</p><p>• S-POP always recommends the most popular items for the current session. • Item-KNN <ref type="bibr" target="#b23">[24]</ref> computes the similarity of items by the cosine distance of two item vectors in sessions. Regularization is also introduced to avoid the rare high similarities for unvisited items. • BPR-MF <ref type="bibr" target="#b21">[22]</ref> proposes a BPR objective function which calculates a pairwise ranking loss. Following <ref type="bibr" target="#b15">[16]</ref>, Matrix Factorization is modified to session-based recommendation by using mean latent vectors of items in a session. • FPMC <ref type="bibr" target="#b22">[23]</ref> is a hybrid model for the next-basket recommendation and it achieves state-of-the-art results. For anonymous session-based recommendation, following <ref type="bibr" target="#b15">[16]</ref>, we omit the user feature directly because of the unavailability. • GRU4REC <ref type="bibr" target="#b8">[9]</ref> stacks multiple GRU layers to encode the session sequence into a final state. It also applies a ranking loss to train the model. • NARM <ref type="bibr" target="#b15">[16]</ref> extends to use an attention layer to combine encoded states of RNN, which enables the model to explicitly emphasize on the more important parts of the input.</p><p>• STAMP <ref type="bibr" target="#b18">[19]</ref> uses attention layers to replace all RNN encoders in previous work to even make the model more powerful by fully relying on the self-attention of the last item in a sequence. • SR-GNN <ref type="bibr" target="#b37">[38]</ref> applies a gated graph convolutional layer <ref type="bibr" target="#b17">[18]</ref> to obtain item embeddings, followed by a self-attention of the last item as STAMP does to compute the sequence level embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metrics</head><p>At a time, a recommender system can give out a few recommended items and a user would choose the first few of them. To keep the same setting as previous baselines, we mainly choose to use top-20 items to evaluate a recommender system and specifically, two metrics, i.e., R@20 and MRR@20. For more detailed comparison, top-5 and top-10 results are considered as well.</p><p>• R@K (Recall calculated over top-K items). The R@K score is the primary metric that calculates the proportion of test cases which recommend the correct item in a top K position in a ranking list,</p><formula xml:id="formula_25">R@K = n hit N ,<label>(18)</label></formula><p>where N represents the number of test sequences S test in the dataset and n hit counts the number that the desired items are in the top K position in the ranking list, which is named the hit. • MRR@K (Mean Reciprocal Rank calculated over top-K items).</p><p>The reciprocal is set to 0 when the desired items are not in the top K position and the calculation is as follows,</p><formula xml:id="formula_26">MRR@K = 1 N l abel ∈S t e s t 1 Rank( l abel ) . (<label>19</label></formula><formula xml:id="formula_27">)</formula><p>The MRR is a normalized ranking of hit, the higher the score, the better the quality of the recommendation because it indicates a higher ranking position of the desired item. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiments Setting</head><p>We apply a three layer WGAT and each with eight heads as our node representation encoder and a three processing steps of our Readout function. The size of the feature vectors of the items are set to 100 for every layer including the initial embedding layer. All parameters of the FGNN are initialized using a Gaussian distribution with a mean of 0 and a standard deviation of 0.1 except for the GRU unit in the Readout function, which is initialized using the orthogonal initialization <ref type="bibr" target="#b24">[25]</ref> because of its performance on RNNlike units. We use the Adam optimizer with the initial learning rate 1e − 3 and the linear schedule decay rate of 0.1 for every 3 epochs. The batch size for mini-batch optimization is 100 and we set an L2 regularization to 1e − 5 to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with Baseline Methods (RQ1)</head><p>To demonstrate the overall performance of FGNN, we compare it with the baseline methods mentioned in Section 5.2 by evaluating the R@20 and MRR@20 scores. The overall results are presented in Table <ref type="table" target="#tab_3">2</ref> with respect to all baseline methods and our proposed FGNN model. Due to the insufficient memory of hardware, we can not initialize FPMC on Yoochoose1/4 as <ref type="bibr" target="#b15">[16]</ref>, which is not reported in Table <ref type="table" target="#tab_3">2</ref>. For more detailed comparisons, in Table <ref type="table" target="#tab_4">3</ref>, we present the results of the most recent state-of-the-art methods for the dataset Yoochoose1/64 when K = 5 and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">General</head><p>Comparison by P20 and MRR20. FGNN utilizes the multi layers of WGAT to easily convey the semantic and structural information between items within the session graph and applies the Readout function to decide the relative significance as the order of nodes in the graph to make the recommendation. According to the results reported in Table <ref type="table" target="#tab_3">2</ref>, obviously, the proposed FGNN model outperforms all the baseline methods on all three datasets for both metrics, R@20 and MRR@20. It is proved that our method achieves state-of-the-art performance on benchmark datasets. We also substitute the two key components, WGAT and the Readout function, with gated graph networks (FGNN-Gated) and the selfattention (FGNN-ATT-OUT) used by previous methods. Both of the variants perform better than previous models, which demonstrates the efficacy of the proposed WGAT and the Readout function respectively. Compared with those traditional algorithms, e.g., POP and S-POP, because of their simple intuition to recommend items based on the frequency of appearance, they perform far worse than FGNN. They tend to recommend fixed items, which leads to the failure of capturing the characteristics of different items and sessions. Taking BPR-MF and FPMC into consideration, which omits the session setting when recommending items, we can see that S-POP can defeat these methods as well because S-POP makes use of the  At the even worse situation when the dataset is large, methods relying on the whole item set undoubtedly fail to scale well. All methods above achieve relatively poor results compared with the recent neural-network-based methods, which fully model the user's preference in the session sequence. Different from the traditional methods mentioned above, all baselines using neural networks achieve a large performance margin. GRU4REC is the first to apply RNN-like units to encode the session sequence. It sets the baseline of neural-network-based methods. Though RNN is perfectly matched for sequence modeling, sessionbased recommendation problems are not merely a sequence modeling task because the user's preference is even changing within the session. RNN takes every input item equally importantly, which introduces bias to the model during training. For the subsequent methods, NARM and STAMP, which both incorporate a self-attention over the last input item of a session, they both outperform GRU4REC in a large margin. They both use the last input item as the representation of short-term user interest. It proves that assigning different attention on different inputs is a more accurate modeling method for session encoding. Looking into the comparison between NARM, combining RNN and attention mechanism, and   STAMP, the complete attention setting, there is a conspicuous gap of performance that STAMP outperforms NARM. This further demonstrates that directly using RNN to encode the session sequence can inevitably introduce bias to the model, which the attention can completely avoid.</p><p>SR-GNN uses a session graph to represent the session sequence, followed by a gated graph layer to encode items. In the final stage, it again uses a self-attention the same as STAMP to output a session embedding. It achieves the best result compared to all methods mentioned above. The graph structure is shown to be more suitable than the sequence structure, the RNN modeling, or a set structure, the attention modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Higher Standard</head><p>Recommendation with K = 5, 10. For more detailed results in Table <ref type="table" target="#tab_4">3</ref>, FGNN also achieves the best results with a higher standard of the top-5 and top-10 recommendation. The proposed FGNN model outperforms all baseline methods above. It has a more accurate node-level encoding tool, WGAT, to learn more representative features and a Readout function, to learn an inherent order of nodes in the graph to avoid the entire random order of items. According to the result, it is demonstrated that a more accurate session embedding is obtained by FGNN to make effective recommendations, which proves the efficacy of the proposed FGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Comparison with Other GNN Layers (RQ2)</head><p>To efficiently convey information between items in a session graph, we propose to use WGAT, which suits the situation of the session better. As mentioned above, there are many different GNN layers that can be used to generate node embeddings, e.g., GCN <ref type="bibr" target="#b11">[12]</ref>, GAT <ref type="bibr" target="#b32">[33]</ref> and gated graph networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38]</ref>. To prove the usefulness of WGAT, we substitute all three WGAT layers with GCN, GAT and gated graph networks respectively in our model. For GCN and GAT, they both initially work for unweighted and undirected graph, which is not the same setting as the proposed session graph. To make both of them work on the session graph, we directly convert the session graph into undirected by replacing the original directed edges with undirected ones, i.e., reverse the source node and target node of edges. And we simply omit the original weight of edges and set all connections between nodes with the same weight 1.For the other one, the Gated graph networks, it can work with the session graph setting in its original form without any modification on the session graph.</p><p>In Figure <ref type="figure" target="#fig_2">3</ref>(a) and Figure <ref type="figure" target="#fig_2">3</ref>(b), results of different GNN layers are shown with R@20 and MRR@20 indices. FGNN is the model proposed in this work, which achieves the best performance. WGAT is more powerful than other GNN layers in session-based recommendation. GCN and GAT are not able to capture the direction and the explicit weight of edges, resulting in performing worse than WGAT and gated graph networks, which holds the ability to capture these information. Between WGAT and gated graph networks, WGAT performs better because of the stronger ability of representation learning.</p><p>For the study of WGAT, we test how the number of layers and heads affect the R@20 index performance on Yooshoose1/64. In Figure <ref type="figure" target="#fig_3">4</ref>, we report the experiment results of different number of layers ranging in {1, 2, 3, 4, 5} and heads ranging in {1, 2, 4, 8, 16}. It shows that stacking three WGAT layers with eight heads performs the best. Lower results are shown for smaller models for the reason that the capacity of them is too low to represent the complexity of the item transition pattern. According to the tendency of results for larger models, it shows that it is difficult to train these models and the overfitting is harmful to the final performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Comparison with Other Graph Embedding Methods (RQ3)</head><p>Different approaches for generating the session embedding after obtaining the node embedding stand for different emphasis of the input item. The Readout function proposed in this work learns an inherent order of the nodes by the query vector, which indicates the relatively different impact on the user's preference along with the item transition. To prove the superiority of our Readout function, we replace the Readout function with other session embedding generators:</p><p>• FGNN-ATT-OUT We apply the widely-used self-attention of the last input item. It directly considers the last input item as the short-term reference and all other items as the longterm reference. • FGNN-GRU To compare the inherent order learned by our Readout function, we use GRU to directly make use of the input session sequence order. • FGNN-SortPool SortPooling is introduced by Zhang et. al. <ref type="bibr" target="#b40">[41]</ref> to perform a pooling on graph level by sorting the features of the nodes. This sorting can be viewed as a kind of order as well.</p><p>In Figure <ref type="figure" target="#fig_5">5</ref>(a) and Figure <ref type="figure" target="#fig_5">5</ref>(b), results of different methods for graph level embedding generation are presented for all the datasets with the R@20 and MRR@20 indices. It is obvious that the proposed Readout function achieves the best result. For FGNN-GRU and FGNN-SortPool, they both contain an order but which is too simple to capture the item transition pattern. FGNN-GRU uses GRU to encode the session sequence with the input order. Such a setting is similar to the RNN-based method. As a consequence, it performs worse than attention-based method FGNN-ATT-OUT, which takes both the short-term and the long-term preference into consideration. As for FGNN-SortPool, it sorts the nodes based on WL colors from previous multiple layers of computations. Though it does not simply rely on the input order of the session sequence, the order used for the node is set according to the relative scale of the features. For the best performance, our Readout function learns the order of the item transition pattern, which is different from using the time order or the hand-crafted split of long-term and shortterm preference. The results prove that there is a more accurate order for the model to make a more accurate recommendation.</p><p>For different session embedding generators, it is also important to look deep into how they perform on sessions with different lengths because the length varies greatly within one dataset. Following previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>, sessions in Yoochoose 1/64 are separated into two groups, i.e., Short and Long. Short indicates that the length of sessions is less than or equal to 5, while sessions longer than 5 are categorized as Long. Length 5 is the closest to the average length of total sessions. 70.1% of Yoochoose1/64 are Short sessions and 29.9% are Long. In addition to different session embedding generators, we take the previous GNN-based baseline method SR-GNN into comparison. For each method, we report the results evaluated in terms of R@20 in Figure <ref type="figure" target="#fig_6">6</ref>. In the aspect of both Short and Long sessions, FGNN achieves the best performance compared with other graph embedding generators and SR-GNN. The proposed Readout function shows superiority to other methods. The order introduced by the Readout function is demonstrated to convey more accurate information of item transition pattern. For the comparison among RNN-based and attention-based methods, it is shown that the performance is relatively better for longer sessions than shorter ones. This indicates that the item transition pattern relies on the latter input items of a sequence more heavily for long sessions, which is more suitable for these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Session-based recommender system is a challenging problem because the user history is unavailable for predicting the user's preference. This work proposes to use WGAT layers to learn item embeddings for items in a session, which are then processed by the Readout function to obtain the session embedding to represent the user's preference for this session. It is demonstrated by experiments that our proposed method achieves state-of-the-art results on benchmark e-commerce datasets. In the future, it is important and promising to make use of inter-session information to more accurately represent the user's preference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2(b) shows an example of how a two-layer GNN calculates the final representation of the node 6 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results with different GNN layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: R@20 index for different number of layers and heads for WGAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results with different aggregation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: R@20 for Short and Long sessions with different aggregation functions and baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• A novel WGAT model is proposed to serve as the item feature encoder by learning to assign different weights to different neighbors. It helps to effectively convey the information between items. • A Readout function is applied to generate appropriate graph level representation for item recommendation. The Readout function can learn the best order of the item transition pattern in the graph. • We conduct extensive experiments on two benchmark ecommerce datasets and achieve state-of-the-art results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure1: Pipeline of FGNN. The input to the model is organized as a session sequence s, which is then converted to a session graph G with node features x. L layers of WGAT serves as the encoder of node features for G. After being processed by WGAT, the session graph now contains different semantic node representations x L but with the same structure as the input session graph. The Readout function is applied to generate a session embedding based on the learned node features. Compared with other items in the item set V, a recommendation score ˆ i is finally generated.</figDesc><table><row><cell>Session 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistic details of datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="5">all the clicks train sessions test sessions all the items avg.length</cell></row><row><cell>Yoochoose1/64</cell><cell>557248</cell><cell>369859</cell><cell>55898</cell><cell>16766</cell><cell>6.16</cell></row><row><cell>Yoochoose1/4</cell><cell>8326407</cell><cell>5917746</cell><cell>55898</cell><cell>29618</cell><cell>5.71</cell></row><row><cell>Diginetica</cell><cell>982961</cell><cell>719470</cell><cell>60858</cell><cell>43097</cell><cell>5.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance compared with other baselines.</figDesc><table><row><cell>Method</cell><cell cols="6">Yoochoose1/64 R@20 MRR@20 R@20 MRR@20 R@20 MRR@20 Yoochoose1/4 Diginetica</cell></row><row><cell>POP</cell><cell>6.71</cell><cell>1.65</cell><cell>1.33</cell><cell>0.30</cell><cell>0.89</cell><cell>0.20</cell></row><row><cell>S-POP</cell><cell>30.44</cell><cell>18.35</cell><cell>27.08</cell><cell>17.75</cell><cell>21.06</cell><cell>13.68</cell></row><row><cell>Item-KNN</cell><cell>51.60</cell><cell>21.81</cell><cell>52.31</cell><cell>21.70</cell><cell>35.75</cell><cell>11.57</cell></row><row><cell>BPR-MF</cell><cell>31.31</cell><cell>12.08</cell><cell>3.40</cell><cell>1.57</cell><cell>5.24</cell><cell>1.98</cell></row><row><cell>FPMC</cell><cell>45.62</cell><cell>15.01</cell><cell>-</cell><cell>-</cell><cell>26.53</cell><cell>6.95</cell></row><row><cell>GRU4REC</cell><cell>60.64</cell><cell>22.89</cell><cell>59.53</cell><cell>22.60</cell><cell>29.45</cell><cell>8.33</cell></row><row><cell>NARM</cell><cell>68.32</cell><cell>28.63</cell><cell>69.73</cell><cell>29.23</cell><cell>49.70</cell><cell>16.17</cell></row><row><cell>STAMP</cell><cell>68.74</cell><cell>29.67</cell><cell>70.44</cell><cell>30.00</cell><cell>45.64</cell><cell>14.32</cell></row><row><cell>SR-GNN</cell><cell>70.57</cell><cell>30.94</cell><cell>71.36</cell><cell>31.89</cell><cell>50.73</cell><cell>17.59</cell></row><row><cell>(ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FGNN-Gated</cell><cell>70.85</cell><cell>31.05</cell><cell>71.5</cell><cell>32.17</cell><cell>51.03</cell><cell>17.86</cell></row><row><cell cols="2">FGNN-ATT-OUT 70.74</cell><cell>31.16</cell><cell>71.68</cell><cell>32.26</cell><cell>50.97</cell><cell>18.02</cell></row><row><cell>FGNN</cell><cell>71.12</cell><cell>31.68</cell><cell>71.97</cell><cell>32.54</cell><cell>51.36</cell><cell>18.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance when K = 5 and 10 for Yoochoose1/64.</figDesc><table><row><cell>Method</cell><cell cols="4">Yoochoose1/64 R@5 MRR@5 R@10 MRR@10</cell></row><row><cell>NARM</cell><cell>44.34</cell><cell>26.21</cell><cell>57.50</cell><cell>27.97</cell></row><row><cell cols="2">STAMP 45.69</cell><cell>27.26</cell><cell>58.07</cell><cell>28.92</cell></row><row><cell cols="2">SR-GNN 47.42</cell><cell>28.41</cell><cell>60.21</cell><cell>30.13</cell></row><row><cell>FGNN</cell><cell>48.23</cell><cell>29.16</cell><cell>60.97</cell><cell>30.85</cell></row><row><cell cols="5">session context information. Item-KNN achieves the best results</cell></row><row><cell cols="5">among the traditional methods, though it only calculates the sim-</cell></row><row><cell cols="5">ilarity between items without considering sequential information.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://2015.recsyschallenge.com/challenge.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://cikm2016.cs.iupui.edu/cikm-cup/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head><p>This work is supported by ARC DP190101985, DP170103954, NSFC 61628206 and NSFC 61806039.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the Item Order in Session-based Recommendation with Graph Neural Networks. In The 28th ACM International Conference on Information and Knowledge Management (CIKM '19), November 3-7, 2019, Beijing, China. ACM, New York, NY, USA, 11 pages.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AIR: Attentional Intention-Aware Recommender Systems</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th IEEE International Conference on Data Engineering</title>
				<meeting>the 35th IEEE International Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, and Xue Li</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014 workshop on Deep Learning</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
				<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Streaming Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viet</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks with Top-k Gains for Session-based Recommendations</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Session-based Recommendations with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<idno>ICLR 2016</idno>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">When Recurrent Neural Networks meet the Neighborhood for Session-Based Recommendation</title>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Ludewig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems</title>
				<meeting>the Eleventh ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From Zero-Shot Learning to Cold-Start Recommendation</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">EAAI</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On both Cold-Start and Long-Tail Recommendation with Social Data</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural Attentive Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph Matching Networks for Learning the Similarity of Graph Structured Objects</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>ICLR 2016</idno>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Refuoe</forename><surname>Mokhosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning, ICML 2010</title>
				<meeting>the 27th International Conference on Machine Learning, ICML 2010</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Content-Based Recommendation Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><surname>Billsus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Adaptive Web, Methods and Strategies of Web Personalization</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorizing personalized Markov chains for next-basket recommendation</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web</title>
				<meeting>the 19th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">George</forename><surname>Badrul Munir Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International World Wide Web Conference</title>
				<meeting>the Tenth International World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2001">2001. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>ICLR 2014</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collaborative Filtering Recommender Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Ben</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Frankowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilad</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Adaptive Web, Methods and Strategies of Web Personalization</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An MDP-based Recommender System</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference in Uncertainty in Artificial Intelligence</title>
				<meeting>the 18th Conference in Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">UAI</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What Can History Tell Us? Identifying Relevant Sessions for Next-Item Recommendation</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM on Conference on Information and Knowledge Management, CIKM</title>
				<meeting>the 29th ACM on Conference on Information and Knowledge Management, CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved Recurrent Neural Networks for Session-based Recommendations</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Kiam Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
				<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>DLRS@RecSys</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D Convolutional Networks for Session-based Recommendation with Content Features</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems</title>
				<meeting>the Eleventh ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Order Matters: Sequence to sequence for sets</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno>ICLR 2016</idno>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Survey on Session-based Recommender Systems</title>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR abs/1902.04864</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SPORE: A sequential personalized spatial item recommender system</title>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shazia</forename><forename type="middle">Wasim</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd IEEE International Conference on Data Engineering, ICDE 2016</title>
				<meeting>the 32nd IEEE International Conference on Data Engineering, ICDE 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="954" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Session-aware Information Embedding for Ecommerce Product Recommendation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">AAAI</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An Endto-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Using Temporal Data for Making Recommendations</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zimdars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, UAI</title>
				<meeting>the 17th Conference in Uncertainty in Artificial Intelligence, UAI</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
