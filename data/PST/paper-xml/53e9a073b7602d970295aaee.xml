<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CVC-MUSCIMA: a ground truth of handwritten music score images for writer identification and staff removal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-06-29">29 June 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alicia</forename><surname>Fornés</surname></persName>
							<email>afornes@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
							<email>adutta@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Gordo</surname></persName>
							<email>agordo@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josep</forename><surname>Lladós</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CVC-MUSCIMA: a ground truth of handwritten music score images for writer identification and staff removal</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-06-29">29 June 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">FD2337C8A73046D5A58E9883580454AF</idno>
					<idno type="DOI">10.1007/s10032-011-0168-2</idno>
					<note type="submission">Received: 15 January 2011 / Revised: 8 April 2011 / Accepted: 31 May 2011 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The analysis of music scores has been an active research field in the last decades. However, there are no publicly available databases of handwritten music scores for the research community. In this paper, we present the CVC-MUSCIMA database and ground truth of handwritten music score images. The dataset consists of 1,000 music sheets written by 50 different musicians. It has been especially designed for writer identification and staff removal tasks. In addition to the description of the dataset, ground truth, partitioning, and evaluation metrics, we also provide some baseline results for easing the comparison between different approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The analysis of music scores <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b36">35]</ref> is a classical area of interest of Document Image Analysis and Recognition (DIAR). Traditionally, the main focus of interest within the research community has been the transcription of printed music scores. Optical Music Recognition (OMR) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">15]</ref> consists in the understanding of information from digitized music scores and its conversion into a machine readable format. It allows a wide variety of applications such as the edition of scores never edited, renewal of old scores, conversion of scores into Braille, production of audio files, adaptation of existing works to other instrumentations, transposing a music sample to some other clef or key signature, producing parts from a given score or a full score from given parts, creation of collecting databases to perform musicological analysis. Since the first works by Prerau and Pruslin in the late 1960s <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b28">27]</ref>, interest in OMR has grown in last decades, appearing several complete OMR systems for printed music (such as Aruspix, Gamera, or Guido <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b34">33]</ref>), braille music approaches <ref type="bibr" target="#b2">[3]</ref>, and even an almost real-time keyboardplaying robot (the Wabot-2 robot <ref type="bibr" target="#b21">[21]</ref>).</p><p>Among the required stages of an Optical Music Recognition system, an special emphasis has been put in the staff removal algorithms <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b33">32]</ref>, since a good detection and removal of the staff lines will allow the correct isolation and segmentation of the musical symbols, and consequently, will ease the correct detection, recognition, and classification of the music symbols. Staff removal is somehow related to form processing <ref type="bibr" target="#b18">[18]</ref>, where ruling lines must be removed prior to recognize the text. The main difference is that staff removal techniques can take advantage of grouping rules, in other words, the algorithm can search a group of five equidistant horizontal lines (the staff).</p><p>In the last decade, there has been a growing interest in the analysis of handwritten music scores <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b35">34]</ref>. In this context, the focus of interest is twofold: the recognition of handwritten music scores and the identification (or verification) of the authorship of a music score. Concerning writer identification, musicologists do not only perform a musicological analysis of the composition (melody, harmony, rhythm, etc.) but also analyze the handwriting style of the manuscript. In this sense, writer identification can be performed by analyzing the shape of the hand-drawn music symbols (e.g., music notes, clefs, accidentals, rests, etc.), because it has been shown (see <ref type="bibr" target="#b10">[10]</ref>) that the author's handwriting style that characterizes a piece of text is also present in a graphic document. Nevertheless, musicologists must work very hard to identify the writer of a music score, especially when there is a large amount of writers to compare with. Recently, several writer identification approaches have been developed for helping musicologists in such a time-consuming task. These approaches are based in many different methodologies, such as Self Organizing Maps <ref type="bibr" target="#b20">[20]</ref>, Bag of Features <ref type="bibr" target="#b14">[14]</ref>, knowledge-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">13]</ref>, or even systems that adapt some writer identification approaches for text documents to music scores <ref type="bibr" target="#b11">[11]</ref>.</p><p>Contrary to printed music scores databases <ref type="bibr" target="#b6">[6]</ref>, there are no public databases of handwritten music scores available for the research community. For this reason, there is a need of a public database and ground truth for validating the different methodologies developed in this research field. With this motivation, in this paper we present the CVC-MUSCIMA 1 ground truth: a ground truth of handwritten music score images. The database and ground truth are available in the Web site: http://www.cvc.uab.es/cvcmuscima. This dataset consists of 1,000 music sheets written by 50 different musicians and has been especially designed for writer identification and staff removal tasks.</p><p>In this paper, we describe the database, evaluation metrics, partitions (data subsets), and baseline results for comparison purposes. We believe that the presented ground truth will serve as a basis for research in handwritten music analysis. Moreover, we will show that the effort of generating ground truth can be reduced using color cues and by applying distortions to both original images and ground truth images.</p><p>The rest of the paper is organized as follows. Section 2 describes the dataset and the staff distortions applied. Section 3 presents the evaluation partitions, metrics, and baseline results for comparison purposes. Finally, concluding remarks are described in Sect. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The dataset consists of 20 music pages of different compositions transcribed by 50 writers, yielding a total of 1,000 music pages. All the 50 writers are adult musicians (aged from 18 to 35) in order to ensure that they have their own characteristic handwriting style. We chose the set of 50 musi-  . The set of writers includes advanced musician students (in conservatories of music or at University), musicologists, music teachers, and professional musicians, but as far as we know, none of them are famous. They all have been studying music for many years, and consequently, they have their own characteristic handwriting style. Figure <ref type="figure" target="#fig_0">1</ref> shows some examples of handwritten music scores written by three different musicians. Having a look at the images, one can see that writer B tends to write in a rectilinear way (with very thin headnotes), while writers A and C draw very round headnotes. In addition, it can be observed that writer C tends to write short symbols (and also short slurs), whereas writers A and B draw taller music symbols and longer slurs. Each writer has been asked to transcribe exactly the same 20 music pages, using the same pen (a black Pilot v7 Hi-Tecpoint) and the same kind of music paper (standard DIN A4 sheets with printed staff lines in blue color). The set of the 20 selected music sheets contains monophonic and polyphonic music, and it consists of music scores for solo instruments (e.g., violin, flute, violoncello, or piano) and music scores for choir and orchestra. It must be noted that the music scores only contain the handwriting text considered as part of the music notation theory (such as dynamics and tempo notation), and for this reason, music scores for choir do not contain lyrics.</p><p>Furthermore, for staff removal tasks, each music page has been distorted using different transformation techniques (please refer to Sect. 2.2 for details), which, together with the originals, yields a grand total of 12,000 base images.</p><p>Next, we describe the data acquisition, the generated deformations, and the different ground truths and data formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Acquisition and preprocessing</head><p>Documents were scanned using an flatbed Epson GT-3000 scanner set at 300 dpi and 24 bpp, as color cues were used in the original templates to ease the elaboration of the staff ground truth. Later, the images were converted to 8-bit gray scale. Care was put into obtaining a good orientation during the scanning stage, and absolutely no digital skew correction was applied once the pages were scanned.</p><p>The staff lines were initially removed using color cues. Afterward, they were binarized and manually checked for correcting errors, specially when some segments of the staff lines were manually added by the writer (see an example in Fig. <ref type="figure" target="#fig_1">2</ref>). Thus, from the gray scale images, we generated the binarized images, the images with only the music symbols (without staff lines), and finally, the images with only the staff lines. Next, we describe the distortions applied to the music scores for staff removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Staff distortions</head><p>To test the robustness of different staff removal algorithms, we have applied a set of distortion models to our music score images. These distortion models are inspired by the work of Dalitz et al. <ref type="bibr" target="#b6">[6]</ref> for testing the performance of staff removal algorithms in printed music scores. In <ref type="bibr" target="#b6">[6]</ref>, the authors describe nine different types of deformations for simulating their dataset with real-world situation: degradation with Kanungo noise, rotation, curvature, Staffline interruption, typeset emulation, Staffline y-variation, Staffline thickness ratio, Staffline thickness variation, and white speckles.</p><p>In order to obtain the same effect, the deformation is simultaneously applied to the original and the ground truth staff images, which correspond to binary images with only the staff lines. A brief description of the individual deformation models is given next:</p><p>-Kanungo noise. Kanungo et al. <ref type="bibr" target="#b17">[17]</ref> have proposed a noise model to create local distortions introduced during scanning. The model mainly affects the contour pixels and has little effect on the inner pixels (see Fig. <ref type="figure">3b</ref>). -Rotation. The distortion rotation (see Fig. <ref type="figure">3c</ref>) consists in rotating the entire staff image by the specified parameter angle. -Curvature. The curvature is performed by applying a half sinusoidal wave over the entire staffwidth. The strength of the curvature is regulated by a parameter, which is a ratio of the amplitude to the staffwidth (see Fig. <ref type="figure">3d</ref>). -Staffline interruptions. The Staffline interruptions consist in generating random interruptions with random size in the staff lines. This model mainly affects the staffline pixels and simulates the scores that are written on already degraded staff lines (see Fig. <ref type="figure">3e</ref>). -Typeset emulation. This particular defect is intended to imitate the sixteenth-century prints that are set by lead types. Consequently, they have Staffline interruptions between symbols and also a random vertical shift of each vertical staff slice containing a symbol (see Fig. <ref type="figure">3f</ref>). -Staffline y-variation and Staffline thickness variation.</p><p>These kind of defects are created by generating a Markov chain describing the evolution of the y-position or the thickness from left to right. This is done since generally the y-position and the staff thickness values for a particular x-position depend on its previous x-position (Fig. <ref type="figure">3g-j</ref> show some examples of these deformations with different parameters). -Staffline thickness ratio. This defect only affects the whole Staffline thickness of the music score, which consists in generating staff lines of different thickness (see Fig. <ref type="figure">3k</ref>). For more information about the parameters and the generation of each distortion, refer to <ref type="bibr" target="#b6">[6]</ref> -White speckles. This degradation model is used to generate white noise within the staff pixels and musical symbols (see Fig. <ref type="figure">3l</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>123</head><p>Table <ref type="table">1</ref> describes the parameters of the respective models. Dalitz et al. <ref type="bibr" target="#b6">[6]</ref> have developed the MusicStaves toolkit 2 , which is available for reproducing the experiments in other datasets. However, these available algorithms for distorting the staff lines have an important drawback: they require computer-generated perfect artificial images, which means perfect horizontal staff lines, equidistant, and also with the same thickness. Since our dataset contains printed and handwritten segments of staff lines (see Fig. <ref type="figure" target="#fig_1">2</ref>), their algorithms cannot be directly applied to our music scores. For this reason, we have modified these algorithms to reproduce the same distortion model in our handwritten music scores (where we do not assume any constraints for perfect staff lines).</p><p>For validating the staff removal algorithms, we have generated a set of 11,000 distorted images by applying the nine already described distortion models, where two of them have been applied twice (see Fig. <ref type="figure">3</ref>). Thus, for each original image, we have obtained 11 distorted images by applying these distortion algorithms with the parameters described in Fig. <ref type="figure">3</ref>. As a result, the dataset for staff removal purposes contains 12,000 images (1,000 original images plus the 11,000 distorted ones). However, since we also provide the code of the staff distortions algorithms, the users can generate the distorted images with their desired parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ground truth</head><p>In this section, we describe the images, evaluation partitions (subsets), evaluation metrics, and some baseline results. Thus, they will serve as a benchmark scenario for a fair comparison between different approaches.</p><p>Concerning the baseline results, it must be said that since the main contribution of this work is the framework for performance evaluation, we include some baseline results just for reference purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Images description</head><p>All the images of the dataset are presented in PNG format. Each document of the dataset (1,000 original images plus the 11,000 distorted images) is labeled with its writer identification code and presented in different image flavors:</p><p>-Original gray scale image (only for the original 1,000 images). -Binary image (with staff lines).</p><p>-Binary staffless image (only music symbols).</p><p>-Binary staff lines image (no music symbols).</p><p>Although all this information is available for all tasks, we encourage the use of certain image flavors for different tasks. The staffless images are particularly useful for writer identification: Since most writer identification methods remove the staff lines in the preprocessing stage, this eases the publication of results which are not dependant on the  performance of the particular staff removal technique applied (see an example in Fig. <ref type="figure" target="#fig_4">4</ref>).</p><p>Similarly, for the staff removal tasks, staff lines images without music symbols (see Fig. <ref type="figure" target="#fig_7">5</ref>) may be useful, not only for the evaluation of the method but also for training purposes. It must be said that the ground truth images only show the pixels that belong only to staff lines. Consequently, these images contain holes, which correspond to the pixels belonging to music symbols.</p><p>Table <ref type="table" target="#tab_1">2</ref> summarizes the provided images and these recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation partitions for writer identification</head><p>For training and evaluation purposes, we devised two sets of ten partitions, which were especially designed for the evaluation of writer identification tasks:</p><p>Set A or constrained. In the first set of partitions, the training pieces of a given fold are the same for each writer,    and so none of the pieces of the test set have been used during the training stage. As an illustrative example, look at Fig. <ref type="figure" target="#fig_9">6a</ref>. Since the first music page of one writer is in the train set of a given fold, all the first music pages of the remaining writers will also be in the train set of that particular fold. Set B or unconstrained. In the second set of partitions, this constraint is not satisfied, and pieces that appear in the training set of one author will appear in the test set of a different one (for example, the first music page will appear in the train set of one author and in the test set of another, as seen in Fig. <ref type="figure" target="#fig_9">6b</ref>).</p><p>These partitions are particularly devised to attest that we are indeed performing writer identification instead of rhythm classification. Indeed, if the method was performing rhythm classification, it is reasonable to think that, in set B, unconstrained, test pieces from one author would be matched with the exact same pieces that appear in the train set of a  different author, and so the classification results would be significantly lower than on set A, where this confusion is not possible. At the same time, a writer identification rate in set B similar to the one in set A will show that the system is classifying according to the handwriting style and not being particularly affected by the kind of music notes and symbols appearing in the music sheet.</p><p>In each partition, 50% of the documents of each writer belong to the training set and the other 50% belong to the test set. Furthermore, effort has been put in guaranteeing that each piece appears approximately 50% of the time in training and 50% in test. The exact partitions can be found in the dataset pack.</p><p>It must be said that instead of the proposed partitions, other strategies (such as "Leave-one-out") can be also used. However, we encourage the use of these partitions to test whether the system is rhythm dependant or not. In any case (partitions or "Leave-one-out"), the metrics described in next subsection can be applied without any modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation metrics and baseline results for writer identification</head><p>In this subsection, we will describe the evaluation metrics and, as an illustrative example, some baseline results for writer identification purposes.</p><p>Metrics. Writer identification systems are evaluated considering two options: if the image has been correctly classified taking into account the n-first authors or only the first writer. In our scenario, we will treat it as a binary problem, in which a music score is correctly classified only if the first nearest writer corresponds to the ground-truthed one.</p><p>Method. As we have commented, it is out of the scope of this work to make a comparison of different writer identification methods in the literature. However, and only for reference purposes, we provide baseline results using a recent writer identification method for musical scores <ref type="bibr" target="#b14">[14]</ref>.</p><p>In the Bag-of-Notes approach described in <ref type="bibr" target="#b14">[14]</ref>, features are computed using the Blurred Shape Model descriptor <ref type="bibr" target="#b8">[8]</ref>. As in the Bag-of-Visual-Words framework, a codebook is built and symbols are assigned to the vocabulary words to represent the musical scores. Finally, they are classified using a SVM trained in a 1 vs. all fashion.</p><p>In that work, the authors presented a vanilla Bag-of-Notes, with the following properties:</p><p>-Unsupervised clustering with k-means.</p><p>-Hard assignment.</p><p>-Linear kernel.</p><p>Afterward, they proposed the following modifications:</p><p>-Supervised clustering (learning a different codebook for each author and then merging the codebooks). -Probabilistic vocabulary (learning the vocabulary with a GMM). -Using a RBF kernel.</p><p>Interestingly, we found that the simpler vanilla implementation of the Bag-of-Notes obtained very similar results than the more complex modifications. In general, the probabilistic vocabularies bring little or no improvement over k-means even when tied with supervised clustering unless some adaptation is performed <ref type="bibr" target="#b26">[25]</ref>; besides, the increasing size of the vocabulary usually makes these approaches impractical or unfeasible as the number of classes increase.</p><p>The RBF kernel provided slightly better results than the linear one. However, RBF has an extra parameter, the bandwitdh γ , which has to be validated. Also, using a linear kernel allows us to use solvers optimized for linear problems such as LIBLINEAR <ref type="bibr" target="#b9">[9]</ref>, which makes use of the cutting-plane algorithm and drastically improves the training speed of the SVM. To set the C trade-off cost of the SVM classifier, we used the same heuristic used by the SVM light <ref type="bibr" target="#b16">[16]</ref> suite. Given a set of N training vectors X = {x 1 , x 2 , . . . , x N }, we set C as follows: This heuristic gave excellent classifications results, better than to those obtained by manually setting the parameter.</p><formula xml:id="formula_0">C = 1/k 2 , k = 1 N N i=1 x i x i . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Because of its simplicity as well as comparatively good performance, we will report results using the vanilla implementation of the Bag-of-Notes (unsupervised clustering with k-means, hard assignment, and linear kernel), without the improvements of <ref type="bibr" target="#b14">[14]</ref>.</p><p>Results. Table <ref type="table" target="#tab_2">3</ref> reports mean classification accuracy and standard deviation as a function of the number of words for the two sets of partitions (please c.f. Sect. 3.2 for details on these partitions). Note that the accuracy results on both sets are quite similar, with a slight advantage for the second set; the higher accuracy and smaller standard deviation are probably caused because this set contains more variety in the training data. The fact that both sets obtain very similar results suggests that the Bag-of-Notes method is indeed performing writer identification and not rhythm identification, as would be the case if the constrained set obtained significantly better results than the unconstrained set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation metrics and baseline results for staff removal</head><p>The goal of staff removal is to delete those pixels that only belong to staff lines. If a pixel belongs to both a staff line and a musical symbol, then the pixel is labeled as belonging to the symbol. Consequently, the staff removal algorithm must be careful when removing the staff line segments, because they should not remove those pixels belonging to music symbols. Next, we describe the evaluation metrics and some baseline results for staff removal purposes.</p><p>Metrics. Different metrics have been used in the literature. For example, in <ref type="bibr" target="#b6">[6]</ref>, the authors use the error rate, segmentation error, and Staffline interruptions, whereas in <ref type="bibr" target="#b33">[32]</ref>, the authors propose to use the percentage of staff lines falsely detected and the percentage of staff lines missed to detect. However, some of these measures are not very easy to compute. For this reason, we have chosen the pixel-based evaluation metric to get the quantitative measurement of the performance of staff removal algorithms. These measures are very well known, easy, efficient, and fast to compute. In this scenario, we consider the staff removal problem as a two-class classification problem at the pixel level. For each of the images, we compute the number of true-positive pixels tp (pixels correctly classified as staff lines), false-positive pixels fp (pixels wrongly classified as staff lines), and false-negative fn (pixels wrongly classified as non-staff lines) by overlapping with the corresponding ground truth images. The Precision and Recognition Rate measures of the classification are computed as:</p><formula xml:id="formula_2">Precision = P = tp tp + fp ,<label>(2)</label></formula><p>Recognition Rate = R = tp tp + fn .</p><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>The third metric error rate E is computed as (# means "number of", sp means "staff pixels"):</p><formula xml:id="formula_4">E =</formula><p>#misclassified sp + #misclassified non sp #all sp + #all non sp . (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method.</head><p>For the sake of illustration, we have chosen one of our staff removal algorithms as the baseline results. The approach proposed in <ref type="bibr" target="#b7">[7]</ref> is based on the criteria of neighboring staff components. It considers a staffline segment as a horizontal linkage of vertical black runs with uniform height, and then, it uses the neighboring properties of a staffline segment to discard the false segments.</p><p>Results. Table <ref type="table" target="#tab_3">4</ref> shows the results of the staff removal algorithm using the proposed evaluation metrics and applied to the 12,000 distorted images. It must be noted that it does not obtain the best results in all cases with respect to the three evaluation metrics, showing that there is still room for research in this field. It should also be noted that these results are over the whole dataset and not only on the testing set, since this method does not require any training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we have described the CVC-MUSCIMA database and ground truth, which has been especially designed for writer identification and staff removal tasks. We have also described the evaluation metrics, partitions, and baseline results in order to ease the comparison between the different approaches that may be developed. It must be said that the main contribution of this work is the framework for performance evaluation, and for this reason, we have included some baseline results just for reference purposes. Concerning ground truthing, we have shown that, although ground truth generation is a time-consuming task (specially when it is manually generated), one can reduce the effort of ground truthing using some simple methods (e.g., using color cues, applying distortions to images, and carrying the ground truth through to the distorted images).</p><p>The database can serve as a basis for research in music analysis. The database and ground truth are considered complete at the current stage. However, further work will be focused on labelling each music note and symbol of the music score images for Optical Music Recognition purposes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Examples of pieces of music scores written by three different musicians. Notice the differences in handwriting styles</figDesc><graphic coords="2,329.83,330.41,190.12,109.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Example of a section of a music score with some segments of hand-drawn staff lines</figDesc><graphic coords="3,70.57,56.09,198.76,135.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 Table 1</head><label>31</label><figDesc>Fig. 3 Staff deformation and their corresponding parameters. a Ideal image. b Kanungo (η, α 0 , α, β 0 , β, k) = (0, 1, 1, 1, 1, 2). c Rotation (θ) = • ). d Curvature (a, p) = (0.05, 1.0). e Staffline interruptions (α, n, p) = (0.5, 3, 0.5). f Typeset emulation (n, p, ns) = (1, 0.5, 10). g, h Staffline y-variation (n, c) = (5, 0.6), and (n, c) = (5, 0.93). i, j Staffline thickness variation (n, c) = (6, 0.5), and (n, c) = (6, 0.93). k Staffline thickness ratio (r ) = (1.0). l White speckles ( p, n, k) = (0.025, 10, 2)</figDesc><graphic coords="4,57.22,510.98,104.68,61.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Gray image (b) Binary image (c) Image without staff lines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Example of the ground truth of music scores for writer identification: a Gray image, b Binary image, c Staffless binary image</figDesc><graphic coords="5,327.04,313.10,195.88,100.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>123</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Curved Image (b) Staff -only Curved Image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Example of the ground truth for staff removal: a Curved image, b Staff-only curved image. Notice that the staff-only image contains holes corresponding to pixels that belong to music symbols</figDesc><graphic coords="6,74.71,187.73,190.12,98.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Constrained set (b) Unconstrained set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Train (black) and test (white) documents of each one of the 50 writers in a given fold in the constrained (left) and unconstrained (right) sets. In the constrained sets, all the writers use the same pieces for training. In the unconstrained sets, pieces used for training by one writer may appear in test for another writer</figDesc><graphic coords="6,308.68,56.69,232.84,241.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>stands for Computer Vision Center -MUsic SCore IMAges.</head><label></label><figDesc></figDesc><table /><note><p>1 CVC-MUSCIMA</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Image flavors designed for writer identification and staff removal tasks</figDesc><table><row><cell>Task</cell><cell>Images provided</cell></row><row><cell>Writer</cell><cell>1,000 Original undistorted grey scale images</cell></row><row><cell>Ident.</cell><cell>1,000 Binary images (with staff lines)</cell></row><row><cell></cell><cell>1,000 Binary staffless images</cell></row><row><cell>Staff</cell><cell>12,000 Binary images with staff lines</cell></row><row><cell>Removal</cell><cell>12,000 Binary images of only staff lines</cell></row><row><cell></cell><cell>12,000 Binary staffless images</cell></row><row><cell cols="2">Recommended images for each task in bold</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Mean classification accuracy (in %) and standard deviation as a function of the number of vocabulary words</figDesc><table><row><cell>N. of words</cell><cell>Set A (const.)</cell><cell>Set B (unconst.)</cell></row><row><cell>16</cell><cell>31.60 ± 4.28</cell><cell>31.82 ± 2.06</cell></row><row><cell>32</cell><cell>43.18 ± 3.73</cell><cell>45.36 ± 3.36</cell></row><row><cell>64</cell><cell>57.08 ± 4.15</cell><cell>59.20 ± 2.83</cell></row><row><cell>128</cell><cell>73.02 ± 3.83</cell><cell>75.00 ± 2.06</cell></row><row><cell>256</cell><cell>84.72 ± 3.20</cell><cell>86.12 ± 1.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Performance of the staff removal algorithm described in<ref type="bibr" target="#b7">[7]</ref> (P = Precission, R = Recognition rate and E = Error rate are shown in %)</figDesc><table><row><cell>Deformation type (%)</cell><cell>P (%)</cell><cell>R (%)</cell><cell>E (%)</cell></row><row><cell>Ideal</cell><cell>99.22</cell><cell>93.56</cell><cell>2.9</cell></row><row><cell>Curvature</cell><cell>97.50</cell><cell>91.99</cell><cell>4.0</cell></row><row><cell>Interrupted</cell><cell>97.73</cell><cell>94.53</cell><cell>1.5</cell></row><row><cell>Kanungo</cell><cell>99.38</cell><cell>91.51</cell><cell>3.9</cell></row><row><cell>Rotated</cell><cell>96.71</cell><cell>93.38</cell><cell>3.7</cell></row><row><cell>Line thickness variation (v1)</cell><cell>95.45</cell><cell>94.96</cell><cell>5.1</cell></row><row><cell>Line thickness variation (v2)</cell><cell>97.45</cell><cell>93.97</cell><cell>4.5</cell></row><row><cell>Line y-variation (v1)</cell><cell>94.54</cell><cell>93.63</cell><cell>6.7</cell></row><row><cell>Line y-variation (v2)</cell><cell>94.73</cell><cell>94.33</cell><cell>6.3</cell></row><row><cell>Thickness ratio</cell><cell>97.87</cell><cell>91.64</cell><cell>8.4</cell></row><row><cell>White speckles</cell><cell>97.95</cell><cell>96.88</cell><cell>2.1</cell></row><row><cell>Typeset emulation</cell><cell>98.86</cell><cell>89.46</cell><cell>4.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://lionel.kr.hs-niederrhein.de/~dalitz/data/projekte/stafflines/ doc/musicstaves.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank all the musicians who contributed to the database presented in this paper. We would also specially thank to Joan Casals from the Universitat Autònoma de Barcelona for contacting with musicians and collecting the music sheets. We would also like to thank Dr. Christoph Dalitz for providing the code, which generates the staff distortions. This work has been partially supported by the Spanish projects TIN2008-04998, TIN2009-14633-C03-03, and CON-SOLIDER-INGENIO 2010 (CSD2007-00018) and 2011 FIB 01022.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The challenge of optical music recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Hum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="121" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Baird</surname></persName>
		</author>
		<title level="m">Structured Document Image Analysis, chap. A Critical Survey of Music Image Analysis</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="405" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bmml: a markup language for braille music</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bortolazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Baptiste-Jessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bertoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers Helping People with Special Needs</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Miesenberger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Klaus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Zagler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Karshmer</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5105</biblScope>
			<biblScope unit="page" from="310" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge-based scribe recognition in historical music archives</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ignatova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Milewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Heery</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Lyon</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3232</biblScope>
			<biblScope unit="page" from="304" to="316" />
		</imprint>
	</monogr>
	<note>Research and Advanced Technology for Digital Libraries</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An adaptive staff line removal in music score images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="964" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparative study of staff removal algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dalitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Droettboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pranzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fujinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="753" to="766" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient staff removal approach from printed musical documents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fornes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Llados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, International Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1965" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blurred Shape Model for binary and grey-level symbol recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lladós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1424" to="1433" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Liblinear: a library for large linear classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/liblinear/" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A symbol-dependent writer identification approach in old handwritten music scores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fornes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Llados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition, International Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="634" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A combination of features for symbol-independent writer identification in old music scores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lladós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Otazu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Anal. Recogn</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="243" to="259" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Visual Perception of Music Notation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fujinaga</surname></persName>
		</author>
		<editor>George, S.</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="39" />
			<pubPlace>Hershey</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Idea Group</orgName>
		</respStmt>
	</monogr>
	<note>Staff detection and removal</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a system for writer identification on handwritten music scores</title>
		<author>
			<persName><forename type="first">R</forename><surname>Göcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IASTED International Conference on Signal Processing, Pattern Recognition, and Applications (SPPRA)</title>
		<meeting>the IASTED International Conference on Signal Processing, Pattern Recognition, and Applications (SPPRA)<address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="250" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A bag of notes approach to writer identification in old handwritten musical scores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lladós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IAPR International Workshop on Document Analysis Systems, DAS&apos;10</title>
		<meeting>the 9th IAPR International Workshop on Document Analysis Systems, DAS&apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Optical music recognition: the case study of pattern recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Homenda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="835" to="842" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>In: Computer Recognition Systems, chap</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Making large-scale support vector machine learning practical</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<ptr target="http://svmlight.joachims.org/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods</title>
		<imprint>
			<publisher>MIT-Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Software</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A statistical, nonparametric methodology for document degradation model validation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuezle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1209" to="1223" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ruling line removal in handwritten page images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopresti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2704" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic identification of music notations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Luth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on WEB Delivering of Music (WEDELMUSIC)</title>
		<meeting>the Second International Conference on WEB Delivering of Music (WEDELMUSIC)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bag of characters and som clustering for script recognition and writer identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Miotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, International Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2182" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An integrated music information processing system: Psb-er</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsushima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ohteru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1989 International Computer Music Conference</title>
		<meeting>1989 International Computer Music Conference</meeting>
		<imprint>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ohio</forename><surname>Columbus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A fast HMM algorithm based on stroke lengths for online recognition of handwritten music scores</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maruyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Frontiers in Handwriting Recognition</title>
		<meeting>the Ninth International Workshop on Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="521" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An online handwritten music symbol recognition system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maruyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Anal. Recogn</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Visual Perception of Music Notation: On-Line and Off-Line Recognition, chap. Optical Music Analysis for Printed Music Score and Handwritten Music Manuscript</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Idea Group Inc</publisher>
			<biblScope unit="page" from="108" to="127" />
			<pubPlace>Hershey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Universal and adapted vocabularies for generic visual categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1243" to="1256" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Computer pattern recognition of standard engraved music notation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Prerau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
	<note type="report_type">Phd thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic recognition of sheet music</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pruslin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
	<note type="report_type">Phd thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Goal-directed evaluation for the improvement of Optical Music Recognition on early music prints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pugin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fujinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries</title>
		<meeting>the 7th ACM/IEEE-CS joint conference on Digital libraries</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="303" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GAMERA versus ARUSPIX. Two Optical Music Recognition approaches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pugin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fujinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Music Infor-mationRetrieval</title>
		<meeting>the 9th International Conference on Music Infor-mationRetrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="419" to="424" />
		</imprint>
	</monogr>
	<note>Lulu. com</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">New methodologies toward an automatic optical recognition of handwritten music scores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rebelo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Universidade do Porto (Portugal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optical recognition of music symbols</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rebelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Capela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Anal. Recogn</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Staff detection with stable paths</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dos Santos Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Capela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rebelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Da Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1134" to="1139" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Guido: a musical score recognition system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szwoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Document Anal. Recogn</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="809" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Musichand: a handwritten music recognition system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Taubman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Brown University</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mask matching for low resolution musical note recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008. 2009</date>
			<biblScope unit="page" from="223" to="226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
