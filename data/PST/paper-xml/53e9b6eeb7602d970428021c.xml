<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel backpropagation learning algorithms on CRAY Y-MP8/864 supercomputer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Hung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>470 Hitchcock Hal~ 2070 Neil Ave</addrLine>
									<postCode>43210-1275</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>470 Hitchcock Hal~ 2070 Neil Ave</addrLine>
									<postCode>43210-1275</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>470 Hitchcock Hall, 2070 Nell Ave</addrLine>
									<postCode>43210-1275</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel backpropagation learning algorithms on CRAY Y-MP8/864 supercomputer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">338B3316391B51B5912C3528622EE184</idno>
					<note type="submission">Received 18 May 1992 Revised 24 September 1992</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Backpropagation</term>
					<term>multitasking</term>
					<term>neural networks</term>
					<term>parallel processing</term>
					<term>vectorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Parallel backpropagation neural networks learning algorithms have been developed employing the vectorization and microtasking capabilities of vector MIMD machines. They have been implemented in C on CRAY Y-MP/864 supercomputer under UNICOS operating system. The algorithms have been applied to two different domains: engineering design and image recognition, and their performance has been investigated. A maximum speedup of about 6.7 is achieved using eight processors for a large network with 5950 links due to microtasking only. When vectodzation is combined with microtasking, a maximum speedup of about 33 is realized using eight processors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Adeli and Yeh <ref type="bibr" target="#b0">[1]</ref> presented a model of machine learning for engineering design based on the concept of self-adjustment of internal control parameters and perceptron. Hung and Adeli <ref type="bibr" target="#b5">[6]</ref> extended that work by developing a two-layer network for design problem solving. The learning performance and convergence speed of these two perceptron models were compared and discussed.</p><p>In order to improve the performance of learning algorithms, one approach, inspired by the human brain neurons performing many operations simultaneously, is the development of learning algorithms on general-purpose parallel computers with the objective of reducing the overall computing time <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this work, parallel learning algorithms have been developed for vector shared memory machines and implemented on CRAY Y-MP8/864 supercomputer under UNICOS operating system. Configured with eight 6-ns (6.0 × 10 -9 second for each clock cycle) clock cycle processors, the CRAY Y-MP8/864 is capable of up to 2667 Mflops with 8 processors (a flop is a floating-point (real number) operation, such as add, multiply, or reciprocal and Mflops means millions of floating-point operation per second of CPU time). The learning algorithms have been implemented in C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Vectorization</head><p>As a vector machine, CRAY Y-MP provides the utility of vectorization, thus reducing program execution time by as much as a factor of ten or more in innermost loops. A small amount of overhead, or start-up time, is associated with each vectorized loop. This start-up time involves initializing the vector registers for vector processing. For long loops, the startup time is only a small portion of the total execution time and is negligible. However, for small loops, the start-up time becomes a rather large portion of the total execution time. In this case, the directive #pragma _CRI nonvector can be used to inhibit vectorization of the loop immediately following the directive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Multitasking</head><p>The CRAY C compiler provides loop-level multitasking, called microtasking, and functionlevel multitasking, called macrotasking. It also provides autotasking which is microtasking performed automatically by the compiler. Autotasking works best when most of the code consists of nested loops <ref type="bibr" target="#b9">[ 10]</ref>.</p><p>C programs are mierotasked by inserting directives at the loop level. The problem of synchronization is handled by inserting directives guard and endguard between the code segment, called guarded region. The pair of case/endcase directives can be used to force only one processor to execute a code block in a parallel region.</p><p>We can maximize the performance by combining vectorization with multitasking in a</p><p>program. An example of vectorized and microtasked matrix multiplication, C = AB, implemented in C is given in Appendix I. In this problem, the directive #pragma _CRI taskloop is used to indicate the microtasked loop and a pair of directives #pragma _CRI guard and #pragma _CRI endguard are used to handle synchronization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Monitoring results and performance</head><p>Three different methods can be used to measure the speedup of a multitasked program on the CRAY Y-MPS/864. The first method is to run the multitasked program on a dedicated number of processors, through the use of the command dedcpu. But, in a multi-user environment dedicating all the processors to one program is not a readily available option. The second method is through the use of an expert system tool, called atexpert. This tool determines the execution time of each parallel region, the overhead time, and the maximum speedup for the whole program. Six different overhead sources can be identified in developing parallel regions <ref type="bibr" target="#b4">[5]</ref>. They are summarized in Fig. <ref type="figure" target="#fig_0">1</ref>. atexpert can estimate the speedup of the code due to vectorization and microtasking on a single processor without running the code in a dedicated multiprocessor environment. Furthermore, it computes the total overhead time due to various overhead sources. The third method to monitor the performance of a multitasked program on the CRAY Y-MP8/864 supercomputer is to measure the Mflops. Using the command hpm (Hardware Performance Monitor), machine performance can be measured in Mflops during the execution of a program under UNICOS operating system. As a 6.0 ns system, the maximum theoretical performance on the CRAY Y-MP8/864 system is 333 Mflops for one processor. For a task executed by only one processor, the value of Mflops due to vectorization falls between 20 Mflops and 300 Mflops <ref type="bibr" target="#b4">[5]</ref>. Lower values indicate mostly a scalar code or short loops, while maximum values indicate a high degree of vectorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Parallel neural network learning algorithms</head><p>The backpropagation (BP) learning is one of the supervised learning methods <ref type="bibr" target="#b8">[9]</ref>. Using the steepest descent method, the BP algorithm tries to find a set of weights, w, that minimizes the system error</p><formula xml:id="formula_0">1 P K E = 1 Ep = (dpk-opk) 2 ,<label>(1)</label></formula><formula xml:id="formula_1">P P p=l k=l</formula><p>where p indicates the number of training instances in the training set, dpk and opk are the desired and computed outputs for the kth output, respectively, and K is the total number of output patterns (nodes). The learning procedure is to update the weights between nodes. The change of weight is</p><formula xml:id="formula_2">Awji(n + 1) = 71t~joi + aAwji(n) . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>The term ~/in Eq. ( <ref type="formula" target="#formula_2">2</ref>) is the learning ratio. Proposed by Rumelhart et al. <ref type="bibr" target="#b8">[9]</ref>, the second term in the right hand side of Eq. ( <ref type="formula" target="#formula_2">2</ref>) is a momentum term. The a, called momentum ratio, is a constant. The momentum term is used to specify that the change to the (n + 1)st step should be somewhat similar to the change in the nth step. We modify and parallelize the serial backpropagation learning algorithm, and implement it on the aforementioned multiprocessor machine. In order to present the parallel algorithms, consider the topology of the m-layer neural network shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of nodes N[I] N[2]</head><p>Total numberof node~ Total numberof weight~</p><formula xml:id="formula_4">N[i] Nlm-ll NlmI m N,=ZNm i--1 m</formula><p>Fig. <ref type="figure" target="#fig_1">2</ref>. Topology of a multi-layer neural network.</p><p>There are two possible approaches to parallelize the aforementioned serial learning algorithm. One approach is to perform all the computations between the nodes concurrently. The training instances are processed sequentially. In this approach, we can maximize the concurrent processing in the neural network. However, the synchronization problem between nodes is taxing in this approach, because the feedforward and error backpropagation steps must be performed layer by layer. Moreover, the output of each node is equal to the summation of the product of the weights of its connecting nodes <ref type="bibr" target="#b8">[9]</ref>. Also, vectorization is inhibited in this case.</p><p>The other approach is to perform training of instances concurrently. In this case, Np (number of available processors) copies of the neural network perform the learning process concurrently and the synchronization problem is reduced to only the computation of summation of deltas of error. Also, the computation in each layer can be vectorized. Thus, this approach is more efficient for vector/parallel machines and thus is employed in this work.</p><p>For balancing the load among the processors, the directive gu/ded is used to divide the outer loop iteration into chunks of varying Size. The size of each chunk is computed at run time using the Guided Self-Scheduling Algorithm that attempts to balance the load on each processor so that all processors complete their work at nearly the same time <ref type="bibr" target="#b3">[4]</ref>. In the algorithms developed in this work, the Ns instances are divided into the same number of groups as the number of available processors (Np). Each processor executes approximately the same number of tasks concurrently. Vectorization is always applied in the innermost loop. The performance of the parallel learning algorithms is optimized on CRAY Y-MP by combining vectorization and microtasking. For instance, the feedforward process of the BP learning algorithm in each layer is done as an inner product computation. Therefore, this process is vectorized and carried out by vector registers.</p><p>There are four steps in each iteration of the modified parallel BP learning algorithms (Fig. <ref type="figure" target="#fig_2">3</ref>). They are the feedforward process and calculating the error term for each training instance, calculating the delta for each training instance, accumulating the total system error term and the deltas of weights, and updating the weight vector. In the following, two different algorithms A and B are presented and their relative efficiency is investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AlgorithmA</head><p>The first three steps have been vectorized and microtasked in this algorithm (see Table <ref type="table" target="#tab_1">1</ref>). The last step of updating the weight vector is only a for loop containing four floating-point operations: one add and three reciprocals. It has been vectorized only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm B</head><p>In order to maximize the concurrent processing, the last step of updating the weight vector is also microtasked in this algorithm. The parallel learning algorithm is presented in Fig. <ref type="figure" target="#fig_2">3</ref> and Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Applications</head><p>The parallel BP learning algorithms have been used for learning in two different domains: engineering design and image processing. We used two examples to investigate and compare the performance of the parallel learning algorithms implemented on a CRAY Y-MP8/864 supercomputer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1 -Engineering design</head><p>An acceptable design must satisfy the requirements of a design code such as the American  Institute of Steel Construction (AISC) Load &amp; Resistance Factor Design (LRFD) specifications <ref type="bibr" target="#b2">[3]</ref> for design of steel structures. This example is selection of a minimum weight steel beam from the AISC LRFD wide-flange (W) shape database for a given loading condition. Similar to Adeli and Yeh <ref type="bibr" target="#b0">[1]</ref> and Hung and Adeli <ref type="bibr" target="#b5">[6]</ref>, we divide the W shapes available into t groups in decreasing order of the plastic section modulus gx. PERHID developed by Hung and Adeli <ref type="bibr" target="#b5">[6]</ref> can learn a satisfactory design by identifying its group number only. In this work, the parallel learning algorithms are used for minimum weight design of beams. F, ach instance consists of 5 input patterns: the member length (L), the unbraced length (Lb), the maximum bending moment in the member (Mmax), the maximum shear force (Vmax), and the bending coefficient (Cb). The output pattern is the plastic modulus (Zx) of the corresponding least weight member.  Note:/* V */: Vectorized.</p><p>A four-layer neural network with two hidden layers was used to learn this problem (Fig. <ref type="figure" target="#fig_4">4</ref>).</p><p>The number of nodes in the input layer, the first and second hidden layers, and the output layer are 5, 5, 3, and 1, respectively. The total number of links in the four-layer neural network is 52. The learning and momentum ratios are chosen as 0.7 and 0.9, respectively. A total of ten training instances was used in this example. The total number of iterations for learning process is limited to 10,000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2 -Image recognition</head><p>The second example is an image recognition problem. That is recognizing numbers 0 to 9. As the noise is important in this problem, 10 noiseless and 20 noisy images were used as training instances in this example. Shown in Fig. <ref type="figure" target="#fig_5">5</ref>, are the seven by seven (7 x 7) binary images of the numerals, 0 to 9. The background value of the binary image is zero and the object pixels have a value of one. This is a hard-to-learn problem because the patterns in the training set share similar features. For instance, the images of the numerals 2, 5, 6, and 9 are similar to each other (see Fig. <ref type="figure" target="#fig_5">5</ref>).  A three-layer neural network with one hidden layer was used to learn this problem (Fig. <ref type="figure" target="#fig_6">6</ref>). The numbers of nodes in the input, the hidden, and the output layers are 49, 99, and 10, respectively. The total numbers of links (weights) in the three-layer neural network is 5950.</p><p>In this example, the learning and momentum ratios are chosen as 0.2 and 0.3, respectively. The total number of iterations for learning process is limited to 2,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Computation results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1</head><p>The pie chart in Fig. <ref type="figure" target="#fig_7">7</ref> shows the time spent in the four steps of the vectorized BP learning algorithm using one processor only. The synchronization section in this example has been reduced to only 0.4%. The speedups achieved for this example using Algorithms A and B due to microtasking only are shown in Fig. <ref type="figure">8</ref> (the speedup is measured with respect to the vectorized code). Algorithm B performs better for this example. Figure <ref type="figure">9</ref> shows the effect of vectorization on the speedup for Algorithm B (the speedup of the upper curve is due to vectorization and microtasking). The maximum speedups achieved in this example using eight processors with and without vectorization are 4.2 and 3.8, respectively. 79% 0.4% 0.6% nStep 1: Feedforward and calculation of sub-system error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I</head><p>Step 2: Calculation of deltas for each instance.</p><p>["~ Step 3: Calculation of total system error (synchronization required).</p><p>]</p><p>Step 4: Updating the vector of weights. The speedup achieved in Example 1 is not high for two reasons. First, the vector length in the vectorized loops is very short (with a maximum of 6). Thus, vectorization is not effective. Second, the overhead in microtasldng is very large (on the average 42% of the total execution time). This is due to the fact that there are only 10 iterations (instances) in the outer loop. The average execution time for this example on a SUN SPARCstation SLC is about 1350 seconds (23 minutes). On a CRAY Y-MP8/864 supercomputer, using one processor the average execution time is 6.7 and 8.2 seconds with and without vectorization, respectively.  Figure <ref type="figure" target="#fig_2">13</ref> shows the effect of vectorization on the speedup using Algorithm A (the speedup of the upper curve is due to vectorization and microtasking). The maximum speedup due to microtasking is 6.7 using eight processors. A maximum speedup of about 33 is achieved when microtasking is combined with vectorization.   We have used 30 training instances in this example. That means the number of iterations in the outermost loop is 30 which must be balanced among the available processors. When the number of training instances is not a multiple of the number of available processors a load imbalance inevitably exists that degrades the performance to some extent. This can be seen in the speedup curve for Algorithm A in Fig. <ref type="figure" target="#fig_0">11</ref> at points of 4 and 7 processors. The performance degradation for Algorithm B in this figure is primarily due to the short length of the second parallel region as explained previously. With 5950 links this example uses a large scale neural network. Consequently, the average CPU time of the sequential code on a SUN SPARCstation SLC is about 3.6 hours. The nonvectorized and vectorized codes on a CRAY Y-MP8/864 supercomputer using a single processor on the average take about 3.7 minutes and 43 seconds, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding remarks</head><p>Two parallel BP neural networks learning algorithms have been developed and implemented on CRAY Y-MP8/864 supercomputer under UNICOS operating system. The algorithms have been applied to two different domains: engineering design and image recognition. The following observations and conclusions are made:</p><p>(1) Performance of a parallel learning algorithm depends on the size of the problem. A parallel algorithm suitable for a small problem may not perform well for large problems. Our Algorithm A is suitable for large neural networks, while Algorithm B is suitable for small neural networks. (2) The synchronization section in a parallel BP algorithm can be reduced to a small portion (about 1%). Thus, a high degree of parallelization can be achieved. Using eight processors we achieved a speedup of 6.7 for a large example with 5950 links due to microtasking only. (3) A vector MIMD machine such as CRAY Y-MP8/864 provides a more effective environment for achieving maximum performance than parallel machines without vectorization capability. We achieved a speedup of about 5.0 due to vectorization only on a single processor for a large example with 5950 links. When vectorization was combined with microtasking, a combined speedup of about 33 was achieved. (4) The image recognition example presented in this paper is a hard-to-learn problem. That is, a large number of iterations is required to train the network (2000 in our example). We</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sources of overhead in a microtasked code on CRAY Y-MP8/864 supercomputer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The number of nodes in the layer i is N[i]. The learning problem is mapped from N[0] input nodes to N[m] output nodes and Ns instances are given as training samples. The total number of weights and nodes is denoted by N,~ and Nn, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Parallel error backpropagation neural network learning algorithm B.</figDesc><graphic coords="6,113.46,94.93,317.16,418.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>i = 1 to Np, do concurrently a. Initialize sub.system_error and sub_delta_weights to zero. b. For j = 1 to chunks/ze, do sequentially bl. Perform feedforward procedure of the BP learning algorithmthe deltas in output layer. /* V */ a,~ = (t,,~ -opt)o,k(1 -o,,~). b4. Calculate the deltas in hidden layers (from layer m -1 to layer 1). /* V */ = o,(1 -o,j) '~-~ (a~,k,). k b5. Calculate the sub_delta_weights in hidden layers (from layer m -I to 1). /* V */ Awj+ = y/(6jo~) + aAwj. Next j. { Guarded seclion -entry } c. Calculate the system error, E, by accumulating sub_system_error. /* V */ 1 E=~-~ EE,. d. Calculate deltas of weights by accumulating the sub.delta_weights. /* V */ Aw+ = Aw~. { Guarded section -end } Next i. { Parallel region 1 -end } Algorithm A B. For i = 1 to total_links, do serially /* V */ a. Update the weight vectors. ,,~(k + 1) = ,,3(k) + :,w~. Next i. Algorithm B { Parallel region 2 -entry } B. For i = I to Np, do concurrently a. For j = 1 to total_links, do serially al. Update the weight vectors. w~(k + 1) = ,~,(k) +/xw,. /* v */ Next j. Next i. { Parallel region 2 -end } WHILE (,,, stop criterion).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Four-layer neural network for the minimum weight steel beam problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The noiseless 7 x 7 binary images (numerals 0-9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The CPU time spent in the four steps of the modified BP algorithm for the minimum weight steel beam problem.</figDesc><graphic coords="10,156.43,331.57,222.12,223.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig</head><label></label><figDesc>Fig. 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 2 1 . 2 :O Step 3 :</head><label>92123</label><figDesc>Fig. 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. The CPU time spent in the four steps of the modified BP algorithm for the 7 x 7 binary image problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Parallel BP Learning Algorithms A and B on Y-MP8/864. 1. Initialize weights using rahdom values and set up the topological structure of neural network.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 summarizes the performance achieved in vectorizing the code using a single processor in terms of Mflops. We achieved a performance of about 93 Mflops for Example 2 which is a large-scale neural network with 5950 links.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Mflops for Example 2 on CRAY Y-MP8/864 supercomputer.</figDesc><table><row><cell></cell><cell>Vectorized</cell><cell cols="2">Nonvectorized</cell></row><row><cell cols="2">Algorithm A Algorithm B</cell><cell cols="2">Algorithm A Algorithm B</cell></row><row><cell>93.17</cell><cell>93.19</cell><cell>11.40</cell><cell>11.33</cell></row><row><cell cols="2">Number of nodes in the input layer: 49.</cell><cell></cell><cell></cell></row><row><cell cols="3">Number of nodes in the hidden layer: 99.</cell><cell></cell></row><row><cell cols="3">Number of nodes in the output layer: 10.</cell><cell></cell></row><row><cell cols="2">Number of links: 5950.</cell><cell></cell><cell></cell></row><row><cell cols="2">Maximum number of iterations: 2000.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><surname>Perceptr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Micr•c•mputers in Civil Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="247" to="256" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vishnubhotla</surname></persName>
		</author>
		<title level="m">ParallelProcessing in Computational Mechanics</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
	<note>Parallel processing and parallel machines</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Manual of Steel Construction, Load and Resistance Factor Design, American Institute of Steel Construction</title>
		<author>
			<persName><surname>Aisc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<pubPlace>Chicago, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Programmer&apos;s Reference Manual</title>
		<author>
			<persName><forename type="first">Cray</forename><surname>Cray</surname></persName>
		</author>
		<author>
			<persName><surname>Standard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>CRAY Research, Inc</publisher>
			<pubPlace>Mendota Heights, MN</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multitasking Programmer&apos;s Manual</title>
		<author>
			<persName><forename type="first">Crayy-Mp</forename><surname>Cray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crayx-Mp</forename><surname>Ea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crayx-Mp</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>CRAY Research, Inc</publisher>
			<pubPlace>Mendota Heights, MN</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A model of perceptron learning with a hidden layer for engineering design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<title level="m">A distributed model of human learning and memory</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="170" to="215" />
		</imprint>
	</monogr>
	<note>Parallel Distributed Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An implementation of network learning on the connection machine</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist Models And Their Implications: Readings from Cognitive Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Waltz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Feldman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="329" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning internal representation by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rurnelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Autotasking, microtasking and macrotasking for optimization of structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Aerospace Engrg. ASCE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
