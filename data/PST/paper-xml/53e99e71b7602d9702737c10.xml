<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generative Model for Music Transcription</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2004-01-19">January 19, 2004</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Ali</forename><surname>Taylan Cemgil</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Bert</forename><surname>Kappen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Barber</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Informatica Instituut</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country>the Netherlands, B</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Kappen is with University of Nijmegen</orgName>
								<orgName type="institution" key="instit2">SNN</orgName>
								<address>
									<postCode>6525 EZ</postCode>
									<settlement>Geert Grooteplein 21, Nijmegen</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">with Edinburgh University</orgName>
								<address>
									<postCode>EH1 2QL</postCode>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Generative Model for Music Transcription</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2004-01-19">January 19, 2004</date>
						</imprint>
					</monogr>
					<idno type="MD5">B9865DD180B07081CCC517A06C97A018</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>music transcription</term>
					<term>polyphonic pitch tracking</term>
					<term>Bayesian signal processing</term>
					<term>switching Kalman filters</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a graphical model for polyphonic music transcription. Our model, formulated as a Dynamical Bayesian Network, embodies a transparent and computationally tractable approach to this acoustic analysis problem. An advantage of our approach is that it places emphasis on explicitly modelling the sound generation procedure. It provides a clear framework in which both high level (cognitive) prior information on music structure can be coupled with low level (acoustic physical) information in a principled manner to perform the analysis. The model is a special case of the, generally intractable, switching Kalman filter model. Where possible, we derive, exact polynomial time inference procedures, and otherwise efficient approximations. We argue that our generative model based approach is computationally feasible for many music applications and is readily extensible to more general auditory scene analysis scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research in this area seeks solutions to a broad range of problems such as the cocktail party problem, (for example automatically separating voices of two or more simultaneously speaking persons, see e.g. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>), identification of environmental sound objects <ref type="bibr" target="#b4">[5]</ref> and musical scene analysis <ref type="bibr" target="#b5">[6]</ref>. Traditionally, the focus of most research activities has been in speech applications. Recently, analysis of musical scenes is drawing increasingly more attention, primarily because of the need for content based retrieval in very large digital audio databases <ref type="bibr" target="#b6">[7]</ref> and increasing interest in interactive music performance systems <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Music Transcription</head><p>One of the hard problems in musical scene analysis is automatic music transcription, that is, the extraction of a human readable and interpretable description from a recording of a music performance.</p><p>Ultimately, we wish to infer automatically a musical notation (such as the traditional western music notation) listing the pitch levels of notes and corresponding time-stamps for a given performance. Such a representation of the surface structure of music would be very useful in a broad spectrum of applications such as interactive music performance systems, music information retrieval (Music-IR) and content description of musical material in large audio databases, as well as in the analysis of performances.</p><p>In its most unconstrained form, i.e., when operating on an arbitrary polyphonic acoustical input possibly containing an unknown number of different instruments, automatic music transcription remains a great challenge. Our aim in this paper is to consider a computational framework to move us closer to a practical solution of this problem.</p><p>Music transcription has attracted significant research effort in the past -see <ref type="bibr" target="#b5">[6]</ref> for a detailed review of early work. In speech processing, the related task of tracking the pitch of a single speaker is a fundamental problem and methods proposed in the literature are well studied <ref type="bibr" target="#b8">[9]</ref>. However, most current pitch detection algorithms are based largely on heuristics (e.g., picking high energy peaks of a spectrogram, correlogram, auditory filter bank, etc.) and their formulation usually lacks an explicit objective function or signal model. It is often difficult to theoretically justify the merits and shortcomings of such algorithms, and compare them objectively to alternatives or extend them to more complex scenarios.</p><p>Pitch tracking is inherently related to the detection and estimation of sinusoidals. The estimation and tracking of single or multiple sinusoidals is a fundamental problem in many branches of applied sciences, so it is less surprising that the topic has also been deeply investigated in statistics, (e.g. see <ref type="bibr" target="#b9">[10]</ref>).</p><p>However, ideas from statistics seem to be not widely applied in the context of musical sound analysis, with only a few exceptions <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> who present frequentist techniques for very detailed analysis of musical sounds with particular focus on decomposition of periodic and transient components. <ref type="bibr" target="#b12">[13]</ref> January <ref type="bibr" target="#b18">19,</ref><ref type="bibr">2004</ref> DRAFT generation of a human readable score includes nontrivial tasks such as tempo tracking, rhythm quantization, meter and key induction <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. As also noted by other authors (e.g. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>), we believe that a model that integrates this higher level symbolic prior knowledge can guide and potentially improve the inferences, both in terms quality of a solution and computation time.</p><p>There are many different natural generative models for piano-rolls. In <ref type="bibr" target="#b24">[25]</ref>, we proposed a realistic hierarchical prior model. In this paper, we consider computationally simpler prior models and focus more on developing efficient inference techniques of a piano-roll representation. The organization of the paper is as follows: We will first present a generative model, inspired by additive synthesis, that describes the signal generation procedure. In the sequel, we will formulate two subproblems related to music transcription: melody identification and chord identification. We will show that both problems can be easily formulated as combinatorial optimization problems in the framework of our model, merely by redefining the prior on piano-rolls. Under our model assumptions, melody identification can be solved exactly in polynomial time (in the number of samples). By deterministic pruning, we obtain a practical approximation that works in linear time. Chord identification suffers from combinatorial explosion. For this case, we propose a greedy search algorithm based on iterative improvement. Consequently, we combine both algorithms for polyphonic music transcription. Finally, we demonstrate how (hyper-)parameters of the signal process can be estimated from real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. POLYPHONIC MODEL</head><p>In a statistical sense, music transcription, (as many other perceptual tasks such as visual object recognition or robot localization) can be viewed as a latent state estimation problem: given the audio signal, we wish to identify the sequence of events (e.g. notes) that gave rise to the observed audio signal.</p><p>This problem can be conveniently described in a Bayesian framework: given the audio samples, we wish to infer a piano-roll that represents the onset times (e.g. times at which a 'string' is 'plucked'), note durations and the pitch classes of individual notes. We assume that we have one microphone, so that at each time t we have a one dimensional observed quantity y t . Multiple microphones (such as required for processing stereo recordings) would be straightforward to include in our model. We denote the temporal sequence of audio samples {y 1 , y 2 , . . . , y t , . . . , y T } by the shorthand notation y 1:T . A constant sampling frequency F s is assumed.</p><p>Our approach considers the quantities we wish to infer as a collection of 'hidden' variables, whilst acoustic recording values y 1:T are 'visible' (observed). For each observed sample y t , we wish to associate a higher, unobserved quantity that labels the sample y t appropriately. Let us denote the unobserved <ref type="bibr">January 19, 2004 DRAFT</ref> quantities by H 1:T where each H t is a vector. Our hidden variables will contain, in addition to a pianoroll, other variables required to complete the sound generation procedure. We will elucidate their meaning later. As a general inference problem, the posterior distribution is given by Bayes' rule</p><formula xml:id="formula_0">p(H 1:T |y 1:T ) ∝ p(y 1:T |H 1:T )p(H 1:T )<label>(1)</label></formula><p>The likelihood term p(y 1:T |H 1:T ) in (1) requires us to specify a generative process that gives rise to the observed audio samples. The prior term p(H 1:T ) reflects our knowledge about piano-rolls and other hidden variables. Our modelling task is therefore to specify both how, knowing the hidden variable states (essentially the piano-roll), the microphone samples will be generated, and also to state a prior on likely piano-rolls. Initially, we concentrate on the sound generation process of a single note.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modelling a single note</head><p>Musical instruments tend to create oscillations with modes that are roughly related by integer ratios, albeit with strong damping effects and transient attack characteristics <ref type="bibr" target="#b25">[26]</ref>. It is common to model such signals as the sum of a periodic component and a transient non-periodic component (See e.g. <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b11">[12]</ref>). The sinusoidal model <ref type="bibr" target="#b28">[29]</ref> is often a good approximation that provides a compact representation for the periodic component. The transient component can be modelled as a correlated Gaussian noise process <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Our signal model is also in the same spirit, but we will define it in state space form, because this provides a natural way to couple the signal model with the piano-roll representation. Here we omit the transient component and focus on the periodic component. It is conceptually straightforward to include the transient component as this does not effect the complexity of our inference algorithms.</p><p>First we consider how to generate a damped sinusoid y t through time, with angular frequency ω.</p><p>Consider a Gaussian process where typical realizations y 1:T are damped "noisy" sinusoidals with angular frequency ω:</p><formula xml:id="formula_1">s t ∼ N (ρ t B(ω)s t-1 , Q)<label>(2)</label></formula><formula xml:id="formula_2">y t ∼ N (Cs t , R)<label>(3)</label></formula><formula xml:id="formula_3">s 0 ∼ N (0, S)<label>(4)</label></formula><p>We use N (µ, Σ) to denote a multivariate Gaussian distribution with mean µ and covariance Σ. Here</p><formula xml:id="formula_4">B(ω) = cos(ω) -sin(ω) sin(ω) cos(ω)</formula><p>is a Givens rotation matrix that rotates two dimensional vector s t by ω degrees counterclockwise. C is a projection matrix defined as C = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref>. The phase and amplitude characteristics of y t are determined by the initial condition s 0 drawn from a prior with covariance S. The Fig. <ref type="figure">1</ref>. A damped oscillator in state space form. Left: At each time step, the state vector s rotates by ω and its length becomes shorter. Right: The actual waveform is a one dimensional projection from the two dimensional state vector. The stochastic model assumes that there are two independent additive noise components that corrupt the state vector s and the sample y, so the resulting waveform y1:T is a damped sinusoid with both phase and amplitude noise.</p><p>damping factor 0 ≤ ρ t ≤ 1 specifies the rate at which s t contracts to 0. See Figure <ref type="figure">1</ref> for an example.</p><p>The transition noise variance Q is used to model deviations from an entirely deterministic linear model.</p><p>The observation noise variance R models background noise.</p><p>In reality, musical instruments (with a definite pitch) have several modes of oscillation that are roughly located at integer multiples of the fundamental frequency ω. We can model such signals by a bank of oscillators giving a block diagonal transition matrix A t = A(ω, ρ t ) defined as</p><formula xml:id="formula_5">        ρ (1) t B(ω) 0 . . . 0 0 ρ (2) t B(2ω) . . . . . . . . . 0 0 . . . 0 ρ (H) t B(Hω)        <label>(5)</label></formula><p>where H denotes the number of harmonics, assumed to be known. To reduce the number of free parameters we define each harmonic damping factor ρ (h) in terms of a basic ρ. A possible choice is to take ρ (h) t = ρ h t , motivated by the fact that damping factors of harmonics in a vibrating string scale approximately geometrically with respect to that of the fundamental frequency, i.e. higher harmonics decay faster <ref type="bibr" target="#b29">[30]</ref>. A(ω, ρ t ) is the transition matrix at time t and encodes the physical properties of the sound generator as a first order Markov Process. The rotation angle ω can be made time dependent for modelling pitch drifts or vibrato. However, in this paper we will restrict ourselves to sound generators that produce sounds with (almost) constant frequency. The state of the sound generator is represented by s t , a 2H dimensional vector that is obtained by concatenation of all the oscillator states in (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. From Piano-Roll to Microphone</head><p>A piano-roll is a collection of indicator variables r j,t , where j = 1 . . . M runs over sound generators (i.e. notes or "keys" of a piano) and t = 1 . . . T runs over time. Each sound generator has a unique fundamental frequency ω j associated with it. For example, we can choose ω j such that we cover all notes of the tempered chromatic scale in a certain frequency range. This choice is arbitrary and for a finer pitch analysis a denser grid with smaller intervals between adjacent notes can be used.</p><p>Each indicator is binary, with values "sound" or "mute". The essential idea is that, if previously muted, r j,t-1 = "mute" an onset for the sound generator j occurs if r j,t = "sound". The generator continues to sound (with a characteristic damping decay) until it is again set to "mute", when the generated signal decays to zero amplitude (much) faster. The piano-roll, being a collection of indicators r 1:M,1:T , can be viewed as a binary sequence, e.g. see Figure <ref type="figure">2</ref>. Each row of the piano-roll r j,1:T controls an underlying sound generator. Fig. <ref type="figure">2</ref>. Piano-roll. The vertical axis corresponds to the sound generator index j and the horizontal axis corresponds to time index t. Black and white pixels correspond to "sound" and "mute" respectively. The piano-roll can be viewed as a binary sequence that controls an underlying signal process. Each row of the piano-roll rj,1:T controls a sound generator. Each generator is a Gaussian process (a Kalman filter model), where typical realizations are damped periodic waveforms of a constant fundamental frequency. As in a piano, the fundamental frequency is a function of the generator index j. The actual observed signal y1:T is a superposition of the outputs of all generators.</p><p>The piano-roll determines the both sound onset generation, and the damping of the note. We consider first the damping effects.</p><p>1) Piano-Roll : Damping: Thanks to our simple geometrically related damping factors for each harmonic, we can characterise the damping factor for each note j = 1, . . . , M by two decay coefficients ρ sound and ρ mute such that 1 ≥ ρ sound &gt; ρ mute &gt; 0.</p><p>The piano-roll r j,1:T controls the damping coefficient ρ j,t of note j at time t by:</p><formula xml:id="formula_6">ρ j,t = ρ sound [r j,t = sound] + ρ mute [r j,t = mute]<label>(6)</label></formula><p>Here, and elsewhere in the article, the notation [x = text] has value equal to 2) Piano-Roll : Onsets: At each new onset, i.e. when (r j,t-1 = mute) → (r j,t = sound), the old state s t-1 is "forgotten" and a new state vector is drawn from a Gaussian prior distribution N (0, S).</p><p>This models the energy injected into a sound generator at an onset (this happens, for example, when a guitar string is plucked). The amount of energy injected is proportional to the determinant of S and the covariance structure of S describes how this total energy is distributed among the harmonics. The covariance matrix S thus captures some of the timbre characteristics of the sound. The transition and observation equations are given by isonset j,t = (r j,t-1 = mute ∧ r j,t = sound) (7)</p><formula xml:id="formula_7">A j,t = [r j,t = mute]A mute j + [r j,t = sound]A sound j (8) s j,t ∼ [¬isonset j,,t ]N (A j,t s t-1 , Q) + [isonset j,t ]N (0, S)<label>(9)</label></formula><formula xml:id="formula_8">y j,t ∼ N (Cs j,t , R)<label>(10)</label></formula><p>In the above, C is a 1 × 2H projection matrix C = [1, 0, 1, 0, . . . , 1, 0] with zero entries on the even components. Hence y j,t has a mean being the sum of the damped harmonic oscillators. R models the variance of the noise in the output of each sound generator. Finally, the observed audio signal is the superposition of the outputs of all sound generators,</p><formula xml:id="formula_9">y t = j y j,t<label>(11)</label></formula><p>The generative model ( <ref type="formula" target="#formula_6">6</ref>)-( <ref type="formula" target="#formula_9">11</ref>) can be described qualitatively by the graphical model in Figure <ref type="figure">3</ref>.</p><p>Equations ( <ref type="formula" target="#formula_8">10</ref>) and ( <ref type="formula" target="#formula_9">11</ref>) define p(y 1:T |s 1:M,1:T ). Equations ( <ref type="formula" target="#formula_6">6</ref>) ( <ref type="formula">8</ref>) and ( <ref type="formula" target="#formula_7">9</ref>) relate r and s and define p(s 1:M,1:T |r 1:M,1:T ). In this paper, the prior model p(r 1:M,1:T ) is Markovian and will be defined in the following sections. </p><p>where the posterior is given by</p><formula xml:id="formula_11">p(r 1:M,1:T |y 1:T ) = 1 p(y 1:T ) s1:M,1:T p(y 1:T |s 1:M,1:T ) ×p(s 1:M,1:T |r 1:M,1:T )p(r 1:M,1:T )</formula><p>The normalization constant, p(y 1:T ), obtained by summing the integral term over all configurations r 1:M,1:T is called the evidence. 1   Unfortunately, calculating this most likely piano-roll configuration is generally intractable, and is related to the difficulty of inference in Switching Kalman Filters <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. We shall need to develop approximation schemes for this general case, to which we shall return in a later section.</p><p>As a prelude, we consider a slightly simpler, related model which aims to track the pitch (melody identification) in a monophonic instrument (playing only a single note at a time), such as a flute. The insight gained here in the inference task will guide us to a practical approximate algorithm in the more general case later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MONOPHONIC MODEL</head><p>Melody identification, or monophonic pitch tracking with onset and offset detection, can be formulated by a small modification of our general framework. Even this simplified task is still of huge practical interest, e.g. in real time MIDI conversion for controlling digital synthesizers using acoustical instruments or pitch tracking from the singing voice in a "karaoke" application. One important problem in real time 1 It is instructive to interpret ( <ref type="formula" target="#formula_10">12</ref>) from a Bayesian model selection perspective <ref type="bibr" target="#b32">[33]</ref>. In this interpretation, we view the set of all piano-rolls, indexed by configurations of discrete indicator variables r1:M,1:T , as the set of all models among which we search for the best model r * 1:M,1:T . In this view, state vectors s1:M,1:T are the model parameters that are integrated over. It is well known that the conditional predictive density p(y|r), obtained through integration over s, automatically penalizes more complex models, when evaluated at y = y1:T . In the context of piano-roll inference, this objective will automatically prefer solutions with less notes. Intuitively, this is simply because at each note onset, the state vector st is reinitialized using a broad Gaussian N (0, S). Consequently, a configuration r with more onsets will give rise to a conditional predictive distribution p(y|r) with a larger covariance. Hence, a piano-roll that claims the existence of additional onsets without support from data will get a lower likelihood. <ref type="bibr">January 19, 2004 DRAFT</ref> pitch tracking is the time/frequency tradeoff: to estimate the frequency accurately, an algorithm needs to collect statistics from a sufficiently long interval. However, this often conflicts with the real time requirements.</p><p>In our formulation, each sound generator is a dynamical system with a sequence of transition models, sound and mute. The state s evolves first according to the sounding regime with transition matrix A sound and then according to the muted regime with A mute . The important difference from a general switching Kalman filter is that when the indicator r switches from mute to sound, the old state vector is "forgotten".</p><p>By exploiting this fact, in the appendix I-A we derive, for a single sound generator (i.e. a single note of a fixed pitch that gets on and off), an exact polynomial time algorithm for calculating the evidence p(y 1:T ) and MAP configuration r * 1:T . 1) Monophonic pitch tracking: Here we assume that at any given time t only a single sound generator can be sounding, i.e. r j,t = sound ⇒ r j ′ ,t = mute for j ′ = j. Hence, for practical purposes, the factorial structure of our original model is redundant; i.e. we can "share" a single state vector s among all sound generators<ref type="foot" target="#foot_1">2</ref> . The resulting model will have the same graphical structure as a single sound generator but with an indicator j t ∈ 1 . . . M which indexes the active sound generator, and r t ∈ {sound, mute} indicates sound or mute. Inference for this case turns out to be also tractable (i.e. polynomial). We allow switching to a new j ′ only after an onset. The full generative model using the pairs (j t , r t ), which includes both likelihood and prior terms is given as</p><formula xml:id="formula_12">r t ∼ p(r t |r t-1 ) isonset t = (r t = sound ∧ r t-1 = mute) j t ∼ [¬isonset t ]δ(j t ; j t-1 ) + [isonset t ]u(j t ) A t = [r t = mute]A mute jt + [r t = sound]A sound jt s t ∼ [¬isonset t ]N (A t s t-1 , Q) + [isonset t ]N (0, S) y t ∼ N (Cs t , R)</formula><p>Here u(j) denotes a uniform distribution on 1, . . . , M and δ(j t ; j t-1 ) denotes a degenerate (deterministic) distribution concentrated on j t , i.e. unless there is an onset the active sound generator stays the same.</p><p>Our choice of a uniform u(j) simply reflects the fact that any new note is as likely as any other. Clearly, more informative priors, e.g. that reflect knowledge about tonality, can also be proposed. The graphical model is shown in Figure <ref type="figure">4</ref>. The derivation of the polynomial time inference algorithm is given in appendix I-C. Technically, it is a simple extension of the single note algorithm derived in appendix I-A.</p><p>In Figure <ref type="figure">5</ref>, we illustrate the results on synthetic data sampled from the model where we show the filtering density p(r t , j t |y 1:t ). After an onset, the posterior becomes quickly crisp, long before we observe a complete cycle. This feature is especially attractive for real time applications where a reliable pitch estimate has to be obtained as early as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Extension to vibrato and legato:</head><p>The monophonic model has been constructed such that the rotation angle ω remains constant. Although the the transition noise with variance Q still allows for small and independent deviations in frequencies of the harmonics, the model is not realistic for situations with systematic pitch drift or fluctuation, e.g. as is the case with vibrato. Moreover, on many musical instruments, it is possible to play legato, that is without an explicit onset between note boundaries. In our framework, pitch drift and legato can be modelled as a sequence of transition models. Consider the generative process for the note index j:</p><formula xml:id="formula_13">r t ∼ p(r t |r t-1 ) isonset t = (r t = sound ∧ r t-1 = mute) issound t = (r t = sound ∧ r t-1 = sound) j t ∼ [issound t ]d(j t |j t-1 ) + [r t = mute]δ(j t ; j t-1 ) + [isonset t ]u(j t )</formula><p>Here, d(j t |j t-1 ) is a multinomial distribution reflecting our prior belief how likely is it to switch between notes. When r t = mute, there is no regime change, reflected by the deterministic distribution δ(j t ; j t-1 ) peaked around j t-1 . Remember that neighbouring notes have also close fundamental frequency ω. To simulate pitch drift, we can choose a fine grid such that ω j /ω j+1 = Q. Here, Q &lt; 1 is the quality factor, a measure of the desired frequency precision not to be confused with the transition noise Q. In this case, we can simply define d(j t |j t-1 ) as a multinomial distribution with support on</p><formula xml:id="formula_14">[j t-1 -1, j t-1 , j t-1 + 1] with cell probabilities [d -1 d 0 d 1 ].</formula><p>We can take a larger support for d(j t |j t-1 ), but in practice we would rather reduce the frequency precision Q to avoid additional computational cost.</p><p>Unfortunately, the terms included by the drift mechanism render an exact inference procedure intractable. We derive the details of the resulting algorithm in the appendix I-D. A simple deterministic pruning method is described in appendix II-A. In Figure <ref type="figure" target="#fig_3">6</ref>, we show the estimated MAP trajectory r * </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. POLYPHONIC INFERENCE</head><p>In this section we return to the central goal of inference in the general polyphonic model described in section II. To infer the most likely piano-roll we need to compute argmax r1:M,1:T p(r 1:M,1:T |y 1:T ) defined in <ref type="bibr" target="#b11">(12)</ref>. Unfortunately, the calculation of ( <ref type="formula" target="#formula_10">12</ref>) is intractable. Indeed, even the calculation of the Gaussian integral conditioned on a particular configuration r 1:M,1:T using standard Kalman filtering equations is prohibitive since the dimension of the state vector is |s| = 2H ×M , where H is the number of harmonics.</p><p>For a realistic application we may have M ≈ 50 and H ≈ 10. It is clear that unless we are able to develop efficient approximation techniques, the model will be only of theoretical interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Vertical Problem: Chord identification</head><p>Chord identification is the simplest polyphonic transcription task. Here we assume that a given audio signal y 1:T is generated by a piano-roll where r j,t = r j for all <ref type="foot" target="#foot_2">3</ref>  The size of the search space in this case 2 M , which is prohibitive for direct computation.</p><p>A simple approximation is based on greedy search: we start iterative improvement from an initial configuration r (0)</p><p>1:M (silence, or randomly drawn from the prior). At each iteration i, we evaluate the probability p(y 1:T , r 1:M ) of all neighbouring configurations of r (i-1) 1:M . We denote this set by neigh(r</p><formula xml:id="formula_15">(i-1) 1:M ).</formula><p>A configuration r ′ ∈ neigh(r), if r ′ can be reached from r within a single flip (i.e., we add or remove single notes). If r (i-1) 1:M has a higher probability than all its neighbours, the algorithm terminates, having found a local maximum. Otherwise, we pick the neighbour with the highest probability and set</p><formula xml:id="formula_16">r (i) 1:M = argmax r1:M ∈neigh(r (i-1) 1:M ) p(y 1:T , r 1:M )</formula><p>and iterate until convergence. We illustrate the algorithm on a signal sampled from the generative model, see Figure <ref type="figure">7</ref>. This procedure is guaranteed to converge to a (possibly local) maxima. Nevertheless, we observe that for many examples this procedure is able to identify the correct chord. Using multiple restarts from different initial configurations will improve the quality of the solution at the expense of computational cost. </p><formula xml:id="formula_17">1 • • • • • • • • • • • • • • • • • • • • • • • • -1220638254 2 • • • • • • • • • • • • • • • • • • • • • • • • -665073975 3 • • • • • • • • • • • • • • • • • • • • • • • • -311983860 4 • • • • • • • • • • • • • • • • • • • • • • • • -162334351 5 • • • • • • • • • • • • • • • • • • • • • • • • -43419569 6 • • • • • • • • • • • • • • • • • • • • • • • • -1633593 7 • • • • • • • • • • • • • • • • • • • • • • • • -14336 8 • • • • • • • • • • • • • • • • • • • • • • • • -5766 9 • • • • • • • • • • • • • • • • • • • • • • • • -5210 10 • • • • • • • • • • • • • • • • • • • • • • • • -4664 True • • • • • • • • • • • • • • • • • • • • • • • • -4664</formula><p>Fig. <ref type="figure">7</ref>. We have first drawn a random piano-roll configuration (a random chord) r1:M . Given r1:M , we generate a signal of length 400 samples with a sampling frequency Fs = 4000 from p(y1:T |r1:M ). We assume 24 notes (2 octaves). The synthesized signal from the generative model and its discrete time Fourier transform modulus are shown above. The true chord configuration and the associated log probability is at the bottom of the table. For the iterative algorithm, the initial configuration in this example was silence. At this point we compute the probability for each single note configurations (all one flip neighbours of silence). The first note that is added is actually not present in the chord. Until iteration 9, all iterations add extra notes. Iteration One of the advantages of our generative model based approach is that we can in principle infer a chord given any subset of data. For example, we can simply downsample y 1:T (without any preprocessing) by an integer factor of D and view the discarded samples simply as missing values. Of course, when D is large, i.e. when we throw away many samples, due to aliasing, higher harmonics will overlap with harmonics in the lower frequency band which will cause a more diffuse posterior on the piano-roll, eventually degrading performance.</p><p>In Figure <ref type="figure" target="#fig_9">8</ref>, we show the results of such an experiment. We have downsampled y 1:T with factor D = 2, 3 and 4. The energy spectrum is quite coarse due to the short length of the data. Consequently many harmonics are not resolved, e.g. we can not identify the underlying line spectrum by visual inspection.</p><p>Methods based on template matching or identification of peaks may have serious problems for such examples. On the other hand, our model driven approach is able to identify the true chord. We note that, the presented results are illustrative only and the actual behaviour of the algorithm (sensitivity to D,  importance of starting configuration) will depend on the details of the signal model.</p><formula xml:id="formula_18">) Init 2 • • • • • • • • • • • • • • • • • • • • • • • • -2685 True • • • • • • • • • • • • • • • • • • • • • • • • -3179 Silence • • • • • • • • • • • • • • • • • • • • • • • • -2685 Random 3 • • • • • • • • • • • • • • • • • • • • • • • • -2057 True • • • • • • • • • • • • • • • • • • • • • • • • -2057 Silence • • • • • • • • • • • • • • • • • • • • • • • • -2616 Random 4 • • • • • • • • • • • • • • • • • • • • • • • • -1605 True • • • • • • • • • • • • • • • • • • • • • • • • -1668 Silence • • • • • • • • • • • • • • • • • • • • • • • • -1591 Random</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Piano-Roll inference Problem: Joint Chord and Melody identification</head><p>The piano-roll estimation problem can be viewed as an extension of chord identification in that we also detect onsets and offsets for each note within the analysis frame. A practical approach is to analyze the signal in sufficiently short time windows and assume that for each note, at most one changepoint can occur within the window.</p><p>Consider data in a short window, say y 1:W . We start iterative improvement from a configuration r (0)</p><p>1:M,1:W , where each time slice r (0) 1:M,t for t = 1 . . . W is equal to a "chord" r 1:M,0 . The chord r 1:M,0 can be silence or, during a frame by frame analysis, the last time slice of the best configuration found in the previous analysis window. Let the configuration at i -1'th iteration be denoted as r (i-1) 1:M,1:W . At each new iteration i, we evaluate the posterior probability p(y 1,W , r 1:M,1:W ), where r 1:M,1:W runs over all neighbouring configuration of r (i-1) 1:M,1:W . Each member r 1:M,1:W of the neighbourhood is generated as follows: For each j = 1 . . . M , we clamp all the other rows, i.e. we set r j ′ ,1:W = r (i-1) j ′ ,1:W for j ′ = j. For each time step t = 1 . . . W , we generate a new configuration such that the switches up to time t are equal to the initial switch r j,0 , and its opposite ¬r j,0 after t, i.e. r j,t ′ r j,0 [t ′ &lt; t] + ¬r j,0 [t ′ ≥ t]. This is equivalent to saying that a sounding note may get muted, or a muted note may start to sound. The computational advantage of allowing only one changepoint at each row is that the probability of all neighbouring configurations for a fixed j can be computed by a single backward, forward pass <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Finally, we pick the neighbour with the maximum probability. The algorithm is illustrated in Figure <ref type="figure" target="#fig_10">9</ref>.</p><p>The analysis for the whole sequence proceeds as follows: Consider two successive analysis windows Y prev ≡ y 1:W and Y ≡ y W +1:2W . Suppose we have obtained a solution R * prev ≡ r * 1:M,1:W obtained by iterative improvement. Conditioned on R * prev , we compute the posterior p(s 1:M,W |Y prev , R * prev ) by Kalman filtering. This density is the prior of s for the current analysis window Y . The search starts from a chord equal to the last time slice of R * prev . In Fig. <ref type="figure" target="#fig_7">10</ref> we show an illustrative result obtained by this algorithm on synthetic data. In similar experiments with synthetic data, we are often able to identify the correct piano-roll. This simple greedy search procedure is somewhat sensitive to location of onsets within the analysis window. Especially, when an onset occurs near the end of an analysis window, it may be associated with an incorrect pitch. The correct pitch is often identified in the next analysis window, when a longer portion of the signal is observed. However, since the basic algorithm does not allow for correcting the previous estimate by retrospection, this introduces some artifacts. A possible method to overcome this problem is to use a fixed lag smoothing approach, where we simply carry out the analysis on overlapping windows.</p><p>For example, for an analysis window Y prev ≡ y 1:W , we find r * 1:M,1:W . The next analysis window is taken as y L+1:W +L where L ≤ W . We find the prior p(s 1:M,L |y 1:L , r * 1:M,1:L ) by Kalman filtering. On the other hand, obviously, the algorithm becomes slower by a factor of L/W . An optimal choice for L and W will depend upon many factors such as signal characteristics, sampling frequency, downsampling factor D, onset/offset positions, number of active sound generators at a given time as well as the amount of CPU time available. In practice, these values may be critical and they need to be determined by trial and error. On the other hand, it is important to note that L and W just determine how the approximation is made but not enter the underlying model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LEARNING</head><p>In the previous sections, we assumed that the correct signal model parameters θ = (S, ρ, Q, R) were known. These include in particular the damping coefficients ρ sound , ρ mute , transition noise variance Q, observation noise R and the initial prior covariance matrix S after an onset. In practice, for an instrument class (e.g. plucked string instruments) a reasonable range for θ can be specified a-priori. We may safely assume that θ will be static (not time dependent) during a given performance. However, exact values for these quantities will vary among different instruments (e.g. old and new strings) and recording/performance conditions.</p><p>One of the well-known advantages of Bayesian inference is that, when uncertainty about parameters is incorporated in a model, this leads in a natural way to the formulation of a learning algorithm. The <ref type="bibr" target="#b19">January 19, 2004</ref> DRAFT piano-roll estimation problem, omitting the time indices, can be stated as follows: </p><formula xml:id="formula_19">r * = argmax</formula><p>Unfortunately, the integration on θ can not be calculated analytically and approximation methods must be used <ref type="bibr" target="#b33">[34]</ref>. A crude but computationally cheap approximation replaces the integration on θ with maximization:</p><formula xml:id="formula_21">r * = argmax r max θ s p(y|s, θ)p(s|r, θ)p(θ)p(r)</formula><p>This leads to the following greedy coordinate ascent algorithm where the steps are iterated until convergence</p><formula xml:id="formula_22">r (i) = argmax r s</formula><p>p(y|s, θ (i-1) )p(s|r, θ (i-1) )p(θ (i-1) )p(r)</p><formula xml:id="formula_23">θ (i) = argmax θ s p(y|s, θ)p(s|r (i) , θ)p(θ)p(r (i) )</formula><p>For a single note, conditioned on θ (i-1) , r (i) can be calculated exactly, using the message propagation algorithm derived in appendix I-B. Conditioned on r (i) , calculation of θ (i) becomes equivalent to parameter in successive windows (bottom). In this example, only the offset time of the lowest note is not estimated correctly. This is a consequence that, for long notes, the state vector s converges to zero before the generator switches to the mute state.</p><p>estimation in a linear dynamical systems, which can be achieved by an expectation maximization (EM) algorithm <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>. In practice, we observe that for realistic starting conditions θ (0) , the r (i) are identical, suggesting that r * is not very sensitive to variations in θ near to a local optimum.</p><p>In Figure <ref type="figure" target="#fig_14">11</ref>, we show the results of training the signal model based on a single note (a C from the low register) of an electric bass. We use this model to transcribe a polyphonic segment performed on the same instrument, see Figure <ref type="figure" target="#fig_15">12</ref>. Ideally, one could train different parameter sets each different note or each different register of an instrument. In practice, we observe that the transcription procedure is not very sensitive to actual parameter settings; a rough parameter estimate, obtained by a few EM iterations, leads often to the correct result. For example, the results in Figure <ref type="figure" target="#fig_15">12</ref> are obtained using a model that is trained by only three EM iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>We have presented a model driven approach where transcription is viewed as a Bayesian inference problem. In this respect, at least, our approach parallels the previous work of <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b35">[36]</ref>. We believe, however, that our formulation, based on a switching state space model, has several advantages. We can remove the assumption of a frame based model and this enables us to analyse music online and to sample precision. Practical approximations to an eventually intractable exact posterior can be carried out frameby-frame, such as by using a fixed time-lag smoother. This, however, is merely a computational issue</p><p>(albeit an important one). We may also discard samples to reduce computational burden, and account for this correctly in our model.  Given some crude first estimate for model parameters θ (0) (S, ρ, Q, R), we estimate r (1) , shown in (a). Conditioned on r (1) , we estimate the model parameters θ (1) and so on. Let S h denote the 2×2 block matrix from the diagonal S, corresponding to the h'th harmonic, similarly for Q h . In (b), we show the estimated parameters for each harmonic sum of diagonal elements, i.e. Tr S h and Tr Q h . The damping coefficient is found as ρsound = (det A h A T h ) 1/4 where A h is a 2 × 2 diagonal block matrix of transition matrix A sound . For reference, we also show the Fourier transform modulus of the downsampled signal. We can see, that on the low frequency bands, S mimics the average energy distribution of the note. However, transient phenomena, such as the strongly damped 7'th harmonic with relatively high transition noise, is hardly visible in the frequency spectrum. On the other hand for online pitch detection, such high frequency components are important to generate a crisp estimate as early as possible.</p><p>An additional advantage of our formulation is that we can still deliver a pitch estimate even when the fundamental and lower harmonics of the frequency band are missing. This is related to so called virtual pitch perception <ref type="bibr" target="#b36">[37]</ref>: we tend to associate notes with a pitch class depending on the relationship between harmonics rather than the frequency of the fundamental component itself.</p><p>There is a strong link between model selection and polyphonic music transcription. In chord identification we need to compare models with different number of notes, and in melody identification we need to deduce the number of onsets. Model selection becomes conceptually harder when one needs to compare models of different size. We partially circumvent this difficulty by using switch variables, which implicitly represent the number of components. Following the established signal processing jargon, we may call our approach a time-domain method, since we are not explicitly calculating a discrete-time Fourier transform. On the other hand, the signal model presented here has close links to the Fourier analysis and sinusoidal modelling. Our analysis can be interpreted as a search procedure for a sparse representa tion on a set of basis vectors. In contrast to Fourier analysis, where the basis vectors are simple sinusoids, we represent the observed signal implicitly using signals drawn from a stochastic process which typically generates decaying periodic oscillations (e.g. notes) with occasional changepoints. The sparsity of this representation is a consequence of the onset mechanism, that effectively puts a mixture prior over the hidden state vector s. This prior is peaked around zero and has broad tails, indicating that most of the sources are muted and only a few are sounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Future work</head><p>Although our approach has many desirable features (automatically deducing number of correct notes, high temporal resolution e.t.c.), one of the main disadvantage of our method is computational cost associated with updating large covariance matrices in Kalman filtering. It would be very desirable to investigate approximation schemas that employ fast transformations such as the FFT to accelerate computations.</p><p>When transcribing music, human experts rely heavily on prior knowledge about the musical structureharmony, tempo or expression. Such structure can be captured by training probabilistic generative models on a corpus of compositions and performances by collecting statistics over selected features (e.g. <ref type="bibr" target="#b37">[38]</ref>).</p><p>One of the important advantages of our approach is that such prior knowledge about the musical structure can be formulated as an informative prior on a piano-roll; thus can be integrated in signal analysis in a consistent manner. We believe that investigation of this direction is important in designing robust and practical music transcription systems.</p><p>Our signal model considered here is inspired by additive synthesis. An advantage of our linear formulation is that we can use the Kalman filter recursions to integrate out the continuous latent state</p><p>analytically. An alternative would be to formulate a nonlinear dynamical system that implements a nonlinear synthesis model (e.g. FM synthesis, waveshaping synthesis, or even a physical model <ref type="bibr" target="#b38">[39]</ref>). Such an approach would reduce the dimensionality of the latent state space but force us to use approximate integration methods such as particle filters or EKF/UKF <ref type="bibr" target="#b39">[40]</ref>. It remains an interesting open question whether, in practice, one should trade-off analytical tractability versus reduced latent state dimension.</p><p>In this paper, for polyphonic transcription, we have used a relatively simple deterministic inference method based on iterative improvement. The basic greedy algorithm, whilst still potentially useful in practice, may occasionally get stuck in poor solutions. We believe that, using our model as a framework, better polyphonic transcriptions can be achieved using more elaborate inference or search methods (deterministic, stochastic or hybrids).</p><p>We have not yet tested our model for more general scenarios, such as music fragments containing percussive instruments or bell sounds with inharmonic spectra. Our simple periodic signal model would be clearly inadequate for such a scenario. On the other hand, we stress the fact that the framework presented here is not only limited to the analysis of signals with harmonic spectra, and in principle applicable to any family of signals that can be represented by a switching state space model. This is already a large class since many real-world acoustic processes can be approximated well with piecewise linear regimes. We can also formulate a joint estimation schema for unknown parameters as in <ref type="bibr" target="#b12">(13)</ref> and integrate them out (e.g. see <ref type="bibr" target="#b18">[19]</ref>). However, this is currently a hard and computationally expensive task.</p><p>If efficient and accurate approximate integration methods can be developed, our model will be applicable to mixtures of many different types of acoustical signals and may be useful in more general auditory scene analysis problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I</head><p>Gaussians for each configuration of r t-1:t . We will highlight this data structure by using the following notation</p><formula xml:id="formula_24">α t ≡    α 1,1 t α 1,2 t α 2,1 t α 2,2 t   </formula><p>where each α i,j t = p(y 1:t , s t , r t = i, r t-1 = j) for i, j = 1 . . . 2 are also Gaussian mixture potentials. We will denote the conditional normalization constants as</p><formula xml:id="formula_25">Z i t ≡ p(y 1:t , r t = i) = rt-1 st α i,rt-1 t</formula><p>Consequently the evidence is given by</p><formula xml:id="formula_26">Z t ≡ p(y 1:t ) = rt rt-1 st α t = i Z i t</formula><p>We also define the predictive density</p><formula xml:id="formula_27">α t|t-1 ≡ p(y 1:t-1 , s t , r t , r t-1 ) = rt-2 st-1 p(s t |s t-1 , r t , r t-1 )p(r t |r t-1 )α t-1</formula><p>In general, for switching Kalman filters, calculating exact posterior features, such as the evidence Z t = p(y 1:t ), is not tractable. This is a consequence of the fact that the number of mixture components to required to represent the exact filtering density α t grows exponentially with time step k (i.e. one Gaussian for each of the exponentially many configurations r 1:t ). Luckily, for the model we are considering here, the growth is polynomial in k only. See also <ref type="bibr" target="#b41">[42]</ref>.</p><p>To see this, suppose we have the filtering density available at time t -1 as α t-1 . The transition models can be organized also in a table where i'th row and j'th column correspond to p(s</p><formula xml:id="formula_28">t |s t-1 , r t = i, r t-1 = j) p(s t |s t-1 , r t , r t-1 ) =    f 1 (s t |s t-1 ) π(s t ) f 2 (s t |s t-1 ) f 2 (s t |s t-1 )    Calculation of the predictive potential is straightforward. First, summation over r k-2 yields rk-2 α t-1 =    α 1,1 t-1 + α 1,2 t-1 α 2,1 t-1 + α 2,2 t-1    ≡    ξ 1 t-1 ξ 2 t-1   </formula><p>Integration over s t-1 and multiplication by p(r t |r t-1 ) yields the predictive potential</p><formula xml:id="formula_29">α t|t-1 =    p 1,1 ψ 1 1 (s t ) p 1,2 Z 2 t-1 π(s t ) p 2,1 ψ 1 2 (s t ) p 2,2 ψ 2 2 (s t )   </formula><p>where we define</p><formula xml:id="formula_30">Z 2 t-1 ≡ st-1 ξ 2 t-1 ψ j i (s t ) ≡ st-1 f i (s t |s t-1 )ξ j t-1</formula><p>January 19, 2004 DRAFT</p><p>The potentials ψ j i can be computed by applying the standard Kalman prediction equations to each component of ξ j t-1 . The updated potential is given by α t = p(y t |s t )α t|t-1 . This quantity can be computed by applying standard Kalman update equations to each component of α t|t-1 .</p><p>From the above derivation, it is clear that α 1,2 t has only a single Gaussian component. This has the consequence that the number of Gaussian components in α  For finding the MAP state, we replace summations over r t by maximization. One potential technical difficulty is that, unlike in the case for evidence calculation, maximization and integration do not commute.</p><p>Consider a conditional Gaussian potential φ(s, r) ≡ {φ(s, r = 1), φ(s, r = 2)} where φ(s, r) are Gaussian potentials for each configuration of r. We can compute the MAP configuration</p><formula xml:id="formula_31">r * = argmax r s φ(s, r) = argmax Z 1 , Z<label>2</label></formula><p>where Z j = s φ(s, r = j). We evaluate the normalization of each component (i.e. integrate over the continuous hidden variable s first) and finally find the maximum of all normalization constants.</p><p>However, direct calculation of r * 1:T is not feasible because of exponential explosion in the number of distinct configurations. Fortunately, for our model, we can introduce a deterministic pruning schema that reduces the number of kernels to a polynomial order and meanwhile guarantees that we will never eliminate the MAP configuration. This exact pruning method hinges on the factorization of the posterior for the assignment of variables r t = 1, r t-1 = 2 (mute to sound transition) that breaks the direct link between s t and s t-1 : φ(s 1:T , r 1:t-2 , r t-1 = 2, r t = 1, r t+1:T ) = φ(s 0:t-1 , r 1:t-2 , r t-1 = 2)φ(s t:T , r t+1:T , r t = 1|r t-1 = 2) (14) <ref type="bibr">January 19, 2004 DRAFT</ref> In this case: max r1:T s0:T φ(s 0:T , r 1:t-2 , r t-1 = 2, r t = 1, r t+1:T ) = max r1:t-1 s0:t-1 φ(s 0:t-1 , r 1:t-2 , r t-1 = 2)</p><p>× max rt:T st:T φ(s t:T , r t+1:T , r t = 1|r t-1 = 2) = Z 2 t × max rt+1:T st:T φ(s t:T , r t+1:T , r t = 1|r t-1 = 2)</p><p>This Equation shows that whenever we have an onset, we can calculate the maximum over the past and future configurations separately. Put differently, provided that the MAP configuration has the form</p><formula xml:id="formula_33">r * 1:T = [r * 1:t-3 , r t-1 = 2, r t = 1, r * t+1:T ], the prefix [r * 1:t-3 , r t-1 = 2]</formula><p>will be the solution for the reduced maximization problem arg max r1:t-1 s0:t-1 φ(s 0:t-1 , r 1:t-1 ).</p><p>1) Forward pass: Suppose we have a collection of Gaussian potentials</p><formula xml:id="formula_34">δ t-1 ≡    δ 1,1 t-1 δ 1,2 t-1 δ 2,1 t-1 δ 2,2 t-1    ≡    δ 1 t-1 δ 2 t-1   </formula><p>with the property that the Gaussian kernel corresponding the prefix r * 1:t-1 of the MAP state is a member of δ t-1 , i.e. φ(s k-1 , r * 1:t-1 ) ∈ δ t-1 s.t. r * 1:T = [r * 1:t-1 , r * t:T ]. We also define the subsets δ i,j t-1 = {φ(s k-1 , r 1:t-1 ) : φ ∈ δ t-1 and r t-1 = i, r t-2 = j} δ i t-1 = j δ i,j t-1</p><p>We show how we find δ t . The prediction is given by</p><formula xml:id="formula_35">δ t|t-1 = st-1</formula><p>p(s t |s t-1 , r t , r t-1 )p(r t |r t-1 )δ t-1</p><p>The multiplication by p(r t |r t-1 ) and integration over s t-1 yields the predictive potential δ t|t-1    p 1,1 st-1 f 1 (s t |s t-1 )δ without changing the optimum solution:</p><formula xml:id="formula_36">δ 1,2 t|t-1 = p 1,2 Z 2 t-1 π(s t )</formula><p>The updated potential is given by δ t = p(y t |s t )δ t|t-1 . The analysis of the number of kernels proceeds as in the previous section.</p><p>2) Decoding: During the forward pass, we tag each Gaussian component of δ t with its past history of r 1:t . The MAP state can be found by a simple search in the collection of polynomially many numbers and reporting the associated tag: We finally conclude that the forward filtering and MAP (Viterbi path) estimation algorithms are essentially identical with summation replaced by maximization and an additional tagging required for decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inference for monophonic pitch tracking</head><p>In this section we derive an exact message propagation algorithm for monophonic pitch tracking.</p><p>Perhaps surprisingly, inference in this case turns out to be still tractable. Even though the size of the configuration space r 1:M,1:T is of size (M + 1) K O(2 K log M ), the space complexity of an exact algorithm remains quadratic in t. First, we define a "mega" indicator node z t = (j t , r t ) where j </p><formula xml:id="formula_37">             <label>(16)</label></formula><p>where the transitions p(z t = (j, r)|z t-1 = (j ′ , r ′ )) are organized at the n'th row and m'th column where n = r × M + j -1 and m = r ′ × M + j ′ -1. <ref type="bibr" target="#b15">(16)</ref>. The transition models p(s t |s t-1 , z t = (j, r), z t-1 = (j ′ , r ′ )) can be organized similarly: Integration over s t-1 and multiplication by p(z t |z t-1 ) yields the predictive potential α t|t-1 . The components are given as α (r,j)(r ′ ,j ′ ) t|t-1</p><formula xml:id="formula_38">              f 1,</formula><formula xml:id="formula_39">=    (1/M )p r,r ′ π(s t )Z (r ′ ,j ′ ) t-1 r = 1 ∧ r ′ = 2 [j = j ′ ] × p r,r ′ ψ (r,j)(r ′ ,j ′ ) t otherwise (<label>17</label></formula><formula xml:id="formula_40">)</formula><p>where we define</p><formula xml:id="formula_41">Z (r ′ ,j ′ ) t-1 ≡ st-1 ξ (r ′ ,j ′ ) t-1 ψ (r,j)(r ′ ,j ′ ) t ≡ st-1</formula><p>f r,j (s t |s t-1 )ξ (r ′ ,j ′ ) t-1</p><p>The potentials ψ can be computed by applying the standard Kalman prediction equations to each component of ξ. Note that the forward messages have the same sparsity structure as the prior, i.e. α (r,j)(r ′ ,j ′ ) t-1 = 0 when p(r t = r, j t = j|r t-1 = r ′ , j t = j ′ ) is nonzero. The updated potential is given by α t = p(y t |s t )α t|t-1 . This quantity can be computed by applying standard Kalman update equations to each nonzero component of α t|t-1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Monophonic pitch tracking with varying fundamental frequency</head><p>We model pitch drift by a sequence of transition models. We choose a grid such that ω j /ω j+1 = Q, where Q is close to one. Unfortunately, the subdiagonal terms introduced to the prior transition matrix p(z t = (1, j t )|z t-1 = (1, j t-1 ))</p><formula xml:id="formula_42">p 1,1 ×            (d 0 + d 1 ) d -1 d 1 d 0 d -1 d 1 . . . . . . . . . d 0 d -1 d 1 (d 0 + d -1 )           <label>(18)</label></formula><p>render an exact algorithm exponential in t. The recursive update equations, starting with α t-1 , are obtained by summing over z t-2 , integration over s t-1 and multiplication by p(z t |z t-1 ). The only difference is that</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Given the polyphonic model described in section II, to infer the most likely piano-roll we need to compute r * 1:M,1:T = argmax r1:M,1:T p(r 1:M,1:T |y 1:T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Simplified Model for monophonic transcription. Since there is only a single sound generator active at any given time, we can represent a piano-roll at each time slice by the tuple (jt, rt) where jt is the index of the active sound generator and rt ∈ {sound, mute} indicates the state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Tracking varying pitch. Top and middle panel show the true piano-roll and the sampled signal. The estimated piano-roll is shown below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>for drifting pitch. We use a model where the quality factor is Q = 2 -120 , (120 generators per octave) with drift probability d -1 = d 1 = 0.1. A fine pitch contour, that is accurate to sample precision, can be estimated.<ref type="bibr" target="#b19">January 19, 2004</ref> DRAFT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>j = 1 . . . M . The task is to find the MAP configuration r * 1:M = argmax r1:M p(y 1:T , r 1:M ) Each configuration corresponds to a chord. The two extreme cases are "silence" and "cacophony" that correspond to configurations r 1:M [mute mute . . . mute] and [sound sound . . . sound] respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>log p(y1:T , r1:M )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>9 and 10</head><label>10</label><figDesc>turn out to be removing the extra notes and iterations converge to the true chord. The intermediate configurations visited by the algorithm are shown in the table below. Here, sound and mute states are represented by •'s and •'s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Iterative improvement results when data are subsampled by a factor of D = 2, 3 and 4, respectively. For each factor D, the top line shows the true configuration and the corresponding probability. The second line is the solution found by starting from silence and the third line is starting from a random configuration drawn form the prior (best of 3 independent runs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Iterative improvement with changepoint detection. The true piano-roll, the signal and its Fourier transform magnitude are shown in Figure 9.(a). In Figure 9.(b), configurations r (i) visited during iterative improvement steps. Iteration numbers i are shown left and the corresponding probability is shown on the right. The initial configuration (i.e. "chord") r1:M,0 is set to silence. At the first step, the algorithm searches all single note configurations with a single onset. The winning configuration is shown on top panel of Figure 9.(b).At the next iteration, we clamp the configuration for this note and search in a subset of two note configurations. This procedure adds and removes notes from the piano-roll and converges to a local maxima. Typically, the convergence is quite fast and the procedure is able to identify the true chord without making a "detour" as in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>, θ)p(s|r, θ)p(θ)p(r)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. A typical example for Polyphonic piano-roll inference from synthetic data. We generate a realistic piano-roll (top) and render a signal using the polyphonic model (middle). Given only the signal, we estimate the piano-roll by iterative improvement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>A single note from an electric bass. Original sampling rate of 22050 Hz is reduced by downsampling with factor D = 20. Vertical lines show the changepoints of the MAP trajectory r1:K. Top to Bottom: Fourier transform of the downsampled signal and diagonal entries of S, Q and damping coefficients ρsound for each harmonic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Training the signal model with EM from a single note from an electric bass using a sampling rate of 22050 Hz. The original signal is downsampled by a factor of D = 20. Given some crude first estimate for model parameters θ (0) (S, ρ, Q, R), we</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Polyphonic transcription of a short segment from a recording of a bass guitar. (Top) The signal, original sampling rate of 22050 Hz is downsampled with a factor of D = 5. (Middle) Spectrogram (Short time Fourier transform modulus) of the downsampled signal. Horizontal and vertical axes correspond to time and frequency, respectively. Grey level denotes the energy in a logarithmic scale. The low frequency notes are not well resolved due to short window length. Taking a longer analysis window would increase the frequency resolution but smear out onsets and offsets. (Bottom) Estimated piano-roll. The model used M = 30 sound generators where fundamental frequencies were placed on a chromatic scale that spanned the 2.5 octave interval between the low A (second open string on a bass) and a high D (highest note on the forth string). Model parametersare estimated by a few EM iterations on a single note (similar to Figure11) recorded from the same instrument. The analysis is carried out using a window length of W = 450 samples, without overlap between analysis frames (i.e. L = W ). The greedy procedure was able to identify the correct pitch classes and their onsets to sample precision. For this example, the results were qualitatively similar for different window lengths W around 300 -500 and downsampling factors D up to 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>1:T , s 0:T , r 1:T ) 0:T , r 1:T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>propagated through f 1 ). The second row sum term ξ 2 t is more costly; it increases at every time slice by the number of components in ξ 1 t-1 . Since the size of ξ 1 t-1 grows linearly, the size of ξ 2</figDesc><table><row><cell>1,1 t</cell><cell>increases only linearly (the first row-sum</cell></row><row><cell cols="2">terms ξ 1 t-1 t</cell></row><row><cell>grows quadratically with time t.</cell><cell></cell></row><row><cell>B. Computation of MAP configuration r  *  1:T</cell><cell></cell></row><row><cell>The MAP state is defined as</cell><cell></cell></row><row><cell>r  *  1:T = argmax</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2,1 st-1 f 2 (s t |s t-1 )δ 1 t-1 p 2,2 st-1 f 2 (s t |s t-1 )δ 2By the (15), we can replace the collection of numbers st-1 δ 2 t-1 with with the scalar Z 2 t-1 ≡ max st-1 δ 2</figDesc><table><row><cell></cell><cell></cell><cell></cell></row><row><cell>1 t-1</cell><cell>p 1,2 π(s t ) st-1 δ 2 t-1</cell><cell></cell></row><row><cell></cell><cell>t-1</cell><cell></cell></row><row><cell></cell><cell></cell><cell>t-1</cell></row></table><note><p>p</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>t ∈ 1 . . . M indicates the index of the active sound generator and r t ∈ {sound, mute} indicates its state. The transition model p(z t |z t-1 ) is a large sparse transition table with probabilities</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>p 1,1</cell><cell cols="3">p 1,2 /M . . . p 1,2 /M</cell></row><row><cell>  </cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell> </cell><cell cols="4">p 1,1 p 1,2 /M . . . p 1,2 /M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell> </cell><cell>p 2,1</cell><cell>p 2,2</cell><cell></cell><cell></cell></row><row><cell>  </cell><cell>. . .</cell><cell></cell><cell>. . .</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>p 2,1</cell><cell></cell><cell></cell><cell>p 2,2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1 π(s t ) . . . π(s t )Here, f r,j ≡ f r,j (s t |s t-1 ) denotes the transition model of the j'th sound generator when in state r. The derivation for filtering follows the same lines as the onset/offset detection model, with only slightly more<ref type="bibr" target="#b19">January 19, 2004</ref> DRAFT tedious indexing. Suppose we have the filtering density available at time t -1 as α t-1 . We first calculate the predictive potential. Summation over z t-2 yields the row sums</figDesc><table><row><cell>ξ</cell><cell>(r,j) t-1</cell><cell>=</cell><cell cols="2">(r,j),(r ′ ,j ′ ) α t-1</cell></row><row><cell></cell><cell></cell><cell>r ′ ,j ′</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>  </cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">f 1,M π(s t ) . . . π(s t )</cell><cell> </cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>f 2,1</cell><cell></cell><cell cols="2">f 2,1</cell><cell></cell><cell> </cell></row><row><cell cols="2">. . .</cell><cell></cell><cell>. . .</cell><cell></cell><cell>  </cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">f 2,M</cell><cell></cell><cell>f 2,M</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p><ref type="bibr" target="#b19">January 19, 2004</ref> DRAFT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We ignore the cases when two or more generators are simultaneously in the mute state.<ref type="bibr" target="#b19">January 19, 2004</ref> DRAFT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We will assume that initially we start from silence where rj,0 = mute for all j = 1 . . . M<ref type="bibr" target="#b19">January 19, 2004</ref> DRAFT</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for a short period in the mute state, i.e. r j,t = mute for some period, the marginal posterior over the state vector s j,t will converge quickly to a zero mean Gaussian with a small covariance matrix regardless of observations y. We exploit this property to save computations by clamping the hidden states for sequences of s j,t:t ′ to zero for r j,t:t ′ = "mute". This reduces the hidden state dimension, since typically, only a few sound generators will be in sound state.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DERIVATION OF MESSAGE PROPAGATION ALGORITHMS</head><p>In the appendix, we derive several exact message propagation algorithms. Our derivation closely follows the standard derivation of recursive prediction and update equations for the Kalman filter <ref type="bibr" target="#b40">[41]</ref>. First we focus on a single sound generator. In appendix I-A and I-B, we derive polynomial time algorithms for calculating the evidence p(y 1:T ) and MAP configuration r * 1:T = argmax r1:T p(y 1:T , r 1:T ) respectively. The MAP configuration is useful for onset/offset detection. In the following section, we extend the onset/offset detection algorithms to monophonic pitch tracking with constant frequency. We derive a polynomial time algorithm for this case in appendix I-C. The case for varying fundamental frequency is derived in the following appendix I-D. In appendix II we describe heuristics to reduce the amount of computations.</p><p>A. Computation of the evidence p(y 1:T ) for a single sound generator by forward filtering</p><p>We assume a Markovian prior on the indicators r t where p(r t = i|r t-1 = j) ≡ p i,j . For convenience, we repeat the generative model for a single sound generator by omitting the note index j.</p><p>For simplicity, we will sometime use the labels 1 and 2 to denote sound and mute respectively. We enumerate the transition models as f rt (s t |s t-1 ) = N (A rt s t-1 , Q). We define the filtering potential as</p><p>We assume that y is always observed, hence we use the term potential to indicate the fact that p(y 1:t , s t , r t , r t-1 )</p><p>is not normalized. The filtering potential is in general a conditional Gaussian mixture, i.e. a mixture of the prediction equation <ref type="bibr" target="#b16">(17)</ref> needs to be changed to α</p><p>where ψ and Z are defined in <ref type="bibr" target="#b17">(18)</ref>. The reason for the exponential growth is the following: Remember that each ψ (r,j)(r ′ ,j ′ ) has as many components as an entire row sum of ξ (r,j)</p><p>. Unlike the inference for piecewise constant pitch estimation, now at some rows there are two or more messages (e.g. α</p><p>(1,j)(1,j) t|t-1 and α</p><p>) that depend on ψ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II COMPUTATIONAL SIMPLIFICATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pruning</head><p>Exponential growth in message size renders an algorithm useless in practice. Even in special cases, where the message size increases only polynomially in T , this growth is still prohibitive for many applications. A cheaper approximate algorithm can be obtained by pruning the messages. To keep the size of messages bounded, we limit the number of components to N and store only components with the highest evidence. An alternative is discarding components of a message that contribute less than a given fraction (e.g. 0.0001) to the total evidence. More sophisticated pruning methods with profound theoretical justification, such as resampling <ref type="bibr" target="#b21">[22]</ref> or collapsation <ref type="bibr" target="#b42">[43]</ref>, are viable alternatives but these are computationally more expensive. In our simulations, we observe that using a simple pruning method with the maximum number of components per message set to N = 100, we can obtain results very close to an exact algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Kalman filtering in a reduced dimension</head><p>Kalman filtering with a large state dimension |s| at typical audio sampling rates F s ≈ 40 kHz may be prohibitive with generic hardware. This problem becomes more severe when the number of notes M is large, (which is typically around 50 -60), than even conditioned on a particular configuration r 1:M , the calculation of the filtering density is expensive. Hence, in an implementation, tricks of precomputing the covariance matrices can be considered <ref type="bibr" target="#b40">[41]</ref> to further reduce the computational burden.</p><p>Another important simplification is less obvious from the graphical structure and is a consequence of the inherent asymmetry between the sound and mute states. Typically, when a note switches and stays <ref type="bibr">January 19, 2004 DRAFT</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bregman</surname></persName>
		</author>
		<title level="m">Auditory Scene Analysis</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computational auditory scene analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A theory and computational model of auditory monaural sound separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weintraub</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University Dept. of Electrical Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One microphone source separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, NIPS*2000</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Prediction-driven computational auditory scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT, Dept. of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Music-listening systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Scheirer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Manipulation, analysis and retrieval systems for audio signals</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machine</forename><surname>Musichanship</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pitch Determination of Speech Signal</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Hess</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Estimation and Tracking of Frequency</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hannan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local harmonic estimation in musical sound signals</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Irizarry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weighted estimation of harmonic components in a musical sound signal</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Time Series Analysis</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real time voice processing with audiovisual feedback: toward autonomous agents with perfect pitch</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, NIPS*2002</title>
		<meeting><address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new approach to frequency analysis with amplified harmonics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Truong-Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statistics Society B</title>
		<imprint>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="203" to="222" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximate Kalman filtering for the harmonic plus noise model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE WASPAA</title>
		<meeting>of IEEE WASPAA<address><addrLine>New Paltz</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Application of bayesian probability network to music scene analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI Workshop on CASA</title>
		<meeting>IJCAI Workshop on CASA</meeting>
		<imprint>
			<publisher>Montreal</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Model-based segmentation of time-frequency images for musical transcription</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sterian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Signal separation of musical instruments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Walmsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian harmonic models for musical signal analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Godsill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A mixed graphical model for rhythmic parsing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raphael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 17th Conf. on Uncertainty in Artif. Int. Morgan Kaufmann</title>
		<meeting>of 17th Conf. on Uncertainty in Artif. Int. Morgan Kaufmann</meeting>
		<imprint>
			<date type="published" when="2001-01-19">2001. January 19. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Cognition of Basic Musical Structures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Temperley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monte Carlo methods for tempo tracking and rhythm quantization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="45" to="81" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sound-source recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust multipitch estimation for the analysis and manipulation of polyphonic musical signals</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klapuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Holm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COST-G6, Conference on Digital Audio Effects</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative model based polyphonic music transcription</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<meeting><address><addrLine>New Paltz, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">October 2003</date>
		</imprint>
	</monogr>
	<note>Proc. of IEEE WASPAA</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rossing</surname></persName>
		</author>
		<title level="m">The Physics of Musical Instruments</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral modeling synthesis: A sound analysis/synthesis system based on deterministic plus stochastic decomposition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="12" to="24" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Musical sound signals analysis/synthesis: Sinusoidal + residual and elementary waveform models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Rodet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Signal Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech analysis/synthesis based on a sinusoidal representation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mcaulay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="744" to="754" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Physical modeling of plucked string instruments with application to real-time sound synthesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Valimaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huopaniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Karjaleinen</surname></persName>
		</author>
		<author>
			<persName><surname>Janosy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Audio Eng. Society</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="331" to="353" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Switching Kalman filters</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of California, Berkeley, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dynamic Bayesian networks: Representation, inference and learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Information Theory, Inference and Learning Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Propagation algorithms for variational Bayesian learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Parameter estimation for linear dynamical systems. (crg-tr-96-2)</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Totronto. Dept. of Computer Science., Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic transcription of piano music</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raphael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMIR</title>
		<meeting>ISMIR</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pitch, consonance and harmony</title>
		<author>
			<persName><forename type="first">E</forename><surname>Terhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1061" to="1069" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Harmonic analysis with probabilistic graphical models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stoddard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMIR</title>
		<meeting>ISMIR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Physical modeling using digital waveguides</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sequential Monte Carlo Methods in Practice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<editor>N. J. Gordon</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bar-Shalom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Estimation and Tracking: Principles, Techniques and Software</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Artech House</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exact and efficient bayesian inference for multiple changepoint problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fearnhead</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Mathematics and Statistics, Lancaster University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Expectation propagation for approximate inference in dynamic Bayesian networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zoeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
