<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RESIDUAL ENERGY-BASED MODELS FOR TEXT GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
							<email>dengyuntian@seas.harvard.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
							<email>myleott@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
							<email>aszlam@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><forename type="middle">' Aurelio</forename><surname>Ranzato</surname></persName>
							<email>ranzato@fb.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RESIDUAL ENERGY-BASED MODELS FOR TEXT GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The dominant approach to parametric text generation is based on large neural auto-regressive models <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. These models can be trained efficiently via maximum likelihood and they can efficiently generate samples of remarkable quality. Key to their success is local normalization, i.e. they are defined in terms of a product of conditional distributions, one for each token in the sequence. Such distributions are relatively cheap to compute with modern hardware given the limited vocabulary size of common sub-word units like BPE <ref type="bibr" target="#b28">(Sennrich et al., 2015)</ref>.</p><p>Unfortunately, local normalization also brings some drawbacks. First, the designer of the model needs to specify the order in which tokens are generated. Second, at training time the model is conditioned on ground truth context while at test time it is conditioned on its own generations, a discrepancy referred to as exposure bias <ref type="bibr" target="#b26">(Ranzato et al., 2016)</ref>. Finally, while heuristics like beam search somewhat help rescore at the sequence level, generation generally lacks long-range coherency because it is produced by the greedy selection of one token at a time without lookahead.</p><p>Energy-based models (EBMs) <ref type="bibr" target="#b11">(Hinton, 2002;</ref><ref type="bibr" target="#b17">LeCun et al., 2006;</ref><ref type="bibr" target="#b25">Ranzato et al., 2007</ref>) are a more general framework which potentially address all these issues, as they do not require any local normalization. They only require the definition of an energy function defined over the whole input sequence. Training aims at shaping the energy function such that regions of high density of training data points have lower energy than elsewhere. In principle, EBMs are ideal for modeling text as they can score the whole input at once, they are not prone to label bias <ref type="bibr" target="#b2">(Bottou, 1991)</ref> and they may enable generation of large chunks of text, which should help improve coherency.</p><p>However, so far EBMs had limited application in text generation, because sampling from the model is intractable, and so is maximum likelihood training. The problem is that shaping the energy function is accomplished by updating the model parameters such that the energy is decreased at the training data points (a.k.a. positive examples) and increased at other data points (a.k.a. negative examples). In maximum likelihood training negatives are generated from the model, but in text application we cannot use gradient-based MCMC methods <ref type="bibr" target="#b30">(Teh et al., 2003;</ref><ref type="bibr" target="#b5">Du &amp; Mordatch, 2019)</ref> and Gibbs sampling <ref type="bibr" target="#b37">(Welling et al., 2005)</ref> is too slow to be practical. Generating negatives by local perturbations of the ground truth would be efficient but hardly useful for generation purposes, when at test time the model needs to generate from scratch.</p><p>Recently, <ref type="bibr" target="#b1">Bakhtin et al. (2019)</ref> carefully studied the problem of training a discriminator to distinguish human written text from language model generations. They experimented with different language model and discriminator architectures, training/test time corpora and concluded that the discriminator can generalize rather well to weaker language models when the training/test corpora match. <ref type="bibr" target="#b1">Bakhtin et al. (2019)</ref> found that the learned discriminator is not robust to random perturbations, and argued that the discriminator operates in the "residual" space of the language model. Concurrently, <ref type="bibr" target="#b9">Grover et al. (2019)</ref> proposed a general approach to "de-bias" a generator, by simply training a discriminator and using its output for importance sampling.</p><p>In this work, we build upon these two works. First, we formalize the residual interpretation by <ref type="bibr" target="#b1">Bakhtin et al. (2019)</ref> and use a generative model of the form:</p><formula xml:id="formula_0">P θ (x) ∝ P LM (x) exp(−E θ (x))<label>(1)</label></formula><p>where P LM (x) is a locally normalized language model which is fixed during training, and E θ is the energy function parameterized by θ. The resulting model P θ (x) is globally normalized due to the energy term. Note that the same residual formulation was also used in <ref type="bibr" target="#b27">Rosenfeld et al. (2001)</ref>; <ref type="bibr" target="#b34">Wang &amp; Ou (2018b)</ref>; <ref type="bibr" target="#b23">Parshakova et al. (2019)</ref>.</p><p>This formulation has multi-fold benefits. First, by incorporating a locally normalized language model, we can leverage recent advancements in locally normalized language modeling. Second, the language model provides a natural proposal distribution for training <ref type="bibr" target="#b1">(Bakhtin et al., 2019)</ref>, and training can be made efficient by using the conditional noise contrastive estimation objective (Gutmann &amp; Hyvärinen, 2010) as we shall see in §3. Lastly, this formulation enables efficient evaluation and generation via importance sampling <ref type="bibr" target="#b15">(Horvitz &amp; Thompson, 1952;</ref><ref type="bibr" target="#b9">Grover et al., 2019)</ref>.</p><p>In some sense, this last point is perhaps the central contribution of the paper, as it allows estimating perplexity of the residual EBM, and thus allows these EBMs to be compared in a standard way to other models. Indeed, in §4 we show that our joint model decreases perplexity on two large datasets, when compared to various auto-regressive language model baselines. Finally, the EBM generations are significantly preferred by humans according to our qualitative evaluation. To the best of our knowledge, this is the first time that an EBM has demonstrated improved generation ability against very strong auto-regressive baselines, both in terms of estimated perplexity and through human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Energy-based models have a long history in machine learning <ref type="bibr" target="#b14">(Hopfield, 1982;</ref><ref type="bibr" target="#b11">Hinton, 2002;</ref><ref type="bibr" target="#b17">LeCun et al., 2006;</ref><ref type="bibr" target="#b25">Ranzato et al., 2007)</ref>. The key challenge of training is mining for good negatives. This can be accomplished explicitly by fantasizing inputs where the energy should be increased or implicitly via global constraints such as sparsity <ref type="bibr" target="#b25">(Ranzato et al., 2007)</ref>. Methods attempting at maximizing the likelihood of the data require to sample from the distribution induced by the model. Unfortunately, gradient-based MCMC approaches like Hybrid Monte Carlo <ref type="bibr" target="#b30">(Teh et al., 2003)</ref> and Langevyn dynamics <ref type="bibr" target="#b25">(Ranzato et al., 2007;</ref><ref type="bibr" target="#b5">Du &amp; Mordatch, 2019;</ref><ref type="bibr" target="#b38">Xie et al., 2016;</ref><ref type="bibr">2017;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b22">2018;</ref><ref type="bibr" target="#b7">Gao et al., 2018;</ref><ref type="bibr" target="#b21">Nijkamp et al., 2019)</ref> are not applicable when the input is discrete like in text applications. Other approaches like Gibbs sampling <ref type="bibr" target="#b11">(Hinton, 2002)</ref> were applied to binary inputs but do not scale well to large dictionaries once the energy function is a large bidirectional transformer model like the one used in this work. Several variants of auto-encoders have also been investigated for representing and generating text <ref type="bibr" target="#b3">(Bowman et al., 2016;</ref><ref type="bibr">Zhao et al., 2018)</ref>, but they have not shown significant improvements in terms of perplexity and they have so far been applied to relatively small datasets only.</p><p>Our approach appears similar to discriminative reranking approaches used in the parsing and machine translation community <ref type="bibr" target="#b29">(Shen et al., 2004)</ref>. However, our approach provides a generative model, and parameters/hyper-parameters are directly tuned to close the gap between the model distribution and the data distribution, rather than relying on surrogate ranking losses. This approach is also related to other sequence level training objectives <ref type="bibr" target="#b6">(Edunov et al., 2018)</ref>, with the major differ-ence that in those works training aims at improving the baseline model, but generation at test time is still greedy.</p><p>Energy Networks have been used for sequence modeling <ref type="bibr" target="#b27">(Rosenfeld et al., 2001;</ref><ref type="bibr" target="#b35">Wang et al., 2015;</ref><ref type="bibr">2017;</ref><ref type="bibr">Wang &amp; Ou, 2017;</ref><ref type="bibr" target="#b33">2018a;</ref><ref type="bibr" target="#b23">Parshakova et al., 2019)</ref>. In particular, our residual modeling form and the training algorithm is the same as in <ref type="bibr" target="#b34">Wang &amp; Ou (2018b)</ref>, where they used an LSTM as the generator and a CNN-LSTM as the energy function, and showed significant gains compared to LSTM baselines in speech recognition. Our work builds on these prior works and develops new lower and upper bounds for the log-probability under the joint model, which makes it possible to show that the residual EBM approach gets better perplexity. We also develop an importance weighting sampling scheme used at generation time, which is focused on conditional generation as opposed to rescoring in speech recognition <ref type="bibr" target="#b34">(Wang &amp; Ou, 2018b)</ref>. The residual EBM formalism makes it very natural to use BERT for language modeling, and we show that empirically this type of approach can outperform modern state-of-the-art language modeling baselines, both in terms of perplexity, and through human evaluation.</p><p>Generative Adversarial Networks <ref type="bibr" target="#b8">(Goodfellow et al., 2014</ref>) also relate to EBMs, except that in EBMs the generator is implicit and negatives samples are produced by the discriminator itself. In our work, the pretrained locally normalized language model can be seen as a fixed generator, like in <ref type="bibr" target="#b1">Bakhtin et al. (2019)</ref>. <ref type="bibr" target="#b0">Azadi et al. (2018)</ref> also share our same goal but their generator is not locally normalized and they propose to improve the sampling from the generator by using the discriminator for rejection sampling. Similar to our work, <ref type="bibr" target="#b9">Grover et al. (2019)</ref> propose to use the discriminator to de-bias the pretrained generator using importance sampling. We adapt this work to the application of text generation. In particular, we adopt the conditional noise contrastive estimation (NCE) objective <ref type="bibr" target="#b19">(Ma &amp; Collins, 2018;</ref><ref type="bibr" target="#b10">Gutmann &amp; Hyvärinen, 2010)</ref> to our residual model energy function and then sample from the joint model using importance sampling. We want to note that the same formulation has been proposed in <ref type="bibr" target="#b34">(Wang &amp; Ou, 2018b;</ref><ref type="bibr" target="#b23">Parshakova et al., 2019)</ref>. While <ref type="bibr" target="#b19">Ma &amp; Collins (2018)</ref> used conditional NCE to predict the next word in a sequence, we apply it to produce a whole sequence at once with the pretrained auto-regressive language model as the noise distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESIDUAL ENERGY-BASED MODELS</head><p>We study the problem of conditional generation of discrete sequences. Given a prefix x 1 , • • • , x p with x j ∈ V where V is the vocabulary, we want to model the probabilities of generating a sequence of total length T &gt; p<ref type="foot" target="#foot_0">1</ref> . The generative model is:</p><formula xml:id="formula_1">P θ (x p+1 , • • • , x T |x 1 , • • • , x p ) = P LM (x p+1 , • • • , x T |x 1 , • • • , x p ) exp(−E θ (x 1 , • • • , x T )) Z θ (x 1 , • • • , x p )<label>(2)</label></formula><p>where Z θ (x 1 , • • • , x p ) is a normalizing factor known as partition function. Computing the partition function is intractable in our case since it involves a sum over |V | T −p terms which grow exponentially with the sequence length: in our experiments the size of the vocabulary is 50,096 and the length of the generation is 40 tokens. We call P θ the joint model, and E θ the residual energy function since P LM is fixed throughout training. The goal of training is to learn the parameters of the energy function such that the joint model distribution gets close to the data distribution. For the sake of reducing clutter in the notation, we will drop the conditioning variables in the following discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRAINING</head><p>When the partition function is intractable, Maximum Likelihood Estimation (MLE) requires samples from the model distribution, which is usually approximated with Monte Carlo sampling or mean field inference <ref type="bibr" target="#b12">(Hinton, 2012;</ref><ref type="bibr" target="#b17">LeCun et al., 2006)</ref> for globally normalized models. Unfortunately, both approaches are too expensive for text applications when using large bidirectional transformer models. For instance, if we were to employ Gibbs sampling exactly, we would need to perform at every position as many forward passes as words in the dictionary to compute each conditional distribution. On large datasets where training locally normalized models on multiple machines already takes days, having such additional overhead means that the model would learn from much less data for the same amount of time, and this is seldom a beneficial strategy for learning models that generalize well. Therefore, we do not use either MCMC nor mean field methods, as the latter would introduce additional variational parameters or an inference network which anyway yields an approximation to MLE learning.</p><p>Instead, we train our residual energy function using Noise Contrastive Estimation (NCE) <ref type="bibr" target="#b10">(Gutmann &amp; Hyvärinen, 2010)</ref>, and more specifically its conditional version <ref type="bibr" target="#b19">(Ma &amp; Collins, 2018)</ref>. NCE requires two distributions: The model distribution and a noise distribution. In our case, the model distribution is the joint model of Eq. 2, P θ , while the noise distribution is the pretrained language model, P LM . NCE then trains a binary classifier on the difference of log-probability scores of these two models. Since our joint model is the product of the energy function (whose parameters we want to learn) with P LM , the difference reduces to: log P θ − log P LM = −E θ . Therefore, under these modeling assumptions of residual learning and noise model, the objective function becomes:</p><formula xml:id="formula_2">max E x+∼P data log 1 1 + exp(E θ (x + )) + E x−∼P LM log 1 1 + exp(−E θ (x − ))<label>(3)</label></formula><p>where x + is a positive sequence taken from the human generated training set, and x − is a negative sequence drawn from P LM (for a given ground truth prefix). In other words, training the energy function reduces to training a binary classifier to discriminate between real text and text generated by an auto-regressive language model. The aim of training is to assign as negative energy as possible to real data, and as positive energy as possible to machine generated data. Interestingly, the role of positive and negative samples is totally symmetric in this loss function, §5 will discuss the consequences of this.</p><p>With the theoretical guarantee of NCE, we can show that the optimum of the above objective is reached at data distribution with infinite amount of data and model with enough capacity, which is also proved in <ref type="bibr" target="#b19">Ma &amp; Collins (2018)</ref> 2 . Theorem 1. If P LM has the same support as P data , then the objective function in Eq. 3 reaches its maximum at log P LM (x) − E θ (x) = log P data , if there exists such θ.</p><p>Proof. This theorem directly follows from the proof in <ref type="bibr" target="#b10">Gutmann &amp; Hyvärinen (2010)</ref>. Note that at optimum,</p><formula xml:id="formula_3">P LM (x) exp(−E θ (x)) is self-normalizing: instead of P θ (x) ∝ P LM (x) exp(−E θ (x)),</formula><p>we have P θ (x) = P LM (x) exp(−E θ (x)). However, we still need to estimate the partition function throughout the rest of this paper, since we cannot guarantee that this optimum can be reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EVALUATION</head><p>A commonly used protocol for evaluating generative sequence models, especially language models, is perplexity (PPL), which is equal to</p><formula xml:id="formula_4">2 − 1 T −p T i=p+1 log 2 P (xi|xi−1,••• ,x1</formula><p>) . PPL can be interpreted as the average number of tokens the model is uncertain of at every time step. Since the log-likelihood required by PPL relies on estimating the partition function</p><formula xml:id="formula_5">Z θ = x P LM (x) exp(−E θ (x)) = E x∼P LM exp(−E θ (x))</formula><p>, we derive two estimators for the log-partition function log Z θ based on the work of <ref type="bibr" target="#b22">Nowozin (2018)</ref>.</p><formula xml:id="formula_6">Theorem 2. Denote T n as the empirical estimate of log E x∼P LM exp(−E(x)) with n samples x i ∼ P LM (i = 1, • • • , n): T n = log 1 n n i=1 exp(−E(x i )), then ∀ &gt; 0, ∃N &gt; 0 such that ∀n &gt; N we have Z θ − &lt; E[T n ] &lt; Z θ &lt; E[(2n − 1)T n − 2(n − 1)T n−1 ] &lt; Z θ + (4)</formula><p>The proof is given in Appendix A.2.</p><p>We can use the above two estimators to estimate the lower and upper bounds of the partition function, but we want to emphasize that they are true only asymptotically (when n is sufficiently large). We also want to note that to get lower variance estimates we use leave-one-out strategy to estimate T n−1 . See <ref type="bibr" target="#b22">Nowozin (2018)</ref> for implementation details and methods to improve numeric stability.</p><p>Similarly to locally normalized models, we can also factorize the probabilities of an entire sequence step by step, as P (x) = </p><formula xml:id="formula_7">s i = E θ (x i ) for each x i ∈ {x 1 , • • • , x n } // Resample from the set of LM samples sample x = x i with probability exp(−s i ) n j=1 exp(−s j ) return x</formula><p>marginalizing over the future, we can derive the following per step probabilities:</p><formula xml:id="formula_8">P (x t |x &lt;t ) = P LM (x t |x &lt;t ) E x t+1 ,••• ,x T ∼P LM (•|x ≤t ) [exp(−E θ (x ≤t , x t+1 , • • • , x T ))] E x t ,••• ,x T ∼P LM (•|x ≤t−1 ) [exp(−E θ (x ≤t−1 , x t , • • • , x T ))]</formula><p>.</p><p>(5)</p><p>The step-wise probabilities in Eq. 5 are an instance of importance sampling <ref type="bibr" target="#b15">(Horvitz &amp; Thompson, 1952)</ref>. The basic P LM distribution is adjusted by the probability assigned to token x t by the energy function (numerator is clamped at x t while denominator sums over all the possible values of the token at position t), with the additional marginalization over all subsequent tokens up to the horizon T . Since the summation involves exponentially many terms, unless t = T , this is approximated by samples drawn by P LM . Since both the numerator and the denominator take the same form as the partition function, we also use Eq. 4 to estimate the upper and lower bounds. E.g., the lower bound of log P (x t |x &lt;t ) can be obtained by using the lower bound of the numerator and the upper bound of the denominator.</p><p>For t = T , we can calculate the log probability by exhaustive enumeration. This gives us an idea of the true performance of our model at the last step, and it also provides a sanity-check of the tightness of our estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GENERATION</head><p>Generating from the joint model is a non-trivial task. A naive way is to generate from the joint model auto-regressively, by marginalizing the future as in Eq. 5, which we term Top-k auto-regressive sampling. However, doing so is expensive and impractical, and we only use this method for a qualitative analysis of the joint model in Appendix A.1.</p><p>In order to generate efficiently, we use self-normalizing importance sampling <ref type="bibr">(Owen, 2013;</ref><ref type="bibr" target="#b9">Grover et al., 2019)</ref>. Under the assumptions that the model from which we wish to draw samples is the joint model, which is the product of the auto-regressive model and the energy function, and that the proposal distribution is the auto-regressive model itself, sampling proceeds simply by: a) sampling from the auto-regressive language model, followed by b) resampling according to the energy function. The algorithm is shown in Algorithm 1, where we introduce an optional top-k constraint on the pretrained language model to improve the quality of samples in the set<ref type="foot" target="#foot_1">3</ref> . Without the top-k constraint, as the number of samples goes to infinity, we would recover exact samples from the joint model distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we describe the experimental set up and the results we obtained by using the residual EBM for text generation, both in terms of perplexity and generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>Datasets We consider two datasets: the Toronto Book Corpus <ref type="bibr" target="#b16">(Zhu et al., 2015;</ref><ref type="bibr" target="#b16">Kiros et al., 2015)</ref> and CC-News <ref type="bibr" target="#b1">(Bakhtin et al., 2019)</ref>. The former dataset consists of fiction books in 16 different genres, totaling about half a billion words. The latter is a de-duplicated subset of the English portion of the CommonCrawl news dataset <ref type="bibr" target="#b20">(Nagel, 2016)</ref>, which totals around 16 Billion words. The book corpus is more challenging because the range of style and topics is more diverse than CC-News. Also, the book corpus is 30 times smaller than CC-News and may pose generalization challenges because of its smaller size.</p><p>In all our experiments we use a prefix of size 120 tokens and we generate the following 40 tokens; with the notation of Eq. 2, p = 120 and T = 160. For training the joint models, for efficiency we generated 16/128 samples per prefix for CC-News/Book Corpus offline, and sample uniformly from those samples at training time.</p><p>Baselines We consider as base language model (BASE LM) used to generate negatives for the residual EBM, a transformer language model with 12 layers, h = 16, d model = 1024, d f f = 4096 (we refer to <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref> for notations). This is also our first baseline model.</p><p>The joint model has as many parameters as the sum of the number of parameters in the base LM and the number of parameters in the energy network. To make a fair comparison, we consider two additional baselines that have the same number of parameters as our joint model.</p><p>The first baseline is a Residual Auto-regressive Language Model baseline (RALM):</p><formula xml:id="formula_9">log P RALM (x t |x &lt;t ) = log P LM (x t |x &lt;t ) + log P φ (x t |x &lt;t ) + const<label>(6)</label></formula><p>where P φ takes the form of another auto-regressive language model. The parameters of P φ are trained by exact maximum likelihood training of P RALM .</p><p>The second baseline is an auto-regressive language model of the same size of our joint model (sum of the base LM and energy function parameters), we dub this model Big Auto-regressive Language Model (BALM). BALM has 12 layers, h = 16, d model = 1568, d f f = 6272, and is trained by standard token level cross-entropy loss.</p><p>Residual EBM Architecture We consider two architectures for our residual EBM, both of them are based on transformers <ref type="bibr" target="#b31">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Devlin et al., 2018)</ref>. The first version uses causal self-attention and is derived from the base LM, a unidirectional transformer (UNIT). It is of the same architecture as BASE LM, except that in the final layer we project the mean-pooled hidden states to a scalar energy value. We initialize its parameters with a language model trained on the same dataset.</p><p>The second version is instead bi-directional (BIT), and the energy function is computed by projecting the mean-pooled top hidden states down to a single scalar value. We consider three variants, a BIT-BASE following the architecture of RoBERTa-Base, and a BIT-LARGE * following RoBERTa-Large <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>, and a BIT-MED with the same number of parameters as UNIT (such that JOINT BIT-MED has roughly the same number of parameters as BALM). We initialize the parameters with a trained BERT, and we use * to mark usage of external data <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>, otherwise it means that BERT was trained on our training set. Notice how our model can be interpreted as a natural way to finetune large bidirectional pretrained models for the text generation task.</p><p>While we expect BIT to yield better results because it can fully leverage context also for intermediate tokens, we also consider UNIT to compare to the RALM baseline, which uses the same architecture and only differs in the way parameters are trained and in the presence of local normalization.</p><p>We train our models on 8 DGX nodes, each with 8 Nvidia V100s. We use the Adam optimizer, with cosine learning rate decay and learning rate warmup. To stabilize training we used gradient norm clipping. Detailed hyper-parameter settings can be found in Appendix A.3.</p><p>For generation, we use top-k sampling with k = 10 for all human evaluations. We take 10,000 samples from BASE LM for our joint sampling.   At each position the lower and upper bounds (Eq. 5 estimated using the method in Eq. 4, see §3.2 for more details) are estimated using 20,000 samples. The shorter the horizon (moving to the right), the tighter the estimation is but also the more limited the gains compared to base LM as un-normalized models are most useful on longer generations. non-residual baseline BALM performs similarly to JOINT UNIT, which might be due to the limitation that P LM is not trained jointly with the residual model in both JOINT UNIT and RALM. However, by using our EBM approach, we can remove the causal attention mask and use bi-directional models, which achieves better performance than baselines and JOINT UNIT: without external data, JOINT BIT-BASE reaches a higher performance than JOINT UNIT with fewer parameters. By initializing from the state-of-the-art pretrained bi-directional transformers RoBERTa-Base and RoBERTa-Large, JOINT BIT-BASE* and JOINT BIT-LARGE* reach even better performance than JOINT BIT-BASE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Evaluation Our main result is reported in</head><p>In the lower part of the table, we show that if we make the big language model baseline BALM deeper (BALM-24L) (24 layers instead of 12, for the same number of parameters) we attain lower perplexity. However, training the joint model JOINT BIT-BASE on the residual of a deeper language model BASE LM-24L yields even lower perplexity, despite having fewer parameters. By using the same number of parameters as BALM-24L, JOINT BIT-MED further decreases perplexity. Finally, by initializing from RoBERTa-Large, JOINT BIT-BASE* obtains the best results.</p><p>One caveat of our evaluation protocol is that the perplexity bounds are only estimates, which might not reflect the true value, particularly since the number of possible sequences grows exponentially with the number of words that are generated. We therefore break down perplexity per position in the generated sequences as in Eq. 5, and compare the estimated PPLs to the true enumerated PPLs at the last position, as shown in Figure <ref type="figure" target="#fig_1">1</ref>. We find that at the final generation step, the estimated bounds agree remarkably well with the exact values, proving that our method at least gets a reasonable PPL estimate at the last generation step, and that JOINT BIT-MED outperforms baselines at the last generation step for sure.</p><p>Human Evaluation Better perplexity results do not necessarily imply better generations. Besides, since generation from the residual EBM requires approximations as in Algorithm 1, the limited sample size might induce approximation errors compared to truly sampling from the joint distribution. Therefore, we conducted human evaluations to compare generations from the residual EBM model to generations from the baseline language models.</p><p>For each prefix, we present one completion from each model, and ask humans to select the one that is a better continuation. More details about human evaluation can be found in the Appendix A.4. The preference rates reported in Table <ref type="table" target="#tab_2">2</ref> confirm that indeed the generation quality of JOINT BIT-BASE and JOINT BIT-LARGE * is better than both language model baselines. Depending on the model variant, our joint model (with bidirectional EBM) is preferred between 56% and almost 60% of the times; interestingly, the preference rate does not change much as we compare against base LM as opposed to BALM. In fact, humans do not seem to have a strong preference for BALM over base LM, despite the former scores two perplexity points lower. Similarly, JOINT UNIT is not strongly preferred over BASE LM despite its lower perplexity score. We surmise that unidirectional scoring functions and auto-regressive models exhibit generation artifacts which are easily detected by humans, and these may overshadow the improvements brought by perplexity gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ANALYSES</head><p>In this section, we analyze some of the results we obtained. First, we check whether we used a sufficient number of samples in our perplexity estimates. Second, we assess whether the joint model produces fewer repetitions compared to the base language model, and finally we check how well some statistics of the model and data distributions match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of samples.</head><p>In Figure <ref type="figure" target="#fig_2">2</ref>, we vary the number of samples we take in order to estimate PPL upper and lower bounds. Beyond 20,000 samples the upper estimate becomes very stable, although  we have to emphasize that these estimates might be biased even though the gap between lower and upper bound closes as we take more samples.</p><p>Repetitions. A typical artifact of auto-regressive language models is their tendency to repeat phrases. It is then interesting to check whether the joint model is able to alleviate this artifact. Fig. <ref type="figure" target="#fig_2">2</ref> shows that indeed the joint model has a slightly higher percentage of unique n-grams compared to the baseline language model with n = 2, 3, 4, although still not as high as the original human generated text.</p><p>A necessary condition for the model to match the data distribution. If the joint model p θ matches the data distribution p d , then statistics computed on a large population of samples from the two distributions should also match. In particular, Fig. <ref type="figure" target="#fig_3">3</ref> show the density plots of log-likelihood scores of the baseline language model (left) and joint model (right) when fed with their own samples versus samples from the test set. We observe that the histogram of samples from the joint model matches the real data distribution more closely: The difference of means in the LM BASE case is 21.64 whereas the difference is 6.20 in the joint approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATIONS</head><p>In the previous sections we highlighted the strengths of residual EBMs, namely their simplicity, efficiency both at training and test time, and their improved perplexity scores against strong autoregressive language model baselines. In this section, we comment on their limitations to caution the reader about when these methods are more likely to succeed and to inform other researchers about what future avenues of research may naturally derive from this work.</p><p>In order to make training efficient and side step costly negative mining using the energy function itself, the current approach uses negatives generated from a pretrained auto-regressive language model. Therefore, our model works as long as the base language model from which we draw samples is strong enough, and as long as the ground truth and other plausible sequences are reachable by the baseline language model.</p><p>If the base language model has poor quality, then generation from our joint model is going to be poor as well, as the joint model merely resamples generations from the original language model. Moreover, training is going to be trivial if the base language model is poor, because the residual energy function merely needs to detect trivial generation artifacts from the base language model. In fact, observe that the role of positive and negative samples is symmetric in the loss of Eq. 3. This means that the energy function can choose to minimize the loss by either modeling the true data or the negative samples; since the latter have much simpler structure, it is going to model the negative samples. Therefore, importance sampling amounts to mostly down-weighing the worst samples from the base language model. The consequence of this is that search with a poor base language model is going to be catastrophically inefficient, as we would need to sample an impractically large number of negatives in order to find samples that are reasonably close to the true data manifold.</p><p>To summarize, this work makes a rather strong implicit assumption on the quality of the base language model, and it is expected to work well only when this is rather strong. In our application, this assumption is met quite well in practice as large auto-regressive language models trained on large datasets have improved significantly in recent years <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. In general however, residual learning always carries liability to its base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>We investigated an EBM trained on the residual of a pretrained autoregressive language model <ref type="bibr" target="#b34">(Wang &amp; Ou, 2018b;</ref><ref type="bibr" target="#b23">Parshakova et al., 2019)</ref>. The resulting joint model scores sequences holistically, thanks to the energy function. Training is very efficient and consists of a binary classification task between positives from the training set and pregenerated negatives from the fixed language model. Generation is also very efficient as it amounts to resampling from the large set of negatives produced by the base language model. Our estimates show that the resulting model has lower perplexity than the base language model. Finally, this approach may be interpreted as a natural way to finetune a large bidirectional transformer like BERT for text generation applications.</p><p>In the future, we plan to investigate other ways to generate negatives that may strike a better tradeoff between the amount of compute each negative requires and their closeness to the joint model distribution. It would also be interesting to explore other loss functions and the generation of longer pieces of text by using this model auto-regressively at the chunk level, as opposed to the token level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 TOP-K AUTO-REGRESSIVE SAMPLING</p><p>In this subsection, we factorize the joint model BIT-BASE auto-regressively, and compare its differences with BASE LM. Since even estimating the per step probabilities according to Eq. 5 is too expensive, we further approximate it by only considering the top 128 words predicted by BASE LM, where we sample 10,000 completions for each of them to estimate P (x t |x &lt;t ). Then we take the top 10 entries and re-normalize, and compare it to the top 10 probabilities of BASE LM.</p><p>Our initial explorations suggested that the joint model tends to generate fewer repetitions. Therefore we picked a few LM samples where there are repetitions at x t , and use the same context x &lt;t to estimate P (x t |x &lt;t ) for the joint model. Some examples of P (x t |x &lt;t ) of BASE LM and BIT-BASE are presented in Table <ref type="table" target="#tab_4">3</ref>. Indeed BIT-BASE usually assigns lower probabilities to repetitions even though the top k words remain the same, which is not surprising given that the existence of repetition is a strong indicator of coming from the LM, which would lead to a higher energy value hence lower joint probability.  </p><formula xml:id="formula_10">(i = 1, • • • , n), and let T n = log 1 n n i=1 exp(−E(x i )), then ∀ &gt; 0, ∃N &gt; 0 such that ∀n &gt; N we have Z θ − &lt; E[T n ] &lt; Z θ &lt; E[(2n − 1)T n − 2(n − 1)T n−1 ] &lt; Z θ + (7)</formula><p>Proof. From <ref type="bibr" target="#b22">Nowozin (2018)</ref> Eq. 35, we can write E[T n ] as</p><formula xml:id="formula_11">E[T n ] = Z θ − µ 2 2µ 2 1 n + 1 3µ 3 µ 3 n 2 − 1 4µ 4 ( 3 n 2 µ 2 2 + 1 n 3 (µ 4 − 3µ 2 2 )) + 1 5µ 5 ( 10 n 3 µ 3 µ 2 + 1 n 4 (µ 5 − 10µ 3 µ 2 )) + o(n −3 ) (8) Where µ = E[T n ], µ k = E[(T n − µ) k ]. Equivalently, E[T n ] = Z θ − µ 2 2µ 2 1 n + o(n −1 ) (9) Therefore, lim n→∞ E[T n ] = Z θ . So ∀ &gt; 0, ∃N 1 &gt; 0 such that when n &gt; N 1 , E[T n ] &gt; Z θ − . On the other hand, lim n→∞ n(Z θ − E[T n ]) = lim n→∞ µ2 2µ 2 + o(1) = µ2 2µ 2 &gt; 0, so ∃N 2 &gt; 0 such that when n &gt; N 2 we have Z θ &gt; E[T n ]. Up to this point, we have proved that Z θ − &lt; E[T n ] &lt; Z θ .</formula><p>For the other half part of the proof, using Eq. 8 we have</p><formula xml:id="formula_12">E[T n ] = Z θ − µ 2 2µ 2 1 n + c n 2 + o(n −2 ) (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where c is a constant. Therefore,</p><formula xml:id="formula_14">E[(2n − 1)T n − 2(n − 1)T n−1 ] = (2n − 1)E[T n ] − 2(n − 1)E[T n−1 ] = Z θ + µ2 2µ 2 1 n + o(n −1 ). Therefore lim n→∞ E[(2n − 1)T n − 2(n − 1)T n−1 ] = Z θ , hence ∀ &gt; 0, ∃N 3 &gt; 0 such that ∀n &gt; N 3 E[(2n − 1)T n − 2(n − 1)T n−1 ] &lt; Z θ + . Furthermore, lim n→∞ n(E[(2n − 1)T n − 2(n − 1)T n−1 ] − Z θ ) = lim n→∞ µ2 2µ 2 + o(1) &gt; 0, so ∃N 4 &gt; 0 such that when n &gt; N 4 we have E[(2n − 1)T n − 2(n − 1)T n−1 &gt; Z θ .</formula><p>Putting the above together, ∀ &gt; 0, let N = max{N 1 , N 2 , N 3 , N 4 }, then ∀n &gt; N , The optimization settings are presented in Table <ref type="table" target="#tab_5">4</ref>. We subsampled 333 sentences from the test set of CC-News, and asked 3 Amazon Mechanical turkers to vote. We consider one continuation better if it gets more votes. To check the quality of the received ratings, we performed a qualification task beforehand, where one of the continuations is real text, and we kept the top half performing turkers for further evaluation (corresponding to higher than 66.67% accuracy for discriminating real from LM samples -for a total of 26 qualified turkers). Then in the actual experiment, we use one out of every four comparisons as an attention check and drop responses if the turker did not pass the check.</p><formula xml:id="formula_15">Z θ − &lt; E[T n ] &lt; Z θ &lt; E[(2n − 1)T n − 2(n − 1)T n−1 ] &lt; Z θ + A.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 QUALITATIVE ANALYSIS</head><p>We present generation examples when our approach BASE LM outperforms baseline BALM in Table <ref type="table" target="#tab_7">5</ref>, and when our approach underperforms in Table <ref type="table" target="#tab_9">6</ref>. Here the judgment is based on human evaluation when all three turkers unanimously voted in favor of one model over the other.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prefix</head><p>Model Generation but as the audio only came on halfway through the incident, it did not record his initial words to the girl or her response. It was also dropped in the hallway during their interaction, so it did not record any of what happened once the two left the library. Jones brought her outside to the library's front steps, where he waited for responding officers to arrive. They did not call for an ambulance, according to the lawsuit. Rather, when the officers arrived, they told her to spit blood on the grass instead of the concrete, and to move from the steps to a bench. Some 20 minutes BASE LM later, she was dead, according to the lawsuit. "It's not uncommon for an officer to spit blood on an unarmed, vulnerable adult. The use of spit is illegal in a public place," BALM later, Jones was arrested for disorderly conduct and released after posting bond, the suit said. She was arrested again on May 2. In the suit, Jones is seeking more than $1 million. A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BIT-BASE</head><p>after the officers' arrival, Jones and the girl left the library and headed to the parking lot of a nearby apartment complex. "This is the type of behavior that is most likely to cause harm to GOLD later, J.G's mother arrived and took her daughter to Cleveland Clinic in Lakewood, where she was treated for a dislocated jaw, a head injury, and an injury to her knee.</p><p>,  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>4. The number of parameters of each model is shown in parentheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Perplexity gain of JOINT BIT-MED and JOINT BIT-LARGE * (using BASE LM-24L) at each position relative to BASE LM-24L on the test set of CC-News. At each position the lower and upper bounds (Eq. 5 estimated using the method in Eq. 4, see §3.2 for more details) are estimated using 20,000 samples. The shorter the horizon (moving to the right), the tighter the estimation is but also the more limited the gains compared to base LM as un-normalized models are most useful on longer generations.</figDesc><graphic url="image-1.png" coords="7,72.62,315.45,431.39,217.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: PPL estimation for joint BIT-BASE on CC-News validation set as we vary the number of samples. Right: Percentage of Unique n-grams found in real data, samples from the joint model BIT-BASE and samples from the base language model. The joint sampling is done with 10,000 samples.</figDesc><graphic url="image-2.png" coords="9,117.90,81.86,178.20,133.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Density plot of log-probability scores using the base language model (left) or the joint model (right).The red curve corresponds to real samples, the black curve to samples from BASE LM and the green curve to samples from BIT-BASE. The joint model provides a much better fit than the base language model.</figDesc><graphic url="image-4.png" coords="9,117.90,259.80,178.20,133.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Screenshot of the human evaluation.</figDesc><graphic url="image-6.png" coords="16,109.98,81.86,392.06,177.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: Top-k Joint Sampling Input: number of samples n drawn from P LM , value of k in top-k // Get a set of samples from P LM sample n samples {x 1 , • • • , x n } from P LM with top-k sampling calculate energies</figDesc><table /><note>T t=1 P (x t |x &lt;t ), and evaluate the PPL for each generation step. By 2 From Ma &amp; Collins (2018) Assumption 2, for conditional NCE the model needs to be flexible enough such that the self-normalizing property can be satisfied conditioned on any prefix.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Table1where we compare models in terms of their perplexity. We can see that on both datasets, residual EBMs with causal attention JOINT UNIT outperforms the baseline RALM with approximately the same number of parameters. The LARGE* (LM-24L+355M) 12.71-12.77 12.10-12.16 13.30-13.34 15.17-15.22 Validation and test perplexity on CC-News and Toronto Book Corpus. * denotes models initialized with RoBERTa trained on additional data. The joint model perplexity ranges are estimated using 100,000 samples, see Eq.</figDesc><table><row><cell>Model (#parameters)</cell><cell>Val</cell><cell cols="2">CC-News</cell><cell>Test</cell><cell>Toronto Book Corpus Val Test</cell></row><row><cell>BASE LM (203M)</cell><cell>18.41</cell><cell></cell><cell></cell><cell>17.57</cell><cell>16.16</cell><cell>18.29</cell></row><row><cell>RALM (LM+203M)</cell><cell>17.01</cell><cell></cell><cell></cell><cell>16.17</cell><cell>15.71</cell><cell>17.85</cell></row><row><cell>BALM (408M)</cell><cell>16.50</cell><cell></cell><cell></cell><cell>15.74</cell><cell>15.00</cell><cell>16.99</cell></row><row><cell>JOINT UNIT (LM+203M)</cell><cell cols="2">16.42-16.44</cell><cell cols="2">15.57-15.58</cell><cell>15.12-15.13</cell><cell>16.98-17.00</cell></row><row><cell>JOINT BIT-BASE (LM+125M)</cell><cell cols="2">15.32-15.35</cell><cell cols="2">14.61-14.64</cell><cell>-</cell><cell>-</cell></row><row><cell>JOINT BIT-BASE* (LM+125M)</cell><cell cols="2">15.11-15.17</cell><cell cols="2">14.37-14.42</cell><cell>14.14-14.16</cell><cell>15.72-15.74</cell></row><row><cell>JOINT BIT-LARGE* (LM+355M)</cell><cell cols="5">14.59-14.61 13.97-14.00 13.80-13.83 15.33-15.36</cell></row><row><cell>BASE LM-24L (203M)</cell><cell>15.71</cell><cell></cell><cell></cell><cell>14.89</cell><cell>15.61</cell><cell>18.14</cell></row><row><cell>RALM-24L (LM-24L+203M)</cell><cell>15.70</cell><cell></cell><cell></cell><cell>14.89</cell><cell>15.63</cell><cell>18.17</cell></row><row><cell>BALM-24L (408M)</cell><cell>14.58</cell><cell></cell><cell></cell><cell>13.92</cell><cell>15.20</cell><cell>18.24</cell></row><row><cell>JOINT UNIT (LM-24L+203M)</cell><cell cols="2">14.59-14.61</cell><cell cols="2">13.81-13.82</cell><cell>15.12 − 15.16</cell><cell>17.46-17.48</cell></row><row><cell>JOINT BIT-BASE (LM-24L+125M)</cell><cell cols="2">13.68-13.69</cell><cell cols="2">13.01-13.03</cell><cell>-</cell><cell>-</cell></row><row><cell>JOINT BIT-BASE* (LM-24L+125M)</cell><cell cols="2">13.60-13.62</cell><cell cols="2">12.93-12.95</cell><cell>14.11-14.12</cell><cell>16.17-16.18</cell></row><row><cell>JOINT BIT-MED (LM-24L+203M)</cell><cell cols="2">12.97-13.01</cell><cell cols="2">12.38-12.42</cell><cell>-</cell><cell>-</cell></row><row><cell>JOINT BIT-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Human evaluation results on a subset of 333 sentences on the CC-News test set. The rate is computed as the percentage of sentences where the number of turkers preferring Model1 is strictly less than (denoted with &lt;) or not greater than (denoted with ≤) those preferring Model2. Attention check is used to drop some votes, so there might exist ties. p-value is based on single-sided binomial test.</figDesc><table><row><cell cols="3">Published as a conference paper at ICLR 2020</cell><cell></cell><cell></cell></row><row><cell>Model1 (baseline)</cell><cell></cell><cell>Model2 (compared model)</cell><cell>Rate</cell><cell>p-value</cell></row><row><cell>BASE LM</cell><cell></cell><cell>JOINT UNIT</cell><cell>52.85%</cell><cell>0.16</cell></row><row><cell>BASE LM</cell><cell></cell><cell>JOINT BIT-BASE</cell><cell>56.25%</cell><cell>0.015</cell></row><row><cell>BASE LM</cell><cell></cell><cell>JOINT BIT-LARGE*</cell><cell cols="2">58.93% 0.00084</cell></row><row><cell>BASE LM</cell><cell></cell><cell>BALM</cell><cell>46.77%</cell><cell>0.88</cell></row><row><cell>BALM</cell><cell>&lt;</cell><cell>JOINT UNIT</cell><cell>50.00%</cell><cell>0.52</cell></row><row><cell>BALM</cell><cell></cell><cell>JOINT BIT-BASE</cell><cell>57.89%</cell><cell>0.0027</cell></row><row><cell>BALM</cell><cell></cell><cell>JOINT BIT-LARGE*</cell><cell cols="2">59.89% 0.00020</cell></row><row><cell>BALM-24L</cell><cell></cell><cell>JOINT BIT-MED (24L)</cell><cell>56.23%</cell><cell>0.015</cell></row><row><cell>JOINT BIT-LARGE* (24L)</cell><cell></cell><cell>HUMAN</cell><cell>55.21%</cell><cell>0.036</cell></row><row><cell>BASE LM</cell><cell cols="2">≤ BALM</cell><cell>54.85%</cell><cell>0.050</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of P (xt|x&lt;t) between BASE LM and BIT-BASE on a few examples. Repetitions are marked with red. Only the top 5 probabilities are shown. Theorem 2. Denote T n as the empirical estimate of log E x∼P LM exp(−E(x)) with n samples x i ∼ P LM</figDesc><table><row><cell>A.2 PROOF OF THEOREM 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Optimization settings. We use the same setting for CC-News and Toronto Book Corpus.</figDesc><table><row><cell cols="2">OPTIMIZATION SETTINGS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">fp16 batch size warmup steps max steps</cell><cell>max lr</cell><cell>max grad norm</cell></row><row><cell>BASE LM</cell><cell>-</cell><cell>32</cell><cell>2,000</cell><cell>180,000</cell><cell>0.0001</cell><cell>10</cell></row><row><cell>RALM</cell><cell>-</cell><cell>64</cell><cell>2,000</cell><cell>180,000</cell><cell>0.0001</cell><cell>10</cell></row><row><cell>BALM</cell><cell>-</cell><cell>32</cell><cell>2,000</cell><cell>180,000</cell><cell>0.0001</cell><cell>10</cell></row><row><cell>JOINT UNIT</cell><cell>+</cell><cell>64</cell><cell>2,000</cell><cell>180,000</cell><cell>0.0003</cell><cell>10</cell></row><row><cell>JOINT BIT-BASE</cell><cell>-</cell><cell>60</cell><cell>2,000</cell><cell>90,000</cell><cell>0.00005</cell><cell>0.25</cell></row><row><cell>JOINT BIT-BASE*</cell><cell>-</cell><cell>60</cell><cell>2,000</cell><cell>90,000</cell><cell>0.00005</cell><cell>0.25</cell></row><row><cell>JOINT BIT-LARGE*</cell><cell>+</cell><cell>64</cell><cell>2,000</cell><cell>90,000</cell><cell>0.0003</cell><cell>10</cell></row><row><cell>BASE LM-24L</cell><cell>-</cell><cell>50</cell><cell>2,000</cell><cell>90,000</cell><cell>0.0003</cell><cell>0.25</cell></row><row><cell>RALM-24L</cell><cell>-</cell><cell>28</cell><cell>1,000</cell><cell>90,000</cell><cell>0.00015</cell><cell>0.25</cell></row><row><cell>BALM-24L</cell><cell>-</cell><cell>28</cell><cell>2,000</cell><cell>90,000</cell><cell>0.0003</cell><cell>0.25</cell></row><row><cell>JOINT UNIT (LM-24L)</cell><cell>+</cell><cell>64</cell><cell>2,000</cell><cell>180,000</cell><cell>0.0003</cell><cell>10</cell></row><row><cell>JOINT BIT-BASE (LM-24L)</cell><cell>-</cell><cell>60</cell><cell>2,000</cell><cell>90,000</cell><cell>0.00005</cell><cell>0.25</cell></row><row><cell>JOINT BIT-BASE* (LM-24L)</cell><cell>-</cell><cell>60</cell><cell>2,000</cell><cell>90,000</cell><cell>0.00005</cell><cell>0.25</cell></row><row><cell>JOINT BIT-MED (LM-24L)</cell><cell>-</cell><cell>32</cell><cell>2,000</cell><cell>90,000</cell><cell>0.00005</cell><cell>0.25</cell></row><row><cell>JOINT BIT-LARGE* (LM-24L)</cell><cell>-</cell><cell>20</cell><cell>2,000</cell><cell>90,000</cell><cell>0.00005</cell><cell>0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Example generations when BIT-BASE outperforms BALM according to human evaluation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Bronson said. "The initiative provides a variety of supports to early childhood programs' children, families and staff. The resources provided through this partnership increase the quality of the participating programs, which benefits the community and impacts our future in such a positive way," Scott said. Visit PNCGrowUpGreat.com/donorschoose. \nHere are Varsity sports headlines for April 13, 2018. Refresh to get the latest as we add news throughout the night as we collect scores: Best of the best in track and field Our Sentinel coverage area top performers lists for girls track and field BASE LM at the Varsity Track &amp; Field Invite.\nThe U.S. Army Corps of Engineers has approved $2 billion in construction work along the U.S.-Mexico boundary as a way to BALM . Check back frequently for updates. The Sentinel also has a feature on the boys basketball and wrestling teams. Boys golf The Sentinel boys golf and wrestling teams have been one of those teams who has had some</figDesc><table><row><cell></cell><cell></cell><cell>.\nA few weeks back, it's been rumored</cell></row><row><cell></cell><cell>BIT-</cell><cell>that the HTC Desire was going to be the</cell></row><row><cell></cell><cell>BASE</cell><cell>companys last flagship phone, and now, a</cell></row><row><cell></cell><cell></cell><cell>new leak has confirmed that it</cell></row><row><cell></cell><cell></cell><cell>and boys track and field are updated go-</cell></row><row><cell></cell><cell></cell><cell>ing into the Saturday district meets. The</cell></row><row><cell></cell><cell>GOLD</cell><cell>season is heating up with more district and region races coming up next week. Click</cell></row><row><cell></cell><cell></cell><cell>these links for girls top performers and</cell></row><row><cell></cell><cell></cell><cell>boys top</cell></row><row><cell></cell><cell></cell><cell>UAA), General Motors (NYSE:GM) on</cell></row><row><cell>leaders meeting November 17-18 in</cell><cell>BASE</cell><cell>November 4; and Procter &amp; Gamble</cell></row><row><cell>Papua New Guinea as potential</cell><cell>LM</cell><cell>(NYSE:PG) for October. On the retail</cell></row><row><cell>Xi-Trump meet dates. If all else fails,</cell><cell></cell><cell>front, Lowe's Companies (NYSE:L</cell></row><row><cell>Trump and Xi are also expected to</cell><cell></cell><cell>UA) on October 30; CVS Health (NAS-</cell></row><row><cell>meet for a bit at the G20 meeting at the end of November. On the economic</cell><cell>BALM</cell><cell>DAQ:CVS) on November 27; Intel Cor-poration (NASDAQ:INTC) on October 28;</cell></row><row><cell>calendar next week, the update on jobs</cell><cell></cell><cell>and Verizon Communications (NYSE:V</cell></row><row><cell>and the U.S. trade deficit are the</cell><cell></cell><cell>UAA), Johnson Controls (NYSE:JCI)</cell></row><row><cell>headliners on November 2. Notable</cell><cell>BIT-</cell><cell>and Cisco Systems (NASDAQ:CSCO) on</cell></row><row><cell>earnings reports: Akamai</cell><cell>BASE</cell><cell>November 6.\nA woman who had to have</cell></row><row><cell>Technologies (NASDAQ:AKAM),</cell><cell></cell><cell>her nose and mouth taped as punishment</cell></row><row><cell>Mondelez International</cell><cell></cell><cell></cell></row><row><cell>(NASDAQ:MDLZ) and Olin Corp.</cell><cell></cell><cell></cell></row><row><cell>(NYSE:OLN) on October 29; Under</cell><cell></cell><cell></cell></row><row><cell>Armour (NYSE:</cell><cell></cell><cell></cell></row></table><note>GOLDUAA), eBay (NASDAQ:EBAY), General Electric (NYSE:GE), Coca-Cola (NYSE:KO), Pfizer (NYSE:PFE) and Electronic Arts (NAS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Example generations when BIT-BASE underperforms BALM according to human evaluation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We assume a fixed T for simplicity of analysis and implementation, but our method generalizes to varying length generation with an end-of-sequence symbol.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Adapting to other types of local constraints such as nucleus sampling<ref type="bibr" target="#b13">(Holtzman et al., 2019)</ref> is straightforward.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Junbo Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, and Yann LeCun. Adversarially regularized autoencoders. In International Conference in Machine Learning, 2018. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06758</idno>
		<title level="m">Discriminator rejection sampling</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Real or fake? learning to discriminate machine from human generated text</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03351</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Une approche thorique de l&apos;apprentissage connexionniste: Applications la reconnaissance de la parole</title>
		<author>
			<persName><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Orsay, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universit de Paris XI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGNLL Conference on Computational Natural Language Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno>CoRR, abs/1903.08689</idno>
		<ptr target="http://arxiv.org/abs/1903.08689" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning generative convnets via multi-grid modeling and sampling</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junpei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9155" to="9164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bias correction of learned generative models using likelihood-free importance weighting</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09531</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="599" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Academy of Sciences of the USA</title>
				<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A generalization of sampling without replacement from a finite universe</title>
		<author>
			<persName><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donovan</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
		<title level="m">Raquel an d Fidler. Skip-thought vectors</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A tutorial on energy-based learning. Predicting Structured Outputs</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Jie</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods for Natural Language Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nagel</surname></persName>
		</author>
		<ptr target="http://web.archive.org/save/http://commoncrawl.org/2016/10/news-dataset-available/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning non-convergent nonpersistent short-run mcmc toward energy-based model</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5233" to="5243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Debiasing evidence approximations: On importance-weighted autoencoders and jackknife variational inference</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<ptr target="https://statweb.stanford.edu/˜owen/mc/.chapter9" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<meeting><address><addrLine>Art B. Owen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2018. 2013</date>
		</imprint>
	</monogr>
	<note>Monte Carlo theory, methods and examples</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global autoregressive models for data-efficient sequence learning</title>
		<author>
			<persName><forename type="first">Tetiana</forename><surname>Parshakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Andreoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unified energy-based framework for unsupervised learning</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11-th International Workshop on Artificial Intelligence and Statistics (AISTATS)</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Whole-sentence exponential language models: a vehicle for linguistic-statistical integration</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="73" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative reranking for machine translation</title>
		<author>
			<persName><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Energy-based models for sparse overcomplete representations</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1235" to="1260" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language modeling with neural trans-dimensional random fields</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="294" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved training of neural trans-dimensional random field language models with dynamic noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page" from="70" to="76" />
			<date type="published" when="2018">2018. 2018a</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning neural trans-dimensional random field language models with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="6134" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Trans-dimensional random fields for language modeling</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning trans-dimensional random fields with applications to language modeling</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="876" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exponential family harmoniums with an application to information retrieval</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingnian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Synthesizing dynamic patterns by spatialtemporal generative convnet</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
				<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7093" to="7101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning descriptor networks for 3d shape synthesis and analysis</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8629" to="8638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning energy-based spatial-temporal generative convnets for dynamic patterns</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
