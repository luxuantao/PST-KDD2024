<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Schrasing</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">James</forename><surname>Glass</surname></persName>
							<email>glass@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76E497ECFAFCA0E46F1E0831F3599664</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the experimental results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low-or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings-continuous-valued vector representations of words-are almost ubiquitous in recent natural language processing research. Most successful methods for learning word embeddings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> rely on the distributional hypothesis <ref type="bibr" target="#b3">[4]</ref>, i.e., words occurring in similar contexts tend to have similar meanings. Exploiting word co-occurrence statistics in a text corpus leads to word vectors that reflect semantic similarities and dissimilarities: similar words are geometrically close in the embedding space, and conversely, dissimilar words are far apart.</p><p>Continuous word embedding spaces have been shown to exhibit similar structures across languages <ref type="bibr" target="#b4">[5]</ref>. The intuition is that most languages share similar expressive power and are used to describe similar human experiences across cultures; hence, they should share similar statistical properties. Inspired by the notion, several studies have focused on designing algorithms that exploit this similarity to learn a cross-lingual alignment between the embedding spaces of two languages, where the two embedding spaces are trained from independent text corpora <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. In particular, recent research has shown that such cross-lingual alignments can be learned without relying on any form of bilingual supervision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, and has been applied to training neural machine translation (NMT) systems in a completely unsupervised fashion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. This eliminates the need for a large parallel training corpus to train NMT systems. Speech, as another form of language, is rarely considered as a source for learning semantics, compared to text. Although there is work that explores the concept of learning vector representations from speech <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, they are primarily based on acoustic-phonetic similarity, and aim to represent the way a word sounds rather than its meaning.</p><p>Recently, the Speech2Vec <ref type="bibr" target="#b23">[24]</ref> model was developed to be capable of representing audio segments excised from a speech corpus as fixed dimensional vectors that contain semantic information of the underlying spoken words. The design of Speech2Vec is based on a Recurrent Neural Network (RNN) Encoder-Decoder framework <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, and borrows the methodology of Skip-grams or continuous bag-of-words (CBOW) from Word2Vec <ref type="bibr" target="#b0">[1]</ref> for training. Since Speech2Vec and Word2Vec share the same training methodology and speech and text are similar media for communicating, the two embedding spaces learned respectively by Speech2Vec from speech and Word2Vec from text are expected to exhibit similar structure.</p><p>Motivated by the recent success in unsupervised cross-lingual alignment <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> and the assumption that the embedding spaces of the two modalities (speech and text) share similar structure, we are interested in learning an unsupervised cross-modal alignment between the two spaces. Such an alignment would be useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low-or zero-resource languages that lack parallel corpora of speech and text for training. In this paper, we propose a framework for unsupervised cross-modal alignment, borrowing the methodology from unsupervised cross-lingual alignment presented in <ref type="bibr" target="#b13">[14]</ref>. The framework consists of two steps. First, it uses Speech2Vec <ref type="bibr" target="#b23">[24]</ref> and Word2Vec <ref type="bibr" target="#b0">[1]</ref> to learn the individual embedding spaces of speech and text. Next, it leverages adversarial training to learn a linear mapping from the speech embedding space to the text embedding space, followed by a refinement procedure.</p><p>The paper is organized as follows. Section 2 describes how we obtain the speech embedding space in a completely unsupervised manner using Speech2Vec. Next, we present our unsupervised cross-modal alignment approach in Section 3. In Section 4, we describe the tasks of spoken word classification and translation, which are similar to ASR and speech-to-text translation, respectively, except that now the input are audio segments corresponding to words. We then evaluate the performance of our unsupervised alignment on the two tasks and analyze our results in Section 5. Finally, we conclude and point out some interesting future work possibilities in Section 6. To the best of our knowledge, this is the first work that achieves fully unsupervised spoken word classification and translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Unsupervised Learning of the Speech Embedding Space</head><p>Recently, there is an increasing interest in learning the semantics of a language directly, and only from raw speech <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Assuming utterances in a speech corpus are already pre-segmented into audio segments corresponding to words using word boundaries obtained by forced alignment, existing approaches aim to represent each audio segment as a fixed dimensional embedding vector, with the hope that the embedding is able to capture the semantic information of the underlying spoken word. However, some supervision leaks into the learning process through the use of forced alignment, rendering the approaches not fully unsupervised.</p><p>In this paper, we use Speech2Vec <ref type="bibr" target="#b23">[24]</ref>, a recently proposed deep neural network architecture that has been shown capable of capturing the semantics of spoken words from raw speech, for learning the speech embedding space. To eliminate the need of forced alignment, we propose a simple pipeline for training Speech2Vec in a totally unsupervised manner. We briefly review Speech2Vec in Section 2.1, and introduce the unsupervised pipeline in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Speech2Vec</head><p>In text, a Word2Vec <ref type="bibr" target="#b0">[1]</ref> model is a shallow, two-layer fully-connected neural network that is trained to reconstruct the contexts of words. There are two methodologies for training Word2Vec: Skip-grams and CBOW. The objective of Skip-grams is for each word w (n) in a text corpus, the model is trained to maximize the probability of words {w (n-k) , . . . , w (n-1) , w (n+1) , . . . , w (n+k) } within a window of size k given w (n) . The objective of CBOW, on the other hand, aims to infer the current word w (n) from its nearby words {w (n-k) , . . . , w (n-1) , w (n+1) , . . . , w (n+k) }.</p><p>Speech2Vec <ref type="bibr" target="#b23">[24]</ref>, inspired by Word2Vec, borrows the methodology of Skip-grams or CBOW for training. Unlike text, where words are represented by one-hot vectors as input and output for training Word2Vec, an audio segment is represented by a variable-length sequence of acoustic features, x = (x 1 , x 2 , . . . , x T ), where x t is the acoustic feature such as Mel-Frequency Cepstral Coefficients at time t, and T is the length of the sequence. In order to handle variable-length input and output sequences of acoustic features, Speech2Vec replaces the two fully-connected layers in the Word2Vec model with a pair of RNNs, one as an Encoder and the other as a Decoder <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. When training Speech2Vec with Skip-grams, the Encoder RNN takes the audio segment (corresponding to the current word) as input and encodes it into a fixed dimensional embedding z (n) that represents the entire input sequence x (n) . Subsequently, the Decoder RNN aims to reconstruct the audio segments {x (n-k) , . . . , x (n-1) , x (n+1) , . . . , x (n+k) } (corresponding to nearby words) within a window of size k from z (n) . Similar to the concept of training Word2Vec with Skip-grams, the intuition behind this methodology is that, in order to successfully decode nearby audio segments, the encoded embedding z (n) should contain sufficient semantic information of the current audio segment x (n) . In contrast to training Speech2Vec with Skip-grams that aims to predict nearby audio segments from z (n) , training Speech2Vec with CBOW sets x (n) as the target and aims to infer it from nearby audio segments. By using the same training methodology (Skip-grams or CBOW) as Word2Vec, it is reasonable to assume that the embedding space learned by Speech2Vec from speech exhibits similar structure to that learned by Word2Vec from text.</p><p>After training the Speech2Vec model, each audio segment is transformed into an embedding vector that contains the semantic information of the underlying word. In a Word2Vec model, the embedding for a particular word is deterministic, which means that every instance of the same word will be represented by one, and only one, embedding vector. In contrast, for audio segments every instance of a spoken word is different (due to speaker, channel, and other contextual differences, etc.), so every instance of the same underlying word is represented by a different (though hopefully similar) embedding vector. Embedding vectors of the same spoken words can be averaged to obtain a single word embedding based on the identity of each audio segment, as is done in <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Speech2Vec</head><p>Speech2Vec and Word2Vec learn the semantics of words by making use of the co-occurrence information in their respective modalities, and are both intrinsically unsupervised. However, unlike text where the content can be easily segmented into word-like units, speech has a continuous form by nature, making the word boundaries challenging to locate. All utterances in the speech corpus are assumed to be perfectly segmented into audio segments based on the word boundaries obtained by forced alignment with respect to the reference transcriptions <ref type="bibr" target="#b23">[24]</ref>. Such an assumption, however, makes the process of learning word embeddings from speech not truly unsupervised.</p><p>Unsupervised speech segmentation is a core problem in zero-resource speech processing in the absence of transcriptions, lexicons, or language modeling text. Early work mainly focused on unsupervised term discovery, where the aim is to find word-or phrase-like patterns in a collection of speech <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. While useful, the discovered patterns are typically isolated segments spread out over the data, leaving much speech as background. This has prompted several studies on full-coverage approaches, where the entire speech input is segmented into word-like units <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>In this paper, we use an off-the-shelf, full-coverage, unsupervised segmentation system for segmenting our data into word-like units. Three representative systems are explored in this paper. The first one, referred to as Bayesian embedded segmental Gaussian mixture model (BES-GMM) <ref type="bibr" target="#b34">[35]</ref>, is a probabilistic model that represents potential word segments as fixed-dimensional acoustic word embeddings <ref type="bibr" target="#b22">[23]</ref>, and builds a whole-word acoustic model in this embedding space while jointly doing segmentation. The second one, called embedded segmental K-means model (ES-KMeans) <ref type="bibr" target="#b35">[36]</ref>, is an approximation to BES-GMM that uses hard clustering and segmentation, rather than full Bayesian inference. The third one is the recurring syllable-unit segmenter called SylSeg <ref type="bibr" target="#b36">[37]</ref>, a fast and heuristic method that applies unsupervised syllable segmentation and clustering, to predict recurring syllable sequences as words.</p><p>After training the Speech2Vec model using the audio segments obtained by an unsupervised segmentation method, each audio segment is then transformed into an embedding that contains the semantic information about the segment. Since we do not know the identity of the embeddings, we use the k-means algorithm to cluster them into K clusters, potentially corresponding to K different word types. We then average all embeddings that belong to the same cluster (potentially the instances of the same underlying word) to obtain a single embedding. Note that by doing so, it is possible that we group the embeddings corresponding to different words that are semantically similar into one cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Alignment of Speech and Text Embedding Spaces</head><p>Suppose we have speech and text embedding spaces trained on independent speech and text corpora. Our goal is to learn a mapping, without using any form of cross-modal supervision, between them such that the two spaces are aligned.</p><p>Let S = {s 1 , s 2 , . . . , s m } ⊆ R d1 and T = {t 1 , t 2 , . . . , t n } ⊆ R d2 be two sets of m and n word embeddings of dimensionality d 1 and d 2 from the speech and text embedding spaces, respectively. Ideally, if we have a known dictionary that specifies which s i ∈ S corresponds to which t j ∈ T , we can learn a linear mapping W between the two embedding spaces such that</p><formula xml:id="formula_0">W * = argmin W ∈R d 2 ×d 1 W X -Y 2 ,<label>(1)</label></formula><p>where X and Y are two aligned matrices of size d 1 × k and d 2 × k formed by k word embeddings selected from S and T , respectively. At test time, the transformation result of any audio segment a in the speech domain can be defined as argmax tj ∈T cos(W s a , t j ). In this paper, we show how to learn this mapping W without using any cross-modal supervision. The proposed framework, inspired by <ref type="bibr" target="#b13">[14]</ref>, consists of two steps: domain-adversarial training for learning an initial proxy of W , followed by a refinement procedure which uses the words that match the best to create a synthetic parallel dictionary for applying Equation <ref type="formula" target="#formula_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain-Adversarial Training</head><p>The intuition behind this step is to make the mapped S and T indistinguishable. We define a discriminator, whose goal is to discriminate between elements randomly sampled from W S = {W s 1 , W s 2 , . . . , W s m } and T . The mapping W , which can be viewed as the generator, is trained to prevent the discriminator from making accurate predictions. This is a two-player game, where the discriminator aims at maximizing its ability to identify the origin of an embedding, and W aims at preventing the discriminator from doing so by making W S and T as similar as possible. Given the mapping W , the discriminator, parameterized by θ D , is optimized by minimizing the following objective function:</p><formula xml:id="formula_1">L D (θ D |W ) = - 1 m m i=1 log P θ D (speech = 1|W s i ) - 1 n n j=1 log P θ D (speech = 0|t j ),<label>(2)</label></formula><p>where P θ D (speech = 1|v) is the probability that vector v originates from the speech embedding space (as opposed to an embedding from the text embedding space). Given the discriminator, the mapping W aims to fool the discriminator's ability to accurately predict the original domain of the embeddings by minimizing the following objective function:</p><formula xml:id="formula_2">L W (W |θ D ) = - 1 m m i=1 log P θ D (speech = 0|W s i ) - 1 n n j=1 log P θ D (speech = 1|t j )<label>(3)</label></formula><p>The discriminator θ D and the mapping W are optimized iteratively to respectively minimize L D and L W following the standard training procedure of adversarial networks <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Refinement Procedure</head><p>The domain-adversarial training step learns a rotation matrix W that aligns the speech and text embedding spaces. To further improve the alignment, we use the W learned in the domain-adversarial training step as an initial proxy and build a synthetic parallel dictionary that specifies which s i ∈ S corresponds to which t j ∈ T .</p><p>To ensure a high-quality dictionary, we consider the most frequent words from S and T , since more frequent words are expected to have better quality of embedding vectors, and only retain their mutual nearest neighbors. For deciding mutual nearest neighbors, we use the Cross-Domain Similarity Local Scaling proposed in <ref type="bibr" target="#b13">[14]</ref> to mitigate the so-called hubness problem <ref type="bibr" target="#b38">[39]</ref> (points tending to be nearest neighbors of many points in high-dimensional spaces). Subsequently, we apply Equation 1 on this generated dictionary to refine W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Spoken Word Classification and Translation</head><p>Conventional hybrid ASR systems <ref type="bibr" target="#b39">[40]</ref> and recent end-to-end ASR models <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> rely on a large amount of parallel audio-text data for training. However, most languages spoken across the world lack parallel data, so it is no surprise that only very few languages support ASR. It is the same story for speech-to-text translation <ref type="bibr" target="#b44">[45]</ref>, which typically pipelines ASR and machine translation, and could be even more challenging to develop as it requires both components to be well trained.</p><p>Compared to parallel audio-text data, the cost of accumulating independent corpora of speech and text is significantly lower. With our unsupervised cross-modal alignment approach, it becomes feasible to build ASR and speech-to-text translation systems using independent corpora of speech and text only, a setting suitable for low-or zero-resource languages.</p><p>Since a cross-modal alignment is learned to link the word embedding spaces of speech and text, we perform the tasks of spoken word classification and translation to directly evaluate the effectiveness of the alignment. The two tasks are similar to standard ASR and speech-to-text translation, respectively, except that now the input is an audio segment corresponding to a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spoken Word Classification</head><p>The goal of this task is to recognize the underlying spoken word of an input audio segment. Suppose we have two independent corpora of speech and text that belong to the same language. The speech and text embedding spaces, denoted by S and T , can be obtained by training Speech2Vec and Word2Vec on the respective corpus. The alignment W between S and T can be learned in an either supervised or unsupervised way. At test time, given an input audio segment, it is first transformed into an embedding vector s in the speech embedding space S by Speech2Vec. The vector s is then mapped to the text embedding space as t s = W s ∈ T . In T , the word that has embedding vector t * = argmax t∈T cos(t, t s ) closest to t s will be taken as the classification result. The performance is measured by accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spoken Word Translation</head><p>This task is similar to the one in the text domain that considers the problem of retrieving the translation of given source words, except that the source words are in the form of audio segments. Spoken word translation can be performed in the exact same way as spoken word classification, but the speech and text corpora belong to different languages. At test time, we follow the standard practice of word translation and measure how many times one of the correct translations (in text) of the input audio segment is retrieved, and report precision@ k for k = 1 and 5. We use the bilingual dictionaries provided by <ref type="bibr" target="#b13">[14]</ref> to obtain the correct translations of a given source word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we empirically demonstrate the effectiveness of our unsupervised cross-modal alignment approach on spoken word classification and translation introduced in Section 4. For our experiments, we used English and French LibriSpeech <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, and English and German Spoken Wikipedia Corpora (SWC) <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>All corpora are read speech, and come with a collection of utterances and the corresponding transcriptions. For convenience, we denote the speech and text data of a corpus in uppercase and lowercase, respectively. For example, EN swc and en swc represent the speech and text data, respectively, of English SWC. In Table <ref type="table" target="#tab_0">1</ref>, column Train is the size of the speech data used for training the speech embeddings; column Test is the size of the speech data used for testing, where the corresponding number of audio segments (i.e., spoken word tokens) is specified in column Segments; column Words provides the number of distinct words in that corpus. Train and test sets are split in a way so that there are no overlapping speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Details of Training and Model Architectures</head><p>The speech embeddings were trained using Speech2Vec with Skip-grams by setting the window size k to three. The Encoder is a single-layer bidirectional LSTM, and the Decoder is a single-layer unidirectional LSTM. The model was trained by stochastic gradient descent (SGD) with a fixed learning rate of 10 -3 . The text embeddings were obtained by training Word2Vec on the transcriptions using the fastText implementation without subword information <ref type="bibr" target="#b2">[3]</ref>. The dimension of both speech and text embeddings is 50. <ref type="foot" target="#foot_0">1</ref>For the adversarial training, the discriminator was a two-layer neural network of size 512 with ReLU as the activation function. Both the discriminator and W were trained SGD with a fixed learning rate of 10 -3 . For the refinement procedure, we used the default setting specified in <ref type="bibr" target="#b13">[14]</ref>. 2    Alignment-Based Approaches Given the speech and text embeddings, alignment-based approaches learn the alignment between them in an either supervised or unsupervised way; for an input audio segment, they perform spoken word classification and translation as described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparing Methods</head><p>By varying how word segments were obtained before being fed to Speech2Vec and how the embeddings were grouped together, the level of supervision is gradually decreased towards a fully unsupervised configuration. In configuration A, the speech training data was segmented into words using forced alignment with respect to the reference transcription, and the embeddings of the same word were grouped together using their word identities. In configuration B, the word segments were also obtained by forced alignment, but the embeddings were grouped together by performing k-means clustering. In configurations C, D, and E, the speech training data was segmented into word-like units using different unsupervised segmentation algorithms described in Section 2.2. Configuration F serves as a baseline by naively segmenting the speech training data into equally sized chunks. Unlike configurations A and B, configurations C, D, E, and F did not require the reference transcriptions to do forced alignment and the embeddings were grouped together by performing k-means clustering, and are thus unsupervised. Configurations A to F all used our unsupervised alignment approach to align the speech and text embedding spaces.</p><p>We also implemented configuration A * , which trained Speech2Vec in the same way as configuration A, but learned the alignment using a parallel dictionary as cross-modal data supervision. The different configurations are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Classifier</head><p>We established an upper bound by using the fully-supervised Word Classifier that was trained to map audio segments directly to their corresponding word identities. The Word Classifier was composed of a single-layer bidirectional LSTM with a softmax layer appended at the output of its last time step. This approach is specific to spoken word classification.</p><p>Majority Word Baseline For both spoken word classification and translation tasks, we implemented a straightforward baseline dubbed Major-Word, where for classification, it always predicts the most frequent word, and for translation, it always predicts the most commonly paired word. Results of the Major-Word offer us insight into the word distribution of the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and Discussion</head><p>Table <ref type="table">3</ref>: Accuracy on spoken word classification. EN ls -en swc means that the speech and text embeddings were learned from the speech training data of English LibriSpeech and text training data of English SWC, respectively, and the testing audio segments came from English LibriSpeech. The same rule applies to Table <ref type="table">5</ref> and Table <ref type="table">6</ref>. For the Word Classifier, EN ls -en swc and EN swc -en ls could not be obtained since it requires parallel audio-text data for training. Spoken Word Classification Table <ref type="table">3</ref> presents our results on spoken word classification. We observe that the accuracy decreases as the level of supervision decreases, as expected. We also note that although the Word Classifier significantly outperforms all the other approaches under all corpora settings, the prerequisite for training such a fully-supervised approach is unrealistic-it requires the utterances to be perfectly segmented into audio segments corresponding to words with the word identity of each segment known. We emphasize that the Word Classifier is just used to establish an upper bound performance that gives us an idea on how good the classification results could be.</p><p>For alignment-based approaches, configuration A * achieves the highest accuracies under all corpora settings by using a parallel dictionary as cross-modal supervision for learning the alignment. However, we see that configuration A using our unsupervised alignment approach only suffers a slight decrease in performance, which demonstrates that our unsupervised alignment approach is almost as effective as it supervised counterpart A * . As we move towards unsupervised methods (k-means clustering) for grouping embeddings, in configuration B, a decrease in performance is observed.</p><p>The performance of using unsupervised segmentation algorithms is behind using exact word segments for training Speech2Vec, shown in configurations C, D, and E versus B. We hypothesize that word segmentation is a critical step, since incorrectly separated words lack a logical embedding, which in turn hinders the clustering process. The importance of proper segmentation is evident in configuration F as it performs the worst.</p><p>The aforementioned analysis applies to different corpora settings. We also observe that the performance of the embeddings learned from different corpora is inferior to the ones learned from the same corpus (refer to columns 1 and 3, versus 5 and 6, in Table <ref type="table">3</ref>). We think this is because the embedding spaces learned from the same corpora (e.g., both embeddings were learned from LibriSpeech) exhibit higher similarity than those learned from different corpora, making the alignment more accurate.</p><p>Spoken Word Synonyms Retrieval Word classification does not display the full potential of our alignment approach. In Table <ref type="table" target="#tab_3">4</ref> we show a list of retrieved results of example input audio segments. The words were ranked according to the cosine similarity between their embeddings and that of the audio segment mapped from the speech embedding space. We observe that the list actually contain both synonyms and different lexical forms of the audio segment. This provides an explanation of why the performance of alignment-based approaches on word classification is poor: the top ranked word may not match the underlying word of the input audio segment, and would be considered incorrect for word classification, despite that the top ranked word has high chance of being semantically similar to the underlying word. We define word synonyms retrieval to also consider synonyms as valid results, as opposed to the word classification. The synonyms were derived using another language as a pivot. Using the cross-lingual dictionaries provided by <ref type="bibr" target="#b13">[14]</ref>, we looked up the acceptable word translations, and for each of those translations, we took the union of their translations back to the original language. For example, in English, each word has 3.3 synonyms (excluding itself) on average. Table <ref type="table">5</ref> shows the results of word synonyms retrieval. We see that our approach performs better at retrieving synonyms than classifying words, an evidence that the system is learning the semantics rather than the identities of words. This showcases the strength of our semantics-focused approach.</p><p>Table <ref type="table">5</ref>: Results on spoken word synonyms retrieval. We measure how many times one of the synonyms of the input audio segment is retrieved, and report precision@k for k = 1, 5.</p><p>Corpora EN ls -en ls FR ls -fr ls EN swc -en swc DE swc -de swc EN ls -en swc EN swc -en ls Average P@k P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5</p><p>Alignment-based approach with cross-modal supervision (parallel dictionary) Spoken word translation Table <ref type="table">6</ref> presents the results on spoken word translation. Similar to spoken word classification, configurations with more supervision yield better performance than those with less supervision. Furthermore, we observe that translating using the same corpus outperforms those using different corpora (refer to EN swc -de swc versus EN ls -de swc ). We attribute this to the higher structural similarity between the embedding spaces learned from the same corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a framework capable of aligning speech and text embedding spaces in an unsupervised manner. The method learns the alignment from independent corpora of speech and text, without requiring any cross-modal supervision, which is especially important for low-or zeroresource languages that lack parallel data with both audio and text. We demonstrate the effectiveness of our unsupervised alignment by showing comparable results to its supervised alignment counterpart Table <ref type="table">6</ref>: Results on spoken word translation. We measure how many times one of the correct translations of the input audio segment is retrieved, and report precision@k for k = 1, 5.</p><p>Corpora EN ls -fr ls FR ls -en ls EN swc -de swc DE swc -en swc EN ls -de swc FR ls -de swc Average P@k P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5</p><p>Alignment-based approach with cross-modal supervision (parallel dictionary) that uses full cross-modal supervision (A vs. A * ) on the tasks of spoken word classification and translation. Future work includes devising unsupervised speech segmentation approaches that produce more accurate word segments, an essential step to obtain high quality speech embeddings. We also plan to extend current spoken word classification and translation systems to perform standard ASR and speech-to-text translation, respectively.</p><formula xml:id="formula_3">A</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The detailed statistics of the corpora.</figDesc><table><row><cell>Corpus</cell><cell cols="2">Train Test Words Segments</cell></row><row><cell cols="2">English LibriSpeech 420 hr 50 hr 37K</cell><cell>468K</cell></row><row><cell cols="2">French LibriSpeech 200 hr 30 hr 26K</cell><cell>260K</cell></row><row><cell>English SWC</cell><cell>355 hr 40 hr 25K</cell><cell>284K</cell></row><row><cell>German SWC</cell><cell>346 hr 40 hr 31K</cell><cell>223K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Different configurations for training Speech2Vec to obtain the speech embeddings with decreasing level of supervision. The last column specifies whether the configuration is unsupervised.</figDesc><table><row><cell>Configuration</cell><cell></cell><cell>Speech2Vec training</cell><cell>Unsupervised</cell></row><row><cell></cell><cell cols="2">How word segments were obtained How embeddings were grouped together</cell></row><row><cell>A &amp; A  *</cell><cell>Forced alignment</cell><cell>Use word identity</cell></row><row><cell>B</cell><cell>Forced alignment</cell><cell>k-means</cell></row><row><cell>C</cell><cell>BES-GMM [35]</cell><cell>k-means</cell></row><row><cell>D</cell><cell>ES-KMeans [36]</cell><cell>k-means</cell></row><row><cell>E</cell><cell>SylSeg [37]</cell><cell>k-means</cell></row><row><cell>F</cell><cell>Equally sized chunks</cell><cell>k-means</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>CorporaEN ls -en ls FR ls -fr ls EN swc -en swc DE swc -de swc EN ls -en swc EN swc -en ls</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Nonalignment-based approach</cell><cell></cell><cell></cell></row><row><cell>Word Classifier</cell><cell>89.3</cell><cell>83.6</cell><cell>86.9</cell><cell>80.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="5">Alignment-based approach with cross-modal supervision (parallel dictionary)</cell><cell></cell></row><row><cell>A  *</cell><cell>25.4</cell><cell>27.1</cell><cell>29.1</cell><cell>26.9</cell><cell>21.8</cell><cell>23.9</cell></row><row><cell></cell><cell cols="5">Alignment-based approaches without cross-modal supervision (our approach)</cell><cell></cell></row><row><cell>A</cell><cell>23.7</cell><cell>24.9</cell><cell>25.3</cell><cell>25.8</cell><cell>18.3</cell><cell>21.6</cell></row><row><cell>B</cell><cell>19.4</cell><cell>20.7</cell><cell>22.6</cell><cell>21.5</cell><cell>15.9</cell><cell>17.4</cell></row><row><cell>C</cell><cell>10.9</cell><cell>12.6</cell><cell>14.4</cell><cell>13.1</cell><cell>6.9</cell><cell>8.0</cell></row><row><cell>D</cell><cell>11.5</cell><cell>12.3</cell><cell>14.2</cell><cell>12.4</cell><cell>7.5</cell><cell>8.3</cell></row><row><cell>E</cell><cell>6.5</cell><cell>7.2</cell><cell>8.9</cell><cell>7.4</cell><cell>4.5</cell><cell>5.9</cell></row><row><cell>F</cell><cell>0.8</cell><cell>1.4</cell><cell>2.8</cell><cell>1.2</cell><cell>0.2</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Majority Word Baseline</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Major-Word</cell><cell>0.3</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.3</cell><cell>0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Retrieved results of example audio segments that are considered incorrect in word classification. The match for each audio segment is marked in bold.</figDesc><table><row><cell>Rank</cell><cell></cell><cell cols="2">Input audio segments</cell><cell></cell></row><row><cell></cell><cell>beautiful</cell><cell>clever</cell><cell>destroy</cell><cell>suitcase</cell></row><row><cell>1</cell><cell>lovely</cell><cell cols="2">cunning destroyed</cell><cell>bags</cell></row><row><cell>2</cell><cell>pretty</cell><cell>smart</cell><cell>destroy</cell><cell>suitcases</cell></row><row><cell>3</cell><cell>gorgeous</cell><cell>clever</cell><cell>annihilate</cell><cell>luggage</cell></row><row><cell>4</cell><cell>beautiful</cell><cell>crafty</cell><cell cols="2">destroying briefcase</cell></row><row><cell>5</cell><cell>nice</cell><cell>wisely</cell><cell>destruct</cell><cell>suitcase</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We tried window size k ∈ {1,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2, 3, 4, 5} and embedding dimension d ∈ {50, 100, 200, 300} and found that the reported k and d yield the best performance<ref type="bibr" target="#b1">2</ref> We also tried multi-layer neural network to model W . However, we did not observe any improvement on our evaluation tasks when using it compared to a linear W . This discovery aligns with<ref type="bibr" target="#b4">[5]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Hao Tang, Mandy Korpusik, and the MIT Spoken Language Systems Group for their helpful feedback and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Distributional structure</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954">1954</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hammerla</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A distribution-based model to learn bilingual word embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning crosslingual word embeddings without bilingual corpora</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Earth mover&apos;s distance minimization for unsupervised bilingual lexicon induction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view recurrent neural acoustic word embeddings</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative acoustic word embeddings: Recurrent neural networkbased approaches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SLT</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional acoustic word using word-pair side information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Word embeddings for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings</title>
		<author>
			<persName><forename type="first">K</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Towards unsupervised automatic speech recognition trained by unaligned speech and text only</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10952</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning word embeddings from speech</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS ML4Audio Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised pattern discovery in speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient spoken term discovery using randomized algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised word segmentation and lexicon discovery using acoustic word embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="679" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised lexicon discovery from acoustic input</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>O'donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="389" to="403" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint training of non-negative tucker decomposition and discrete density hidden markov models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="969" to="988" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A hierarchical system for word discovery exploiting dtw-based initialization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Korthals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A segmental framework for fully-unsupervised large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="154" to="174" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An embedded segmental k-means model for unsupervised segmentation and clustering of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised word discovery from speech using automatic segmentation into syllable-like units</title>
		<author>
			<persName><forename type="first">O</forename><surname>Räsänen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spoken language translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fugen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">LibriSpeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Augmenting Librispeech with French translations: A multimodal corpus for direct speech translation evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kraif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mining the spoken wikipedia for speech data and beyond</title>
		<author>
			<persName><forename type="first">A</forename><surname>Köhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
