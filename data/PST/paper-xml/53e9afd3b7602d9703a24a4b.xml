<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dead-Block Prediction &amp; Dead-Block Correlating Prefetchers</title>
				<funder>
					<orgName type="full">Intel corporation</orgName>
				</funder>
				<funder ref="#_zVWevQ2">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">An-Chow</forename><surname>Lai</surname></persName>
							<email>laia@ecn.purdue@edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cem</forename><surname>Fide</surname></persName>
							<email>cem.fide@eng.sun</email>
							<affiliation key="aff1">
								<address>
									<addrLine>Sun Microsystems 901 San Antonio Rd</addrLine>
									<postCode>94303</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dead-Block Prediction &amp; Dead-Block Correlating Prefetchers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective data prefetching requires accurate mechanisms to predict both "which" cache blocks to prefetch and "when" to prefetch them. This paper proposes the Dead-Block Predictors (DBPs), trace-based predictors that accurately identify "when" an L1 data cache block becomes evictable or "dead". Predicting a dead block significantly enhances prefetching lookahead and opportunity, and enables placing data directly into L1, obviating the need for auxiliary prefetch buffers. This paper also proposes Dead-Block Correlating Prefetchers (DBCPs), that use address correlation to predict "which" subsequent block to prefetch when a block becomes evictable. A DBCP enables effective data prefetching in a wide spectrum of pointerintensive, integer, and floating-point applications.</p><p>We use cycle-accurate simulation of an out-of-order superscalar processor and memory-intensive benchmarks to show that: (1) dead-block prediction enhances prefetching lookahead at least by an order of magnitude as compared to previous techniques, (2) a DBP can predict dead blocks on average with a coverage of 90% only mispredicting 4% of the time, (3) a DBCP offers an address prediction coverage of 86% only mispredicting 3% of the time, and (4) DBCPs improve performance by 62% on average and 282% at best in the benchmarks we studied.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Increasing processor clock speeds along with microarchitectural innovation have led to a tremendous gap between processor and memory performance. Architects have primarily relied on deeper cache hierarchies, where each level trades off faster lookup speed for larger capacity, to reduce this performance gap. Conventional cache hierarchies employ a demand-fetch memory access model, in which data are fetched into higher levels upon processor requests. Unfortunately, the limited capacity in higher cache levels and the simple data placement mechanisms used in conventional hierarchies often result in high miss rates and reduce performance. While superscalar engines</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Babak Falsafi Electrical &amp; Computer Engineering</head><p>Carnegie Mellon University Pittsburgh, PA 15213 babak@ece.cmu.edu http://www.ece.cmu.edu/-impetus with non-blocking caches <ref type="bibr" target="#b18">[19]</ref> allow overlapping the miss latency among the higher cache levels, limited available instruction-level parallelism and long access latencies to lower cache levels often expose the miss latency in many important classes of applications.</p><p>Many architects have additionally relied on the prefetch memory access model to mitigate the shortcomings of the demand-fetch model. Prefetching helps fetch data in advance to hide the memory latency by predicting future memory requests. While prefetching can be initiated in either hardware <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> or software <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>, many researchers and vendors opt for hardware implementations for transparency and due to availability of runtime information which can significantly improve prefetching's effectiveness. Most previous proposals for hardware prefetchers target specific memory access patterns --such as strided accesses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> and accesses to linked data structures <ref type="bibr" target="#b16">[17]</ref>. While effective for the targeted access patterns, these prefetchers have limited general applicability across a wide spectrum of applications.</p><p>There are a number of prefetcher proposals in the literature that target generalized memory access patterns <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref> -including strided accesses, and indirect accesses to linked data structures and arrays. These proposals primarily rely on miss address correlation <ref type="bibr">[1]</ref> as a technique to predict and prefetch memory addresses. These prefetchers, which we refer to as Miss Correlating Prefetchers (MCPs), record a history of prior L1 cache miss addresses, and correlate the history to a subsequent miss to trigger a prefetch.</p><p>Unfortunately, MCPs suffer from several key shortcomings. First, LI cache misses are often clustered, especially in out-of-order engines with high-bandwidth L I caches, significantly limiting the lookahead and opportunity for timely prefetching. Second, rather than predicting block evictability, these prefetchers place the (prefetched) data in small associative buffers, and look them up either in parallel with L1 thereby increasing Ll's critical access path or upon an L1 miss thereby increasing the prefetch hit latency. Finally, miss address correlation has not been shown to offer both high prediction accuracy (i.e., correct predictions as a fraction of all predictions) and high coverage (i.e., cor-rect predictions as a fraction of all misses) <ref type="bibr" target="#b4">[5]</ref>.</p><p>This paper proposes the Dead-Block Predictors (DBPs) and the Dead-Block Correlating Prefetchers (DBCPs). A DBP is a novel hardware mechanism that predicts "when" a block in a data cache becomes evictable. In a recent paper <ref type="bibr" target="#b6">[7]</ref>, we proposed trace-based predictors that record a trace of shared memory references to predict a last reference to a cache block prior to an invalidation in a multiprocessor. Similarly, a DBP records a trace of memory references that accurately predict the lastreference to a block in an L1 data cache, prior to the block's eviction. A DBCP uses address correlation in conjunction with dead-block traces to predict a subsequent address upon a dead-block prediction. Accurate predicton of a block's evictability enables timely prefetching of data directly into an L1 data cache.</p><p>We use a cycle-accurate simulation of an aggressive outof-order superscalar processor and a spectrum of memoryintensive benchmarks to show the following:</p><p>? For critical cache misses (that are not fully overlapped by computation and incur stalls), on average 92% of the intervals between a last reference to a block until its eviction from L1 are larger than L2 latency, indicating excellent lookahead opportunity for DBCP. In contrast, on average only 38% of the intervals between two subsequent cache misses are larger than L2 latency, indicating a much lower opportunity for MCPs. The rest of the paper is organized as follows. Section 2 describes previous work on miss correlating prefetchers. In Section 3, we present the design details of our predictors and prefetchers. In Section 4, we present the methodology and results. Finally, we discuss the related work in Section 5 and conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Miss correlating prefetchers</head><p>There is a myriad of proposals for hardware and software data prefetching. We will briefly describe these previous proposals in the related work in Section 5. In this section, we will focus on Miss Correlating Prefetchers (MCPs) <ref type="bibr" target="#b2">[3]</ref>, the most effective of current hardware prefetchers applicable to generalized memory access patterns. Prefetching relies on two key mechanisms to success- fully fetch and place data prior to a processor reference: (1) an accurate memory address predictor to predict "which" data to prefetch, and ( <ref type="formula">2</ref>) and an accurate predictor of "when" to prefetch the data. MCPs rely on correlating cache miss addresses to predict both which data to prefetch and when to prefetch it. Figure <ref type="figure">1</ref> depicts the anatomy of an MCP. An MCP uses a miss address predictor and a prefetch buffer. Much as two-level branch predictors, the miss address predictor consists of two storage levels. A history register maintains an encoding of the most recent miss addresses. A correlation table, organized as a cache, records a prediction for a subsequent address given a history encoding. Upon prediction, MCP prefetches a block and places it in the prefetch buffer for lookup by the processor. The lookup occurs either in parallel with L1, increasing Ll's critical access path, or upon an L1 miss incurring high prefetch hit latency.</p><p>A recent study <ref type="bibr" target="#b4">[5]</ref> evaluated MCPs in detail and concluded that miss address correlation alone results in low prediction accuracy and coverage independently of the number of addresses recorded in the history. They proposed Markov prefetchers that recorded and prefetched up to four subsequent missing addresses for every history entry. We present results in this paper that indicate that encoding two previous addresses results in high prediction accuracy and coverage in the spectrum of pointer-intensive, integer, and floating-point applications we studied.</p><p>Despite a high address prediction accuracy and coverage, prefetching using MCPs is often not timely. Figure <ref type="figure">1</ref> depicts an example of MCPs' shortcomings. A reference to block B 1 results in a cache miss which triggers a prefetch to cache block C2. Cache blocks C 1 and C2 are mapped to the same block frame in the cache. While the last reference (or "last touch") to C 1 may occur well in advance of the reference to C2, the block frame in the cache holds C1 until C2 is moved from the prefetch buffer into the frame upon a processor reference. In contrast, predicting the last touch to C1 would allow replacing C1 by C2 in the corresponding block frame earlier.</p><p>Recent out-of-order superscalar engines further reduce lookahead in MCPs. These processors rely on non-blocking L1 caches and often issue multiple accesses in parallel, reducing the distance between two misses. Furthermore, these engines issue cache accesses out of program order, resulting in a re-ordering of miss addresses and address misprediction. The degree of re-ordering and its impact on prediction accuracy highly depends on the available parallelism in the application and a processor's issue-window size. While using the ordered (i.e., committed) miss address stream <ref type="bibr" target="#b4">[5]</ref> would help increase accuracy, it significantly reduces the timeliness in prefetching because MCPs have limited prefetching lookahead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dead-block correlating prefetchers</head><p>In this paper, we propose the Dead-Block Correlating Prefetchers (DBCPs), that predict a last reference to a block frame in a data cache, replace the contents of the block frame upon the last reference, and subsequently predict and prefetch a new cache block. Cache block frames alternate between two states: (a) a "live" state which begins with a miss and is followed by a sequence of hits to the frame, and (b) a "dead" state which begins after the last hit to the frame and ends with a subsequent miss. The key observation behind a DBCP is that the time during which a frame is "dead" is quite long <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref> and well more than the time needed to fetch a cache block from the bottom of the hierarchy. Therefore, by accurately predicting both when a frame becomes "dead" and what cache block the processor will reference next, an DBCP can eliminate the miss and improve performance.</p><p>A DBCP uses a two-level predictor to predict both a cache block replacement and a subsequent address to prefetch for the corresponding block frame. In a recent paper <ref type="bibr" target="#b6">[7]</ref>, we proposed Last-Touch Predictors (LTPs) to predict memory invalidations for shared data in a multiprocessor. In this paper, we derive predictors from LTPs that predict the last reference to a cache block prior to its eviction (i.e., when the block "dies") in the L1 cache and correlate a subsequent address to prefetch with every last touch.</p><p>Much like MCPs, DBCPs rely on repetitive memory access behavior in programs to prefetch effectively. Unlike MCPs, DBCPs primarily capitalize on repetitive instruction sequences --rather than memory address sequences --to predict memory access behavior. Figure <ref type="figure">2</ref> depicts prefetching using a DBCP for our example of memory references. The predictor encodes the trace of memory references to A2 from the time it is fetched (by the reference at PCi) into L1. Upon a last reference to A2 by PCk, the DBCP predicts that A2 is "dead", replaces A2 and prefetches A3. Because the last reference to A2 often arrives much earlier than a subsequent reference to A3 (at PCI), DBCP hides the latency for fetching A3. In the rest of the section, we will describe in detail DBCP's prediction/prefetching mechanisms and their implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A dead-block predictor</head><p>The first predictor we derive is a Dead-Block Predictor (DBP) for L1 data caches. Figure <ref type="figure">3</ref> (top) depicts the anatomy of a trace-based DBP. A history table duplicates the L1 tag array and stores a trace encoding associated with every tag. We use truncated addition (as before <ref type="bibr" target="#b6">[7]</ref>) to maintain a fixed-size encoding for every instruction trace. In practice, truncated addition allows a compact encoding (i.e., -12 bits) while offering high prediction accuracy and coverage. We also studied using xor (used in other predictors <ref type="bibr" target="#b12">[13]</ref>) and found that repetition of PCs (due to iterative control flow) prevents xor from accurately encoding a trace. While other encoding functions are possible, a more detailed study of encoding is beyond the scope of this paper.</p><p>A dead-block table maintains encoded traces, called signatures, that end with a dead block. Upon a new history encoding, a DBP looks up in the dead-block table to match the trace against a signature. When learning, an L1 replacement places the block's history encoding as a signature in the dead-block table. To reduce misprediction frequency, a DBP uses two-bit saturating counters for every signature to estimate prediction confidence.</p><p>Due to control flow irregularities in applications, multiple cache blocks may have dead-block signatures that are proper subsequences of each other resulting in subtrace aliasing <ref type="bibr" target="#b6">[7]</ref>. To prevent aliasing, DBP maintains deadblock signatures per cache block address. Because, the number of signatures highly varies across blocks (as data structure usage varies across application phases), we use a simple hash function to distribute the signatures across the table. Results in Section 4 indicate that xor works well as a hash function in practice.</p><p>The figure depicts prediction in DBP using our example of memory references. The trace in the history table for block A2 is {PCi,PCj}. A memory reference to A2 from</p><formula xml:id="formula_0">history table A2] Pci,Pcl I 1 I r-*(enc?de ) Id/st A2 @ PCk dead-block table ~ -~ A2,.IIPCk ? -\ =? A2 is dead! history table [---I~Q encode ) &lt; I ~( dead-block correlation table A2,AI..,PCk A3 F =? Id/st A2 @ PCk ~, A2 is dead! replace A2, prefetch A3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIGURE 3. A dead-block predictor (top) and a dead-block correlating address predictor (bottom).</head><p>PCk updates the corresponding history table entry and looks it up in the dead-block table. Because the table indicates a match, the DBP predicts that block A2 is dead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A dead-block correlating address predictor</head><p>We derive a Dead-Block Correlating address Predictor, from DBPs, that correlates the trace leading to a dead block to a subsequent memory address. In this paper, we use the abbreviation DBCP to refer to both the address predictor and the prefetcher we propose. In its minimal form, a DBCP is a DBP where each dead-block entry also includes a (prediction for) subsequent address. The dead-block table simply keeps track of the address referenced the last time a recorded trace resulted in a dead block. In general, a DBCP can also include prior address information in the deadblock traces for improved address prediction accuracy and coverage at the cost of higher storage overhead. In Section 4.3, however, we show that in practice, unlike MCPs, DBCPs exhibit high address prediction accuracy and coverage without prior address correlation.</p><p>Figure <ref type="figure">3</ref> (bottom) depicts the anatomy of a DBCP. A history table entry encodes a list of prior memory addresses (mapped to the block frame) along with the current PC trace. A dead-block correlation table records history entries that result in a dead block, and includes a prediction for a subsequent memory address. For instance, in our example in the figure, the predictor maintains a history of a prior memory address, A1, previously mapped to the same block frame as A2, with the trace { PCi,PCj }. Upon a reference to A2 by PCk, the dead-block correlation table predicts that A2 is dead and predicts A3 for prefetching.</p><p>A key difference between MCPs and DBCPs is that MCPs correlate and predict the miss address stream across block frames whereas DBCPs correlate and predict the miss address stream in a given block frame. Intuitively, neither address stream is fundamentally more predictable than the other because MCPs' cache address stream is just an interleaving of DBCP's block frame address stream. In practice, we present results in Section 4.3 that indicate that the best achievable address prediction accuracy and coverage for MCP and DBCP are comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Predictor &amp; prefetcher implementation</head><p>Unlike previous MCP studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref> that evaluated the prefetcher's effectiveness for single-issue in-order processors, we incorporate and evaluate our prefetchers in a wideissue out-of-order superscalar engine (Figure <ref type="figure" target="#fig_0">4</ref>). Because cache block deadtimes are typically large (e.g., hundreds to thousands of cycles), a DBCP can tolerate high latencies in dead-block and prefetch address prediction without sacrificing timeliness. Moreover, the execution order of instructions and the instruction issue width have little impact on a DBCP's effectiveness because of the large prediction/ prefetch lookahead. Therefore, unlike MCPs, DBCPs can monitor and record the program ordered (i.e., committed) memory reference stream. Because the history table maintains a copy of the L1 tags, the ordered memory reference stream in the history table also produces an ordered L1 miss address stream.</p><p>As in any table-based predictor, a key design parameter affecting a DBCP's accuracy is the size and organization of the correlation table. Due to DBCP's large tolerance for latency, the table can be built as a highly associative structure to minimize the number of conflicts among the signatures and increase accuracy and coverage. Moreover, the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We use SimpleScalar 3.0 to simulate an aggressive outof-order processor with a cache hierarchy. Table <ref type="table" target="#tab_5">1</ref> depicts the configuration parameters for the base system. We have augmented the simulator to accurately model contention at the LI/L2 and memory buses accurately. The buses always give priority to processor requests over prefetch requests.</p><p>To gauge the full potential of the predictors and the effectiveness of the entropy they encode independently of storage, we assume correlation tables with an unlimited  The correlation table entries consist of a 19-bit data (17bit next cache block address equal to an L1 block tag and a two-bit saturating counter) and a 27-bit tag and index (including the current address and the dead-block signature). We use 27 bits from the L1 cache block's tag and index and xor it with the 12-bit signature from the history table. We use this resulting 27 bits as tag and index for the correlation table. We have found that this placement function in practice eliminates conflicts in the tables.</p><p>All prefetchers use a request queue with 128 entries. When the request queue is full, new entries in the queue replace the old (unissued) ones at the queue head. The MCPs also use a 128-entry (fully-associative) prefetch buffer with a 1-cycle access latency in parallel with L1. As in the request queue, new entries in the buffer remove old (unreferenced) entries. Prefetch requests are only issued at most one per cycle when the L1/L2 bus is free.</p><p>Table <ref type="table" target="#tab_7">2</ref> describes the benchmarks we studied, their inputs, and the corresponding L1 and L2 miss ratios. These benchmarks include five pointer-intensive applications (i.e., bh, em3d, health, mst, and treeadd) from the Olden suite [2], and four integer and five floating-point applications from the SPEC2K (i.e., gcc, mcf, ammp, art, and equake) and SPEC95 (i.e., compress, perl, mgrid, and swim). All tested SPEC benchmarks use reference inputs except for compress. In the interest of reduced simulation time, we simulated the benchmarks for three billions cycles (six billion cycles for equake) after skipping an initialization period of one billion cycles. Compress requires simulating the entire input to benefit from repetitive memory access behavior. Instead we simulated its train input.</p><p>The benchmarks in this study all exhibit a large fraction of memory stall cycles in their execution with varying degrees of memory parallelism and overlap.l On one end of the spectrum, some of the Olden benchmarks primarily exhibit a high degree dependent memory accesses, exposing all the miss latencies on the critical path. On the other end, the floating-point benchmarks exhibit a high degree of memory parallelism, but are primarily limited by the processor's inability to hide long L2 miss latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Lookahead opportunity</head><p>In this section, we compare the lookahead opportunity for DBCPs and MCPs. Figure <ref type="figure" target="#fig_1">5</ref> (left), illustrates the cumulative distribution of distances (in cycles) between a last I. We evaluated DBP's and DBCP's prediction accuracy/coverage for all of SPEC95 and those SPEC2K benchmarks compiled for SimpleScalar. While the benchmarks we omit exhibit as high a prediction accuracy/coverage as those presented in this paper, they incur a small number of memory stalls and do not benefit from prefetching. reference to a cache block prior to the block's eviction due to a subsequent miss to another block. The graphs illustrate the deadtimes for critical misses, those misses which stall the reorder buffer because their latencies cannot be fully overlapped. The graphs indicate that except in perl, 80% of the deadtimes in all applications are 500 cycles are more, several times larger than the memory latency. Perl exhibits a high fraction of conflict misses that are clustered in time, reducing the fraction of deadtimes over 500 cycles to 50%. These results corroborate previous findings on cache block deadtimes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11 ]</ref>, and show that the deadtimes offer excellent prefetch lookahead for DBCPs.</p><p>The results on block deadtimes have several implications. First, because deadtimes are on average several times longer than memory latency, as the gap between processor and memory speeds increases, lookahead opportunity remains high allowing effective data prefetching. Second, prediction and prefetching techniques relying on block deadtimes may trade off speed for higher accuracy; the latency of off-chip storage (or slow but large on-chip storage) for dead-block tables is not as critical if larger storage helps improve prediction accuracy and coverage.</p><p>Figure <ref type="figure" target="#fig_1">5</ref> (right) illustrates the cumulative distribution of distance between two critical misses in the cache. As we can see, in 9 out of 14 applications, 20%-80% of the miss intervals are smaller than L2 latency, allowing insufficient lookahead to prefetch and overlap these misses. Moreover, in I0 of the applications, 50% or more of the miss intervals are smaller than memory latency preventing a prefetcher from overlapping L2 misses. These results indicate that cache misses are highly clustered, therefore even if MCPs offer high address prediction accuracy and coverage, they may be limited significantly by prefetching lookahead.</p><p>The results on cache miss intervals also have several implications. First, as the gap between processors and memory speeds increases, lookahead opportunity using miss intervals relative to memory latency decreases. Second, aggressive wide-issue engines exacerbate the negative impact of miss clustering by issuing a larger number of memory references every cycle, thereby reducing the lookahead. Third, due to limited lookahead, MCPs require fast address prediction and prefetching mechanisms and cannot trade off accuracy for speed. Unlike DBCPs which can use the program ordered reference streams, MCPs must use the speculative reference stream which may be re-ordered and may reduce prediction accuracy and coverage.  (misses), and the fraction of dead blocks not predicted due to predictor training. The graphs indicate that on average a DBP predicts 90% of dead blocks and mispredicts (i.e., prematurely predicts live blocks as dead) only 4% of the dead blocks. We also evaluated DBPs for set-associative caches an LRU replacement policy and various sizes and found similar prediction accuracy and coverage. However, we omit these results in this paper in the interest of brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dead-block prediction accuracy &amp; coverage</head><p>These results indicate that the potential for trace-based predictors to predict memory system events is beyond just predicting memory invalidation and sharing for scientific applications in multiprocessors <ref type="bibr" target="#b6">[7]</ref>. The results corroborate the intuition that because memory instructions drive the movement of data in the cache hierarchy, repetitive code fragments and the resulting instruction traces can help predict the movement.</p><p>Prediction coverage in rest is only 47% because mst's primary heap-based data structure is constantly modified, resulting in many of the dead-block signatures to be obsolete upon creation. The two-bit saturating counters in DBP filter these signatures, preventing them from triggering a prediction. In health, DBP exhibits a subtrace aliasing problem because of the irregular control flow in the application's main two-Ievel nested loop, giving rise to a large number of mispredicted dead blocks. Compress has a prediction accuracy and coverage of 80% because of low predictability in accesses to specific segments of the main data structure which is a compression hash table.</p><p>Figure <ref type="figure" target="#fig_2">6</ref> also compares the prediction accuracy and coverage of DBP against simple LRU stacks (bars labeled "B" and "C") proposed by Peir, et al. <ref type="bibr" target="#b15">[16]</ref> to predict evictability of L1 blocks. The LRU stacks simply maintain a list of least-recently used addresses. The key idea is that an application's working set has a finite number of cache blocks. The stacks estimate the maximum size of the working set. When an address falls out of the stack, the stack predicts that the corresponding block is dead. Unfortunately, because working set sizes in a cache may largely vary both within and across applications <ref type="bibr" target="#b17">[ 18]</ref>, LRU stacks fail to predict dead blocks accurately. The graphs corroborate previous findings <ref type="bibr" target="#b15">[16]</ref> on stacks and indicate that a 64-entry stack achieves a coverage of 77% while prematurely predicting dead blocks by over 100% (not shown). A 256-entry stack predicts dead blocks more conservatively and reduces the fraction of mispredicted dead blocks to 5%, only slightly over DBP's. However, the 256-entry stack's coverage is much lower than DBR and is on average 60%. Moreover, the stack fails to cover any dead blocks for perl and ammp due to conflict misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Address prediction accuracy &amp; coverage</head><p>While dead-block prediction offers high accuracy and ? coverage, the success of a prefetcher also relies on accurate address prediction. In general, either MCP or DBCP can use an arbitrary history depth --i.e., record a history of an arbitrary number of previous addresses to predict a next address. Larger history depth in MCP and DBCP increases accuracy by reducing subtrace aliasing --i.e., identical address sequences leading to different subsequent addresses --at the cost of higher storage. Too large a history depth, however, also reduces coverage by increasing the learning time for predictions that do not benefit from larger depth. Figure <ref type="figure">7</ref> illustrates prediction accuracy and coverage for MCP and DBCP with depths of one and two. We experimented with varying the depth and found little improvement in accuracy and a decrease in coverage with a higher depth.</p><p>MCP-I and MCP-2 in the graphs correspond to MCPs with a history depth and one and two respectively. The graphs show that there is a significant improvement in prediction coverage from, 52% to 80%, for MCPs when the depth increases from one to two. Moreover, the fraction of mispredicted addresses on average drops from 47% to 13%, increasing the prediction accuracy. Previous studies evalu-ating MCPs for technical and commercial workloads <ref type="bibr" target="#b4">[5]</ref> reported no added advantage to more history than a single address. Our results clearly indicate that for the wide spectrum of applications we study, the addition of another prior address enhances the predictor's effectiveness. Moreover, the previous study evaluated MCPs using a programordered stream of miss addresses from a single-issue inorder engine. The numbers we evaluate are for the addresses generated by our wide-issue out-of-order engine. We found that while the prediction coverage is slightly higher for the ordered stream, the ordered stream has minimal lookahead opportunity.</p><p>The key intuition behind why a larger history depth increases an MCP's accuracy and coverage is that while data structures are often referenced in multiple distinct program contexts or phases <ref type="bibr" target="#b6">[7]</ref> (e.g., a given sequence procedure invocations), they are not always used in conjunction with the same other data structures in every phase. Moreover, a group of data structures referenced together are not always referenced in the same order. An increase in history depth helps correlate a reference (with a given address) to a program phase and consequently to a subsequent address.</p><p>In DBCP, dead-block signatures can precisely pinpoint which program phase a reference is from and therefore what subsequent address is following the given reference in that phase. DBCP-1 and DBCP-2 correspond to a DBCP with a history depth of one and two prior addresses respectively. The graphs show that DBCP-I achieves a high coverage of 82% and with only 4% misprediction. However, the addition of a small number of extra bits increases coverage and decreases the fraction of mispredicted addresses in DBCP-2 to 86% and 3% respectively. We experimented with varying the number of bits used from tile second prior address and found four bits to offer the best coverage while mm~mlzmg storage.</p><p>There is a single application in which the addition of bits from a second prior address helps significantly improve the predictor's coverage. In em3d, the entire program runs in a single phase, generating common dead-block signatures. Because the program marches down a bipartite graph in which one graph node shares both multiple incoming edges and outgoing edges with other nodes, a dead-block signature and the node's address alone cannot distinguish the subsequent addresses along the different edges.</p><p>On average, MCP-2 and DBCP-2 achieve roughly the same prediction accuracy and coverage, with DBCP-2 having only a slight advantage over MCP-2. On a closer look, we also found that the number of entries the predictors maintain are roughly the same. Therefore, DBCP-2's primary advantage is in prefetching lookahead opportunity. However, DBCP-2 effectively encodes the information as to when to prefetch and what to prefetch simultaneously, obviating the need for decoupling the predictors, and opti- mizing for storage. Figure <ref type="figure">8</ref> compares DBCP-2 with a Markov predictor -i.e., an MCP-1 predictor that predicts four LRU addresses rather than a single address. Markov predictors increase coverage at the cost of a much higher fraction of mispredicted addresses (only a single address out of the four predicted can be correct). On average, Markov achieves a coverage of 81% (slightly less than DBCP-2) but increases the traction of mispredicted addresses to 229%. The mispredicted addresses may significantly increase traffic in the memory hierarchy and place a large demand on bandwidth. Fortunately, not all of the mispredicted addresses go to waste. By predicting multiple subsequent addresses and prefetching them into a prefetch buffer, Markov actually somewhat offsets the negative impact of miss address reordering due to either re-ordering of references to data structures across program phases or re-ordering of miss addresses in the out-of-order engine.</p><p>In equake and mgrid, the number of different miss addresses after the current miss is often larger than four, because these applications reference different sets of arrays in different program phases (e.g., different procedures). Hence, Markov is unable to capture a large fraction of the misses in these applications. Because DBCP-2 identifies where the program is executing, it exhibits high prediction accuracy and coverage in these applications. DBCP-2 performs worse than Markov in health and rest because these two benchmarks have dynamically alternating sequences of memory references to their main data structures. Consequently, DBCP-2 spends much time in correlating these references, resulting in a low prediction coverage. In contrast, Markov captures these changes in groups of four, and yields a higher prediction coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prefetching performance</head><p>In this section, we evaluate the prefetcher's effectiveness in improving performance. We compare DBCP against MCP implementations (i.e., both MCP-2 and Markov). To gauge MCPs' best performance independently of storage size, we assume correlation tables for MCPs that are large enough to fit all of the generated address encoding with a fast lookup latency of 12 cycles, equal to L2's latency (we also evaluated Markov with an ideal lookup latency of one cycle and observed less than 5% improvement in speedups). Because Markov can potentially benefit from extra bandwidth, we also present Markov numbers with unlimited bandwidth to L2 and memory.</p><p>Table <ref type="table">3</ref> compares a DBCP against an "ideal" L1 with no capacity/conflict misses. The comparison to "ideal" LI helps determine what fraction of the memory stalls the prefetchers can eliminate. The table presents speedups over a demand-fetched system. Our first observation is that all prefetchers improve performance over the demand-fetched system even though our base system is quite aggressive. On average, DBCP eliminates 62% of the memory stalls in L1 and achieves a 62% speedup. In contrast, MCP and Markov only eliminate 22% and 30% of the memory stalls and achieve a speedup of only 14% and 17% respectively. While Markov benefits from extra L2 and memory bandwidth, the improvement is only on average an additional 4%, not enough to break even with DBCP.</p><p>Figure <ref type="figure">9</ref> breaks down the fraction of memory stalls removed and incurred in a prefetching system relative to a demand-fetched system. The incurred stalls are either stalls originally present in the demand-fetched system or extra stalls due to incorrect or late prefetching. The removed stalls are either hits in the LI (for the case of DBCP) or prefetch buffer due to a successful (accurate and timely) prefetch, or hits in the prefetch buffer due to an earlier mispredicted prefetch.</p><p>The figure indicates that DBCP is timely in most applications. DBCP prefetches late in art, compress, gcc, and mcfdue to the bursty prefetch requests. These applications would benefit from multiple L2 cache ports and a higher bandwidth memory system. The request queue in these applications often becomes full and drops requests. Nevertheless, DBCP eliminates a significant fraction of the memory stalls in these applications even with a single L2 port.</p><p>DBCP is extremely effective in Olden benchmarks which exhibit a high degree of data dependence through memory in linked data structures; DBCP virtually eliminates the dependence bottleneck in two applications and significantly reduces memory stalls in another two. Despite low prediction accuracy and coverage in mst, DBCP still speeds up execution by 18%. Mispredictions somewhat offset the gains from prefetching in health and compress, due to cache pollution. Mcf and ammp have large footprints which do not fit in L2. DBCP, however, accurately predicts the references and successfully fetches the data into LI, reducing the memory stalls. Equake generates a large number of correlation signatures due to a large working set of data. While DBCP only allocates signatures for critical misses, there are still more signatures than can fit in a 2M on-chip table. The off-chip table, however, fits all the signatures and significantly improves speedup in equake.  In contrast, MCPs are timely for many of the Olden benchmarks. The benchmarks exhibit a high degree of data dependence and large cache miss intervals (Figure <ref type="figure" target="#fig_1">5</ref>) allowing for prefetching lookahead. In SPEC benchmarks, the misses are bursty and often independent allowing little prefetching lookahead. Markov always improves performance over MCP-2 even though the predictors achieve comparable coverage (Section 4.3). Markov's effective coverage is higher than MCP-2 because some of the mispredicted prefetches placed in the buffer actually hit for subsequent misses. The bandwidth loss in MCPs is not significant because most prefetches are either late but accurate or early but inaccurate. In the former case, prefetches do not increase traffic as misses are merged with outstanding prefetch requests. In the latter case, inaccurate prefetches which are buffered are useful for subsequent misses.</p><p>Besides equake, MCPs only improve performance over DBCP in mst and perl. In mst, Markov improves prediction coverage over DBCP (as discussed in Section 4.3) and removes relatively more stalls. Perl primarily incurs L1 conflict misses satisfied by L2 and therefore neither prefetcher is timely for perl. However, hits in the prefetch buffer due to mispredicted prefetches significantly improve performance in Markov over DBCE Table <ref type="table" target="#tab_11">4</ref> compares the on-chip 2M DBCP with a base IM L2 (with a 12-cycle hit latency) against an 3.1M 6-way L2 with approximately equal storage cost including the tag overhead. We evaluate both an aggressive large L2 implementation with the same 12-cycle hit latency as the base L2, and a slower but more realistic L2 hit latency of 18 cycles. The table indicates that the addition of an on-chip DBCP is much more cost-effective than increasing L2's size. Half of the applications do not benefit from a larger L2. A larger L2 slightly outperforms DBCP only in two applications, health and gcc, in which the larger L2 captures a significant fraction of their working sets. Moreover, in a realistic 3.1M L2 implementation with a longer access latency, seven of the applications actually exhibit slowdown because the out-of-order engine fails to overlap the long L2 latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>There are a number of hardware-based data prefetching techniques, many of which customize hardware for specific memory reference patterns. Chert and Baer proposed stride prefetchers <ref type="bibr" target="#b3">[4]</ref> that correlate non-unit data address strides with a memory instruction PC in a small table and prefetch based on the stride. Jouppi proposed stream buffers <ref type="bibr" target="#b5">[6]</ref> to detect unit stride cache miss address sequences and correlated them with a "starting" address to prefetch them sequentially. Palacharla and Kessler <ref type="bibr" target="#b14">[15]</ref> extended stream buffers to non-unit stride stream. Mehrorta and Harrison <ref type="bibr" target="#b9">[10]</ref> proposed the indirect reference buffers to identify data address dependence in recursive or linked data structures. Roth, et al. <ref type="bibr" target="#b16">[ 17]</ref>, proposed prefetchers that capture memory reference dependence in linked data structures and associate them with instruction PCs to initiate a prefetch. Charney and Reeves <ref type="bibr" target="#b2">[3]</ref> were first to use address correlation in hardware on the L1 miss stream to prefetch. Joseph and Grunwald <ref type="bibr" target="#b4">[5]</ref> proposed Markov prefetchers that are miss correlating prefetchers associating multiple subsequent addresses with each correlation.</p><p>Many software prefetchers rely on accurate compiletime analysis of memory access patterns to detect both what memory addresses are subsequently referenced and when the data can be placed in the cache. Mowry, et al. <ref type="bibr" target="#b11">[12]</ref>, show that for numerical and scientific applications, software prefetchers can successfully hide the memory access latency. Luk and Mowry <ref type="bibr" target="#b8">[9]</ref>, Lipasti, et al. <ref type="bibr" target="#b7">[8]</ref>, and Ozawa, et al. <ref type="bibr" target="#b13">[14]</ref>, also evaluate the effectiveness of heuristicsbased techniques which insert compile-time prefetch instructions in pointer-intensive applications and applications with recursive data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed and evaluated Dead-Block Correlating Prefetchers (DBCPs). These prefetchers, use a novel mechanism, Dead-Block Predictors (DBPs), to predict when LI data cache blocks become evictable. Previous techniques for data prefetching primarily relied on correlating the L1 data miss address stream to predict and trigger a prefetch. Predicting a dead block, however, significantly enhances prefetching lookahead over previous techniques. DBCPs predict and prefetch a subsequent block address upon predicting a block's eviction. DBCPs enable effective data prefetching in a wide spectrum of pointer-intensive, integer, and floating-point applications with arbitrary mem-ory access patterns.</p><p>We used cycle-accurate simulation of an aggressive wide-issue out-of-order superscalar processor and memoryintensive benchmarks to show that: (1) dead-block prediction enhances prefetch lookahead by at least an order of magnitude over previous techniques, (2) a DBP can predict dead blocks on average with a coverage of 90% with only 4% misprediction, (3) a DBCP offers an address prediction coverage of 86% with only 3% misprediction, and (4) DBCPs offer timely prefetching of data directly into L1 and help improve performance by 62% on average and 282% at best. In contrast, the best current proposal for prefetching generalized memory access patterns achieves a speedup of only 17% and at best 51%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 4 .</head><label>4</label><figDesc>FIGURE 4. Using a DBCP in an out-of-order core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 5 .</head><label>5</label><figDesc>FIGURE 5. Lookahead analysis: cumulative distribution of distance in processor cycles from a last touch to a subsequent miss (left), and between two consecutive misses (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 (</head><label>6</label><figDesc>Figure 6 (bars labeled "A") presents the prediction accuracy and coverage of a DBP for a 32K LI direct-mapped cache. The graphs plot the fraction of correct dead-block predictions (hits), the fraction of incorrect predictions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 8 . 2 .</head><label>82</label><figDesc>FIGURE 8. Accuracy and coverage of Markov and DBCP-2. The graphs only present bars with up to 40% of mispredicted L1 misses. The numbers on top indicate the maximum values for each bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Id/st C1 miss Id/st C1 hit last touch</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">parallel lookup</cell></row><row><cell></cell><cell>Id/st B~ ,.</cell><cell></cell><cell></cell></row><row><cell>Id/st A1 miss Id/st B1 miss</cell><cell>At</cell><cell></cell><cell></cell></row><row><cell>prefetch C2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Id/st C2 miss waiting</cell><cell>L1</cell><cell>MCP</cell><cell>A~</cell></row><row><cell>dynamic stream of memory references</cell><cell>miss B1</cell><cell>C2</cell><cell>~refetch C2</cell></row><row><cell cols="4">FIGURE 1. A Miss Correlating Prefetcher.</cell></row></table><note><p>correlation table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>PCi: Id/st A2 miss PCj: Id/st A2 hit PCk:: ld/st A2 hit,q-~ last touch prefetch A3 PCI: Id/st A3 hit~ prefetched d namic stream of memory references A2 Id/st A2 @ PCk history correlation table table j~ce A3</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>o::,: ,,</cell></row><row><cell>A3</cell><cell>prefetch A3</cell></row><row><cell>FIGURE 2.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>A Dead-Block Correlating Prefetcher.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>L1 I/D</cell><cell>32K, 32-byte block</cell></row><row><cell></cell><cell></cell><cell></cell><cell>direct-mapped d-cache</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4.way i-cache, 1-cycle</cell></row><row><cell></cell><cell></cell><cell>L1 D ports</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>L1 D MSHRs</cell><cell>unlimited</cell></row><row><cell></cell><cell></cell><cell>L2 I/D</cell><cell>each 1M 64-byte brock</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell>4-way, 12-cycle</cell></row><row><cell></cell><cell>128 entries (FIFO)</cell><cell>L2. D ports</cell><cell>1</cell></row><row><cell></cell><cell>128 entries</cell><cell>L2 D MSHRs</cell><cell>unlimited</cell></row><row><cell></cell><cell>unlimited, 12-cycle</cell><cell>L1/L2 bus</cell><cell>32-byte wide, 2GHz</cell></row><row><cell></cell><cell>1K entries</cell><cell></cell><cell>Memory</cell></row><row><cell></cell><cell>2M 8-way, 18-cycle</cell><cell>Latency</cell><cell>70 processor cycles</cell></row><row><cell></cell><cell>7.6M 16-way, 70-cycle 4</cell><cell>Bus</cell><cell>64-byte wide, 400 MHz</cell></row><row><cell>. S</cell><cell cols="2">,stem configuration.</cell></row></table><note><p><p>table can be built either on chip to reduce interference with off-chip traffic, or off chip to optimize for size at the cost of a higher required bandwidth. Alternatively, the table can be built as a hierarchy with the on-chip storage holding the signatures for all the cache blocks in L1 backed up by main memory. Prefetching a block into L1 would simultaneously initiate bringing the block's dead-block signatures into the on-chip table. In this paper, we evaluate standalone (fast) on-chip and (slow) off-chip table implementations.</p>A DBCP can use reference filters to reduce storage and lookup bandwidth requirements for the tables. In out-oforder engines with non-blocking caches, cache misses in L 1 that hit in L2 (e.g., conflict misses that are temporally close) are often overlapped and do not introduce memory stalls on the execution path. In this paper, we augment the processor's re-order buffer with a single bit that indicates whether a load/store instruction at the head of the buffer stalls (i.e., takes two or more cycles to retire). Such a load/ store instruction incurs a critical miss. The correlation table only allocates entries for these critical misses. All signatures that do not result in a critical miss are therefore filtered and are not placed in the correlation table. In general, a DBCP can benefit from more elaborate hardware or software techniques to accurately identify the instructions responsible for a high fraction of memory stalls.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 2 . Benchmarks and input parameters.</head><label>2</label><figDesc></figDesc><table /><note><p>number of entries in our predictor studies (Sections 4.2 and 4.3). For performance evaluation (Section 4.4), we consider practical implementations of DBCP with cache-like correlation tables. We consider an on-chip configuration with 2M (including tag and data) 8-way set-associative table with 18-cycle access latency (including -64K entries), and an off-chip configuration with 7.6M (including tag and data) 16-way set-associative table with 70-cycle access latency (containing -460K entries). Both configurations use a 1K-entry (as many entries as the tag array in the 32K-LI D) history table and 12-bit signatures. The off-chip configuration eliminates all capacity and conflict misses in the correlation table and is the maximum size needed by all the applications we studied.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The table depicts percent speedup over our base demand-fetched system. Ideal L1 numbers correspond to an L1 with no capacity/conflict misses. MCP and DBCP use address history depth of 2 (i.e., MCP-2 and DBCP-2). The table also depicts Uarkov numbers with unlimited bandwidth to L2 and memory (Markov inf b/w). The DBCP numbers correspond to an on-chip implementation with 2M 8-way storage and 18-cycle latency and off-chip storage with 7.6M 16-way storage and 70-cycle latency. The numbers appear in italic are those which Markov outperforms DBCE All results are normalized against the base system (no prefetch) using 12-cycle 1M L2.</figDesc><table><row><cell cols="2">Ideal L1</cell><cell>MOP-2</cell><cell cols="5">Markov Markov DBCP-2 DBCP-2 infb/w on chip offchip</cell></row><row><cell>bh</cell><cell>61</cell><cell>28</cell><cell></cell><cell>38</cell><cell>38</cell><cell>59</cell><cell>59</cell></row><row><cell>em3d</cell><cell>60</cell><cell>6</cell><cell></cell><cell>6</cell><cell>13</cell><cell>35</cell><cell>42</cell></row><row><cell>health</cell><cell>135</cell><cell>26</cell><cell></cell><cell>47</cell><cell>50</cell><cell>88</cell><cell>89</cell></row><row><cell>mst</cell><cell>123</cell><cell>11</cell><cell></cell><cell>24</cell><cell>28</cell><cell>18</cell><cell>18</cell></row><row><cell>treeadd</cell><cell>36</cell><cell>17</cell><cell></cell><cell>16</cell><cell>16</cell><cell>33</cell><cell>33</cell></row><row><cell>compress</cell><cell>18</cell><cell>4</cell><cell></cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>6</cell></row><row><cell>perl</cell><cell>10</cell><cell>5</cell><cell></cell><cell>8</cell><cell>8</cell><cell>5</cell><cell>5</cell></row><row><cell>gcc</cell><cell>46</cell><cell>2</cell><cell></cell><cell>9</cell><cell>11</cell><cell>24</cell><cell>27</cell></row><row><cell>mcf</cell><cell>185</cell><cell>15</cell><cell></cell><cell>38</cell><cell>53</cell><cell>125</cell><cell>131</cell></row><row><cell>ammp</cell><cell>303</cell><cell>62</cell><cell></cell><cell>63</cell><cell>69</cell><cell>282</cell><cell>283</cell></row><row><cell>art</cell><cell>155</cell><cell>4</cell><cell></cell><cell>1</cell><cell>3</cell><cell>52</cell><cell>54</cell></row><row><cell>equake</cell><cell>50</cell><cell>2</cell><cell></cell><cell>10</cell><cell>11</cell><cell>2</cell><cell>27</cell></row><row><cell>mgrid</cell><cell>56 i</cell><cell>4</cell><cell></cell><cell>21</cell><cell>30</cell><cell>27</cell><cell>48</cell></row><row><cell>swim</cell><cell>94</cell><cell>7</cell><cell></cell><cell>25</cell><cell>33</cell><cell>31</cell><cell>51</cell></row><row><cell>"ABLE 3.</cell><cell cols="3">Performance</cell><cell cols="2">comparison</cell><cell></cell><cell>of the</cell></row><row><cell>)refetchers</cell><cell cols="7">against an ideal demand-fetched</cell></row><row><cell>system.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The DBCP numbers correspond to an off-chip table implementation. Incurred stalls are stalls that the prefetchers are unable to remove. These stalls are due to prefetcher training and late (not timely) prefetching. The extra prefetching stalls are extra memory stalls incurred because of prefetching. These stalls are bandwidth loss due to misprediction and cache pollution (in DBCP) because of incorrect prefetching into LI.</figDesc><table><row><cell cols="4">Removed stalls [Incurred stalls</cell><cell cols="2">Extra prefetching stalls]</cell></row><row><cell cols="2">prefetch hit</cell><cell>]~ late</cell><cell></cell><cell cols="2">II bandwidth loss</cell><cell>I</cell></row><row><cell cols="2">D other hit</cell><cell cols="2">[[-] training</cell><cell cols="2">[7 cache pollution</cell><cell>]</cell></row><row><cell>120</cell><cell cols="2">C=MCP-2</cell><cell>M=Markov</cell><cell cols="2">D=DBCP-2</cell></row><row><cell>110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CM?em3d</cell><cell cols="2">mst compress gcc</cell><cell cols="2">ammp equake sw/n:</cell></row><row><cell>bh</cell><cell cols="3">health treeadd perl</cell><cell>mcf</cell><cell>art</cell><cell>mgrid</cell></row><row><cell cols="6">FIGURE 9. Breakdown of all memory stalls</cell></row><row><cell cols="5">relative to a demand-fetched system.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 4 . Performance comparison of DBCP against demand-fetched systems with larger L2.</head><label>4</label><figDesc></figDesc><table><row><cell>The table presents percent speedups over our base demand-fetched sys-</cell></row><row><cell>tem. The L2 numbers correspond to an ideal 12-cycle lookup latency and</cell></row><row><cell>a more realistic 18-cycle latency. The numbers appear in italic are those</cell></row><row><cell>which 3.1M L2 outperforms DBCP.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This work is supported in part by a grant from <rs type="funder">Intel corporation</rs> and an <rs type="funder">NSF</rs> <rs type="grantName">CAREER Award</rs> titled "<rs type="projectName">Prediction and Speculation in High-Performance Memory Systems</rs>". We also thank <rs type="person">Milo Martin</rs>, <rs type="person">Se-Hyun Yang</rs>, and the anonymous reviewers for feedback on an earlier draft of this paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_zVWevQ2">
					<orgName type="grant-name">CAREER Award</orgName>
					<orgName type="project" subtype="full">Prediction and Speculation in High-Performance Memory Systems</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic improvements of locality in virtual memory systems</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">lEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="1976-03">March 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Early experiences with Olden</title>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">C</forename><surname>Carlisle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">H</forename><surname>Reppy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurie</forename><forename type="middle">J</forename><surname>Hendren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Languages and Compilers for Parallel Computing</title>
		<meeting>the Sixth Languages and Compilers for Parallel Computing</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generalized correlation-based hardware prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">P</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><surname>Reeves</surname></persName>
		</author>
		<idno>EE- CEG-95-1</idno>
		<imprint>
			<date type="published" when="1995-02">February 1995</date>
		</imprint>
		<respStmt>
			<orgName>School of Electrical Engineering, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reducing memory latency via non-blocking and prefetching caches</title>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Baer</surname></persName>
		</author>
		<idno>U. Washington CS TR 92-06-03</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Architectural Support for Programmbzg Languages and Operating Systems (ASPLOS V)</title>
		<meeting>the Fifth International Conference on Architectural Support for Programmbzg Languages and Operating Systems (ASPLOS V)</meeting>
		<imprint>
			<date type="published" when="1992-10">October 1992</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prefetching using Markov Predictors</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="133" />
			<date type="published" when="1999-02">February 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual hlternational Symposium on Computer Architecture</title>
		<meeting>the 17th Annual hlternational Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990-05">May 1990</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selective, accurate, and timely self-invalidation using last-touch prediction</title>
		<author>
			<persName><forename type="first">An-Chow</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual h2ternational Symposium on Computer Architecture</title>
		<meeting>the 27th Annual h2ternational Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Software prefetching in pointerand call-intensive environments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mikko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Lipasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">R</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName><surname>Roediger</surname></persName>
		</author>
		<author>
			<persName><surname>Spaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual IEEE/ACM blternational Symposium on Microarchi-tecture (MICRO 28)</title>
		<meeting>the 28th Annual IEEE/ACM blternational Symposium on Microarchi-tecture (MICRO 28)</meeting>
		<imprint>
			<date type="published" when="1995-11">November 1995</date>
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compiler based prefetching for recursive data structures</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh bzternational Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII)</title>
		<meeting>the Seventh bzternational Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII)</meeting>
		<imprint>
			<date type="published" when="1996-10">October 1996</date>
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Examination of a memory access classification scheme for pointer-intensive and numeric programs</title>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luddy</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 International Conference on Supercomputing</title>
		<meeting>the 1996 International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="page" from="133" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Modeling live and dead lines in cache memory systems</title>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Thi'ebaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhiraj</forename><surname>Pradhan</surname></persName>
		</author>
		<idno>TR-90-CSE-14</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering, University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Desiga and evaluation of a compiler algorithm for prefetching</title>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Archi= tectural Support for Programming Languages and Operating Systems (ASPLOS V)</title>
		<meeting>the Fifth International Conference on Archi= tectural Support for Programming Languages and Operating Systems (ASPLOS V)</meeting>
		<imprint>
			<date type="published" when="1992-10">October 1992</date>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic path-based branch prediction</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual IEEE/A CM International Symposium on Microarchitecture (MICRO 29)</title>
		<meeting>the 29th Annual IEEE/A CM International Symposium on Microarchitecture (MICRO 29)</meeting>
		<imprint>
			<date type="published" when="1996-12">December 1996</date>
			<biblScope unit="page" from="142" to="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cache miss heuristics and preloading techniques for general-purpose programs</title>
		<author>
			<persName><forename type="first">Toshihiro</forename><surname>Ozawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasunori</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin'ichiro</forename><surname>Nishizaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 28)</title>
		<meeting>the 28th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 28)</meeting>
		<imprint>
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluatir~g stream buffers as a secondary cache placement</title>
		<author>
			<persName><forename type="first">Subbarao</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual h~ternational Symposium on Computer Architecture</title>
		<meeting>the 21st Annual h~ternational Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1994-04">April 1994</date>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Capturing dynamic memory reference behavior with adaptive cache topology</title>
		<author>
			<persName><forename type="first">Jih-Kwon</forename><surname>Peir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Windsor</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VIII)</title>
		<meeting>the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VIII)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="240" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dependence based prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VIII)</title>
		<meeting>the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VIII)</meeting>
		<imprint>
			<date type="published" when="1998-10">October 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cache memories</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="530" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-bandwidth data memory systems for superscalar processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gurindar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Sohi</surname></persName>
		</author>
		<author>
			<persName><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV)</title>
		<meeting>the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV)</meeting>
		<imprint>
			<date type="published" when="1991-04">April 1991</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A model for estimating trace-sample miss ratios</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="79" to="89" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
