<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineer-ing</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B88DF1B069E7F5C991F98BF79EBD782</idno>
					<idno type="DOI">10.1109/TIP.2014.2308414</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Tracking via Discriminative Sparse Similarity Map</head><p>Bohan Zhuang, Huchuan Lu, Senior Member, IEEE, Ziyang Xiao, and Dong Wang Abstract-In this paper, we cast the tracking problem as finding the candidate that scores highest in the evaluation model based upon a matrix called discriminative sparse similarity map (DSS map). This map demonstrates the relationship between all the candidates and the templates, and it is constructed based on the solution to an innovative optimization formulation named multitask reverse sparse representation formulation, which searches multiple subsets from the whole candidate set to simultaneously reconstruct multiple templates with minimum error. A customized APG method is derived for getting the optimum solution (in matrix form) within several iterations. This formulation allows the candidates to be evaluated accurately in parallel rather than one-by-one like most sparsity-based trackers do and meanwhile considers the relationship between candidates, therefore it is more superior in terms of cost-performance ratio. The discriminative information containing in this map comes from a large template set with multiple positive target templates and hundreds of negative templates. A Laplacian term is introduced to keep the coefficients similarity level in accordance with the candidates similarities, thereby making our tracker more robust. A pooling approach is proposed to extract the discriminative information in the DSS map for easily yet effectively selecting good candidates from bad ones and finally get the optimum tracking results. Plenty experimental evaluations on challenging image sequences demonstrate that the proposed tracking algorithm performs favorably against the state-of-the-art methods.</p><p>Index Terms-Object tracking, sparse representation, appearance model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V ISUAL tracking, one of the fundamental topics in com- puter vision, has long been playing a critical role in numerous applications such as surveillance, military reconnaissance, motion recognition and traffic monitoring. While much breakthrough has been made within the last decades (like <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b15">[16]</ref>, etc), it still remains challenging in many aspects including pose variation, illumination change, partial Fig. <ref type="figure">1</ref>.</p><p>Challenges during tracking in real-world environments, including heavy occlusions (Woman), abrupt motion (Face), illumination change (Singer1), pose variation (Girl) and complex background (Cliffbar). We use blue, green, black, yellow, magenta, cyan and red rectangles to represent the tracking results of the OSPT <ref type="bibr" target="#b0">[1]</ref>, APGL1 <ref type="bibr" target="#b1">[2]</ref>, LSAT <ref type="bibr" target="#b2">[3]</ref>, ASLAS <ref type="bibr" target="#b3">[4]</ref>, MTT <ref type="bibr" target="#b4">[5]</ref>, SCM <ref type="bibr" target="#b5">[6]</ref> and the proposed method, respectively. occlusion, camera motion and background clutter, like we demonstrate in Fig. <ref type="figure">1</ref>.</p><p>A general way to construct a robust tracking system involves two key components: a motion model, e.g., particle filter <ref type="bibr" target="#b16">[17]</ref> or Kalman filter <ref type="bibr" target="#b17">[18]</ref>, that forecasts the likely movements of the target over time to supply the tracker with a number of candidate states; an observation model (or an appearance model) that evaluates the likelihood of each candidate state being the true target state and selects the best candidate as the tracking result for the current frame, which is the core of a tracking system.</p><p>Since the first time sparse representation is introduced into visual tracking by Mei and Ling <ref type="bibr" target="#b18">[19]</ref>, it has been employed to build various efficient trackers (we refer them as sparse trackers in the following paper) with favorable experimental performance against other state-of-the-art trackers. In <ref type="bibr" target="#b18">[19]</ref>, the feature vector of a candidate state is reconstructed by both the target templates and the trivial templates (accounting for noisy pixels) with the sparsity and nonnegativity constraints on the reconstruction coefficients. Later, the likelihood of this candidate being the true target is measured upon its error in being reconstructed by the target templates. This method requires solving the 1 minimization problem as many times as the number of candidates, making it quite computational expensive. To explore more efficient solutions within the same framework, in <ref type="bibr" target="#b19">[20]</ref> an approximate solution is developed to reduce the number of particles that need to be sparsely decomposed, and in <ref type="bibr" target="#b1">[2]</ref> an efficient gradient descent approach is introduced to accelerate the solving process of the 1 minimization problem.</p><p>Sharing the candidate evaluation scheme in <ref type="bibr" target="#b18">[19]</ref>, some other sparsity based tracking algorithms build new formulations with customized sparse constraint terms. In <ref type="bibr" target="#b2">[3]</ref>, Liu et al. select a sparse set of features for representing target objects and extend the sparsity constraint to the dynamic group sparsity constraint considering the contiguous distribution of noised pixels. Zhang et al. <ref type="bibr" target="#b4">[5]</ref> formulate the tracking problem using sparse representation within the multi-task learning framework in which the similarities between candidates are exploited by enforcing joint group sparsity with mixed norm constraints. An algorithm also considering the relevance among candidates is presented in <ref type="bibr" target="#b20">[21]</ref> where the tracking problem is posed as a low-rank matrix learning problem.</p><p>Although these new formulations are effective in modeling the object, the reconstruction error based candidate evaluating scheme that they share is neither efficient nor robust. Therefore, several sparse trackers not only propose new sparsity involved models but also introduce improvements on the candidate evaluation scheme. Liu and Sun <ref type="bibr" target="#b21">[22]</ref> propose to use a dictionary composed by all candidates and trivial templates to represent a static object template and view the decomposition coefficient as the similarity between all candidates and the templates. Wang et al. <ref type="bibr" target="#b0">[1]</ref> replace the target templates with online updated PCA basis vectors, which can better express the target object subspace. Meanwhile, they use an occlusion mask to explicitly consider the effect of occluded pixels when evaluating a candidate. Jia et al. <ref type="bibr" target="#b3">[4]</ref> propose a structural local sparse appearance model that integrates local and global information of an observed image through an alignment pooling method, and the coefficients after pooling are summed to sort the candidates. Zhong et al. <ref type="bibr" target="#b5">[6]</ref> develop two independent sparsity-based models and evaluate the candidates by integrating the information from both models.</p><p>All the aforementioned sparsity based methods yield impressive tracking performance, however, most of them focus on measuring how a candidate is resembling the foreground object while ignoring the background information, which makes them subject to drifts when objects are similar to the target appearance or when the target appearance bears some similarity with the background objects due to partial occlusion. Although Zhong et al. <ref type="bibr" target="#b5">[6]</ref> employ a discriminative model, it is more of an assistant to the generative model and it makes the tracker redundant since hundreds of the candidates are all evaluated twice, which entails the number of involved 1 minimization problem to be doubled, greatly aggravating the computational complexity.</p><p>Through the above analysis, we propose a reversed multitask sparse tracking framework which projects the templates matrix (both positive and negative templates) into the candidates space. By selecting and weighting the discriminative sparse coefficients, the DSS map and pooling method lead to the best candidate. Our contributions can be summed up in the following three aspects:</p><p>• First, we propose an innovative optimization formulation named multi-task reverse sparse representation. In our work, a single task means to reconstruct a template with a few candidates that bear more similarity with the template than the others, which is inverse to the traditional sparsity based formulations (like those in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>) and multi-task means that we seek to simultaneously reconstruct multiple templates. A customized APG method is derived for getting the optimum solution (in matrix form) within several iterations. A Laplacian term is also included to keep the coefficients similarity level in accordance with the candidates similarities, which makes our tracker more robust as the experimental observations show. This formulation provides the tracker with the similarity relationship between all the candidates and templates through solving only one optimization problem without loss of accuracy. Therefore, this formulation is more superior in terms of cost-performance ratio. • Second, we construct a discriminative sparse similarity map (DSS map) based upon those similarity relationship. The discriminative information containing in this map comes from a large template set composed by multiple positive target templates and hundreds of negative templates. Both the target templates and the background templates are updated online to accommodate the appearance change in and near the target area. With this DSS map, candidates are evaluated in both directions: not only how similar it is to the target object but also how different it is from the background. This is also one of the key difference from most previous sparse trackers like <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, making our tracker more robust when similar objects appear near the target or when the target appearance bears some similarity with the background due to partial occlusion. • Third, we propose a simple yet useful additive pooling method to make the best use of the information in the DSS map and before this step the DSS map would be refined with adaptive weights to get rid of potential instability. Through this pooling scheme, the information for each candidate is integrated to be a single score and the candidate with the highest score is regarded as the tracking result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BAYESIAN INTERFERENCE FRAMEWORK</head><p>We carry out the object tracking in a Bayesian interference framework, a technique for estimating the posterior distribution of state variables that characterize a dynamic system, to form a robust tracking algorithm. We define the observation set of target Z t = [z 1 , z 2 , . . . , z t ], and let x t be the state variable of an object at time t. In the tracking frame, we use the affine transformation to model the object motion between two consecutive frames. Then the optimal state xt can be computed by the maximum a posterior (MAP) estimation, xt = arg max</p><formula xml:id="formula_0">x i t p (x i t |Z t ) (1)</formula><p>where x i t indicates the state of the i -th sample. The posterior probability can be inferred from the Bayesian framework recursively,</p><formula xml:id="formula_1">p(x t |Z t ) ∝ p(z t |x t ) p(x t |x t -1 ) p(x t -1 |Z t -1 )dx t -1 (2)</formula><p>where p(x t |x t -1 ) is the dynamic model and p(z t |x t ) denotes the observation likelihood. The state variable x t is composed of six independent parameters { 1 , 2 , 3 , 4 , 1 , 2 }, in which { 1 , 2 , 3 , 4 } are the deformation parameters and { 1 , 2 } contain the 2D transformation information. As the dynamic model can be modeled by the Gaussian distribution, it can be represented by</p><formula xml:id="formula_2">p(x t |x t -1 ) = N(x t ; x t -1 , σ ) (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where σ is a diagonal covariance matrix whose elements are the variances of the affine parameters.</p><p>Through this method, we get the candidates set Y = [y 1 , y 2 , ..., y m ] ∈ R d×m , in which d is the feature dimension and m is the number of candidates. The observation model p(z t |x t ) essentially reflects the likelihood of observing z t at state x t . In this paper, p(z t |x t ) is proportional to the discriminative score obtained by exploiting the additive pooling scheme on the DSS map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Discriminative Reverse Sparse Representation</head><p>To construct a robust tracker, the number of templates and candidates always amount to hundreds or even thousands and high-dimension features must be used to keep the profuse target information. However, traditional sparse coding based trackers perform computationally expensive 1 regularizations at each frame for each candidate. Hundreds of 1 regularizations per frame make the computational load so high that the tracker is unsuitable to process high-dimensional image features for fast and robust tracking applications under dynamic environment. As a reverse thought to conventional sparse representation, where a candidate (an observed image patch associated with a state) is reconstructed mainly by several target templates, we construct the dictionary with the candidate set Y to represent each target template as in Eq. 4 with the sparsity and nonnegativity constraints,</p><formula xml:id="formula_4">arg min c ||t -Yc|| 2 2 + λ||c|| 1 , s.t. c 0 (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where t denotes a representative template, λ is the parameter to adjust the sparsity penalty term and c represents the coefficient vector.</p><p>With the sparsity constraint and the goal to minimize the reconstruction error term, only a few candidates that bear more similarity to the template would be involved in representing the template. Their associated elements in c are positive and the magnitudes of these elements are assumed to imply the similarity levels. Thus, we add a constraint entry, c 0, which means all the elements of c are nonnegative for the reason that each element represents the similarity between the corresponding template and candidate, and negative elements are meaningless.</p><p>Beyond that, although through using the 1 minimization can the tracker be efficient and adaptive to appearance change, the lack of negative templates makes its discriminative power poor for ignoring the background information around the target, which may cause the tracker gradually drift away from the target. Therefore, in this work, multiple positive target templates are exploited so as to make the tracker more responsive to a variety of appearance change. Meanwhile, in order to better capitalize on the distinction between the foreground and the background to locate the target, we use plenty of negative templates, which are capable of fully sketching out the periphery of the target area.</p><p>The positive and negative template sets are respectively defined as T pos = [t 1 , t 2 , ..., t p ] and T neg = [t p+1 , t p+2 , ..., t p+n ], where p and n denote the number of positive and negative templates.</p><p>With these assumptions, our problem formulation is equivalent to an ensemble of sparse decomposition problems that the templates are effectively expressed by finding the combination of the particles and the corresponding coefficients as the following: </p><formula xml:id="formula_6">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ arg min c 1 ||t 1 -Yc 1 || 2 2 +</formula><p>where</p><formula xml:id="formula_8">c i = [c 1 i , c 2 i , . . . , c m i ]</formula><p>expresses the sparse coefficients of the i -th template and c i 0, i = 1, 2, . . . , ( p + n) means all the elements in c i are nonnegative.</p><p>In this formulation, one template is decomposed in each sparse representation procedure through 1 optimization and the whole process terminates until all the positive and negative templates have been represented.</p><p>We give an illustration of the basic idea of this formulation in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><p>The matrix formed by the reconstruction coefficient vectors of all templates are defined as the sparse map C = [c 1 , . . . , c p+n ], which fundamentally reflects a mapping relationship between the reference templates and the candidates, i.e., the value of a map element c j i can be comprehended as an indicator of similarity between the i -th template and the j -th candidate.</p><p>The candidates that contribute more to reconstruct one template should correspond to a large map element while those involve little information of the template should correspond </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Laplacian Multi-Task Reverse Sparse Representation</head><p>Overall, the formulation presented in Eq. 5 suffers from two principal problems. First, it still requires solving multiple 1 minimization problems per frame, which is computationally expensive especially when a large number of template set is maintained. Second, the dependence information among features of particles is ignored, even similar features may have unreasonable difference in the responses of sparse representation, which specifically embodies in the disparity of the corresponding coefficients .</p><p>In order to alleviate these defects, we reformulate the problem of calculating decomposition coefficients for multiple templates into a single optimization procedure, where the optimum similarity map C can be calculated as a whole. Intuitively, we propose a multi-task concept here, in which a single task means one template can be represented in the form of linear combination of a few similar candidates, and further, the multi-task refers to reconstruct multiple templates simultaneously. We name this procedure a multi-task reverse sparse representation problem as Eq. 6</p><formula xml:id="formula_9">arg min C ||T -YC|| 2 2 + λ i ||c i || 1 s.t. c i 0, i = 1, 2, . . . , ( p + n). (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>In addition, to preserve the similarity of sparse codes for the similar candidate features, we introduce a customized Laplacian regularization term inspired by the success of similar implementation for image classification <ref type="bibr" target="#b24">[25]</ref>. To begin with, we have the following formulation:</p><formula xml:id="formula_11">arg min C ||T -YC|| 2 2 + λ i ||c i || 1 + δ 2 i j ||c i -c j || 2 B i j s.t. c i 0, i = 1, 2, . . . , ( p + n), (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where δ is the parameter to adjust the new regularization term and B is a binary matrix indicating the relationship between any two candidate features with</p><formula xml:id="formula_13">B i j = 1 if c i is among the K nearest neighbors of c j , otherwise, B i j = 0.</formula><p>The last part of this formula can be transformed as:</p><formula xml:id="formula_14">1 2 i j ||c i -c j || 2 2 B i j = i ||c i || 2 D i + j ||c j || 2 D j -2 i j c i c j B i j = 2tr(CLC )<label>(8)</label></formula><p>where L = D -B is the Laplacian matrix , the degree of c i is defined as</p><formula xml:id="formula_15">D i = p+n j =1 B i j and D = di ag(D 1 , D 2 , . . . , D p+n ).</formula><p>So, the Laplacian multi-task reverse optimization problem is reformulated as:</p><formula xml:id="formula_16">arg min C ||T -YC|| 2 2 + λ i ||c i || 1 + δtr(CLC ) s.t. c i 0, i = 1, 2, . . . , ( p + n). (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>Let 1 ∈ R m (m is the number of candidates) denote the column vector whose entries are all ones and denote (c i ) as:</p><formula xml:id="formula_18">ψ(c i ) = 0 c i 0 +∞</formula><p>other wise <ref type="bibr" target="#b9">(10)</ref> With this non-negative constraint, Eq. 9 can be optimized alternately as:</p><formula xml:id="formula_19">arg min C ||T -YC|| 2 2 + λ1 C1 + δtr(CLC ) + ψ(C) (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>where a i represents the i -th template. Then we apply the accelerated proximal gradient (APG) approach <ref type="bibr" target="#b1">[2]</ref> to solve this minimization problem with</p><formula xml:id="formula_21">F(C) = arg min C ||T -YC|| 2 2 + λ1 C1 + δtr(CLC ) G(C) = ψ(C) (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>where F(C) is a differentiable convex function and G(C) is a non-smooth convex function. Following the APG method, we need to solve an optimization problem:</p><formula xml:id="formula_23">μ k+1 = arg min C ξ 2 ||C -ε k+1 + ∇ F(ε k+1 ) ξ || 2 2 + G(C) (<label>13</label></formula><formula xml:id="formula_24">)</formula><p>where ξ is the Lipschitz constant as function F(ε k+1 ) has the nature of continuous gradient and the variable k denotes the current interation time. Then we define</p><formula xml:id="formula_25">g k+1 = ε k+1 + ∇ F (ε k+1 ) ξ . Since ∇ F(ε k+1 ) = -Y (T -Yε k+1 ) + δε k+1 (L + L) + λ11 * ,<label>(14)</label></formula><p>we can easily get the formulation:</p><formula xml:id="formula_26">g k+1 = ε k+1 + 1 ξ [-Y (T-Yε k+1 )+δε k+1 (L +L)+λ11 * ]<label>(15)</label></formula><p>where 1 * ∈ R ( p+n) ((p+n) is the number of templates) denotes the vector whose entries are all ones. Based on the above assumption, Eq. 13 is equivalent to</p><formula xml:id="formula_27">μ k+1 = max(0, g k+1 )<label>(16)</label></formula><p>The algorithm for solving our Laplacian multi-task reverse sparse representation problem is summarized in Algorithm 1. The computational complexity of each iteration in Algorithm 1 is dominated by step 3. Thus, we can easily compute the perframe complexity to be O(kd</p><formula xml:id="formula_28">( p + n)),</formula><p>where k is the iteration number, d is the feature dimension and p + n is the total number of templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OBJECT TRACKING VIA THE DSS MAP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Weighted Discriminative Sparse Similarity Map 1) Discriminative Sparse Similarity Map:</head><p>In this subsection, we further interpret the discriminative sparse similarity map. As being introduced in the above sections and demonstrated in Fig. <ref type="figure" target="#fig_0">2</ref>, each column of C denotes the coefficients of a certain template decomposed by all candidates. However, it is worth explaining that each row of C corresponds to the responses of one candidate on all templates, which can be viewed as a discriminative feature of this candidate. For the i -th candidate, we have</p><formula xml:id="formula_29">f i = [C i1 , . . . , C ip , C i( p+1) , . . . , C i( p+n) ] (<label>17</label></formula><formula xml:id="formula_30">)</formula><p>Algorithm 1 Algorithm for Optimizing the Laplacian Multi-Task Reverse Sparse Representation.</p><p>where C i j is the element in the i -th row and the j -th column of C. From this perspective, the similarity map can be represented as</p><formula xml:id="formula_31">F = [f 1 , . . . , f m ] = C , (<label>18</label></formula><formula xml:id="formula_32">)</formula><p>where each column is a discriminative feature of a candidate, indicating its similarity levels to p positive templates and n negative templates.</p><p>The discriminative nature of this feature can be reflected from its larger elements distribution as shown in Fig. <ref type="figure" target="#fig_1">3</ref>. For a good candidate, the index of larger elements in f must be in the range <ref type="bibr">[1, p]</ref>, corresponding to several positive templates. Likewise, a bad candidate should be more similar to some negative templates, which results in larger coefficients index ranging in [ p + 1, p + n] while small or even zero coefficients on representing positive templates. For the subsequent implementation, we define two sub similarity maps as F pos = C pos and F neg = C neg .</p><p>2) Refined Discriminative Sparse Similarity Map: To get rid of potential instability and achieve better robustness, we refine the DSS map with adaptive weights. The weight W i j for an element F i j in the similarity map is constructed based on the difference between the j -th candidate y j and the i -th template t i :</p><formula xml:id="formula_33">W i j ∝ exp(-||t i -y j || 2 2 ). (<label>19</label></formula><formula xml:id="formula_34">)</formula><p>A candidate with smaller difference from a foreground template indicates they share higher similarity with each other, representing that the candidate is more likely to be a target object, and vice versa. For the following employment, we separate the weight map into two submaps:</p><formula xml:id="formula_35">W pos = [w 1 , . . . , w p ] , W neg = [w p+1 , . . . , w p+n ] , (<label>20</label></formula><formula xml:id="formula_36">)</formula><p>where</p><formula xml:id="formula_37">w i = [W i1 , . . . W im ] for i = 1, 2, . . . , ( p + n).</formula><p>Then we can get two weighted DSS maps through:</p><formula xml:id="formula_38">Fpos = W pos F pos , (<label>21</label></formula><formula xml:id="formula_39">)</formula><p>and</p><formula xml:id="formula_40">Fneg = W neg F neg , (<label>22</label></formula><formula xml:id="formula_41">)</formula><p>where is the Hadamard product (element-wise product). In the weighted DSS map, an element Fij = W i j • C i j is supposed to be large only when the j -th candidate has small difference from the i -th template and it plays a significant role in decomposing the i -th template with other candidates. Otherwise, Fij will have a small or even zero value, indicating that j -th candidate bears little similarity with the i -th template.</p><p>An example is shown in Fig. <ref type="figure" target="#fig_1">3(c</ref>) to illustrate the benefit of this refinement process. For the bad candidate, the sub-feature related to the positive templates (in red) are non-zero since the templates might account for some minor parts of the bad candidate in the 1 minimization process. Although their values are small, they might cause unexpected tracking result. However, by applying the adaptive weight, the refined subfeature related to positive templates (in red) are suppressed to be close to zero, which means the bad candidate just bears similarity to some negative templates instead of any positive templates. In terms of this view, we can get the most accurate feature for the candidate in order to get the convincing final candidate score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additive Pooling</head><p>For the i -th candidate, we view the i -th column in the refined similarity map F as a refined discriminative feature: fi = [ F1i , . . . . . . , Fpi , F(p+1)i , . . . . . . , F(n+p)i ] , <ref type="bibr" target="#b22">(23)</ref> and we have two sub features each representing the candidate's resemblance to the positive and negative templates:</p><formula xml:id="formula_42">fi-pos = [ F1i , . . . . . . , Fpi ] , fi-neg = [ F(p+1)i , . . . . . . , F(n+p)i ]<label>(24)</label></formula><p>from which we calculate the candidate's confidence in being the true target object s i through an intuitive additive pooling method which consists of two steps. First, we separately plus the largest l coefficients in fi-pos and fi-neg to get the scores s i-pos and s i-neg indicating what extent can the i -th candidate be related to the positive and the negative template sets. This process can be concluded as the following:</p><formula xml:id="formula_43">s i-pos = L( fi-pos , 1)+, • • • + L( fi-pos , l), s i-neg = L( fi-neg , 1)+, • • • + L( fi-neg , l) (<label>25</label></formula><formula xml:id="formula_44">)</formula><p>where L( f, k) denotes the k-th largest element in f and in this work we set l half the number of positive templates. Discarding the small map values that may come from uncertain interference ensures that we get more robust scores.</p><p>Second, the discriminative score for the i -th candidate is formulated by <ref type="bibr" target="#b25">(26)</ref> and the score set for all the candidates is denoted as S = {s i } i=1,...,m . This formulation is based on the assumption that a candidate with a larger foreground score and a smaller background score is more likely to be the target object, and vice versa. Namely, a target observation should have large discriminative score while a bad candidate has a relatively small one. Thus the additive pooling process is completed after two steps defined by Formulation 25 and 26.</p><formula xml:id="formula_45">s i = s i-pos -s i-neg</formula><p>The likelihood of the observation y i being the target at state x t can be constructed within the Bayesian framework by</p><formula xml:id="formula_46">p(y i |x t ) ∝ s i (27)</formula><p>Finally, the target observation y t can be located by maximizing</p><formula xml:id="formula_47">p(y t |x t ) = max p(y i |x t ) (28)</formula><p>We give a summary of this additive pooling scheme in Fig. <ref type="figure" target="#fig_2">4</ref>. Here, we could notice that some discriminative scores are similar to each other. This result is rational because we sample numerous candidates, and inevitably, some of them share the similar features, which leads to the similar responses to the additive pooling scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPORTANT IMPLEMENTATION SCHEMES</head><p>To make this work clear and complete, we will briefly introduce some less novel but rather important implementation schemes in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Locally Normalized Features</head><p>In this work we adopt locally normalized features to withstand partial occlusion and moderate appearance variation. An observed image patch A is partitioned into E local patches, each of which is independently expressed in gray scale values, vectorized and normalized to be a vector with unit 2 norm. Then we concatenate these local feature vectors so that the global structural information is maintained. The candidates and templates in this work are all represented with this locally normalized features to handle partial occlusion and to moderate appearance variation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Initial Discriminative Template Sets</head><p>The first tracking result is a manually chooser rectangle area. Let us assume point Q(h, v) be the center of the rectangle region, and we sample p patches as the initial positive templates around Q(h, v) within a circular area which satisfies ||Q i -Q(h, v)|| &lt; ϕ, where Q i is the center of the i -th sampled patch. Similarly, the initial negative template set, that is updated dynamically, is sampled from the annular region</p><formula xml:id="formula_48">ϕ &lt; ||Q j -Q(h, v)|| &lt;</formula><p>a few pixels away from Q, where Q j is the center of the j -th sampled image, ϕ and are the inner and outer radius of the annular region respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Update Scheme</head><p>For the positive template set, as the target in the first frame is always the ground-truth, we keep the first template in the positive template set unchanged to alleviate the drifting problem. We denote η = [η 1 , η 2 , . . . , η p ] as the similarity vector and set a threshold θ to describe the degree of similarity.</p><p>In each frame, we measure the similarity η i between the current tracking result and the i -th positive training template by applying the Euclidean distance. Then we compare the maximum similarity value = max η i , i = 1, 2, . . . , m with the threshold θ . If &gt; θ, then we use the tracking result to replace the corresponding positive template which has the largest similarity with the new target appearance. Otherwise, it means there is an incredibly large appearance change in adjacent frames or a significant part of the target object is occluded. Then, we discard this bad sample without update.</p><p>On the other hand, for the negative templates, although the background information varies a lot along the tracking process, we only sample negative templates around the tracking result in the last frame. Since the backgrounds of two successive frames are quite similar, the negative templates could be well represented by the current candidates that contain much background information. In this way, these bad candidates would achieve lower scores in the following pooling step without being considered as possible tracking result for they take part in representing negative templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>The proposed algorithm is implemented in MATLAB and runs at 2 frames per second on a 2.5 GHz i5-2450M Core PC with 4GB memory. The parameters, which are fixed for each sequence, are summarized as follows. In Eq. 9, the sparse regularization constant λ is set to be 0.04 and the Laplacian constraint δ is 0.8. The iteration number is 5 and the Lipschitz constant ξ is equal to 1/0.00018, respectively. The variables p and n (the number of positive and negative templates) are set to be 10 and 150 respectively. The update threshold θ is 0.4. We resize the target image patch to 32×32 pixels and extract 4×4 local patches within the target region. We update both positive templates and negative templates in each frame. The MATLAB source code and datasets will be made available on our website (http://ice.dlut.edu.cn/lu/publications.html).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Key Component Validation</head><p>In this section, we qualitatively discuss the effect of the Laplacian constraint term and the negative templates. It is worth noticing that the OWN (our algorithm without the negative template set) and the OWL (our algorithm without the Laplacian constraint) perform relatively good as well, from which we can conclude that the overall framework is effective. But without negative templates or the Laplacian constraint, the robustness of our tracker indeed decreases to some extent. As is shown in Tables <ref type="table" target="#tab_1">I</ref> and<ref type="table" target="#tab_2">II</ref>, all the results of the proposed algorithm are better than the ones of the OWN and the OWL. Compared with OWL, we can come to a conclusion that the Laplacian constraint serves to increase the stability of the proposed algorithm. The OWN tracker performs relatively poor compared with OWL and the proposed algorithm in those sequences undergoing heavy occlusion or severe background clutter, which can demonstrate the significant role of negative templates in handling occlusion and segregating the foreground target from the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Evalution</head><p>We use fifteen challenging videos in the experiment to evaluate the performance of the proposed algorithm. The challenging factors of these videos include heavy occlusion, motion blur, pose variation, background clutter and illumination change. The proposed approach is compared with eleven state-of-the-art algorithms, including the IVT <ref type="bibr" target="#b6">[7]</ref>, APGL1 <ref type="bibr" target="#b1">[2]</ref>, PN <ref type="bibr" target="#b7">[8]</ref>, VTD <ref type="bibr" target="#b8">[9]</ref>, MILTrack <ref type="bibr" target="#b9">[10]</ref>, FragTrack <ref type="bibr" target="#b10">[11]</ref>, MTT <ref type="bibr" target="#b4">[5]</ref>, OSPT <ref type="bibr" target="#b0">[1]</ref>, ASLAS <ref type="bibr" target="#b3">[4]</ref>, LSAT <ref type="bibr" target="#b2">[3]</ref> and SCM <ref type="bibr" target="#b5">[6]</ref> methods. Also two extra algorithms, OWL and OWN, are introduced for  self-comparison. For fair evaluation, we use the source code provided by the authors and run these codes with the same initial position of the target.</p><p>For the purpose of assessing the performance of the proposed tracker, two criteria, the center location error as well as the overlap rate, are employed in our paper. It should be noted that a smaller average error or a bigger overlap rate means a more accurate result. Given the tracking result of each frame R T and the corresponding ground truth R G , we can get the overlap rate by the PASCAL VOC <ref type="bibr" target="#b25">[26]</ref> criterion, score = area(R T ∩R G ) area(R T ∪R G ) . Tables I and II report the quantitative comparison results in terms of the average center location errors and average overlap rates respectively. As shown in the tables, the proposed tracker yields favorable performance against other state-of-the-art methods.</p><p>Regarding the computational loads, in the last row of Table <ref type="table" target="#tab_2">II</ref> we report the comparison result in terms of fps, which is obtained by running all the algorithms on computers with same configuration and using the same dataset for fair comparison. From the result we could tell that although the sparse representation based trackers (the APGL1, MTT, OSPT, ASLAS, LSAT, SCM trackers and the proposed tracker) are slower than some classic trackers like the IVT and MIL trackers, they generally yield superior performance. Among the sparsity based trackers, the proposed tracker is best in terms of accuracy and second in speed, striking a good balance between performance and computational load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Evaluation</head><p>As the sparse trackers generally perform better than the other state-of-the-art methods and they are more related to our work, we only demonstrate the comparison results with them in Fig. <ref type="figure" target="#fig_3">5</ref>, including the OSPT <ref type="bibr" target="#b0">[1]</ref>, APGL1 <ref type="bibr" target="#b1">[2]</ref>, LSAT <ref type="bibr" target="#b2">[3]</ref>, ASLAS <ref type="bibr" target="#b3">[4]</ref>, MTT <ref type="bibr" target="#b4">[5]</ref>, SCM <ref type="bibr" target="#b5">[6]</ref> and the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heavy Occlusion:</head><p>We test four sequences (Occlusion1, Woman, Caviar1, Caviar2) characterizing in having either severe occlusion or long-time partial occlusion. Fig. <ref type="figure" target="#fig_3">5</ref>(a) and (b) confirms the truth of the robustness of the proposed algorithm in dealing with rotation and scale change when the target undergoes heavy occlusion. Since two sub discriminative features are formulated to evaluate the candidate's similarity to the positive and the negative template set respectively, although a good candidate with occlusion bears some similarity with the background, other misaligned candidates bears more, which make their final scores lower than the good candidate. What's more, as we use the particles to reconstruct the templates, the influence from occluded parts of the particles is effectively suppressed for the reason that they contribute little to the reconstruction progress. Motion Blur: Fig. <ref type="figure" target="#fig_3">5(c</ref>) presents the tracking results on the sequences Face, Jumping and Deer. As the target object undergos abrupt motion, it is rather tough to accurately locate its position and account for the blurs which reduce the discriminative information in feature vectors. It is worth noticing that the proposed method performs better than other algorithms. Thanks to the discriminative template set and the update scheme, it is easier for our tracker to maximally capture the appearance change information in and near the target area and accurately select the target from the background even with limited discriminative information in the feature vectors when the blurs occurs.</p><p>Illumination Change: Fig. <ref type="figure" target="#fig_3">5(d</ref>) demonstrates the tracking results on the sequences DavidIndoor, Singer1 and Car4 with drastic illumination change. Our tracker can successfully tail the target throughout entire sequences, which can be attributed to the locally normalized features that have great effect in resisting the light change. We also observe that due to the template update strategy with the incremental subspace learning which enables the tracker to capture light change, the ASLAS algorithm achieves good performance in these sequences as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation:</head><p>The sequences, Sylvester2008b, Girl and Dudek, involving both in-plane and out-of-plane rotations are reported in Fig. <ref type="figure" target="#fig_3">5(e</ref>). As we use the affine transformation parameters that include the rotation angle modeling, we can capture the rotating candidates for further selection. We also observe that some trackers do not adapt to scale or in-plane rotation (e.g., LSAT, APGL1 and MTT).</p><p>Background Clutter: Fig. <ref type="figure" target="#fig_3">5</ref>(f) shows the tracking results in the Cliffbar and Car11 with complex background. By introducing both the positive template set and the negative template set to model the foreground and the background information respectively, we can obtain enough discriminative information and store them in the DSS map. Meanwhile, the additive pooling method effectively extract the discriminative information in the DSS map and enables our method to accurately calculate the discriminative scores and find the optimum candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we propose an efficient tracking algorithm based on a discriminative sparse similarity map which is obtained via a multi-task reverse sparse coding approach with Laplacian constraint. The proposed formulation enjoys advantages including light computational load through using a customized APG method and ideal stability by incorporating a Laplacian term. The employment of dynamically updated positive and negative template sets supplies our tracker with sufficient discriminative information, which is stored in the DSS map and accurately integrated via an additive pooling scheme. Both quantitative and qualitative evaluations against several state-of-the-art algorithms based on challenging image sequences demonstrate the accuracy and the robustness of the proposed tracker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Problem Formulation. This figure illustrates the basic idea of the multi-task reverse sparse representation scheme. (a) The positive and the negative template sets. (b) The sampled candidates. (c) The discriminative sparse similarity map (DSS map).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. This figure illustrates how the discriminative sparse similarity map indicates whether a candidate is good or not. (a) The original discriminative similarity map. A typical good candidate and a bad one are picked as examples. (b) The process regarding how to obtain the refined discriminative feature for the good candidate. The notion is the Hadamard product (element-wise product). (c) The process regarding how to obtain the refined discriminative feature for the bad candidate. The sub features related to the positive/negtive templates are shown in red/green. We notice that the positive part in the refined discriminative feature for the bad candidate is weakened by the adaptive weights.</figDesc><graphic coords="5,68.63,118.61,417.74,125.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. This figure intuitively illustrates how to get the discriminative scores for all candidates and choose the best candidate state based on it. (a) The weighted DSS map. (b) Two score vectors after the first step of additive pooling, and they respectively indicate the degree of resemblance to the positive (upper one) and the negative (bottom one) template set for all candidates. (c) The final discriminative score vector after the second step of additive pooling. (d) The optimal state corresponding to the candidate that scores the highest.</figDesc><graphic coords="7,225.59,127.37,98.18,61.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sample tracking results on fifteen challenging sequences. (a) Occlusion1 and Woman with heavy occlusion and in-plane rotation. (b) Caviar1 and Caviar2 with heavy occlusion and in-plane rotation. (c) Face, Jumping and Deer with abrupt motion. (d) DavidIndoor, Singer1 and Car4 with illumination change. (e) Sylvester2008b, Girl and Dudek with pose variation. (f) Cliffbar and Car11 with background clutter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>RESULTS IN TERMS OF AVERAGE CENTER ERROR (IN PIXELS). THE BEST THREE RESULTSARE SHOWN IN RED, BLUE, AND GREEN FONTS.(The Last Two Columns are for Self Comparison and do not Participate in Ranking)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>RESULTS IN TERMS OF AVERAGE OVERLAP RATE (IN PIXELS). THE BEST THREE RESULTS ARE SHOWN IN RED, BLUE, AND GREEN FONTS. THE LAST ROW SHOWS COMPARISON RESULTS ABOUT COMPUTATIONAL LOADS IN TERMS OF Fps. (The Last Two Columns are for Self Comparison and do Not Participate in Ranking)</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Natural Science Foundation of China under Grants 61071209 and 61272372, and in part by the Joint Foundation of China Education Ministry and China Mobile Communication Corporation under Grant MCM20122071. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Richard J. Radke.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Online object tracking with sparse prototypes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="314" to="325" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real time robust L1 tracker using accelerated proximal gradient approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1830" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust tracking using local sparse appearance model and k-selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulikowsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive structural local sparse appearance model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1822" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multi-task sparse learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2042" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparsitybased collaborative model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1838" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">P-N learning: Bootstrapping binary classifiers by structural constraints</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual tracking decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1269" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="983" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust fragments-based tracking using the integral histogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hough-based tracking of non-rigid objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distribution fields for tracking</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Lara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multiple kernel boosting with affinity constraints</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="254" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning structured visual dictionary for object tracking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="992" to="999" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Color-based probabilistic tracking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="661" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="577" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust visual tracking using 1 minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimum error bounded efficient 1 tracker with occlusion detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-rank sparse learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="470" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual tracking using sparsity induced similarity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ICPR</title>
		<meeting>20th ICPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1702" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On-line learning parts-based representation via incremental orthogonal projective non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1608" to="1623" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Least soft-threshold squares tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2371" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Local features are not lonely-Laplacian sparse coding for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3555" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
