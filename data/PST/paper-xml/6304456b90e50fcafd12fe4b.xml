<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MentorGNN: Deriving Curriculum for Pre-Training GNNs</title>
				<funder ref="#_yGjsKUT">
					<orgName type="full">IIS</orgName>
				</funder>
				<funder ref="#_ACkfYSw #_kpqDWhf #_6E8B7VB">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_YQffUya">
					<orgName type="full">C3.ai Digital Transformation Institute</orgName>
				</funder>
				<funder ref="#_Tu2X4hb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-21">21 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
							<email>zhoud@vt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
							<email>lecheng4@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
							<email>dongqif2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
							<email>jingrui@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech Blacksburg</orgName>
								<address>
									<region>Virginia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">CIKM &apos;22</orgName>
								<address>
									<addrLine>October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MentorGNN: Deriving Curriculum for Pre-Training GNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-21">21 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3511808.3557393</idno>
					<idno type="arXiv">arXiv:2208.09905v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain Adaptation</term>
					<term>Pre-Training Strategies</term>
					<term>GNNs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph pre-training strategies have been attracting a surge of attention in the graph mining community, due to their flexibility in parameterizing graph neural networks (GNNs) without any label information. The key idea lies in encoding valuable information into the backbone GNNs, by predicting the masked graph signals extracted from the input graphs. In order to balance the importance of diverse graph signals (e.g., nodes, edges, subgraphs), the existing approaches are mostly hand-engineered by introducing hyperparameters to re-weight the importance of graph signals. However, human interventions with sub-optimal hyperparameters often inject additional bias and deteriorate the generalization performance in the downstream applications. This paper addresses these limitations from a new perspective, i.e., deriving curriculum for pre-training GNNs. We propose an end-to-end model named Men-torGNN that aims to supervise the pre-training process of GNNs across graphs with diverse structures and disparate feature spaces. To comprehend heterogeneous graph signals at different granularities, we propose a curriculum learning paradigm that automatically re-weighs graph signals in order to ensure a good generalization in the target domain. Moreover, we shed new light on the problem of domain adaption on relational data (i.e., graphs) by deriving a natural and interpretable upper bound on the generalization error of the pre-trained GNNs. Extensive experiments on a wealth of real graphs validate and verify the performance of MentorGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Learning latent representations; Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the era of big data, graph presents a fundamental data structure for modeling relational data of various domains, ranging from physics <ref type="bibr" target="#b48">[49]</ref> to chemistry <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, from neuroscience <ref type="bibr" target="#b3">[4]</ref> to social science <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b64">65]</ref>. Graph neural networks (GNNs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61]</ref> provide a powerful tool to distill knowledge and learn expressive representations from graph structured data. Despite the successes of GNNs developments, many GNNs are trained in a supervised manner that requires abundant human-annotated data. Nevertheless, in many high-impact domains (e.g., brain networks constructed by fMRI) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>, collecting high-quality labels is quite resource-intensive and time-consuming. To fill this gap, the recent advances <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b59">60]</ref> have focused on pre-training GNNs by directly learning from proxy graph signals (shown in Figure <ref type="figure" target="#fig_0">1</ref>) extracted from the input graphs. The key assumption is that the extracted graph signals are informative and task-invariant, and as such, the learned graph representation can be generalized well to tasks that have not been observed before.</p><p>Nevertheless, the current GNN pre-training strategies are still at the early stage and suffer from multiple limitations. Most prominently, the performance of the existing pre-training strategies largely relies on the quality of graph signals. It has been observed that noisy and irrelevant graph signals often lead to negative transfer <ref type="bibr" target="#b47">[48]</ref> and marginal improvement in many application scenarios <ref type="bibr" target="#b19">[20]</ref>. For example, one may want to predict the chemical properties <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> of a family of novel molecules (e.g., the emerging COVID-19 variants). However, the available data (e.g., the known coronavirus) for pre-training are homologous but with diverse structures and disparate feature spaces. In this case, how can we control the risk of negative transfer and guarantee the generalization performance in the downstream tasks (e.g., characterizing and predicting a new COVID-19 variant)? Even worse, the risk of negative transfer can be exacerbated when encountering heterogeneous graph signals, i.e., the graph signals can be categorized into distinct types (e.g., class-memberships, attributes, centrality scores, and temporal dependencies) at different granularities (e.g., nodes, edges, and subgraphs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref>. To accommodate the heterogeneity of graph signals, the current pre-training strategies are often handengineered by using some hyperparameters <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Consequently, the inductive bias from humans might be injected into the pretrained models thus deteriorating the generalization performance.</p><p>We identify the following two challenges for alleviating the negative transfer phenomenon of pre-training GNNs. First (C1. Cross-Graph Heterogeneity), how can we distill the informative knowledge from source graphs and translate it effectively to the target graphs for solving novel tasks? Second (C2. Graph-Signal Heterogeneity), how can we combine and tailor complex graph signals to further harmonize their unique contributions and maximize the overall generalization performance? To fill the gap, it is crucial to obtain a versatile GNN pre-training model that can enable knowledge transfer from the source domain to the target domain and carefully exploit the relevant graph signals for downstream tasks.</p><p>In this paper, we propose an end-to-end framework, namely Men-torGNN, to seamlessly embrace both aforementioned objectives for pre-training GNNs. In particular, to address C1, we propose a multiscale encoder-decoder architecture that automatically summarizes graph contextual information at different granularities and learns a mapping function across different graphs. Moreover, to address C2, we develop a curriculum learning paradigm <ref type="bibr" target="#b23">[24]</ref>, in which a teacher model (i.e., graph signal re-weighting scheme) gradually generates a domain-adaptive curriculum to guide the pre-training process of the student model (i.e., GNNs) and enhance the generalization performance in the tasks of interest. In general, our contributions are summarized as follows.</p><p>? Problem. We formalize the domain-adaptive graph pre-training problem and identify multiple unique challenges inspired by the real applications. ? Algorithm. We propose a novel method named MentorGNN, which 1) automatically learns a knowledge transfer function and 2) generates a domain-adaptive curriculum for pre-training GNNs across graphs. In addition, we derive a natural and interpretable generalization bound for domain-adaptive graph pre-training. ? Evaluation. We systematically evaluate the performance of MentorGNN under two settings: 1) single-source graph transfer and 2) multi-source graph transfer. Extensive results prove the superior performance of MentorGNN under both settings.</p><p>The rest of our paper is organized as follows. In Section 2, we introduce the preliminary and problem definition, followed by the details of our proposed framework MentorGNN in Section 3. Experimental results are reported in Section 4. We review the related literature in Section 5. Finally, we conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>Here we introduce the notations (shown in Table <ref type="table" target="#tab_0">1</ref>) and the background of our problem setting. In this paper, we use regular letters to denote scalars (e.g., ?), boldface lowercase letters to denote vectors (e.g., v), and boldface uppercase letters to denote matrices (e.g., A). We denote the source graph and the target graph using </p><formula xml:id="formula_0">? , G (?) ? the ?-th layer coarse graphs of G ? , G ? . X (?) ? , X (?) ? the hidden representation of G (?) ? , G (?) ? . A (?) ? , A (?) ? the adjacency matrices of G (?) ? , G (?) ? . P (?) ? , P (?) ? the perturbation matrices of G (?) ? , G (?) ? . ? (?) ? , ? (?) ? # of supernodes in G (?) ? , G (?) ? . ? # of layers. ? # of graph signal types. ? Hadamard product. | ? | ? ? 2 -norm of a vector. G ? = (V ? , E ? , B ? ) and G ? = (V ? , E ? , B ? ), where V ? (V ? ) is the set of nodes, E ? (E ? )</formula><p>is the set of edges, and B ? (B ? ) is the node attributes in G ? (G ? ). Next, we briefly review the graph pre-training strategies and a theoretical model for domain adaptation.</p><p>Pre-training strategies for GNNs. Previous studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref> have been proposed to use easily-accessible graph signals to capture domain-specific knowledge and pre-train GNNs. These attempts are mostly designed to parameterize and optimize the GNNs by predicting the masked graph signals (e.g., node attributes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref>, edges <ref type="bibr" target="#b50">[51]</ref>, subgraphs <ref type="bibr" target="#b20">[21]</ref>, and network proximity <ref type="bibr" target="#b21">[22]</ref>) from the visible ones in the given graph G. In Figure <ref type="figure" target="#fig_0">1</ref>, we instantiate the masked graph signals at the level of nodes, edges, and subgraphs, respectively. To predict a given proxy graph signal s ? G, we use ? : s ? [0, 1] to denote the true labeling function and ? : s ? [0, 1] to denote the learned hypothesis by GNNs. The risk of the hypothesis ?(?) can be computed as ? (?, ? ) := E s? G [|?(s) -? (s)|]. In general, the overall learning objective of pre-training models for GNNs can be formulated as arg max</p><formula xml:id="formula_1">? log ?(G| ?, ? ) = ?? s? G ? s log ?(s| ?, ? )<label>(1)</label></formula><p>where ? is the corrupted graph with some masked graph signals s, ? s is a hyperparameter that balances the weight of the graph signal s, and ? is the hidden parameters of the hypothesis ?(?). By maximizing Eq. 1, we can encode the contextual information of the selected proxy graph signals into pre-trained GNNs ?(?). With that, the end-users can fine-tune the pre-trained GNNs and make predictions in the downstream tasks. Domain adaptation. Following the conventional notations in domain adaptation, we let ? ? (?) and ? ? (?) be the true labeling functions in the source and the target domains. Given ? ? (?) and ? ? (?), ? ? (?) = ? ? (?, ? ? ) and ? ? (?) = ? ? (?, ? ? ) denote the corresponding risks with respect to hypothesis ?(?). With this, Ben-David et al. proved a domain-adaptive generalization bound in terms of domain discrepancy and empirical risks <ref type="bibr" target="#b0">[1]</ref>. To approximate the empirical risks, one common approach <ref type="bibr" target="#b0">[1]</ref> is to assume the data points are sampled i.i.d. from both the source and the target domains. However, this assumption does not hold for the graph-structured data, as the samples in graphs (nodes, edges, subgraphs) are relational and non-i.i.d. in nature. To study the generalization performance of graph mining models, an alternative approach is to define the generalization bound based on the true data distribution <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b65">66]</ref>. For instance, Theorem 1 <ref type="bibr" target="#b65">[66]</ref> provides a population result, which does not rely on the i.i.d. assumption and can be deployed on graphs. Theorem 1. <ref type="bibr" target="#b65">[66]</ref> Let ?D ? , ? ? ? and ?D ? , ? ? ? be the source and the target domains, for any function class H ? [0, 1] X on the input space X, and ?? ? H , the following inequality holds:</p><formula xml:id="formula_2">? ? (?) ? ? ? (?) + ? H (D ? , D ? ) + min{E D ? [|? ? -? ? |], E D ? [|? ? -? ? |]}.</formula><p>Problem definition. In this paper, our goal (as shown in Figure <ref type="figure" target="#fig_0">1</ref>) is to learn a knowledge transfer function denoted as ?(?), such that the knowledge obtained by a GNN model in the source graph G ? can be transferred to the target graph G ? and pre-train the GNNs in G ? . Without loss of generality, we assume that 1) the source graph G ? is well studied by ? ? (?) and comes with rich label information for diverse graph signals s ? G ? , i.e., L ? = {(s, ?)|s ? G ? , ? ? Y ? }; and 2) the task of interest in the target graph G ? is novel, and we are only given a handful of annotated graph signals, i.e., L ? = {(s, ?)|s ? G ? , ? ? Y ? }. Given the notations above, we formally define the problem as follows.</p><p>Problem 1. Domain-Adaptive Graph Pre-Training Given: (i) source graph G ? = (V ? , E ? , B ? ) with rich label information L ? , (ii) target graph G ? = (V ? , E ? , B ? ) with scarce label information L ? , and (iii) user-defined graph neural network architecture. Find: the pre-trained model ?(?) that leverages the knowledge obtained from the source graph G ? and the target graph G ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED FRAMEWORK MENTORGNN</head><p>In this section, we introduce our proposed framework MentorGNN (shown in Figure <ref type="figure" target="#fig_5">2</ref>) to address Problem 1. The key challenges of Problem 1 lie in the dual-level data heterogeneity, namely crossgraph heterogeneity and graph-signal heterogeneity. Then, we dive into two major modules of MentorGNN, i.e., cross-graph adaptation (colored in blue in Figure <ref type="figure" target="#fig_5">2</ref>) and curriculum learning (colored in orange in Figure <ref type="figure" target="#fig_5">2</ref>), that are designed specifically for addressing the dual-level data heterogeneity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cross-Graph Adaptation via Multi-Scale Encoder-Decoder</head><p>The core obstruction of cross-graph adaptation lies in how to effectively translate the knowledge learned from G ? to G ? without any supervision of cross-graph association (e.g., partial network alignments <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b64">65]</ref>). Specifically, given G ? and G ? , we want to learn a transformation function ?(?) that leverages both network structures and node attributes over the entire graph and translate the knowledge between G ? and G ? , i.e., (V ? , E ? , B ? ) ? ?((V ? , E ? , B ? )). However, learning the transformation function ?(?) may require a large parameter space ? (? ? ? ? ) and become computationally intractable, especially when both G ? and G ? are large. In order to alleviate the computational cost, we propose to learn the translation function ?(?) at a coarser resolution instead of directly translating knowledge between G ? and G ? . The underlying intuition is that many real graphs from distinct domains may share similar highlevel organizations. For instance, in Figure <ref type="figure" target="#fig_0">1</ref> (Top), the book review network (G ? ) and the movie review network (G ? ) come from two distinct domains, but may share similar high-level organizations (e.g., alignments between book genres and movie genres). That is to say, the communities (the adventure books) on the book review network may have related semantic meanings to the communities (the adventure movies) on the movie review network. Motivated by this observation, we develop a multi-scale encoderdecoder architecture (the module colored in blue in Figure <ref type="figure" target="#fig_5">2</ref>), which explores the cluster-within-cluster hierarchies of graphs to better characterize the graph signals at multiple granularities. In particular, the encoder P learns the multi-scale representation of G ? by pooling the source graph from the fine-grained representation X ? to the coarse-grained representation X (?) ? , while the decoder U aims to reconstruct the target graph from the coarse-grained representation X (?) ? to the fine-grained representation X ? . In our paper, we set an identical number of layers ? in the encoder and the decoder to make G ? and G ? comparable with each other.</p><p>Encoder: The encoder is defined as a set of differentiable pooling matricies P = {P (1) , . . . , P (?) }, where</p><formula xml:id="formula_3">P (?) ? R ? (? ) ? ?? (? +1) ? , ? (?)</formula><p>? and ? (?+1) ? are the number of super nodes at layer ? and layer ? + 1. Specifically, following <ref type="bibr" target="#b63">[64]</ref>, the differentiable pooling matrix P (?) at layer ? is defined as follows.</p><formula xml:id="formula_4">P (?) = softmax(GNN ?,???? (A (?) ? , X (?) ? ))<label>(2)</label></formula><p>where each entry P (?) (?, ?) indicates the assignment coefficient from node ? at layer ? to the corresponding supernode ? at layer ? + 1, and GNN ?,???? is the corresponding surrogate GNN to generate the assignment matrix P (?) . It is noteworthy that the number of granularity levels ? ? 2 and the maximum number of supernodes in each layer ? (?)</p><p>? are both hyperparameters selected by the endusers. More implementation details are discussed in Section 4.1. With the differentiable pooling matrix P (?) , the ? th -layer coarsegrained representation X (?) ? of G ? can be approximated by X (?)</p><formula xml:id="formula_5">? = P (?-1) ? . . . P (1) ? X ? .</formula><p>Decoder: Different from the encoder, our decoder is composed of a translation function ?(?) and a set of differentiable unpooling matrices U = {U (1) , . . . , U (?) }. To be specific, the translation function ?(?) learns a non-linear mapping between G ? and G ? at the coarsest layer ?. In our implementation, ?(?) is parameterized by a multilayer perceptron (MLP), and the differentiable pooling matrix</p><formula xml:id="formula_6">U (?) ? R ? (? +1) ? ?? (? ) ?</formula><p>at each layer ? can be computed as follows.</p><formula xml:id="formula_7">U (?) = softmax(GNN ?,?????? (A (?) ? , X (?) ? ))<label>(3)</label></formula><p>In contrast to GNN ?,???? , GNN ?,?????? is a reverse operation that aims to reconstruct X (?-1) from its coarse representation X (?) . With the learned translation function ?(?) and the differentiable unpooling matrices U, the hidden representation of G ? can be computed by X? = U (1) ? . . . U (?) ? X(?) ? , where X(?)</p><formula xml:id="formula_8">? = ?(X (?)</formula><p>? ) is the translated representation from G ? to G ? at the ? th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Signal Comprehension via Curriculum Learning</head><p>In the presence of various types of graph signals, the current GNNs pre-trained models are mostly designed in the form of a weighted combination of multiple graph signal encoders by incorporating some hyperparameters. However, manually selecting hyperparameters are often challenging and may lead to sub-optimal performance due to the inductive bias from humans. Here we propose a graph signal re-weighting scheme (the module colored in orange in Figure <ref type="figure" target="#fig_5">2</ref>) to capture the contribution of each graph signal towards the downstream tasks. The learned sample weighting scheme specifies a curriculum under which the GNNs will be pre-trained gradually from the easy concepts to the hard concepts. , and to provide guidance to pre-train GNNs on the target graph G ? . To regularize the learning curriculum, one prevalent choice is to employ some pre-defined curriculums <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref>, which have been extensively explored in the literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b77">78]</ref>. Here we consider a curriculum regularizer derived from the robust non-convex penalties <ref type="bibr" target="#b22">[23]</ref>.</p><formula xml:id="formula_9">? (? ? (s (?,?) ? , ?), ? 1 , ? 2 ) = 1 2 ? 2 ? 2 ? (s (?,?) ? ; ?) -(? 1 + ? 2 )? ? (s (?,?) ? ; ?)</formula><p>where ? = {? 1 , ? 2 } are both positive hyperparameters. Note that</p><formula xml:id="formula_10">? (? ? (s (?,?) ? , ?), ? 1 , ? 2 ) is a convex function in terms of ? ? (s (?,?) ?</formula><p>, ?) and thus the closed-form solution of our learning curriculum can be easily obtained as follows.</p><formula xml:id="formula_11">? ? (s (?,?) ? ; ? * ) = ? ? ? ? ? ? ? 1(J (Y ? , ?( X(?) ? , s (?,?) ? , ? )) ? ? 1 ) ? 2 = 0 min(max(0, 1 - J (Y ? ,? ( X(? ) ? ,s (?,? ) ,? ))-? 1 ? 2 ), 1) Otherwise<label>(5)</label></formula><p>The learning threshold ? = {? 1 , ? 2 } plays a key role in controlling the learning pace of MentorGNN. When ? 2 = 0, the algorithm will only select the "easy" graph signals of J (Y ? , ?( X(?) ? , s (?,?) ? , ? )) ? ? 1 in training labeling function ?(?), which is close to the binary scheme in self-paced learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>. When ? 2 ? 0, ? ? (s  Update ? by taking a gradient step for L.</p><formula xml:id="formula_12">(?,?) ? ; ? * )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute learning curriculum ? ? (s (?,?) ? ; ?) via the closedform solution in Eq. 5. Augment the values of ? 1 and ? 2 by the ratio ?. , ? )) ? ? 1 + ? 2 will not be selected in pre-training. In practice, we gradually augment the value of ? 1 + ? 2 to enforce MentorGNN learning from the "easy" graph signals to the "hard" graph signals by mimicking the cognitive process of humans and animals <ref type="bibr" target="#b1">[2]</ref>. The pseudo-code of MentorGNN is provided in Algorithm 1, which is designed in an alternativeupdating fashion <ref type="bibr" target="#b52">[53]</ref>. In each iteration, when we train the student model ?( X(?) ? , s (?,?) ? , ? )), we keep ? fixed and minimize the prediction loss J (Y ? , ?( X(?) ? , s (?,?) , ? )); when we train the teacher model ? ? (s (?,?) ?</p><p>; ?), we keep ? fixed and update the learning curriculum to be used by the student model in the next iteration; at last, we augment the value of Augment the values of ? 1 and ? 2 by the ratio ? for the next iteration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalization Bound of MentorGNN</head><formula xml:id="formula_13">? ? (?) ?? ? (?) + ? H (G ? , G ? ) + max ?,? {min{E G ? [|? ? -? (?,?) ? |], E G ? [|? ? -? (?,?) ? |]}} (6)</formula><p>The worst-case generalization bound shown in Corollary 1 could be pessimistic in practice, especially when the graphs are large and noisy. However, extensive work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46]</ref> has empirically shown that leveraging multiple types of graph signals often leads to improved performance in many application domains, even in the presence of noisy data and irrelevant features. The key observation is that the information from multiple types of graph signals is often redundant and complementary. That is to say, when the majority of graph signals are related, a few irrelevant graph signals may not hurt the overall generalization performance too much. Hence, the natural question is: can we obtain a better generalization bound than the one shown in Corollary 1? To answer this question, we present a re-weighting case of the generalization bound for MentorGNN, that is developed based on the obtained learning curriculum ? ? (s (?,?) , ?) as follows. , ?) = 1, the following inequality holds:</p><formula xml:id="formula_14">? ? (?) ?? ? (?) + ? H (G ? , G ? ) + ?? ?,?,? ? ? (s (?,?) ? , ?) ? min{E G ? [|? ? -? (?,?) ? |], E G ? [|? ? -? (?,?) ? |]}}</formula><p>Remark 1: Corollary 1 and Corollary 2 are both based on the true data distribution, and as such, the derived generalization bound might slightly deviate from the empirical results in practice. However, we argue that the state-of-the-art GNNs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61]</ref> are reliable and accurate, whose empirical performance is very close to the true labeling function. Therefore, our theoretical results can well approximate the generalization bound of MentorGNN in practice.</p><p>Remark 2: Corollary 2 reduces the multi-type multi-granularity of graph signals into an aggregated version with a linear combination using ? ? (s (?,?) ? , ?). In fact, the worst-case generalization bound can be considered as a special case of the generalization bound shown in Corollary 2. Based on the following inequality, it is easy to prove that Corollary 2 provides a much tighter generalization bound than Corollary 1. ?? ?,?,? ? ? (s</p><formula xml:id="formula_15">(?,?) ? , ?) min{E G ? [|? ? -? (?,?) ? |], E G ? [|? ? -? (?,?) ? |]}} ? max ?,? {min{E G ? [|? ? -? (?,?) ? |], E G ? [|? ? -? (?,?) ? |]}}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate the performance of MentorGNN on both synthetic and real graphs in the following aspects: ? Effectiveness: We report comparison results with a diverse set of baseline methods, including state-of-the-art GNNs, graph pretraining models, and transfer learning models, in the single-source graph transfer and the multi-source graph transfer settings. ? Case study: We conduct a case study to study the generalization performance of MentorGNN in the dynamic protein-protein interaction graphs. ? Parameter sensitivity and scalability analysis: We study the parameter sensitivity and the scalability of MentorGNN on both synthetic and real graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Data sets: Cora <ref type="bibr" target="#b40">[41]</ref> data set is a citation network consisting of 2,708 scientific publications and 5,429 edges. Each edge in the graph represents the citation from one paper to another. CiteSeer <ref type="bibr" target="#b40">[41]</ref> data set consists of 3,327 scientific publications, which could be categorized into six classes, and this citation network has 9,228 edges. PubMed <ref type="bibr" target="#b44">[45]</ref>  Implementation details: The data and code are available in the link * . In the implementation of MentorGNN, we consider two types (? = 2) of graph signals, i.e., node attributes and edges, at ? = 3 levels of granularity. The output dimension of the first level of granularity is 500, and the output dimension of the second level of granularity is 100. We use Adam <ref type="bibr" target="#b30">[31]</ref> as the optimizer with a learning rate of 0.005 and a two-layer GAT <ref type="bibr" target="#b54">[55]</ref> with a hidden layer of size 50 as our backbone structure. MentorGNN and its variants are trained for a maximum of 2000 episodes. The experiments are performed on a Windows machine with eight 3.8GHz Intel Cores and a single 16GB RTX 5000 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Single-Source Graph Transfer</head><p>In this subsection, we first consider the single-source graph transfer setting (following <ref type="bibr" target="#b19">[20]</ref>), where we are given a single source graph (e.g., Cora <ref type="bibr" target="#b40">[41]</ref>, CiteSeer <ref type="bibr" target="#b40">[41]</ref>, and PubMed <ref type="bibr" target="#b44">[45]</ref>) and a single target graph (e.g., Reddit1, Reddit 2, Reddit 3 <ref type="bibr" target="#b17">[18]</ref>). Our goal is to pre-train GNNs across two graphs and then fine-tune the model to perform node classification on the target graph. We split the data set into training, validation, and test sets with the fixed ratio of 4%:16%:80%, respectively. Each experiment is repeated five times, and we report the average accuracy and the standard deviation of all methods across different data sets in Tables <ref type="table" target="#tab_4">2</ref><ref type="table" target="#tab_5">3</ref><ref type="table" target="#tab_6">4</ref>. Results reveal that our proposed method and its variants outperform all baselines. Specifically, we have the following observations: (1) compared with the traditional GNNs, e.g., GCN, GAT and DGI, our method and its variants can further boost the performance by utilizing the knowledge learned from both the source graph and the target graph; (2) * https://github.com/Leo02016/MentorGNN the performance of GPA is worse than GCN, GAT and DGI. One conjecture is that the performance of GPA largely suffers from the label scarcity issue in our setting. In particular, the results of GPA in <ref type="bibr" target="#b19">[20]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-Source Graph Transfer</head><p>In this subsection, we evaluate proposed model MentorGNN in the multi-Source graph transfer setting, where multiple source graphs are given for pre-training GNNs. In our experiments, we use three source citation networks (i.e., Cora, CiteSeer, and PubMed) as the source graphs, and our goal is to transfer the knowledge extracted from multiple source graphs to improve the performance of the node classification task on a single target graph (e.g., Reddit1, Reddit 2, Reddit 3). Similar to the single-source graph transfer setting, we use the same data split scheme to generate the training set, the validation set, and the test set. The full results in terms of prediction accuracy and standard deviation over five runs are reported in Table <ref type="table" target="#tab_7">5</ref>. Note that some of our baseline graph pretraining methods (i.e., UDA-GCN, GPT-GNN, Pretrain-GNN) are not designed for the multi-source graph transfer setting. Thus, we exclude them from Table <ref type="table" target="#tab_7">5</ref>. In general, our proposed method and its variants outperform all the baselines. Moreover, by combining the results (Table <ref type="table" target="#tab_4">2</ref> -4) in the single source-graph setting, it is interesting to see that the performances of GPA and MentorGNN are both slightly improved when we leverage multiple source graphs simultaneously. For instance, when we consider the Reddit1 as the target graph, the accuracy of GPA and our method is improved by 0.42% and 0.72% respectively compared with the single-source graph transfer setting (i.e., Cora ? Reddit1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study: Metabolic Pattern Modeling on Protein-Protein Interaction Graph</head><p>Protein analysis is of great significance in many biological applications <ref type="bibr" target="#b56">[57]</ref>. In this case study, we aim to apply MentorGNN to study and capture the metabolic patterns of yeast cells in the dynamic setting. To be specific, given a dynamic protein-protein interaction (PPI) graph, e.g., DPPIN-Breitkreutz <ref type="bibr" target="#b10">[11]</ref>, it consists of five snapshots G = {G 1 , . . . , G 5 }. Each node represents a protein, and each timestamped edge stands for the interaction between two proteins.   </p><formula xml:id="formula_16">{ G 1 , G 2 }, { G 1 , G 2 , G 2 , G 3 }, and { G 1 , G 2 , G 2 , G 3 , G 3 , G<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Sensitivity Analysis</head><p>In this subsection, we study the impact of the learning thresholds ? 1 and ? 2 on MentorGNN. In Figure <ref type="figure" target="#fig_11">4</ref>, we conduct the case study in the single-source graph transfer setting of Cora ? Reddit2. Particularly, we report the training accuracy and the test accuracy of our model with a diverse range of ? 1 and ? 2 . In general, we observe that: (1) the model often achieves the best training accuracy and test accuracy when ? 2 = 1; and (2) given a fixed ? 2 , both the training accuracy and test accuracy are improved with increasing ? 1 . It is because the learned teacher model enforces the pre-trained GNNs to learn from the "easy concepts" (i.e., small ? 1 ) to the "hard concepts" (i.e., large ? 1 ). In this way, the pre-trained GNNs encode more and more informative graph signals in the learned graph representation and thus achieve better performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Scalability Analysis</head><p>In this subsection, we study the scalability of MentorGNN in the setting of single-source graph transfer, where the source graph is Cora while the target graphs are a collection of synthetic graphs with an increasing number of nodes. To control the size of the target graphs, we generate the synthetic target graphs via ER algorithm <ref type="bibr" target="#b6">[7]</ref>.</p><p>In Figure <ref type="figure" target="#fig_12">5</ref>, we present the running time of MentorGNN with the different number of layers (? = 2, 3, 4) over the increasing number of nodes in the target graphs. All the results reported in Figure <ref type="figure" target="#fig_12">5</ref> are based on 1000 training epochs. In general, we observe that 1) the complexity of the proposed method is roughly quadratic w.r.t. the number of nodes, and 2) the running time only slightly increases when we increase the number of layers in MentorGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>In this section, we briefly review the related works in the context of pre-training strategies for graphs and domain adaptation.</p><p>Pre-Training Strategies for Graphs. To tackle the label scarcity in graph representation learning, pre-training strategies for GNNs have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>. The core idea is to capture the generic graph information across different tasks, and transfer it to the target task to reduce the required amount of labeled data and domain-specific features. Some recent methods effectively extract knowledge at the level of both individual nodes and the entire graphs via various techniques, including contrastive predictive coding <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref>, context prediction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b45">46]</ref>, and mutual information maximization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref>. To be specific, in <ref type="bibr" target="#b20">[21]</ref>, the authors design three topology-based tasks for pre-training GNNs to learn the transferable structural information, (i.e., node-level pre-training, subgraph-level pre-training, graphlevel pre-training); <ref type="bibr" target="#b20">[21]</ref> propose a pre-training framework for GNNs by combining the self-supervised pre-training strategy on the node level (i.e., neighborhood prediction and attributes masking) and the supervised pre-training strategy on the graph level (i.e., graph property prediction). However, the current pre-training strategies often lack the capability of domain adaptation, thus limiting their ability to leverage valuable information from other data sources. In this paper, we propose a novel method named MentorGNN that enables pre-training GNNs across graphs and domains. Domain Adaptation. To ensure that the learned knowledge is transferable between the source domains (or tasks) and the target domains (or tasks), many efforts tend to learn the domain-invariant or task-invariant representations, such as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74]</ref>. After the domain adaptation demand meets the graph-structured data, the transferability of GNNs has been theoretically analyzed in terms of convolution operations <ref type="bibr" target="#b34">[35]</ref>. Then to learn the domain-invariant representations with GNNs, several domain-adaptive GNN models are proposed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b76">77]</ref>. For example, UDA-GCN is proposed <ref type="bibr" target="#b58">[59]</ref> in the unsupervised learning setting to learn invariant representations via a dual graph convolutional network component. Pragmatically, in order to efficiently label the nodes in a target graph to reduce the annotation cost of training, GPA <ref type="bibr" target="#b19">[20]</ref> is proposed to learn a transferable policy with reinforcement learning techniques among fully labeled source domain graphs, which can be directly generalized to unlabeled target domain graphs. Despite the success of domain adaptation on graphs, little effort has been made to analyze the generalization performance of the deep learning models on relational data (e.g., graphs). Here we propose a new generalization bound for domain-adaptive pre-training on graph-structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Pre-training deep learning models is of key importance in many graph mining tasks. In this paper, we study a novel problem named domain-adaptive graph pre-training, which aims to pre-train GNNs from diverse graph signals across disparate graphs. To address this problem, we present an end-to-end framework named Men-torGNN, which 1) automatically learns a knowledge transfer function and 2) generates a domain-adaptive curriculum for pre-training GNNs from the source graph to the target graph. We also propose a new generalization bound for MentorGNN in the domain-adaptive pre-training. Extensive empirical results show that the pre-trained GNNs under MentorGNN with fine-tuning using a few labels achieve significant improvements in the settings of single-source graph transfer and multi-source graph transfer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative example of domain-adaptive graph pre-training. (Top) Domain-adaptive knowledge transfer between the book review network and the movie review network. (Bottom) Proxy graph signals (red masks) at the granularity of nodes, edges, and subgraphs.</figDesc><graphic url="image-1.png" coords="3,134.51,83.69,342.99,155.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 : 4 :</head><label>24</label><figDesc>Figure 2: An overview of the MentorGNN, which is composed of two modules, namely cross-graph adaptation (colored in blue) and curriculum learning (colored in orange).</figDesc><graphic url="image-2.png" coords="5,139.55,83.69,332.91,179.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>13: end while will return continuous values in [0, 1], and the graph signals with the loss of J (Y ? , ?( X(?) ? , s (?,?) ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Given a source graph G ? and a target graph G ? , what generalization performance can we achieve with MentorGNN in pre-training GNNs? Here, we present two approaches towards the generalization bound of MentorGNN in the presence of multiple types of graph signals at different granularities: one by a union bound argument and the other relying on the graph signal learning curriculum. Let ? ? (?) be the true labeling function in G ? , and ? (?,?) ? (?) be the true labeling function for the ? th type graph signals s (?,?) at the ? th level of G ? . One straightforward idea is to leverage the generalization error between ? ? (?) and each ? (?,?) ? (?) by applying Theorem 1 ? ? ? times. Following this idea, we can obtain the following worst-case generalization bound of MentorGNN, which largely depends on the largest generalization error between ? ? (?) and ? (?,?) ? (?). Corollary 1. (Worst Case) Given G ? and G ? , ? ? (?) is the labeling function in G ? , and ? (?,?) ? (?) is the labeling function for the ? th type graph signals s (?,?) at the ? th level of granularity in G ? . Then, for any function class H ? [0, 1] X , and ?? ? H , the following inequality holds:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Corollary 2 .</head><label>2</label><figDesc>(Re-weighting Case) Given G ? and G ? , ? ? (?) is the labeling function in G ? , and ? (?,?) ? (?) is the labeling function for the ? th type graph signals s (?,?) at the ? th level of granularity in G ? . Then, for any function class H ? [0, 1] X , ?? ? H , and ?,?,? ? ? (s (?,?) ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>require 67% labeled samples for training on Reddit1, while we are only given 4% labeled samples of Reddit1 for training. (3) UDA-GCN, GPT-GNN and Pretrain-GNN outperform the traditional GNNs in most cases, which implies that these pre-training algorithms indeed transfer the useful knowledge from the source graph and thus boost the performance of node classification in the target graph; (4) compared with the pre-training algorithms (UDA-GCN, GPT-GNN and Pretrain-GNN), MentorGNN further boosts the performance by more than 3% in the setting of CiteSeer ? Reddit2. (5) compared with MentorGNN-V which discards the link signal, MentorGNN could further improve the performance by up to 3.31% by utilizing the structural information of the graph; and (6) compared with MentorGNN-C which does not utilize the curriculum learning, MentorGNN boosts the performance by up to 1.88% in the setting of CiteSeer ? Reddit2. Overall, the comparison experiments verify the fact that MentorGNN largely alleviates the impact of negative transfer and improves the generalization performance across all data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) One-training pair. (b) Two-training pairs. (c) Three-training pairs. (d) Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Metabolic pattern modeling on DPPIN-Breitkreutz graph. (a)-(c) show the network layouts of the generated G 5 by learning from{ G 1 , G 2 }, { G 1 , G 2 , G 2 , G 3 }, and { G 1 , G 2 , G 2 , G 3 , G 3 , G 4 }, respectively; and (d) shows</figDesc><graphic url="image-5.png" coords="8,329.79,173.69,103.31,73.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 3: Metabolic pattern modeling on DPPIN-Breitkreutz graph. (a)-(c) show the network layouts of the generated G 5 by learning from{ G 1 , G 2 }, { G 1 , G 2 , G 2 , G 3 }, and { G 1 , G 2 , G 2 , G 3 , G 3 , G 4 }, respectively; and (d) shows the network layout of the ground-truth G 5 .In order to train MentorGNN, we use the past snapshot as the source graph and the future snapshot as the target graph. The five snapshots naturally form four pairs of ?source graph, target graph? in the chronological order, i.e.,G 1 , G 2 , G 2 , G 3 , G 3 , G 4 , G 4 , G 5 .In our implementation, we treat the first three pairs as the training set and use the remaining one for testing. After obtaining the knowledge transfer function ?(?) via MentorGNN, we aim to reconstruct the last snapshot G 5 by adapting from G 4 . Figure3shows the synthetic G 5 generated by MentorGNN using different portions of the training set as well as the ground-truth G 5 . In detail, Figure3(a)-(c) show the generated G 5 by learning from{ G 1 , G 2 }, { G 1 , G 2 , G 2 , G 3 }, and { G 1 , G 2 , G 2 , G 3 , G 3 , G 4 }, respectively; and Figure3(d)shows the ground-truth G 5 by using t-SNE<ref type="bibr" target="#b53">[54]</ref>. Each dot is the projected node in the embedding space, and the different colors correspond to three substructures of the PPI network. In general, we observe that Figure3(c) is most similar to the ground-truth in Figure 3(d), Figure 3(b) is the next, and Figure 3(a) is the least similar one. Through this comparison, we know that our MentorGNN captures the temporal evolution pattern, and the generated molecule graphs are more trustworthy given more temporal information.</figDesc><graphic url="image-6.png" coords="8,443.06,173.69,103.31,73.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Parameter analysis w.r.t. the learning thresholds ? 1 and ? 2 on Cora ? Reddit2.</figDesc><graphic url="image-7.png" coords="9,58.78,130.09,116.23,65.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Scalability analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Symbols and notation. ? , G ? input source and target graphs. V ? , V ? the set of nodes in G ? and G ? . E ? , E ? the set of edges in G ? and G ? . B ? , B ? the node attribute matrices in G ? and G ? . X ? , X ? the hidden representation of G ? , G ? . L ? , L ? the set of annotated data in G ? and G ? . ? ? , ? ? # of nodes in G ? and G ? . ? ? , ? ? # of edges in G ? and G ? . G</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>G (?)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>each graph signal without the requirements of the pre-defined hyperparameters, and 2) supervises the pre-training process to control the risk of negative transfer. Consider a GNN-based pre-training problem with ? types of graph signals extracted from graphs at ? levels of resolutions, we formulate the learning objective as follows. ? denotes the ? th sample of the type-? graph signals (e.g., edges, node attributes, centrality scores) extracted from the ? th -layer of G ? , ? ? (s ?) is the curriculum regularizer parameterized by the learning threshold ?. It is worthy to mention that such labeled examples can be any type of graph signals, including the class-memberships of nodes, edges, and even subgraphs. The goal of L is to automatically learn attention weight</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">L = arg min</cell><cell>??</cell><cell>??</cell><cell cols="2">??</cell><cell cols="2">? ? (s ? (?,?)</cell><cell>; ?)J (Y ? , ?(</cell><cell>X(?) ? , s ? (?,?)</cell><cell>, ? ))</cell></row><row><cell cols="3">?,?</cell><cell>?=1</cell><cell>?=1</cell><cell>s ? (?,? )</cell><cell>(? ) ? ? G</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">+ ? (? ? (s ? (?,?)</cell><cell>, ?), ?)</cell><cell>(4)</cell></row><row><cell>where s</cell><cell cols="2">(?,?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(?,?) ?</cell><cell cols="3">; ?) is a function that predicts the atten-</cell></row><row><cell cols="9">tion weights for each training graph signal s ? (?,?)</cell><cell>, ?(</cell><cell>X(?) ? , s ? (?,?)</cell><cell>, ? )</cell></row><row><cell cols="9">denotes the pre-trained GNNs using s ? (?,?)</cell><cell>, J (Y ? , ?(</cell><cell>X(?) ? , s ? (?,?)</cell><cell>, ? ))</cell></row><row><cell cols="9">denotes the prediction loss of a downstream task over a handful</cell></row><row><cell cols="9">of labeled examples Y ? , and ? (? ? (s ? (?,?) , ?), ? ? (s (?,?) ? ; ?) towards each graph signal s (?,?) ?</cell><cell>in order to guide the</cell></row><row><cell cols="9">pre-training process of GNNs ?(?) via curriculum learning, thus</cell></row><row><cell cols="9">mitigating the risk of negative transfer.</cell></row><row><cell cols="9">Intuitively, the learning objective in Eq. 4 can be interpreted as</cell></row><row><cell cols="9">a teacher-student training paradigm [8, 34, 70, 78]. In particular,</cell></row><row><cell cols="6">the labeling function ?(</cell><cell cols="2">X(?) ? , s ? (?,?)</cell><cell>, ? ) parameterized by ? serves</cell></row><row><cell cols="9">as the student model, which aims to learn expressive represen-</cell></row><row><cell cols="9">tation by predicting the graph signal s ? (?,?)</cell><cell>; the teacher model</cell></row><row><cell cols="2">? ? (s ? (?,?)</cell><cell cols="7">; ?) parameterized by ? aims to measure the importance</cell></row><row><cell cols="7">(?,?) ? of each graph signal s</cell><cell></cell></row></table><note><p>In our problem setting, the curriculum is presented as a sequence of graph signals that are extracted from the given graph. Compared with the previous GNNs pre-training methods, MentorGNN automatically 1) assigns an attention weight towards</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>is a diabetes data set, which consists of 19,717 scientific publications in three classes and 88,651 edges. The Reddit<ref type="bibr" target="#b17">[18]</ref> data set was extracted from Reddit posts in September 2014.</figDesc><table><row><cell>After pre-processing, three Reddit graphs are extracted and denoted</cell></row><row><cell>as Reddit1, Reddit2, and Reddit3, respectively. Reddit1 consists of</cell></row><row><cell>4,584 nodes and 19,460 edges; Reddit2 consists of 3,765 nodes and</cell></row><row><cell>30,494 edges; and Reddit3 consists of 4,329 nodes and 35,191 edges.</cell></row><row><cell>Baselines: We compare our method with the following baselines:</cell></row><row><cell>(1) GCN [33]: graph convolutional network, which is directly trained</cell></row><row><cell>and tested on the target graph; (2) GAT [55]: graph attention net-</cell></row><row><cell>work, which is directly trained and tested on the target graph; (3)</cell></row><row><cell>DGI [56]: deep graph infomax, which is directly trained and tested</cell></row><row><cell>on the target graph; (4) GPA [20]: a policy network for transfer</cell></row><row><cell>learning across graphs. Since GPA is designed for zero-shot setting,</cell></row><row><cell>we fine-tune the pre-trained model with 100 iterations in the down-</cell></row><row><cell>stream tasks, and then make the final prediction; (5) UDA-GCN [59]:</cell></row><row><cell>an unsupervised domain-adaptive graph convolutional network;</cell></row><row><cell>(6) GPT-GNN [22]: a self-supervised attributed graph generative</cell></row><row><cell>framework to pre-train a GNN by capturing both the structural</cell></row><row><cell>and semantic properties of the graph; (7) Pretrain-GNN [21]: a self-</cell></row></table><note><p>supervised pre-training model that learns both local and global representations of each node; (8) MentorGNN-V: one variant of MentorGNN, which only considers node attributes as graph signals; and (9) MentorGNN-C: one variant of MentorGNN, which is designed without the curriculum learning module.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of node classification in the single-source graph transfer setting, from a single source graphs (Cora) to a single target graph (Reddit1 or Reddit2 or Reddit3).</figDesc><table><row><cell>Method</cell><cell cols="3">Cora ? Reddit1 Cora ? Reddit2 Cora ? Reddit3</cell></row><row><cell>GCN</cell><cell>0.8736 ? 0.0151</cell><cell>0.8996 ? 0.0158</cell><cell>0.8816 ? 0.0050</cell></row><row><cell>GAT</cell><cell>0.9420 ? 0.0154</cell><cell>0.9241 ? 0.0094</cell><cell>0.8985 ? 0.0088</cell></row><row><cell>DGI</cell><cell>0.7845 ? 0.0208</cell><cell>0.9062 ? 0.0071</cell><cell>0.8388 ? 0.0098</cell></row><row><cell>GPA</cell><cell>0.7011 ? 0.0116</cell><cell>0.7157 ? 0.0053</cell><cell>0.7271 ? 0.0063</cell></row><row><cell>UDA-GCN</cell><cell>0.9323 ? 0.0106</cell><cell>0.9378 ? 0.0079</cell><cell>0.9528 ? 0.0058</cell></row><row><cell>GPT-GNN</cell><cell>0.9570 ? 0.0097</cell><cell>0.9614 ? 0.0065</cell><cell>0.9548 ? 0.0092</cell></row><row><cell>Pretrain-GNN</cell><cell>0.9484 ? 0.0105</cell><cell>0.9351 ? 0.0100</cell><cell>0.9574 ? 0.0090</cell></row><row><cell>MentorGNN-V</cell><cell>0.9448 ? 0.0131</cell><cell>0.9584 ? 0.0045</cell><cell>0.9454 ? 0.0071</cell></row><row><cell>MentorGNN-C</cell><cell>0.9508 ? 0.0097</cell><cell>0.9640 ? 0.0060</cell><cell>0.9655 ? 0.0072</cell></row><row><cell>MentorGNN</cell><cell>0.9562 ? 0.0059</cell><cell cols="2">0.9815 ? 0.0053 0.9741 ? 0.0046</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of node classification in the single-source graph transfer setting, from a single source graphs (CiteSeer) to a single target graph (Reddit1 or Reddit2 or Reddit3).</figDesc><table><row><cell>Method</cell><cell cols="3">CiteSeer ? Reddit1 CiteSeer ? Reddit2 CiteSeer ? Reddit3</cell></row><row><cell>GCN</cell><cell>0.8736 ? 0.0151</cell><cell>0.8996 ? 0.0158</cell><cell>0.8816 ? 0.0050</cell></row><row><cell>GAT</cell><cell>0.9420 ? 0.0154</cell><cell>0.9241 ? 0.0094</cell><cell>0.8985 ? 0.0088</cell></row><row><cell>DGI</cell><cell>0.7845 ? 0.0208</cell><cell>0.9062 ? 0.0071</cell><cell>0.8388 ? 0.0098</cell></row><row><cell>GPA</cell><cell>0.6932 ? 0.0105</cell><cell>0.7162 ? 0.0055</cell><cell>0.7273 ? 0.0065</cell></row><row><cell>UDA-GCN</cell><cell>0.9027 ? 0.0082</cell><cell>0.9540 ? 0.0092</cell><cell>0.9610 ? 0.0131</cell></row><row><cell>GPT-GNN</cell><cell>0.9544 ? 0.0078</cell><cell>0.9528 ? 0.0068</cell><cell>0.9541 ? 0.0100</cell></row><row><cell>Pretrain-GNN</cell><cell>0.9386 ? 0.0084</cell><cell>0.9488 ? 0.0098</cell><cell>0.9581 ? 0.0113</cell></row><row><cell>MentorGNN-V</cell><cell>0.9589 ? 0.0043</cell><cell>0.9637 ? 0.0072</cell><cell>0.9763 ? 0.0034</cell></row><row><cell>MentorGNN-C</cell><cell>0.9521 ? 0.0091</cell><cell>0.9646 ? 0.0064</cell><cell>0.9779 ? 0.0045</cell></row><row><cell>MentorGNN</cell><cell>0.9617 ? 0.0028</cell><cell>0.9834 ? 0.0048</cell><cell>0.9806 ? 0.0026</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Accuracy of node classification in the single-source graph transfer setting, from a single source graphs (PubMed) to a single target graph (Reddit1 or Reddit2 or Reddit3).</figDesc><table><row><cell>Method</cell><cell cols="3">PubMed ? Reddit1 PubMed ? Reddit2 PubMed ? Reddit3</cell></row><row><cell>GCN</cell><cell>0.8736 ? 0.0151</cell><cell>0.8996 ? 0.0158</cell><cell>0.8816 ? 0.0050</cell></row><row><cell>GAT</cell><cell>0.9420 ? 0.0154</cell><cell>0.9241 ? 0.0094</cell><cell>0.8985 ? 0.0088</cell></row><row><cell>DGI</cell><cell>0.7845 ? 0.0208</cell><cell>0.9062 ? 0.0071</cell><cell>0.8388 ? 0.0098</cell></row><row><cell>GPA</cell><cell>0.6907 ? 0.0114</cell><cell>0.7146 ? 0.0050</cell><cell>0.7210 ? 0.0103</cell></row><row><cell>UDA-GCN</cell><cell>0.9381 ? 0.0064</cell><cell>0.9460 ? 0.0335</cell><cell>0.9506 ? 0.0099</cell></row><row><cell>GPT-GNN</cell><cell>0.9582 ? 0.0087</cell><cell>0.9546 ? 0.0060</cell><cell>0.9553 ? 0.0063</cell></row><row><cell>Pretrain-GNN</cell><cell>0.9405 ? 0.0059</cell><cell>0.9625 ? 0.0131</cell><cell>0.9566 ? 0.0088</cell></row><row><cell>MentorGNN-V</cell><cell>0.9574 ? 0.0129</cell><cell>0.9761 ? 0.0059</cell><cell>0.9454 ? 0.0071</cell></row><row><cell>MentorGNN-C</cell><cell>0.9602 ? 0.0028</cell><cell>0.9749 ? 0.0043</cell><cell>0.9734 ? 0.0104</cell></row><row><cell>MentorGNN</cell><cell>0.9575 ? 0.0072</cell><cell>0.9839 ? 0.0025</cell><cell>0.9785 ? 0.0050</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of node classification in the setting of multi-source graph transfer, from multiple source graphs (Cora, CiteSeer, and PubMed) to a single target graph (Reddit1 or Reddit2 or Reddit3).</figDesc><table><row><cell>Method</cell><cell>Reddit1</cell><cell>Reddit2</cell><cell>Reddit3</cell></row><row><cell>GCN</cell><cell>0.8736 ? 0.0151</cell><cell>0.8996 ? 0.0158</cell><cell>0.8816 ? 0.0050</cell></row><row><cell>GAT</cell><cell>0.9420 ? 0.0154</cell><cell>0.9241 ? 0.0094</cell><cell>0.8985 ? 0.0088</cell></row><row><cell>DGI</cell><cell>0.7845 ? 0.0208</cell><cell>0.9062 ? 0.0071</cell><cell>0.8388 ? 0.0098</cell></row><row><cell>GPA</cell><cell>0.7053 ? 0.0091</cell><cell>0.7193 ? 0.0027</cell><cell>0.7308 ? 0.0033</cell></row><row><cell>MentorGNN-V</cell><cell>0.9586 ? 0.0054</cell><cell>0.9848 ? 0.0033</cell><cell>0.9801 ? 0.0024</cell></row><row><cell cols="2">MentorGNN-C 0.9634 ? 0.0055</cell><cell>0.9844 ? 0.0045</cell><cell>0.9795 ? 0.0051</cell></row><row><cell>MentorGNN</cell><cell>0.9621 ? 0.0015</cell><cell cols="2">0.9857 ? 0.0020 0.9811 ? 0.0025</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by <rs type="funder">National Science Foundation</rs> under Award No. <rs type="grantNumber">IIS-1947203</rs>, <rs type="grantNumber">IIS-2117902</rs>, <rs type="grantNumber">IIS-2137468</rs>, <rs type="grantNumber">IIS-19-56151</rs>, <rs type="grantNumber">IIS-17-41317</rs>, and <rs type="funder">IIS</rs> <rs type="grantNumber">17-04532</rs> and the <rs type="funder">C3.ai Digital Transformation Institute</rs>. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agencies or the government.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ACkfYSw">
					<idno type="grant-number">IIS-1947203</idno>
				</org>
				<org type="funding" xml:id="_kpqDWhf">
					<idno type="grant-number">IIS-2117902</idno>
				</org>
				<org type="funding" xml:id="_6E8B7VB">
					<idno type="grant-number">IIS-2137468</idno>
				</org>
				<org type="funding" xml:id="_Tu2X4hb">
					<idno type="grant-number">IIS-19-56151</idno>
				</org>
				<org type="funding" xml:id="_yGjsKUT">
					<idno type="grant-number">IIS-17-41317</idno>
				</org>
				<org type="funding" xml:id="_YQffUya">
					<idno type="grant-number">17-04532</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Pohoreckyj Danyluk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</editor>
		<meeting>the 26th Annual International Conference on Machine Learning<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-06-14">2009. 2009. June 14-18, 2009</date>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GRAM: Graph-based Attention Model for Healthcare Representation Learning</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-08-13">2017. August 13 -17, 2017</date>
			<biblScope unit="page" from="787" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph analysis of functional brain networks: practical issues in translational neuroscience</title>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vico</forename><surname>Fallani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Richiardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Achard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page">1653</biblScope>
			<date type="published" when="2014">2014. 2014. 20130521</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">New Frontiers of Multi-Network Mining: Recent Developments and Future Trend</title>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4038" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Molecular Fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On random graphs I</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdds</surname></persName>
		</author>
		<author>
			<persName><surname>R&amp;wi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. math. debrecen</title>
		<imprint>
			<date type="published" when="1959">1959. 1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to Teach</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial Graph Contrastive Learning with Information Regularization</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;22: The ACM Web Conference 2022, Virtual Event</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-25">2022. April 25 -29, 2022</date>
			<biblScope unit="page" from="1362" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meta-Learned Metrics over Multi-Evolution Temporal Graphs</title>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liri</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vetle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>Torvik</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-08-14">2022. August 14 -18, 2022</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DPPIN: A biological repository of dynamic protein-protein interaction network data</title>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02168</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A View-Adversarial Framework for Multi-View Network Embedding</title>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event</title>
		<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			<biblScope unit="page" from="2025" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">2017. August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Armin Moharrer, Jennifer Dy, and Stratis Ioannidis. 2021. Graph Transfer Learning</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimia</forename><surname>Shayestehfard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph theoretical analysis of structural and functional connectivity MRI in normal and pathological brain networks</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Guye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaelle</forename><surname>Bettus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Bartolomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Cozzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Magnetic Resonance Materials in Physics</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="409" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs: Methods and Applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Loyalty in Online Communities</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justine</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Web and Social Media</title>
		<meeting>the Eleventh International Conference on Web and Social Media<address><addrLine>Montr?al, Qu?bec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-05-15">2017. 2017. May 15-18, 2017</date>
			<biblScope unit="page" from="540" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive Transfer Learning on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Xueting</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-14">2021. August 14-18, 2021</date>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Policy Network for Transferable Active Learning on Graphs</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc-Alexandre</forename><surname>C?t?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Rajesh</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">Aditya</forename><surname>Prakash</surname></persName>
		</editor>
		<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-Paced Curriculum Learning</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015-01-25">2015. January 25-30, 2015</date>
			<biblScope unit="page" from="2694" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Men-torNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pre-training on Large-Scale Heterogeneous Graph</title>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianrui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-14">2021. August 14-18, 2021</date>
			<biblScope unit="page" from="756" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contrastive Pre-Training of GNNs on Heterogeneous Graphs</title>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;21: The 30th ACM International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-11-01">2021. November 1 -5, 2021</date>
			<biblScope unit="page" from="803" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HDMI: High-order Deep Multiplex Infomax</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conference 2021, Virtual Event / Ljubljana</title>
		<meeting><address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04-19">2021. April 19-23, 2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2414" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graph-MVP: Multi-View Prototypical Contrastive Learning for Multiplex Graphs</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejia</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03560</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">COIN: Co-Cluster Infomax for Bipartite Graphs</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00006</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised Contrastive Learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-Paced Learning for Latent Variable Models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Pawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010-12">2010. December 2010</date>
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transferability of Spectral Graph Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gitta</forename><surname>Kutyniok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph Communal Contrastive Learning</title>
		<author>
			<persName><forename type="first">Bolian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;22: The ACM Web Conference 2022, Virtual Event</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-25">2022. April 25 -29, 2022</date>
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning Invariant Representations and Risks for Semi-Supervised Domain Adaptation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yezhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1104" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Disentangled Contrastive Learning on Graphs</title>
		<imprint>
			<biblScope unit="page" from="21872" to="21884" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pre-training Molecular Graph Representation with 3D Geometry</title>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural subgraph isomorphism counting</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mutian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1959" to="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Link-based Classification</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003)</title>
		<editor>
			<persName><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nina</forename><surname>Mishra</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003-08-21">2003. August 21-24, 2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to Pre-train Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. February 2-9, 2021</date>
			<biblScope unit="page" from="4276" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Progressive Graph Learning for Open-Set Domain Adaptation</title>
		<author>
			<persName><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 18 July 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="6468" to="6478" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Supervised Domain Adaptation using Graph Embedding</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Hedegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morsing</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Ali Sheikh-Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Iosifidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition, ICPR 2020</title>
		<meeting><address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-01-10">2020. January 10-15, 2021</date>
			<biblScope unit="page" from="7841" to="7847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Querydriven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">th International Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pretraining Graph Neural Networks with Kernels</title>
		<author>
			<persName><forename type="first">Nicol?</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Van Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06930</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zvika</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2005 workshop on transfer learning</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to Simulate Complex Physics with Graph Networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web, WWW 2015</title>
		<meeting>the 24th International Conference on World Wide Web, WWW 2015<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-05-18">2015. May 18-22, 2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis</title>
		<author>
			<persName><forename type="first">Siyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Kamal Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Dubost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Lee-Messer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Convergence of a block coordinate descent method for nondifferentiable minimization</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of optimization theory and applications</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="475" to="494" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno>CoRR abs/1809.10341</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamic protein interaction network construction and applications</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang-Xiang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteomics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="338" to="352" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Curgraph: Curriculum learning for graph classification</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1238" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptive Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<idno>ACM / IW3C2</idno>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference 2020</title>
		<editor>
			<persName><forename type="first">Irwin</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tie-Yan</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Van Steen</surname></persName>
		</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. April 20-24, 2020</date>
			<biblScope unit="page" from="1457" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">InfoGCL: Information-Aware Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="30414" to="30425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamic knowledge graph alignment</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4564" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bright: A bridging algorithm for network alignment</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3907" to="3917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">FINAL: Fast Attributed Network Alignment</title>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13">2016. August 13-17, 2016</date>
			<biblScope unit="page" from="1345" to="1354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On Learning Invariant Representations for Domain Adaptation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019. 2019, 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7523" to="7532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13798</idno>
		<title level="m">Tackling oversmoothing of gnns with contrastive learning</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Contrastive Learning with Complex Heterogeneity</title>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-08-14">2022. August 14 -18, 2022</date>
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Heterogeneous Contrastive Learning</title>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09401</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">SPARC: Self-Paced Network Representation for Few-Shot Rare Category Characterization</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<editor>
			<persName><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Faisal</forename><surname>Farooq</surname></persName>
		</editor>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08-19">2018. August 19-23, 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="2807" to="2816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A Local Algorithm for Structure-Preserving Graph Cut</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Yigit Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Alcorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasan</forename><surname>Davulcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-08-13">2017. August 13 -17, 2017</date>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">High-Order Structure Exploration on Massive Graphs: A Local Graph Clustering Perspective</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Yigit Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Alcorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasan</forename><surname>Davulcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A data-driven graph generative model for temporal interaction networks</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="401" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Misc-GAN: A multi-scale generative model for graphs</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiejun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unlearn What You Have Learned: Adaptive Crowd Teaching with Exponentially Decayed Memory Learners</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Reddy Nelakurthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08-19">2018. 2018. August 19-23, 2018</date>
			<biblScope unit="page" from="2817" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Crowd Teaching with Imperfect Labels</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Reddy Nelakurthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference 2020</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. April 20-24, 2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Transfer Learning of Graph Neural Networks with Ego-graph Information Maximization</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1766" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Blai</forename><surname>Usa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sven</forename><surname>Bonet</surname></persName>
		</editor>
		<editor>
			<persName><surname>Koenig</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015-01-25">2015. January 25-30, 2015</date>
			<biblScope unit="page" from="4083" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
