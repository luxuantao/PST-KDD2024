<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Proxemic Interaction: Designing for a Proximity and Orientation-Aware Environment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Till</forename><surname>Ballendat</surname></persName>
							<email>tballend@ucalgary.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<addrLine>2500 University Drive NW</addrLine>
									<postCode>T2N 1N4</postCode>
									<settlement>Calgary</settlement>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolai</forename><surname>Marquardt</surname></persName>
							<email>nicolai.marquardt@ucalgary.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<addrLine>2500 University Drive NW</addrLine>
									<postCode>T2N 1N4</postCode>
									<settlement>Calgary</settlement>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saul</forename><surname>Greenberg</surname></persName>
							<email>saul.greenberg]@ucalgary.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<addrLine>2500 University Drive NW</addrLine>
									<postCode>T2N 1N4</postCode>
									<settlement>Calgary</settlement>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Proxemic Interaction: Designing for a Proximity and Orientation-Aware Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">74E09E68DF97EDD170A0715F7E4BCB1F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ACM Classification: H5.2 [Information interfaces and presentation]: User Interfaces -Input devices and strategies Proximity</term>
					<term>proxemics</term>
					<term>location and orientation aware</term>
					<term>implicit interaction</term>
					<term>explicit interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the everyday world, much of what we do is dictated by how we interpret spatial relationships, or proxemics. What is surprising is how little proxemics are used to mediate people's interactions with surrounding digital devices. We imagine proxemic interaction as devices with fine-grained knowledge of nearby people and other devices -their position, identity, movement, and orientation -and how such knowledge can be exploited to design interaction techniques. In particular, we show how proxemics can: regulate implicit and explicit interaction; trigger such interactions by continuous movement or by movement of people and devices in and out of discrete proxemic regions; mediate simultaneous interaction of multiple people; and interpret and exploit people's directed attention to other people and objects. We illustrate these concepts through an interactive media player running on a vertical surface that reacts to the approach, identity, movement and orientation of people and their personal devices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Spatial relationships play an important role in how we physically interact, communicate, and engage with other people and with objects in our everyday environment. Proxemics is Edward Hall's theory of these interpersonal spatial relationships <ref type="bibr" target="#b7">[8]</ref>. It describes how people perceive, interpret and use distance, posture and orientation to mediate relations to other people, and to the fixed (immobile) and semi-fixed (movable) features in their environment <ref type="bibr" target="#b7">[8]</ref>. Proxemic theory correlates physical distance with social distance (albeit in a culturally dependent manner): intimate 6-18", personal 1.5-4', social 4-12', and public 12-&gt;25' distances. As the terms suggest, the distances lend themselves to a progression of interactions ranging from highly intimate to personal, to social and then to public. Each distance also defines a close and far phase that affects that interaction <ref type="bibr" target="#b7">[8]</ref>.</p><p>Hall emphasizes the role of proxemic relationships as a form of people's implicit communication -a form of communication that interactive computing systems have yet to understand. In spite of the opportunities presented by people's natural understanding of proxemics, only a relatively small number of research installations -usually within Ubiquitous Computing (Ubicomp) explorations -incorporate spatial relationships within interaction design. Yet these installations are somewhat limited. For example, a variety of systems trigger activity by detecting the presence or absence of people within a space, e.g., reactive environments have devices in a room react to presence <ref type="bibr" target="#b1">[2]</ref>, or digital surfaces that detect and react to a device within a given range <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b14">[15]</ref>. While useful, this is a crude measure of proxemics, as it only considers distance as a binary value, i.e., within or outside a given distance. True proxemics demand fine-grained knowledge of people's and device's continuous movement in relationship with each other, and how this would affect interaction. Two projects stand out here <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b20">[21]</ref>; both have a vertical digital surface reacting to people's distance from it to control the information displayed. We take their work even further, where we extend previous notions of proxemic interactions.</p><p>Our contributions consider the complete ecology present in a small space Ubicomp environment (illustrated in Figure <ref type="figure">1</ref>): the relationships of people to devices, of devices to devices, and of non-digital objects to people and devices. For this, we exploit continuous knowledge of distance, orientation, movements, and identity as part of an extended notion of proxemics to drive the possible interactions. Building upon Vogel's <ref type="bibr" target="#b20">[21]</ref> and Ju's <ref type="bibr" target="#b10">[11]</ref> work, we demonstrate how proxemic information can regulate both implicit and explicit interaction techniques within a realistic application, either based on continuous movement, or by movement in and out of discrete proxemic zones. By implicit, we mean actions the computer takes based on its interpretation of implied user actions vs. explicit control actions stated by the end user. We explain how proxemic interactions consider aspects of the fixed and semi-fixed feature environment, and how they extend attentive interfaces. Proxemic interactions also extend beyond pairwise interaction and consider one person or multiple people in relation to an ecology of multiple devices and objects in their nearby environment.</p><p>We illustrate these concepts with the design of an interactive vertical display surface that recognizes the proximity of surrounding people, digital devices, and non-digital objects. Our example application is an interactive home video media player centered around a vertical surface in a living room. It implicitly reacts to the approach and orientation of people, and their personal devices and objects. Depending on the distance of people to the display and their movements, the application implicitly changes information displayed on the screen, and reacts by implicitly triggering application functions. Furthermore, we explain how explicit interaction is supported from these varying distances to the interactive display surface.</p><p>The remainder of the paper is structured as follows. After summarizing related work, we provide a scenario of people using our proxemic media player. Next, we introduce four dimensions describing the possible proxemic relationships involving people and their things. We then introduce concepts for designing proxemic interactions in Ubicomp, which we illustrate via our proxemic media player. We close with a brief description of our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>We sample related work out of two research areas: interactive wall surfaces that sense the presence of nearby devices and of people to mediate implicit and explicit interaction, and devices that sense the presence of other devices to mediate connectivity and information exchange.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proximity-Aware Surfaces and Displays</head><p>The majority of HCI research involving digital wall displays explores direct touch or gestural interaction, but otherwise ignores proximity. Some techniques do expect people to be at a certain distance from the display to work (e.g., ray casting, or pick and drop <ref type="bibr" target="#b13">[14]</ref>), but this is just a function of where people have to stand for the technique to work.</p><p>Several early works considered how a spatially-aware mobile device would interact with a large digital surface. Notably, Chameleon <ref type="bibr" target="#b5">[6]</ref> was a palmtop computer aware of its position and orientation. When used relative to a vertical display, Chameleon's contents would vary depending on its spatial orientation to that surface. Similarly, Rekimoto's spatially-aware M-Pad mobile device behaved like a clickthrough toolglass whose attributes affect the nearby items on the surface <ref type="bibr" target="#b13">[14]</ref>.</p><p>Somewhat later, several researchers considered vertical surfaces that react to the spatial presence of people. For example, Shoemaker <ref type="bibr" target="#b17">[18]</ref> introduced techniques for a person to directly interact with digital content on a vertical wall surface through real or virtual shadows. The person's movement in the space and resulting changes of the shadow projections become part of the interaction. Hello.Wall <ref type="bibr" target="#b12">[13]</ref> introduced the notion of 'distance-dependent semantics', where the distance of an individual from the wall defined the interactions offered and the kind of information shown. Technically, Hello.Wall could discriminate people's rough positions as three spatial zones. Vogel et al. <ref type="bibr" target="#b20">[21]</ref> took this concept even further, where they directly applied Hall's theory to define four proxemic zones of interaction. From far to close, these ranged from ambient display of information, then to implicit, then subtle, and finally personal interaction. A major idea in their work -developed even further by Ju <ref type="bibr" target="#b10">[11]</ref> -is that interaction from afar is public and implicit, and becomes more private and explicit as people move towards the surface.</p><p>Researchers have also considered a person's proximity to a small display. Lean and Zoom, for example, used the distance between the user's head and a notebook display to control a zoom effect <ref type="bibr" target="#b8">[9]</ref>: the smaller the distance, the larger the displayed content.</p><p>As mentioned earlier, we extend this prior work by exploiting continuous distance, orientation, movement and identity to tune surface interaction, where we incorporate multiple people and features of the fixed and semi-fixed environment as a complete ecology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Device to Device Connectivity Via Proximity Sensing</head><p>A major problem in Ubicomp is how to control the connectivity of devices. Consequently, various researchers have considered how spatial distance can be used to connect devices. Most approaches define a single discrete spatial region -which often depends on the sensing technology used -where a connection (or user interaction leading to a connection) is triggered when the spatial regions between devices overlap. With Smart-its friends <ref type="bibr" target="#b9">[10]</ref>, such a connection can be established once two devices sense similar values through attached sensors (such as accelerometers). By shaking a pair of devices simultaneously, an interdevice connection can be established. Want <ref type="bibr" target="#b21">[22]</ref> introduced the technique of detecting nearby objects and devices through attached RFID tags, while Rekimoto <ref type="bibr" target="#b15">[16]</ref> combined RFID and infrared for establishing device connectivity. These techniques are powerful for connecting de-vices that are in very close proximity or -like in many cases -are even directly touching one another. Swindells <ref type="bibr" target="#b18">[19]</ref> introduced a similar technique that worked from a larger distance, where he applied it to the gesturePen for initiating remote pointing for device selection. We extend this prior work, where we contribute techniques that go beyond a binary device connection state: we introduce techniques that move from awareness at a larger distance, to gradually revealing of higher level of detail, to direct interaction for transferring digital information between devices. Spatial relations have also been used to mediate the information exchanged between devices. For example, Kray's group coordination negotiation <ref type="bibr" target="#b11">[12]</ref> introduced spatial regions around mobile phones. Their scenario used these regions to negotiate exchange of information with others and to visualize the regions on a tabletop. Depending on how devices were moved in and out of three discrete regions, the transfer of media data between the devices is initiated. We extend their approach to interaction around large surfaces, where the degree of shared information between devices depends not only on their relative distance, but also orientation.</p><p>Gellersen's RELATE Gateways <ref type="bibr" target="#b6">[7]</ref> provided a spatial-aware visualization of nearby devices. A graphical map showed the spatial room layout, and icons indicated the position of other nearby devices. Alternatively, icons at the border of a mobile device screen represented the type and location of surrounding devices (see also <ref type="bibr" target="#b15">[16]</ref>). We extend this notion with: visualizations that include proximitydependent level of detail, and with techniques that move from awareness to direct interaction depending on a person's distance and orientation to the display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THE PROXEMIC MEDIA PLAYER APPLICATION</head><p>We use the example of people interacting with a home media player application located in a living room. Later sections, which present concepts for designing proxemic interactions, will use episodes from this scenario to anchor the discussion.</p><p>Our scenario follows Fred who is approaching the display from a distance. We explain how the system supports Fred's implicit and explicit interaction with the digital surface as a function of his distance and orientation. The primary interface of the interactive media player application supports browsing, selection, and playback of videos on a large wall-mounted digital surface: a 52 inch touchsensitive SmartBoard from Smart Technologies, Inc. (Figure <ref type="figure" target="#fig_0">2</ref>, top). A Vicon motion capture system tracks, via reflective infrared markers, the location and orientation of nearby people, objects, and other digital devices. All equipment is situated in a room that resembles a domestic living room. When Fred enters the room at position (a'), the media player recognizes Fred and where he is standing. It activates the display, shows a short animation to indicate it is activated, and then displays four large video preview thumbnails held in Fred's media collection (Figure <ref type="figure" target="#fig_1">2a</ref>). As Fred moves closer to the display (b'), the video preview thumbnails and titles shrink continuously to a smaller size, thus showing an increasing number of available videos (2b). When Fred is very close to the surface (c'), he can select a video directly by touching its thumbnail on the screen. More detailed information about the selected video is then shown on the display (2c), which includes a preview playback that can be played and paused (2c, top), as well as its title, authors, description and release date (2c, right). When Fred moves away from the screen to sit on the couch (d'), his currently selected video track starts playing in fullscreen view (2d). If Fred had previously seen part of this video, the playback is resumed at Fred's last viewing position, otherwise it starts from the beginning.</p><p>Fred tires of this video, and decides to select a second video from the collection. He pulls out his mobile phone and points it towards the screen (Figure <ref type="figure" target="#fig_3">4b</ref>). From its position and orientation, the system recognizes the phone as a pointer, and a row of preview videos appears at the bottom of the screen (as in Figure <ref type="figure" target="#fig_3">4b</ref>). A visual pointer on the screen provides feedback of the exact pointing position of Fred's phone relative to the screen. Fred then selects the desired videos by flicking the hand downwards, and the video starts playing. Alternately, Fred could have used a non-digital pen to do the same interaction (Figure <ref type="figure" target="#fig_3">4a</ref>).</p><p>Somewhat later, Fred receives a phone call. The video playback automatically pauses when he answers the phone (Figure <ref type="figure" target="#fig_2">3b</ref>), but resumes playback after he finishes the call. Similarly, if Fred turns away from the screen to (say) read a magazine (Figure <ref type="figure" target="#fig_2">3a</ref>), the video pauses, but then continues when Fred looks back at the screen.</p><p>As Fred watches the video while seated on the couch, George enters the room. The title of the currently playing video shows up to at the top of the screen to tell George what video is being played (Figure <ref type="figure" target="#fig_5">6a</ref>). When George approaches the display, more detailed information about the current video becomes visible at the side of the screen where he is standing (Figure <ref type="figure" target="#fig_5">6b</ref>). When George moves directly in front of the screen (thus blocking Fred's view), the video playback pauses and the browsing screen is shown (Figure <ref type="figure" target="#fig_5">6c</ref>). George can now select other videos by touching the screen. The view changes back into full screen view once both sit down to watch the video. If Fred and George start talking to each other, the video pauses until one of them looks back at the screen (Figure <ref type="figure" target="#fig_2">3c</ref>).</p><p>Fred takes out his personal portable media player from his pocket. A small graphic representing the mobile device appears on the border of the large display, which indicates that media content can be shared between the surface and portable device (Figure <ref type="figure" target="#fig_4">5a</ref>). Fred moves closer to the surface while pointing his device towards it; the graphic on the surface responds by progressively and continuously revealing more information about the content held on the media device (Figure <ref type="figure" target="#fig_4">5b</ref>). When Fred moves directly in front of the surface while holding the device, he sees large preview images of the device's video content, and can then transfer videos to and from the surface and portable device by dragging and dropping their preview images (Figure <ref type="figure" target="#fig_4">5c</ref>). The video playback on the large screen resumes as Fred puts his portable device back in his pocket and sits down on the couch. When all people leave the room, the application stops the video playback and turns off the display.</p><p>While this media player is a simple application domain, it provided a fertile setting to develop and explore concepts of proxemic interaction. In the next section we introduce the dimensions of input that are essential for designing proximity aware interfaces. Then we will discuss the details of proxemic interaction concepts associated with a single person or multiple people interacting with a large digital surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIMENSIONS OF PROXEMIC RELATIONSHIPS</head><p>While many dimensions are used by people to mediate their interpersonal proxemic interactions, we identify four dimensions as essential if a system is to determine the basic proxemic relationships between entities (people, digital devices, and non-digital objects): position, orientation, movement, and identity. These four dimensions are part of our extended notion of proxemics that differs from Hall's understanding of discrete proxemic zones that are based primarily on the actual spatial distance between individuals.</p><p>Position of an entity can be described in absolute or relative terms. For the absolute position we have to know the distance of the entity from a defined fix point in the space.</p><p>Once such a fixed point in space is defined, the absolute position of every entity can be described as the three dimensional position relative to this fixed point. Relative position, on the other hand, can be determined from knowing the spatial relationship between two entities (e.g., between a person and object), and does not require a common fixed point of reference. Through the knowledge of absolute or relative position, we can calculate information about distance (e.g., imperial or metric units) between objects and people.</p><p>Orientation provides the information about which direction an entity is facing. This makes sense only if an entity has a well-defined 'front' (e.g., a person's eyes, the point of a pencil). Similar to location, we can differentiate between the absolute orientation of an entity (e.g., described through yaw, pitch, and roll) or relative orientation (e.g., a quantitative description such as "this person is facing that object").</p><p>From orientation, determine where a ray cast from one entity would intersect with another entity (ray casting).</p><p>Movement lets us understand the changes of position and orientation of an entity over time. This also means we can calculate the velocity of these changes. These movements, for example, reveal how a person is approaching a particular device or object.</p><p>Identity uniquely describes the entities in the space. The most detailed information provides the exact identity of a person or object (e.g., "Fred", "Person A", "Fred's Cell phone"). Other less detailed forms of identity are possible, such as identifying a category precisely (e.g., "book", "person"), or roughly ("non-digital object"), or even affiliation to a group (e.g., "family member", "visitor").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DESIGNING FOR PROXEMIC INTERACTION</head><p>We now describe concepts of applying these four input dimensions in meaningful ways to people's proxemic interactions with Ubicomp systems. To ground our explanation, we highlight particular examples from the scenario that illustrate how each concept can be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating the Fixed-and Semi-fixed Feature Space</head><p>One promise of Ubicomp is to situate technology in people's everyday environments, in a way that lets people interact with information technology in their familiar places and environment. Dourish framed this concept as embodied interactions <ref type="bibr" target="#b4">[5]</ref>; technology that is seamlessly integrated into people's everyday practices, rather than separated from them. Context-aware computing is one outcome of this, where some kind of context-aware sensing <ref type="bibr" target="#b16">[17]</ref> provided devices with knowledge about the situation around them. This sensing usually involved measuring a coarser subset of our dimensions, e.g., very rough positions, and other factors such as noise, light, or tilting. We contribute to this by introducing the notion of having context-aware systems mediate embodied interaction by understanding the proxemic relationships (as defined by our dimensions) of people to the fixed-and semi-fixed feature space <ref type="bibr" target="#b7">[8]</ref> surrounding them.</p><p>For an interactive system (such as the interactive wall display in our media player application), knowledge about the fixed feature space includes the layout of the fixed aspects of the room, such as existing walls, doors and windows. It also includes knowledge about fixed displays -such as a digital surface -located in this environment. For instance, the knowledge about the position of the fixed entrance doors allows our system to recognize a person entering the room from the doorway, and then take implicit action by awaking from standby mode. Similarly, knowing the position of the fixed display means that the interface on that display can react as a person approaches it.</p><p>Semifixed features in the environment include all furniture, such as bookshelves, chairs, and tables whose position may change over time. While it is somewhat object-dependant, semi-fixed features often remain at specific locations, but are per se movable objects that people rearrange to adapt to changed situations (such as moving a group of chairs around a table). Unlike fixed features whose position needs to be configured only once, knowledge about the positions of semi-fixed features will have to be updated over time as changes are noticed.</p><p>Knowledge of semi-fixed features can also mediate interaction. To illustrate this point, we compare two stages of a person relative to the media player's interactive surface: approaching from a distance (see Figure <ref type="figure" target="#fig_0">2</ref>, position a') and watching the video when seated at the semi-fixed couch (Figure <ref type="figure" target="#fig_1">2 position d'</ref>). The actual distance of the person relative to the surface is similar in both situations, yet they suggest very different forms of interaction. The fact that the person is seated on a couch or chair facing the display becomes an indicator for watching the video. Yet standing at the same distance and then moving closer to the screen is used to infer that the person is increasingly interested in getting more information about the available videos in the media collection. (Of course, inferences may not always be correct. This will be discussed later).</p><p>Thus, information about distance and orientation of a person relative to the fixed and semi-fixed feature space provides cues that can mediate implicit interactions with the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpreting Directed Attention to People and Objects</head><p>Proxemic interactions can be used to extend the concept of attentive user interfaces (AUIs) that are designed to "support users' attentional capacities." <ref type="bibr" target="#b19">[20]</ref>. In AUIs, the system reaction depends on whether a person is directing his or her attention to the device that holds the system (usually through detection of eye gaze) <ref type="bibr" target="#b19">[20]</ref>. We take this AUI concept one step further, where we also incorporate information about: what entity a person is attending, and the importance of distance and orientation in that context. Attending to the system itself occurs if the device reacts to how it is being looked at. This is how most traditional AUIs work. We include an example of this behaviour <ref type="bibr" target="#b19">[20]</ref> in our media player application: the system plays the video as long as at least one person faces the large display, but pauses when that person looks away for a length of time.</p><p>Attention to other surrounding objects and devices. We enrich the concept of AUIs by including how a person's directed attention to other surrounding objects of the semifixed feature space can trigger implicit system reactions. In our system, the fact that a person is holding and facing towards a newspaper (shown in Figure <ref type="figure" target="#fig_2">3a</ref>) provides cues about the focus of this person's attention, i.e., the system infers that Fred is reading, and pauses video playback until Fred stops reading and looks back at the screen. If Fred had a similar gaze to (say) a bowl of popcorn, the video would not have paused. A shift of attention can also be suggested by the relative distance of an object to the person. For example, our system detects when Fred is holding his mobile phone close to his ear (as shown in Figure <ref type="figure" target="#fig_2">3b</ref>). It infers that Fred is having a phone conversation, and pauses the video until Fred moves his phone away from his head. The measurement of relative distance of phone to the person's head, as well as their orientation towards each other, provided the necessary information for the system to implicitly react to this situation.</p><p>Attention to other people. We can discriminate how one person attends other people as a means to trigger implicit system reactions. For example, consider Fred and George when they turned towards each other to converse (see Figure <ref type="figure" target="#fig_2">3c</ref>). Our scenario illustrated how the system implicitly reacts to this situation by pausing the video. However, by knowing that they are in conversation (rather than just knowing that they are looking away from the display), the system could have just turn down its volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Fine Grained Explicit Interaction</head><p>Instead of implicitly reacting to a person's proxemic relation to other semi-fixed environment objects, these relationships can also facilitate a person's explicit forms of interaction with the system. We introduce the concept of using physical objects as mobile tokens that people can use to mediate their explicit interaction with an interactive surface. The meaning of these tokens is adjusted based upon the token's distance and orientation to other entities in the space.</p><p>To illustrate this concept, consider the explicit interaction in our scenario where Fred pointed his cell phone or a pencil at the surface to view and select content. The way this works is that all mobile tracked objects are interpreted as mobile tokens. Three units of information caused our system to interpret that token as a pointing device: it is held in front of a person, it is roughly oriented towards the display, and it is within a particular distance from the display. Indeed, we showed how two quite different devices can serve as similar tokens: the pen in Figure <ref type="figure" target="#fig_3">4a</ref>, and the mobile phone in Figure <ref type="figure" target="#fig_3">4b</ref>. We emphasize that we are not using any of the digital capabilities of the mobile digital phone to make this inference. Rather (and as with the physical pen) we are using only the knowledge of its position and orientation to switch to a certain interaction mode. Thus, the particular proxemic relationship between a person and a mobile token is interpreted as a method of signaling <ref type="bibr" target="#b2">[3]</ref>, as discussed in Clark's theory of pointing and placing as forms of communication. Further, the specific orientation and distance of the token to other devices (e.g., the large display) are interpreted to establish an intrinsic connection <ref type="bibr" target="#b2">[3]</ref> to control that particular device.</p><p>A key advantage is that the use of these mobile tokens as identifiers can disambiguate similar looking gestures. For example, a gesture recognition system cannot tell if the intent of a person pointing their hand towards the screen is to interact with the screen, or that it is just a gesture produced as part of a conversation. Mobile tokens, on the other hand, create a specific context to disambiguate and interpret gestures, where it uses the distance and location of the objects relative to the person and other objects to infer a certain explicit interaction mode. Many of these behaviours can be triggered by approximate knowledge of proxemic relationships. Yet having exact knowledge is helpful for minimizing errors that can occur where the system misinterprets a person's manipulation of a mobile token as an explicit action. For example, consider a person playing with a pen in their hand vs. pointing the pen at the screen to select an item. If proxemic measures are reasonably precise, the triggering event could rely solely on the pen being a specific distance from the person's body and a specific orientation towards the screen for a particular length of time.</p><p>Another example includes the multiple meanings held by a mobile token. Consider how the meaning of the mobile phone depended on its proxemic relation to its holder and to the display. The distance of the phone to a person's head indicated an ongoing phone conversation, while holding the same device in front and towards the display shifts its meaning to an interaction pointer.</p><p>For the actual explicit interaction with the digital video content displayed on the large surface, the person can move the position of the mobile token. Changes of the orientation angle allow fine grained positioning of a pointer icon on the screen, while fast acceleration downwards can be used for selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpreting Continuous Movements or Discrete Proxemic Zones</head><p>Another concept is that the behaviours of proxemic interfaces can react to the position and distance of its entities as either continuous movements, or as movements in and out of discrete proxemic zones.</p><p>For continuous movement, the calculated distances between people and devices function as input variables that continually affect the interactive system's behaviour. For example, as a person approaches a screen of the media player appli- cation, the number of visible video preview thumbnails shown continually increase with distance (see Figures <ref type="figure" target="#fig_1">2a,</ref><ref type="figure">b</ref>). To do this, the system gradually resizes the preview images to a smaller size (zoom out effect); thus more content is visible as the person approaches the screen. Depending on the situation, an inverse behaviour might be applied, where the system actually zooms into the content to make it larger when the person is approaching the screen (similar to Lean and Zoom <ref type="bibr" target="#b8">[9]</ref>). Another example of continuous mapping of distance as an input regulator are the awareness icons of nearby digital devices (visible in Figure <ref type="figure" target="#fig_4">5</ref>). These icons grow continuously, from a small circular icon indicating its presence, to a large area on the screen that displays rich content and allows direct touch interaction with it (as in the progression from Figure <ref type="figure" target="#fig_4">5a-c</ref>).</p><p>With discrete proxemic zones we can divide the space into discrete regions. When a person enters or leaves the thresholds of these zones, certain actions are triggered in the system. Indeed, the use of zones is inspired by the interpersonal proxemic distance zones defined by Hall <ref type="bibr" target="#b7">[8]</ref>, and others have applied zones as a way to mediate interaction with public ambient displays <ref type="bibr" target="#b20">[21]</ref> and digital whiteboards <ref type="bibr" target="#b10">[11]</ref>.</p><p>Our media player uses discrete zones in several ways. We use it to trigger an associated implicit action (e.g., we activate a display screen when entering the room). We also use zones to allow certain forms of explicit interaction (e.g., switching to an interface that allows direct touch interaction when the person is standing in close distance to the screen).</p><p>A problem associated with discrete zones occurs when the interface rapidly switches back and forth between two states; this occurs when the person stands exactly at a border of one of the discrete zones. This is solved via the concept of a hysteresis tolerance: the entry and exit point of each region are not at the same distance, but are two sepa-rate distances. For example, we use a 15-20% hysteresis tolerance for proxemic regions around the interactive wall display (percentage of the region dimension) to avoid this rapid switching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moving From Awareness to Direct Interaction</head><p>Next, we can combine both continuous movements and discrete proxemic zones to design system interfaces that move fluently from awareness to direct explicit interaction. Two examples illustrate this combination.</p><p>Our media player begins by providing peripheral awareness information about its capabilities and content when a person enters the room.</p><p>The system detects the presence of the person at a distance (around 4m), activates the display, displays a welcome animation, and plays a subtle acoustic signal. This indicates to the person that the system is active. Here, we used a discrete proxemic zone around the digital display that triggers this activation behaviour. At this point, if the person just walks pass the display, or does not face the display, the media player application would revert to sleep mode. If, however, the person does move closer, the system shows preview images of video content, where it gradually reveals more preview items on the screen as the person approaches the screen. Here, we use the continuous mapping of distance to the size and quantity of preview items shown. When the person stands within reach of the screen, we enter another discrete zone: direct touch interaction. At that distance, the person can use their hands for direct touch interaction with the screen content; thus the continuous resizing of the displayed preview thumbnails stops as it would otherwise make selection difficult.</p><p>So far, we have focused on implicit and explicit interactions mediated through changes of a person's distance and orientation relative to the large digital surface. Interactions, however, increasingly take place in an environment comprising an ecology of devices -from shared large displays to portable personal devices. Using our four proximity dimensions, we can recognize nearby devices and thus facilitate using them in conjunction with one another. This opens new possibilities for interaction, communication, and information exchange. However, to make sense of device interaction, people require awareness of device interconnections and a means to move into direct interaction over them.</p><p>To explain, we illustrate device-to-device proxemic awareness and interactions with the interactive vertical display, where the surface reacts to nearby portable devices carried by a person. This time the system reacts to distance, con- tinuous movement, and orientation of a person's portable digital device when approaching the media player displayed on the surface. Again, we illustrate how we use discrete zones and continuous movements to move from awareness to direct interaction.</p><p>When a person takes a portable media player out of their pocket while sitting at a distance, the system recognizes the device and indicates a possible interaction through a visual icon at the border of the display (visible in Figure <ref type="figure" target="#fig_4">5a</ref>). This icon represents the portable device, where it indicates to the person that there is now an opportunity to share content between the large surface and the portable device. While this icon visualization is inspired by earlier approaches (e.g., <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b15">[16]</ref>) for visualizing spatial relationship between devices, it differs in how it incorporates proxemic distance and orientation information leading from awareness to direct interaction. If the person then orients this portable device towards the large screen, more detailed information about that device and its contents becomes visible. Depending on the orientation between the device and the large surface, the icons continuously and instantly update their position at the border of the interactive wall screen, so that they always face the direction of the portable device. As the person moves the personal device closer to the large display, even more details about the content (e.g., titles) become visible and the preview thumbnails are shown at a larger size (Figure <ref type="figure" target="#fig_4">5b</ref>). When the person holding the device is within reach of the interactive screen (i.e., a discrete zone is entered), the size of the icon grows to a large area of the screen (visible in Figure <ref type="figure" target="#fig_4">5c</ref>). The icon not only provides detailed information about the content of the device, but also allows full direct touch interaction. The person can now drag and drop video items from the portable device to the large surface and vice versa. When putting the device back in the pocket the visualization immediately disappears and the media player continues its playback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leveraging People's Identity</head><p>The concepts introduced so far only require knowledge about "a person" approaching the display, but they do not require the actual identity of a person. We now discuss examples that leverage the knowledge about the actual identity of individuals.</p><p>History. Knowing which person is interacting with the system is used to continue activities that this person began in the past. For instance, when a person enters the room and immediately sits down, the media application will resume playback of a last video that a person previously watched but did not finish.</p><p>Personalization. The media player could save one's settings as a personal profile. This can include personal configurations, idiosyncrasies of how the system should respond to that particular person, and that person's media content. For example, when a particular person approaches the display, our media player would then display content out of that person's media library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Safeguards.</head><p>Identifying the person interacting with the system can also function as a safeguard to restrict access. For instance, children may only be allowed to access the media player application during pre-defined time slots, or access to available media content could be restricted to movies rated for their age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mediating People's Simultaneous Interaction</head><p>Proxemic interactions should also mediate the interaction of multiple people in the same space. In the simplest case, as long as all people are in the same proxemic state relative to the display's surface, the system's behaviour could be similar to the proxemic interactions introduced for a single person interacting with the surface. In reality, however, we expect people to be in different proxemic stages, where the system would need to reason about how it should mediate its behaviour to reflect people's simultaneous interaction possibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging multiple proxemic distances.</head><p>In situations where people have different proxemic distances to the interactive display of our application, the system can be designed to individually address people's diverse proxemic needs, albeit as a compromise.</p><p>For example, we saw George enter the room while Fred was watching a video. George wants to know what was being played, while Fred wants to keep watching. To compromise between these needs, the system displayed the title of the currently playing video at the top of the screen, thus subtly informing George while still letting Fred watch without too much distraction (Figure <ref type="figure" target="#fig_5">6a</ref>). If George sits at the couch or on a chair, the title disappears.</p><p>If George approaches the screen instead of sitting down, the display animates and splits off a small region of the screen. This region provides further information of the video being played: its description, author information, and the release date (Figure <ref type="figure" target="#fig_5">6b</ref>). The positioning of this region also depends on George's spatial relation to the display -if he moves between the left to right side, the information panel smoothly animates to that side the display.</p><p>When both people are in the same proxemic state, the views merge. For instance, both people can watch the video in full screen when seated, or both can explore and choose from the videos available when standing in front of the display.</p><p>Handling conflicts. When multiple people are present within a proximity-aware application, situations will arise where the system has to handle two conflicting individual possibilities. For example, consider the scenario situation of Figure <ref type="figure" target="#fig_5">6c</ref>: Fred is sitting in front of the large display watching a movie, while George moves directly in front of the display to browse a media collection.</p><p>Several strategies are possible to handle these situations.</p><p>The system could favour the person in closer proximity; e.g., George standing directly in front of the display would have priority over Fred sitting at a larger distance. This is the solution shown in Figure <ref type="figure" target="#fig_5">6c</ref>, where George gets full access to the media library to select videos; a strategy that makes sense as Fred's view is already blocked. Alternately, the system could have given the video player priority, disallowing George's interaction, where they would have to resolve this through social means (e.g., both standing up to make a selection). Or the system could create some kind of composite view, i.e., by moving the video so that Fred could still see some of it, while still giving George interactive controls in the blocked part of the screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences in Perceiving and Interpreting Proxemics</head><p>People's perception of proxemic relationships is influenced by gender, cultures, age, work hierarchies, and other factors <ref type="bibr" target="#b7">[8]</ref>. These differences also affect the design of proxemic interactions. Imagine a system that requires people to stand in very close proximity to each other to collaboratively interact with an interactive surface, e.g., to exchange digital documents. This close proximity might be perceived as adequate by some, but as too intimate by others. Therefore, the design of proxemic interactions has to consider these variations in proxemic perception.</p><p>In this regard, our implementation -while fully functional -serves just as an example that illustrates design possibilities. We do not suggest that our media player is the ideal, nor that it achieves the perfect balance between adjudicating proxemic information and implicit or explicit interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMPLEMENTATION</head><p>We briefly describe the technical setup and software implementation behind our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracking position and orientation of people and objects.</head><p>Similar to <ref type="bibr" target="#b20">[21]</ref>, we use a Vicon infrared camera tracking system [www.vicon.com] to acquire fine-grained sensing information about people, objects, and digital devices moving around the interactive wall display. Six cameras emitting infrared light are placed around the SmartBoard to capture the position of passive infrared reflective markers. These markers are attached to tracked objects, digital devices, and people (peoples' head positions are tracked via hats with reflective markers attached). With this setup, we can detect markers up to a distance of four meters around the wall display. The Vicon software returns the triangulated position of all detected markers.</p><p>However, our implementation goes beyond this low level data acquisition. Our Proximity Toolkit <ref type="bibr" target="#b3">[4]</ref> wraps the Vicon real time raw data, where it transforms that data into a much more usable form available to the programmer via an object-oriented API. Internally, it maintains a 3D model of all fixed and semi-fixed features, and of moving tagged entities in the environment. Entity positions are updated at 50 Hz. Programmers use the Proximity Toolkit's API to receive detailed information about the relative and absolute proximity and orientation between identified people, objects, and devices (including ray-casting information) <ref type="bibr" target="#b3">[4]</ref>.</p><p>Our toolkit also includes extensive but easy to use configuration options for specifying the fixed and semi-fixed features of the environment.</p><p>Distributed access to proxemic information. The Ubicomp ecology includes multiple digital devices -such as a personal portable media player -that also need to be notified about their movements in the environment and the position and orientation of nearby objects and people. Our system maintains a distributed data structure (provided by the .NetworkingGT Toolkit <ref type="bibr" target="#b0">[1]</ref>) shared over a wireless network connection. This data structure contains precise information about the position and orientation of the tracked objects. We designed a hierarchical data structure that stores information like distance, angle, proxemic areas, and file transfers for every device. Both the client and server application subscribe to the relevant values and are instantly notified about changes. The applications running on the mobile devices can then trigger actions in response to sensed spatial movements and proxemic relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternative tracking technologies.</head><p>We recognize that the Vicon tracking system is expensive and thus not a realistic platform for commercialization. However, we believe that technology for sensing proximity and orientation will soon be cheaply available to the public at a lower and more reasonable price. In particular, 3D depth cameras measuring the time of flight can provide marker-less position-and orientation detection of arbitrary objects in a 3D space; such cameras will soon be cheaply available as part of game consoles. What is more important is that our design concepts for proxemic interactions apply are independent of the tracking technology, as long as the technology returns the four dimensions of proxemic relationships: position, orientation, movement, and identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>We contribute extended notions of proxemic interaction, which is based on fine grained sensing of nearby people, objects, and digital devices. Through a scenario, we showed how proxemic interactions enable a multitude of implicit and explicit interactions with an interactive vertical display. In particular, we explained how knowing the continuous movement of an approaching identified person along with the position, orientation, and usage of identified digital devices and objects can be exploited in interface design, e.g., how the system should implicitly respond to proxemic entities and how the system can afford opportunities for explicit interactions. We generalize these as concepts for the design of proxemic interactions, all illustrated with episodes extracted from our fully-functional proximity-aware interactive media player.</p><p>We will continue our work in several ways. This includes: how proxemic interactions can mediate people's interaction with particular devices (e.g., digital cameras, picture frames); how proxemic interactions can facilitate interdevice connectivity and information transfer; and how security / access between devices can be done by knowing who, what, and where people are within an environment. We are also interested in investigating the scalability of these device ecologies, where many people and devices of different types may enter and leave the environment.</p><p>The largest unsolved issue in proxemic interaction is how one can configure the 'rules of behaviour', i.e., how the system should react to the proxemic information it gathers. While computers can take action based on its inference of the proxemic dynamics, it will sometimes get it wrong. Creating meaningful behaviours and repairing mistakes <ref type="bibr" target="#b10">[11]</ref> will, we believe, become a central issue in the design of such systems. Even with this caveat, we believe that proxemic interactions will become a powerful way to realize embodied interaction, where -ideally -the system naturally responds to people's social expectations and practices in their everyday environments, and where mistakes are easily repaired <ref type="bibr" target="#b10">[11]</ref> or of little consequence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 (</head><label>2</label><figDesc>Figure 2 (top) shows Fred approaching the display at four distances (a' -d'), while the four scenes at the bottom shows what Fred would see at those distances. Initially, the proxemic media player is 'asleep' as the room is empty.When Fred enters the room at position (a'), the media player recognizes Fred and where he is standing. It activates the display, shows a short animation to indicate it is activated, and then displays four large video preview thumbnails held in Fred's media collection (Figure2a). As Fred moves closer to the display (b'), the video preview thumbnails and titles shrink continuously to a smaller size, thus showing an increasing number of available videos (2b). When Fred is very close to the surface (c'), he can select a video directly by touching its thumbnail on the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Proxemic Interaction: a) activating the system when a person enters the room, b) continuously revealing of more content with decreasing distance of the person to the display, c) allowing explicit interaction through direct touch when person is in close distance, and d) implicitly switching to full screen view when person is taking a seat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Integrating attentive interface behaviour: pausing the video playback when the person is (a) reading a magazine, (b) answering a call, or (c) talking to another person</figDesc><graphic coords="5,55.13,109.16,378.97,51.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Explicit interaction triggered through distance and orientation between a person and digital / non-digital physical artefacts: a) pen, b) cell phone.</figDesc><graphic coords="6,192.95,144.80,225.41,86.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Proximity mediates device to device interaction: from awareness to direct interaction.</figDesc><graphic coords="7,57.65,206.00,341.65,75.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Mediating between multiple people: a) incoming person sees basic information such as video title; b) as one moves closer, the split view provides a more detailed video description; c) when within reach of the display, the person gets full control.</figDesc><graphic coords="8,56.81,176.36,371.05,60.67" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>November 7-10, 2010, Saarbr ücken, Germany</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Partially funded by the iCORE/NSERC/SMART Chair in Interactive Technologies, Alberta Innovates Technology Futures, NSERC, and SMART Technologies Inc. Thanks to Dan Vogel, Wendy Ju, and their collaborators for their two inspiring papers on proxemics <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b20">[21]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SD: performance and simplicity in a groupware toolkit</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Alwis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Gt/</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EICS &apos;09</title>
		<meeting>of EICS &apos;09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Living in Augmented Reality: Ubiquitous Media and Reactive Environments</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A S</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Video Mediated Communication</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Finn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Sellen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Wilber</surname></persName>
		</editor>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="363" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointing and Placing</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pointing. Where language, culture, and cognition meet</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Kita</surname></persName>
		</editor>
		<meeting><address><addrLine>Erlbaum, Hillsdale, N.J., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="243" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The proximity toolkit and ViconFace: the video. Ext. Abstracts of CHI</title>
		<author>
			<persName><forename type="first">R</forename><surname>Diaz-Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="4793" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Where the Action Is: The Foundations of Embodied Interaction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dourish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Situated information spaces and spatially aware palmtop computers</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Fitzmaurice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supporting device discovery and spontaneous interaction with spatial references</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gellersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pers. Ubiq. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="255" to="264" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Hidden Dimension</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<pubPlace>Doubleday</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lean and zoom: proximityaware user interface and content magnification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI &apos;08</title>
		<meeting>of CHI &apos;08</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="507" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smart-Its Friends: A Technique for Users to Easily Establish Connections between Smart Artefacts</title>
		<author>
			<persName><forename type="first">L</forename><surname>Holmquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Ubicomp &apos;01</title>
		<meeting>of Ubicomp &apos;01</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Range: exploring implicit interaction through electronic whiteboard design</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CSCW &apos;08</title>
		<meeting>of CSCW &apos;08</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Group Coordination and Negotiation through Spatial Proximity Regions around Mobile Devices on Augmented Tabletops</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kratz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TABLETOP &apos;08</title>
		<meeting>of TABLETOP &apos;08</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Prante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Röcker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Streitz</surname></persName>
		</author>
		<title level="m">Wall-Beyond Ambient Displays. Adj. Proc. of Ubicomp &apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Direct Manipulation Technique For Multiple Computer Environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</author>
		<author>
			<persName><surname>Pick-And-Drop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST &apos;97</title>
		<meeting>of UIST &apos;97</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A multiple device approach for supporting whiteboard-based interactions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI &apos;98</title>
		<meeting>of CHI &apos;98</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="344" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proximal Interactions: A Direct Manipulation Technique for Wireless Networking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ayatsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERACT &apos;03</title>
		<meeting>of INTERACT &apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context-Aware Computing Applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schilit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Want</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HotMobile &apos;94</title>
		<meeting>of HotMobile &apos;94</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shadow reaching: a new perspective on interaction for large displays</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shoemaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Booth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST &apos;07</title>
		<meeting>of UIST &apos;07</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="53" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">That one there! Pointing to establish device identity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Swindells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tory</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST &apos;02</title>
		<meeting>of UIST &apos;02</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attentive user interfaces: the surveillance and sousveillance of gaze-aware objects</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Shell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Information</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="275" to="298" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive public ambient displays: transitioning from implicit to explicit, public to personal, interaction with multiple users</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST &apos;04</title>
		<meeting>of UIST &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bridging Physical and Virtual Worlds with Electronic Tags</title>
		<author>
			<persName><forename type="first">R</forename><surname>Want</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Fishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gujar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI &apos;99</title>
		<meeting>of CHI &apos;99</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
