<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DUCATI: High-performance Address Translation by Extending TLB Reach of GPU-accelerated Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
							<email>ajaleel@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">S. H. Duncan</orgName>
								<address>
									<addrLine>37 Wollaston Ave</addrLine>
									<postCode>02476</postCode>
									<settlement>Arlington</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">S. H. Duncan</orgName>
								<address>
									<addrLine>37 Wollaston Ave</addrLine>
									<postCode>02476</postCode>
									<settlement>Arlington</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
							<email>eebrahimi@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">S. H. Duncan</orgName>
								<address>
									<addrLine>37 Wollaston Ave</addrLine>
									<postCode>02476</postCode>
									<settlement>Arlington</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>NVIDIA</roleName><forename type="first">Sam</forename><surname>Duncan</surname></persName>
							<email>sduncan@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">S. H. Duncan</orgName>
								<address>
									<addrLine>37 Wollaston Ave</addrLine>
									<postCode>02476</postCode>
									<settlement>Arlington</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Northborough, MA</roleName><forename type="first">Hudson</forename><surname>St</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">S. H. Duncan</orgName>
								<address>
									<addrLine>37 Wollaston Ave</addrLine>
									<postCode>02476</postCode>
									<settlement>Arlington</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DUCATI: High-performance Address Translation by Extending TLB Reach of GPU-accelerated Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3309710</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts:</term>
					<term>Computer systems organization ? Architectures</term>
					<term>? Hardware ? Semiconductor memory</term>
					<term>Emerging technologies</term>
					<term>Emerging interfaces</term>
					<term>? Software and its engineering ? Memory management</term>
					<term>GPU, virtual memory, TLB, caches, high bandwidth memory</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional on-chip TLB hierarchies are unable to fully cover the growing application working-set sizes. To make things worse, Last-Level TLB (LLT) misses require multiple accesses to the page table even with the use of page walk caches. Consequently, LLT misses incur long address translation latency and hurt performance. This article proposes two low-overhead hardware mechanisms for reducing the frequency and penalty of ondie LLT misses. The first, Unified CAche and TLB (UCAT), enables the conventional on-die Last-Level Cache to store cache lines and TLB entries in a single unified structure and increases on-die TLB capacity significantly. The second, DRAM-TLB, memoizes virtual to physical address translations in DRAM and reduces LLT miss penalty when UCAT is unable to fully cover total application working-set. DRAM-TLB serves as the next larger level in the TLB hierarchy that significantly increases TLB coverage relative to on-chip TLBs. The combination of these two mechanisms, DUCATI, is an address translation architecture that improves GPU performance by 81% (up to 4.5?) while requiring minimal changes to the existing system design. We show that DUCATI is within 20%, 5%, and 2% the performance of a perfect LLT system when using 4KB, 64KB, and 2MB pages, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Heterogeneous systems composed of latency optimized cores (e.g., CPUs) and throughput optimized cores (e.g., GPUs, MIC) <ref type="bibr" target="#b16">(Duran and Klemm 2012)</ref> are becoming the defacto organization of future high-performance computing platforms. Such systems typically consist of a hybrid memory system <ref type="bibr" target="#b26">(Jeffers et al. 2016;</ref><ref type="bibr" target="#b37">Macri 2015</ref>; GP100 2016) that is composed of commodity DRAM (JEDEC 2013a) and stacked DRAM <ref type="bibr">(JEDEC 2013b;</ref><ref type="bibr">hmc 2013)</ref>. Furthermore, such systems also support Shared Virtual Memory (HSA Foundation 2014; <ref type="bibr" target="#b39">Negrut et al. 2014)</ref> where both CPUs and GPUs access the entire hybrid memory address space using a unified virtual address space.</p><p>6:2 A. Jaleel et al. In the virtual memory framework, depending on the page table implementation, address translation requires one or more page table accesses <ref type="bibr" target="#b4">(Bhargava et al. 2008)</ref>. To avoid the long memory access latency, processor architects cache recent address translations using an on-chip multi-level translation look-aside buffer (TLB) hierarchy. Growing application working-set sizes continue to stress the on-chip Last-Level TLB (LLT) <ref type="bibr" target="#b2">(Barr et al. 2011;</ref><ref type="bibr" target="#b3">Basu et al. 2013;</ref><ref type="bibr" target="#b6">Bhattacharjee et al. 2011;</ref><ref type="bibr" target="#b41">Pham et al. 2012)</ref>. LLT misses are latency sensitive operations that require one or more serial accesses to the page table. Therefore, virtual memory performance is dependent on the performance of the LLT. Reducing LLT miss latency enables instructions depending on the missing TLB entry to make faster forward progress.</p><p>A recent real system measurement study shows significant opportunity to improve shared virtual memory performance of heterogeneous CPU-GPU systems <ref type="bibr" target="#b56">(Vesely et al. 2016)</ref>. Specifically, they show that LLT misses are an order of magnitude slower on the GPU relative to the CPU. Thus, we focus on improving GPU LLT miss overhead in CPU-GPU systems by reducing both LLT miss frequency and LLT miss latency.</p><p>A simple solution to reduce LLT miss frequency would be to increase LLT size to cover the application working-set size. Figure <ref type="figure" target="#fig_0">1</ref> shows GPU performance sensitivity to LLT size relative to a 1,024-entry shared LLT. <ref type="foot" target="#foot_1">1</ref> The x-axis shows TLB sizes, while the y-axis shows average performance relative to the baseline for three different page sizes: 4KB, 64KB, and 2MB. The figure shows that performance is highly sensitive to the number of LLT entries with smaller page sizes. On average, a two million entry LLT improves performance by 2? with a 4KB page size. Similarly, a 256kentry LLT improves performance by 60% with 64KB page size. Finally, an 8k-entry LLT improves performance by 10% with 2MB page size. Note that as application memory working-set sizes grow larger, the performance sensitivity to larger LLT size also increases with large page sizes.</p><p>Unfortunately, on-die area limitations prohibit simply increasing the LLT size, especially with smaller page sizes. Consequently, prior work has investigated improving performance of virtual memory translation using large pages to help TLB coverage. However, large pages can create unintended OS performance overheads <ref type="bibr" target="#b53">(Talluri and Hill 1994;</ref><ref type="bibr" target="#b54">Talluri et al. 1992</ref>) due to memory imbalance <ref type="bibr" target="#b18">(Gaud et al. 2014)</ref>, memory fragmentation, paging <ref type="bibr" target="#b11">(Chou et al. 2014)</ref>, page creation, and page splitting <ref type="bibr" target="#b42">(Pham et al. 2015)</ref> in emerging Non-Uniform Memory Access (NUMA) GPU systems <ref type="bibr">(Young et al. 2018b</ref>). As such, there is a growing need to improve LLT coverage without relying on large pages alone. DUCATI: High-performance Address Translation by Extending TLB Reach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:3</head><p>We propose hardware mechanisms to improve LLT coverage and LLT miss penalty on emerging GPU systems. Our first mechanism, Unified Cache and TLB (UCAT), reduces the frequency of ondie LLT misses by enabling the conventional unified Last-Level Cache (LLC) to also hold TLB entries. UCAT augments the existing on-die LLT and increases on-die TLB coverage by potentially allowing as many TLB entries as there are cache lines in the conventional on-chip LLC.</p><p>However, UCAT does not reduce the LLT miss penalty incurred from walking the page table. This is because an LLT miss still requires multiple long-latency, serial, accesses to the page table. To this end, we extend GPUs with a hardware mechanism that memoizes virtual to physical translations directly in memory. Similar in design to the recently proposed PoM-TLB <ref type="bibr" target="#b48">(Ryoo et al. 2017)</ref>, we propose a TLB in GPU DRAM (DRAM-TLB) that serves as the next larger TLB in the processor TLB hierarchy whose contents are identical to the contents of on-chip TLBs. A DRAM-TLB is consulted upon LLT (or UCAT) misses before walking the page table.</p><p>Overall, this article makes the following contributions:</p><p>(1) We extend on-die TLB coverage by designing the conventional LLC to serve as a Unified Cache and TLB (UCAT). UCAT holds cache lines and TLB entries in a single hardware structure. Thus, UCAT improves on-die TLB coverage by enabling as many on-die TLB entries as there are cache lines in the on-chip LLC.</p><p>(2) We reduce LLC miss penalty by architecting TLBs in the GPU DRAM (DRAM-TLB). A DRAM-TLB is a very large hardware-managed structure that logically sits between the LLT (or UCAT) and the page table. A DRAM-TLB can be arbitrarily sized to provide the desired TLB coverage by only occupying a very small portion of gigantic DRAM sizes in today's systems. Unlike a page table walk, a DRAM-TLB provides low-latency address translation with a single memory access.</p><p>(3) We propose DUCATI, an address translation architecture for GPUs that combines DRAM-TLB and UCAT benefits to improve TLB coverage and LLT miss latency in highperformance computing systems.</p><p>For a set of high-performance computing workloads simulated on a heterogeneous CPU-GPU system with 4KB page size, Unified Cache and TLB (UCAT) improves performance by 65% on average (up to 4?). However, DRAM-TLB improves performance by 22% on average (up to 2.25?). Both proposals combined, DUCATI, improves performance by 81% (up to 4.5?). We also show that DUCATI requires negligible hardware changes and scales to larger page sizes and improves performance by 56% and 8% when using 64KB and 2MB pages, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>Virtual memory is ubiquitous in nearly all computing systems today and is responsible for translating virtual addresses to physical addresses. Address translation is performed by maintaining an application page table in system memory. Figure <ref type="figure" target="#fig_1">2</ref> shows a typical four-level hierarchical page  memory reference). Next, the MMU indexes the PDP to fetch the Page Directory (PD) or secondlevel page table (second memory reference). Next, the MMU indexes the PD to fetch the first-level of the Page Table (PT) (third memory reference). Finally, the MMU indexes the PT (fourth memory reference) to fetch the physical address mapped to the virtual address.</p><p>When walking the page table, commercial processors reduce memory accesses by using Page Walk Caches (PWC) for each level of the page table <ref type="bibr">(Barr et al. 2010;</ref><ref type="bibr" target="#b5">Bhattacharjee 2013)</ref>. PWCs exploit temporal and spatial locality in the page table access stream and avoid memory accesses on PWC hits. Consequently, the frequency of TLB misses and PWC hit rates impacts address translation performance. In the previous section, Figure <ref type="figure" target="#fig_0">1</ref> motivated increasing TLB coverage by demonstrating its direct impact on performance. To provide further insight, Table <ref type="table" target="#tab_2">2</ref> shows the number of LLT misses for the evaluated workloads. We observe that these workloads incur frequent LLT misses when using 4KB pages. To make things worse, each LLT miss requires more than one memory access to the page table (maximum of two in some cases). Consequently, these workloads experience significant performance overheads due to TLB misses. With the number of levels in the page table hierarchy expected to grow to 5-levels <ref type="bibr" target="#b14">(Corbet 2017)</ref>, there is a need for reducing the number of memory accesses on an LLT miss in addition to improving LLT coverage. Before we describe our solutions to address both these problems, we first provide details on our experimental methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL METHODOLOGY</head><p>We assume a CPU-GPU heterogeneous system with a single CPU and a single GPU supporting shared virtual memory <ref type="bibr" target="#b30">(Junkins 2015;</ref><ref type="bibr" target="#b38">Moammer 2016;</ref><ref type="bibr">GP100 2016)</ref>. Figure <ref type="figure" target="#fig_2">3</ref> illustrates our baseline system where GPU LLT misses are handled by the CPU IOMMU using Address Translation Services (ATS) <ref type="bibr" target="#b56">(Vesely et al. 2016;</ref><ref type="bibr" target="#b1">ATS 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Configuration</head><p>We use an industry proprietary performance simulator to simulate a GPU (Table <ref type="table" target="#tab_1">1</ref>) with a memory hierarchy similar to the NVIDIA Pascal GPU system (GP100 2016). We model 128 Streaming Multiprocessors (SM) that support 64 warps each. A warp scheduler selects warp instructions each cycle. The baseline GPU memory system consists of a multi-level non-inclusive cache and TLB hierarchy. The first-level cache and TLB are private to each SM while the memory-side LLC and LLT are shared by all the SMs <ref type="bibr" target="#b6">(Bhattacharjee et al. 2011)</ref>. All caches use 128B cache line size with 32B   <ref type="bibr" target="#b23">(Jaleel et al. 2010)</ref>.</p><p>We model a hybrid memory subsystem consisting of 16GB of High Bandwidth Memory (HBM) technology (JEDEC 2013b) (referred to as stacked memory) and 256GB of CPU-attached commodity DRAM (referred to as system memory) using conventional DDR4 technology (JEDEC 2013a). We assume similar interconnect latency to the HBM controller and the DDR4 controller (50ns oneway). However, the stacked memory has 5? the bandwidth of system memory with similar random access latency. To ensure full utilization of the total available system bandwidth, physical pages are allocated based on the bandwidth ratio of the hybrid memory system <ref type="bibr" target="#b0">(Agarwal et al. 2015;</ref><ref type="bibr">Chou et al. 2015a</ref>). In doing so, both system memory and stacked memory satisfy memory requests. The memory controller supports 128-entry read and write queues per channel, open-page policy, minimalist address mapping policy <ref type="bibr" target="#b32">(Kaseridis et al. 2011</ref>) and FR-FCFS scheduling policy (prioritizing reads over writes). Writes are issued in batches when the write queue starts to fill up.</p><p>We model a virtual memory system that maps virtual addresses to physical addresses. We use a combination of bandwidth-aware page allocation <ref type="bibr" target="#b0">(Agarwal et al. 2015;</ref><ref type="bibr">Chou et al. 2015a</ref>) and random page replacement policy. Our baseline system assumes 4KB page size; however, we also evaluate 64KB and 2MB page size. We model a four-level hierarchical page table <ref type="bibr">(Barr et al. 2010</ref>): Page Map Level 4 (PML4), Page Directory Pointer (PDP), Page Directory (PD), and Page Table (PT). All application page tables reside in system memory where GPU LLT misses are serviced by the IOMMU on the CPU using ATS. We also evaluate a system where the page tables are stored in GPU memory and the GPU MMU (GMMU) services LLT misses instead. The IOMMU (and GMMU) is configured with a highly-threaded hardware page table walker <ref type="bibr" target="#b44">(Power et al. 2014;</ref><ref type="bibr" target="#b43">Pichai et al. 2014</ref>) and on-chip Page Walk Caches (PWCs) <ref type="bibr">(Barr et al. 2010;</ref><ref type="bibr" target="#b5">Bhattacharjee 2013)</ref> for each level of the page table. The PWCs are small, fully associative hardware structures that are indexed by portions of the virtual address <ref type="bibr" target="#b5">(Bhattacharjee 2013)</ref>. We model a 16-entry PML4-cache, 16-entry PDP-cache, a 16-entry PD-cache, and a 16-entry PT-cache (Bhattacharjee 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Workloads and Metric of Interest</head><p>We use selected CUDA-based high-performance computing applications from CORAL (CORAL 2014), <ref type="bibr">Mantevo (Heroux et al. 2009), and</ref><ref type="bibr">LoneStar (Kulkarni et al. 2009</ref>) suites (see Table <ref type="table" target="#tab_2">2</ref>). Our study also includes a graph-traversal workload (MaxFlow <ref type="bibr" target="#b28">(Jiang et al. 2013)</ref>) and a random memory access workload (GU PS <ref type="bibr" target="#b36">(Luszczek et al. 2006)</ref>). All workloads are run with large inputs to stress the hybrid memory system. We collected representative program regions and warm up the caches, TLBs, and PWCs by executing four billion warp instructions. After functional warmup, we enable detailed timing simulation for two billion warp instructions.</p><p>Reduction in total execution time is our primary metric for performance. We also compare address translation latency to correlate the change in performance. We compute translation latency as the cycles spent in translating a virtual address to physical address averaged across all memory instructions executed by the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNIFYING CACHES AND TLBS</head><p>Modern chip multiprocessors use multi-level TLB and cache hierarchies for high performance. The last-level in each hierarchy is architected as a single large unified structure that holds both instruction and data entries and is shared among all cores. For example, the unified LLC contains hundreds of thousands of cache lines (e.g., 256k lines in an 8MB cache with 32B cache lines). Similarly, the unified LLT contains 512-1024 TLB entries <ref type="bibr" target="#b52">(Sodani et al. 2016;</ref><ref type="bibr" target="#b21">Intel 2009)</ref>. The LLT and LLC are cache structures with a tag array and a data array. The LLT tag array stores virtual addresses while the LLC tag array stores physical addresses. <ref type="foot" target="#foot_2">2</ref> The LLT data array stores the physical address corresponding to the virtual address translation (roughly 8 bytes), while the LLC data array stores a copy of the data from memory at a cache line granularity (e.g., 128-byte lines with 32 byte sectors).</p><p>The LLT and LLC have similar sized tag arrays. However, since they provide different types of data, the LLC has a 4?-8? larger data array. To increase on-die LLT capacity, we observe that the LLC can potentially also serve as gigantic TLB with as many entries as there are LLC lines. Given that LLCs typically have low hit-rates and are often inefficiently used<ref type="foot" target="#foot_3">3</ref> (Lee and Kim 2012; <ref type="bibr" target="#b23">Jaleel et al. 2010;</ref><ref type="bibr" target="#b45">Qureshi et al. 2007;</ref><ref type="bibr" target="#b58">Wu et al. 2011;</ref><ref type="bibr" target="#b29">Jim?nez 2013;</ref><ref type="bibr" target="#b33">Khan et al. 2010</ref>), the conventional LLC can be used to store virtual to physical translations just like a TLB (in addition to caching data from memory). Based on this insight, we propose to re-architect the conventional LLC as a Unified Cache and TLB (UCAT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">UCAT Architecture</head><p>Figure <ref type="figure" target="#fig_3">4</ref>(a) illustrates a baseline system with separate LLT and LLC structures, while Figure <ref type="figure" target="#fig_3">4</ref>(b) demonstrates a system with a separate LLT and a Unified Cache and TLB (UCAT). In the latter system, the UCAT holds both cache lines and TLB entries in a single hardware structure. Figure <ref type="figure" target="#fig_3">4(c)</ref> shows how this is done. When a UCAT entry stores a cache line similar to what the baseline system does, the tag-array stores a portion of the physical address while the data array stores the cache line (e.g., 32-bytes).<ref type="foot" target="#foot_4">4</ref> However, when the UCAT entry serves as a TLB entry, the UCAT tag-array stores portions of the virtual address and the address space identifier (ASID), while the data-array stores the virtual to physical address translation, and additional information such as page protection bits, and possibly some ASID bits. Storing some ASID bits in the data array may be necessary if all the ASID bits do not fit in the existing LLC tag array size. In such situations, we propose to store the top bits of the ASID in the tag array. Doing so enables a partial tag match and avoids false positives when multiple programs with identical virtual addresses concurrently execute on the system. Note that the bottom bits of the ASID (stored in the data array) must be compared for a match before supplying the translation stored in the UCAT-entry. We use 16 bytes of the UCAT data array for storing the address translation.</p><p>Even though we only use 16-bytes of the 32-byte sector to store the translation, UCAT improves upon the existing LLC inefficiency. This is because recent CPU and GPU studies show (and this study independently verifies) that the majority of LLC entries are unused after cache insertion (Lee and Kim 2012; <ref type="bibr" target="#b23">Jaleel et al. 2010;</ref><ref type="bibr" target="#b45">Qureshi et al. 2007;</ref><ref type="bibr" target="#b58">Wu et al. 2011;</ref><ref type="bibr" target="#b29">Jim?nez 2013;</ref><ref type="bibr" target="#b33">Khan et al. 2010</ref>). As such, UCAT utilizes the conventional LLC space more efficiently by storing TLB entries in addition to cache lines. Note that UCAT space efficiency can be improved further by compressing multiple TLB entries in the unused portion of the data array. However, we leave these optimizations for future work.</p><p>We now discuss the lookup and fill operations for a UCAT. When the GPU misses in the LLT, it consults the UCAT to determine if the missing translation is in the UCAT. If so, then the translation is returned to the LLT. However, on a UCAT miss, the request is sent to the MMU to walk the page table and retrieve the missing translation. The missing translation is then inserted into the UCAT (for future hits) and also returned to the LLT. For the purpose of our evaluations we assume a non-inclusive TLB hierarchy.</p><p>In a UCAT, cache lines and TLB-entries dynamically contend for the available UCAT space. Like the baseline LLC architecture, we leverage the existing replacement policy to manage allocating UCAT entries in a set. UCAT hits update replacement state while UCAT misses utilize the baseline replacement policy to select the victim. The baseline UCAT replacement policy allocates entries based on demand, rather than utility. This can potentially create performance problems in DUCATI: High-performance Address Translation by Extending TLB Reach 6:9 TLB-sensitive workloads that frequently stream through a large number of cache lines and constantly discard performance critical UCAT TLB entries. Based on this insight, we enhance the baseline UCAT design by improving the replacement policy. To this end, we leverage the Dynamic Re-Reference Interval Prediction (DRRIP) replacement policy <ref type="bibr" target="#b23">(Jaleel et al. 2010)</ref>. In this policy, all UCAT insertions follow the same insertion policy. We propose enhancing the insertion policy for TLB entries by inserting them with a near-immediate prediction rather than the default far prediction. By doing so, TLB entries reside in the UCAT for a longer duration. We refer to this enhancement as UCAT with Insertion (UCAT-I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">UCAT Performance</head><p>Figure <ref type="figure" target="#fig_4">5</ref> shows UCAT performance relative to the baseline system(on the y-axis) with workloads on the x-axis. The first bar in the figure shows UCAT performance where cache lines and TLBentries contend for UCAT space without any restrictions. The figure shows that UCAT significantly improves performance of workloads like XSBench, dmr, MaxFlow, and MCB by more than 2? (up to 4? in the latter two). Other workloads like GUPS, SNAP, UMT, and CoMD observe more than 15% performance gains. Overall, UCAT improves performance across all workloads by 53%.</p><p>To understand the performance benefits of UCAT, Figure <ref type="figure">6</ref> and Figure <ref type="figure" target="#fig_5">7</ref> illustrate relative translation latency and effective on-die TLB size respectively. The effective on-die TLB size is reported as the average number of TLB entries in the UCAT (sampled every 10 million cycles) during the course of application execution.</p><p>Workloads with more than 2? performance improvement observe a reduction in TLB miss penalty by 70% or more. This is because these workloads experience a 4?-64? increase in on-die TLB size. On average, UCAT improves address translation latency by 51% (up to 86% for MCB). The improvement in translation latency stems from an increase in on-chip TLB coverage by 16? (up to 64? for dmr). We note that some workloads (e.g., SNAP) experience modest performance benefits despite a significant reduction in TLB miss penalty. This is because SNAP has abundant parallel work that can be performed in the shadow of a TLB miss. As such, SNAP despite its frequent TLB misses is fairly insensitive to TLB miss latency.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> also shows that intelligently managing UCAT by prioritizing TLB-entries over cache lines improves average UCAT performance by an additional 12%. We observed that UCAT-I also has negligible increase in memory traffic (&lt;1%) and TLB-sensitive workloads like XSBench and dmr experience additional performance improvements of up to 90%. Additionally, workloads like LULESH, SNAP, and CoMD experience an additional 7-10% performance improvement over UCAT by intelligently managing the UCAT-entries. This is evident from Figures <ref type="figure">6</ref> and<ref type="figure" target="#fig_5">7</ref> where UCAT-I increases effective TLB coverage by nearly 2? while improving average translation latency by 7%. Figure <ref type="figure">8</ref> shows that the increase in effective TLB size halves the baseline TLB miss rate from roughly 60% to about 30%. Overall, the reduction in misses improves UCAT-I performance by 65% relative to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">UCAT Impact on LLC Performance</head><p>Since UCAT decreases the effective capacity of cache lines compared to the separate LLC, we investigated the increase in memory traffic relative to the baseline design. Figure <ref type="figure" target="#fig_6">9</ref> show that on average, UCAT increases the number of memory accesses by 0.5% (maximum 5% for dmr). We find the additional increase in memory traffic has negligible performance impact. This illustrates that trading off conventional LLC space for TLB entries provides substantial performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">UCAT Design Overhead</head><p>UCAT requires a mechanism to distinguish between cacheline entries and translation entries. We propose using the UCAT-entry state bits (i.e., MESI bits) and insert TLB-entries into UCAT with the exclusive and shared status bits both set to valid. Since cache lines can either be in exclusive state or shared state and not both, this proposal enables distinguishing between TLB entries and cacheline entries at zero storage overhead. Besides distinguishing between cacheline and translation entries, UCAT also requires efficient support for handling TLB shootdown and TLB flush requests. We discuss these design issues in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ARCHITECTING TLBS IN DRAM</head><p>When the application working set exceeds the TLB coverage provided by UCAT (e.g., in GU PS), UCAT does not reduce the number of memory accesses on an LLT miss. Reducing the number of memory accesses on an LLT miss reduces effective LLT miss latency. As such, we now investigate increasing TLB coverage by extending the TLB hierarchy using TLBs in DRAM (DRAM-TLB). DRAM-TLB logically sits between the on-die shared LLT (or UCAT) and the page tables in memory. DRAM-TLB is first consulted on an LLT (or UCAT) miss before walking the page table (see Figure <ref type="figure" target="#fig_7">10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DRAM-TLB Architecture</head><p>DRAM-TLBs can either be architected using stacked memory or system memory. A DRAM-TLB architected using stacked memory is referred to as a Stacked-TLB while a DRAM-TLB architected using system memory is referred to as a SysMem-TLB <ref type="bibr" target="#b48">(Ryoo et al. 2017)</ref>. DRAM-TLBs are physically placed in a large contiguous segment of memory and are entirely hardware-managed. As such, DRAM-TLBs do not contribute to the OS-visible memory space (see Figure <ref type="figure" target="#fig_7">10(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">DRAM-TLB Organization.</head><p>Like conventional on-chip SRAM TLBs, and the proposed UCAT architecture, a DRAM-TLB entry maintains the TLB tag, physical address, and meta data information such as valid bits, page permission bits, address space identifier (ASID). To accommodate this information, we use 16 bytes of storage per DRAM-TLB entry. Figure <ref type="figure" target="#fig_7">10</ref> illustrates the layout of the DRAM-TLB in the DRAM array. For example, a 2KB DRAM row buffer holds 128 DRAM-TLB entries per row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">DRAM-TLB Lookups.</head><p>Unlike on-chip TLBs, all DRAM-TLB operations occur on the DRAM data bus at the granularity of the DRAM interface. Thus, on an LLT miss, if implemented as a stacked-TLB, the lookup fetches data at the granularity of the stacked memory interface: 32-byte cacheline (two TLB entries), and if implemented in system memory, the lookup fetches data at the granularity of the DDR interface: 64-byte cacheline (four TLB entries).</p><p>Reading multiple TLB entries enables architecting a set-associative DRAM-TLB without incurring additional latency or bandwidth (Qureshi and Loh 2012; Loh and Hill 2011). We evaluate both set-associative and direct-mapped designs and observe similar performance among these design points with large DRAM-TLB sizes. These results match the behavior of existing work on large DRAM Caches where conflict misses tend to be low (Qureshi and Loh 2012). Thus, we architect a direct-mapped DRAM-TLB where consecutive DRAM-TLB sets map to the same row buffer in memory (to exploit DRAM row buffer locality). Adjacent DRAM-TLB entries fetched are also inserted into the PWC for potential future hits.</p><p>We could further take advantage of reading multiple TLB entries with a single read operation. Retrieving the co-located DRAM-TLB entries on a DRAM-TLB read naturally enables prefetching of neighboring address translations. For example, prefetched entries can be stored in an on-chip TLB prefetch buffer. This approach is similar to caching co-located page table entries in the PWCs on a conventional page table walk. We leave the evaluation of such designs to future work.</p><p>Designing DRAM-TLB as a direct-mapped structure can incur frequent conflict misses especially when multiple virtual addresses (from the same or different processes) map to the same DRAM-TLB set. Further enhancements such as hashing the lower order bits of the set index with the ASID and coordinating way installation and way prediction <ref type="bibr">(Young et al. 2018a</ref>) can address conflict misses in direct-mapped DRAM-TLBs. In practice, we find conflict misses are rare (see Figure <ref type="figure" target="#fig_3">14</ref>). As such, evaluation of such designs are also left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">DRAM-TLB Insertions and Updates.</head><p>The DRAM-TLB must be updated on misses, shootdowns, and page permission changes. We propose to modify the TLB-entry directly in DRAM on updates. To ensure that updating a single DRAM-TLB-entry does not corrupt the contents of colocated DRAM-TLB entries, we leverage existing DRAM interfaces that allow partial writes (e.g., byte-level writes) to memory without the power and bandwidth overhead of a full read-modifywrite operation (JEDEC 2013b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">DRAM-TLB Implementation.</head><p>Since the DRAM-TLB is stored in physical memory, the physical address for the DRAM-TLB entry must be computed. We use an 8-byte register in the Memory Management Unit (MMU) to store the base address (BaseAddr ) for the DRAM-TLB. Given the DRAM-TLB set index SI, DRAM-TLB associativity A, and DRAM-TLB entry size s (16 bytes in our case), the MMU computes the physical address for the DRAM-TLB entry using combinational logic:</p><formula xml:id="formula_0">PhysAddr = BaseAddr + SI * A * s.</formula><p>(1)</p><p>Once the DRAM-TLB entry is retrieved from memory, the MMU compares the tag to determine hit or miss. On a DRAM-TLB hit, the missing translation is returned to the processor. However, on a DRAM-TLB miss, the MMU walks the application page table to determine the virtual to physical translation. This translation is returned to the processor and also inserted into the DRAM-TLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">DRAM-TLB Example.</head><p>Assume a 1M-entry direct-mapped DRAM-TLB (with 4KB pages) starting at memory location BaseAddr=0. An LLT miss for virtual page 0xff2212345000 requires fetching the 16-byte DRAM-TLB entry at set index 0x12345. With A = 1 and s = 16, the physical memory location for this DRAM-TLB entry is 0x123450 (i.e., 0 + 0x12345 * 1 * 16). The MMU compares the missing tag (0xff22) with the DRAM-TLB entry to determine DRAM-TLB hit or miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DRAM-TLB Performance</head><p>We evaluate SysMem-TLB and Stacked-TLB performance assuming an 8-million entry DRAM TLB (32GB TLB coverage with 4KB pages). Figure <ref type="figure" target="#fig_8">11</ref> shows that DRAM-TLBs improve performance relative to the baseline system across all TLB-sensitive workloads. On average, SysMem-TLB improves performance by 11% while Stacked-TLB improves performance by 22%.  Figure <ref type="figure" target="#fig_9">12</ref> shows that SysMem-TLBs reduce address translation latency by 12% while Stacked-TLBs reduce address translation latency by 40%. In general, Stacked-TLBs perform better than SysMem-TLBs, because they are implemented with stacked memory, a technology that provides lower memory queuing delay due to the high memory bandwidth. In fact, our evaluations show that the loaded stacked memory latency tends to be an order of magnitude lower than system memory access latency.</p><p>DRAM-TLBs decrease address translation latency by reducing the number of memory accesses on an LLT miss. Figure <ref type="figure" target="#fig_10">13</ref> shows that the baseline system incurs 1.5 memory accesses per LLT miss (up to 2) on average. However, both SysMem-TLB and Stacked-TLB incur only 1.05 memory access per LLT miss (max of 1.21) on average. These results correspond to a near 100% hit rate in the DRAM-TLBs (see Figure <ref type="figure" target="#fig_3">14</ref>), implying that the only memory access required on an LLT miss is to fetch the translation from the DRAM-TLB. Note that minor differences in memory accesses per LLT miss between SysMemTLB and StackedTLB are due to timing variations that cause conflict misses in page walk caches. Overall, DRAM-TLBs boost performance of TLB-sensitive workloads like GU PS by 2.2? and MaxFlow by 70%. Furthermore, workloads like XSBench, dmr , LU LESH , and MiniAMR experience more than 15% performance gain. These results show that DRAM-TLBs decrease LLT miss latency by reducing the number of memory accesses on an LLT miss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">DRAM-TLB Design Overhead</head><p>Figure <ref type="figure" target="#fig_12">15</ref> shows simple modifications to the GPU Memory Management Unit (MMU) state machine to support DRAM-TLBs. On an LLT miss, the GMMU first consults the DRAM-TLB to determine if the translation is already present in the DRAM-TLB. If the request misses in the DRAM-TLB, then the MMU walks the different levels of the page table. Thus, we simply introduce a new state in the MMU state machine for the DRAM-TLB lookup before walking the page table.</p><p>The in-memory storage overhead for DRAM-TLBs depends on the desired TLB coverage. For example, achieving full system memory (256GB in our baseline) coverage with 4KB, 64KB, and 2MB pages requires 1GB, 64MB, and 2MB of storage overhead, respectively (assuming 16-byte DRAM-TLB entries). In general, this storage overhead is impractical for on-chip SRAM TLBs. However, these sizes are an insignificant fraction of emerging multi-gigabyte DRAM systems. For example, the aforementioned storage overheads correspond to 6% (4KB pages), 0.4% (64KB pages), and 0.01% (2MB pages) storage overhead for a 16GB stacked memory system. Consequently, DRAM-TLBs improve TLB coverage using small pages with minimal storage overhead and require no significant changes to the existing address translation architecture.</p><p>Like UCAT, DRAM-TLBs also require efficient support for handling TLB shootdown and TLB flush requests. We discuss these design issues in Section 7. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">COMBINING DRAM-TLBS AND UCAT</head><p>UCAT and DRAM-TLB independently improve on-die processor TLB coverage and LLT miss overhead, respectively. These mechanisms can be combined to collectively improve processor performance. To this end, we propose DUCATI, an address translation architecture that combines DRAM-TLBs and UCAT-I. While DUCATI can be architected with Stacked-TLBs or with SysMem-TLBs, we limit our analysis of DUCATI to Stacked-TLBs. This is because stacked memory technology provides better bandwidth and latency than conventional DDR memory technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DUCATI Performance</head><p>Figure <ref type="figure" target="#fig_13">16</ref> illustrates the relative performance of our proposals to the baseline system. The xaxis shows the different workloads while the y-axis illustrates performance. For each workload, we present the relative performance for DRAM-TLBs architected with stacked memory (i.e., Stacked-TLB), Unified Cache and TLB (UCAT) enhanced with insertion (UCAT-I), DUCATI, and for comparison, a hypothetical Perfect LLT system. <ref type="foot" target="#foot_5">5</ref> To provide insight into the reason for performance improvement, Figure <ref type="figure" target="#fig_5">17</ref> presents relative address translation latencies of the different proposals.</p><p>Figure <ref type="figure" target="#fig_13">16</ref> shows that stacked memory DRAM-TLBs improve average performance by 22%, UCAT with insertion (UCAT-I) improves average performance by 61%, while DUCATI combines the benefits of both to improve average performance by 81%. In fact, workloads like XSBench, dmr , MaxFlow, and MCB experience 4? or more performance improvement. Most of the performance gain for these workloads is due to increase in on-die LLT coverage. However, GU PS sees a 2.5? performance boost due to reduced LLT miss penalty (i.e., the DRAM-TLB component of DUCATI).</p><p>Figure <ref type="figure" target="#fig_5">17</ref> illustrates the relative address translation latency for the different proposals. On average, DRAM-TLB and UCAT reduce address translation latency by 40% and 60%, respectively, while DUCATI reduces address translation latency by 75%. In doing so, DUCATI improves performance by 1.81? while a perfect LLT system improves performance by 2.24?. Thus, DUCATI bridges two-thirds of the performance gap between the baseline system and a perfect LLT system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">DUCATI Sensitivity to Page Size</head><p>We now discuss the performance of our proposals using larger page sizes. Note that while large page sizes improve TLB coverage, unrestricted use of them can create various performance overheads <ref type="bibr" target="#b53">(Talluri and Hill 1994;</ref><ref type="bibr" target="#b54">Talluri et al. 1992;</ref><ref type="bibr" target="#b18">Gaud et al. 2014;</ref><ref type="bibr" target="#b11">Chou et al. 2014;</ref><ref type="bibr" target="#b42">Pham et al. 2015)</ref>. Nonetheless, Figure <ref type="figure" target="#fig_15">18</ref> illustrates the average performance across all workloads for 4KB, 64KB, and 2MB pages. Overall, DUCATI improves performance by 81% with 4KB pages, 56% with 64K pages, and 8% with 2MB pages. In fact, DUCATI is within 20%, 5%, and 2% the performance of an unrealistic perfect LLT system when using 4KB, 64KB, and 2MB pages, respectively. Thus, DUCATI bridges the performance gap between the baseline system and a perfect LLT system regardless of page size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">DUCATI Sensitivity to LLC Size</head><p>Table <ref type="table" target="#tab_3">3</ref> presents DUCATI performance averaged across all our workloads with 4KB pages and LLC sizes ranging from 2MB to 32MB. The table shows that DUCATI improves performance across all LLC sizes studied. In general, performance increases with growing LLC size. For example, performance improves by 61% with a 2MB LLC while performance improves by 93% with a 32MB LLC. This is because larger LLCs enable more translation entries to be cached in the UCAT structure. Overall, we observe that DUCATI improves performance regardless of the system LLC size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Thus far we have shown that DUCATI can improve TLB miss overhead when the GPU relies on the CPU for addresses translation support. We now investigate DUCATI performance when the GPU itself can walk the page table. We also discuss how DUCATI can potentially impact the behavior of virtual memory operations such as TLB shootdowns and TLB flushes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Performance When GPU Walks Page Table</head><p>We also investigate the performance of our mechanisms when the application page table is placed in GPU memory. In this scenario, the GPU MMU (and not the CPU IOMMU) walks the application page table. In doing so, the GPU MMU now has high-bandwidth and low-latency access to the application page table. Furthermore, a GPU page table walk has additional locality benefits of caching the application page table in the memory-side GPU LLC. Table <ref type="table" target="#tab_4">4</ref> presents the average performance of UCAT, DRAM-TLB, and DUCATI for the different page sizes when the GPU walks the application page table on an LLT miss. The table shows that DUCATI improves performance by 32%, 33%, and 1% for the 4KB, 64KB, and 2MB pages, respectively. These results show that there is still significant opportunity to improve TLB miss overheads even when the GPU walks the application page table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Impact on TLB Shootdowns</head><p>Virtual memory operations such as remapping the virtual to physical address or changes to page permissions require efficient handling of page table update requests such as TLB shootdowns. It would be highly desirable that DUCATI does not impact the overall latency of TLB shootdowns. TLB shootdowns normally occur as a part of an interrupt flow that requires 5-30?s <ref type="bibr" target="#b61">(Zheng et al. 2016)</ref>. The access latency for UCAT and DRAM-TLB tends to be a few ten to a few hundred nanoseconds. Thus, even in the worst case, the additional shootdown latency is a small fraction of the overall interrupt latency (2-5%). As such, DUCATI has negligible performance impact on TLB shootdown latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Impact on TLB Flushes</head><p>TLB flushes normally occur when the virtual to physical mappings of an entire ASID need to be updated. Flushing conventional on-die TLB sizes (e.g., 1,024 entries in the baseline) can be done quickly. However, flushing thousands of TLB entries (as in UCAT) or millions of TLB entries (as in DRAM-TLB) can be a very long latency operation. To address this problem, we provide architecture support to flush large TLBs with zero latency overhead. To this end, we propose to associate an Epoch Counter (EPCTR) with each ASID. This 4-byte counter tracks the virtual to physical translations for an ASID in a given epoch. In the DUCATI framework, the DRAM-TLB and UCAT store the EPCTR (in addition to the physical address and page permission bits) with each TLB entry. When the ASID requires a TLB flush, the EPCTR can simply be incremented. Thus, DUCATI provides a valid translation only if the ASID and EPCTR stored in the TLB-entry match the current EPCTR of the ASID. Both DRAM-TLB and UCAT have sufficient storage space to support a 16-bit EPCTR per entry. As such, the TLB flush overhead can be completely eliminated with negligible hardware (4 bytes per ASID) and zero latency overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Handling Multiple Page Sizes</head><p>On computing systems that simultaneously support multiple page sizes, DUCATI requires support for translating multiple page sizes. A naive implementation would require consulting the UCAT and DRAM-TLB for each page size for hit/miss confirmation. To avoid the bandwidth and latency for the multiple accesses, we leverage recent work that uses a simple predictor to learn the page size for any given virtual address <ref type="bibr" target="#b48">(Ryoo et al. 2017)</ref>. In this work, the authors propose a 512-entry table of 2-bit saturating counters indexed by the lower bits of the virtual address. The value of the 2-bit counter predicts the page size: small or large. Mispredicts still incur the latency and bandwidth of multiple memory accesses, however in the common case we find that such a predictor can provide near-perfect predictions on GPUs in steady state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>While significant literature exists on improving TLB performance, we discuss recent work that is most closely related to the work described in this article.</p><p>Improving TLB Performance. Recent work improve TLB coverage through compression <ref type="bibr" target="#b41">(Pham et al. 2012</ref><ref type="bibr" target="#b40">(Pham et al. , 2014) )</ref> and enhanced TLB organizations <ref type="bibr" target="#b6">(Bhattacharjee et al. 2011;</ref><ref type="bibr" target="#b9">Chen et al. 1992</ref>) that modify the existing TLB structures. Other work has investigated mechanisms to accelerate page walks by caching portions of the page table <ref type="bibr">(Barr et al. 2010;</ref><ref type="bibr" target="#b5">Bhattacharjee 2013)</ref>, prefetching TLB entries <ref type="bibr" target="#b7">(Bhattacharjee and Martonosi 2010;</ref><ref type="bibr" target="#b31">Kandiraju and Sivasubramaniam 2002;</ref><ref type="bibr" target="#b49">Saulsbury et al. 2000;</ref><ref type="bibr" target="#b44">Power et al. 2014)</ref>, or speculating on the address translation on TLB misses <ref type="bibr" target="#b2">(Barr et al. 2011)</ref>. When the memory footprint of workloads is extremely large, such proposals are unable to avoid TLB miss overhead. Our work focuses both on increasing TLB coverage and reducing TLB miss overheads by storing TLB entries in the conventional LLC (i.e., UCAT) and embedding a TLB in DRAM (i.e., DRAM-TLB). UCAT is similar to prior work on in-cache address translation <ref type="bibr" target="#b57">(Wood et al. 1986</ref>) where a VIVT L1 cache is used to store the PTE entry. This work is only applicable to VIVT caches, requires page tables to be aligned at a fixed granularity (e.g., 256MB), and is primarily targeted on reducing TLB hardware without sacrificing TLB coverage. However, UCAT significantly improves TLB coverage and can be applied to any level of the cache hierarchy (VIVT or PIPT) and requires no restrictions on page table placement.</p><p>DUCATI has two components: UCAT and DRAM-TLB. DRAM-TLBs resemble the softwaremanaged SPARC Translation Storage Buffer (TSB) architecture <ref type="bibr" target="#b17">(Feehrer et al. 2013)</ref>. Unlike the SPARC TSB, DRAM-TLBs are entirely hardware-managed and incur no software overhead (e.g., expensive traps). DRAM-TLBs also resemble recent CPU-centric work that extends TLB reach of CPUs (referred to as PoM-TLB) <ref type="bibr" target="#b48">(Ryoo et al. 2017)</ref>. Our work explores the DRAM-TLB design space on GPUs when architected in CPU memory space as proposed in the original work. However, we show that architecting TLBS in CPU memory (see Section 5.2) incurs additional latency and bandwidth overhead for accessing CPU memory. This is due to the low bandwidth system level interconnection network to access system memory from the GPU. As such, we propose architecting TLBs in the GPU attached high bandwidth memory. Unlike PoM-TLB, we also extend DRAM-TLBs with support for virtualized environments. Specifically, we propose low overhead TLB flushes without incurring any additional performance overheads on context switches.</p><p>PoM-TLB also improves on-die translation coverage by caching portions of the PoM-TLB in the on-chip last level cache. However, this proposal requires changes to the existing memory controller to physically support a PoM-TLB. Our UCAT work however is independent of a large memoryresident TLB. Instead, UCAT requires localized changes to the existing on-chip LLC that enable the LLC to serve simultaneously as a data cache and as a TLB. While on-chip caching of PoM-TLB and UCAT share the same spirits, the architecture and implementation of UCAT is entirely different from caching portions of the PoM-TLB. UCAT is a novel TLB and LLC structure for improving TLB reach while PoM relies on caching recent accesses in the LLC. We show that the combination of DRAM-TLBs and UCAT are complementary and additive. As such, we believe that UCAT can provide additional improvements with PoM-TLB when applied to CPU systems.</p><p>Large Pages and Direct Segments. Our studies show that the use of large pages (e.g., 2MB, 64MB, 1GB) and direct segments <ref type="bibr" target="#b3">(Basu et al. 2013</ref>) can significantly improve TLB coverage. However, recent work has shown that unrestricted use of large pages creates unintended performance overheads <ref type="bibr" target="#b53">(Talluri and Hill 1994;</ref><ref type="bibr" target="#b54">Talluri et al. 1992;</ref><ref type="bibr" target="#b18">Gaud et al. 2014;</ref><ref type="bibr" target="#b11">Chou et al. 2014;</ref><ref type="bibr" target="#b42">Pham et al. 2015)</ref> in emerging NUMA GPU systems <ref type="bibr">(Young et al. 2018b)</ref>. Alternatively, the use of direct segments can improve TLB coverage. However, direct segments requires both hardware and software support to redesign the existing address translation system. Our proposals provide a complementary approach to improve address translation using smaller pages (4KB, 64KB) without the overhead of large pages and the design complexity of direct segments.</p><p>Alternate DRAM Architectures. Recent proposals extend the processor cache hierarchy by architecting stacked memory as a DRAM cache <ref type="bibr">(Chou et al. 2015b;</ref><ref type="bibr" target="#b46">Qureshi and Loh 2012;</ref><ref type="bibr" target="#b27">Jevdjic et al. 2014;</ref><ref type="bibr" target="#b35">Loh and Hill 2011;</ref><ref type="bibr" target="#b50">Sim et al. 2012</ref><ref type="bibr" target="#b51">Sim et al. , 2013))</ref>. The tradeoffs for architecting DRAM-TLBs are different from those considered in DRAM cache designs. For example, DRAM-TLB entries are much smaller than DRAM cache entries and as such, DRAM-TLBs expose different tradeoffs for reading and updating multiple entries without incurring higher latency or bandwidth. Overall, DRAM caches are orthogonal to the proposals of this article.</p><p>Data Placement in Hybrid Memory. Application data placement in hybrid memory systems as well as NUMA systems has been well studied. Hybrid memory placement policies attempt to fully utilize total system bandwidth by distributing pages between system memory and stacked memory based on the bandwidth ratio <ref type="bibr" target="#b0">(Agarwal et al. 2015;</ref><ref type="bibr">Chou et al. 2015a</ref>). NUMA aware placement on the other hand focuses on data placement near computing resources to minimize overall latency <ref type="bibr" target="#b15">(Dashti et al. 2013;</ref><ref type="bibr" target="#b55">Verghese et al. 1996;</ref><ref type="bibr" target="#b8">Bolosky et al. 1989</ref>). Our work is orthogonal these proposals.</p><p>Address Translation Support for GPUs. Efficient and high performing address translation on GPUs is an important area of research. Recent studies <ref type="bibr" target="#b44">(Power et al. 2014;</ref><ref type="bibr" target="#b43">Pichai et al. 2014)</ref> show that simply extending CPU-style TLBs and page walkers to GPUs do not perform well. Consequently, novel mechanisms such as highly threaded page walkers <ref type="bibr" target="#b44">(Power et al. 2014</ref>) and intelligent page walk scheduling <ref type="bibr" target="#b43">(Pichai et al. 2014</ref>) have been proposed. Our work focuses on improving GPU TLB coverage by allowing the GPU LLC to hold TLB entries. Additionally, we improve GPU TLB miss latency by architecting TLBs in DRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternate Page Table Implementation.</head><p>To reduce memory accesses, Inverted Page Tables <ref type="bibr" target="#b22">(Jacob and Mudge 1998;</ref><ref type="bibr">Rashid et al. 1987</ref>) by indexing the page table using a hash of the virtual address.</p><p>However, the number of memory accesses depends on the frequency of hash collisions. Inverted page tables also do not support mapping of two different virtual addresses to the same physical address <ref type="bibr" target="#b22">(Jacob and Mudge 1998)</ref>. DRAM-TLBs provide benefits of inverted page tables (i.e., address translation in a single memory access) while retaining the benefits of hierarchical page tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">SUMMARY</head><p>Increasing application working-set sizes and growing on-die thread-level parallelism has made TLB performance an important performance bottleneck in today's systems. Even page walk caching mechanisms are unable to accommodate emerging application working-set sizes. As a result, LLT coverage can be very important to overall application performance. However, LLT misses suffer long latency due to multiple memory accesses to the page table. As such, when LLT misses occur, reducing address translation latency is necessary to maintain high performance.</p><p>This article proposes low-cost hardware mechanisms to accelerate virtual to physical address translation by improving on-die LLT coverage and LLT miss penalty. We improve on-die TLB coverage by proposing Unified Cache and TLB (UCAT), which enables the conventional unified LLC to also hold TLB entries. UCAT increases on-die LLT coverage by allowing as many TLB entries as there are cache lines in the conventional on-chip LLC. We show that UCAT improves performance by 60% (up to 4?) on average across a set of memory intensive GPU workloads. These improvements can be realized with negligible changes to the existing LLC architecture.</p><p>While UCAT improves performance significantly, it does not decrease the number of page table accesses on an LLT miss. To address this problem we propose embedding TLBs in DRAM (DRAM-TLB). DRAM-TLB serves as the next larger level in the TLB hierarchy architected using DRAM technology. DRAM-TLB can be arbitrarily sized to provide the desired TLB coverage. In steady state, DRAM-TLB can avoid multiple page table accesses and provides address translation with a single memory access. Consequently, DRAM-TLB is a low latency alternative to walking the page table on an LLT miss. Our studies show that DRAM-TLB architected using stacked memory technology improves performance by 22% on average (up to 2.25?). We show that DRAM-TLB requires less than 1% the capacity of our baseline 16GB stacked memory system.</p><p>Finally, we propose DUCATI, an address translation architecture that combines the benefits of DRAM-TLBs and UCAT while requiring negligible hardware overhead. DUCATI improves performance by 81% (up to 4.5?) for 4KB pages and by 56% and 8% when applied to 64KB and 2MB page systems, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Performance sensitivity to LLT entries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A four-level hierarchical page table.</figDesc><graphic url="image-1.png" coords="4,105.77,82.98,273.96,153.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Baseline system.</figDesc><graphic url="image-2.png" coords="5,95.95,93.19,294.24,136.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. UCAT architecture.</figDesc><graphic url="image-3.png" coords="8,54.15,114.85,377.32,178.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. UCAT performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Effective on-die TLB size with UCAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The impact of UCAT on LLC performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Improving TLB coverage by embedding TLBs in DRAM (DRAM-TLB). A DRAM-TLB architected using commodity DRAM is called SysMem-TLB and a DRAM-TLB architected with stacked DRAM is called Stacked-TLB.</figDesc><graphic url="image-4.png" coords="12,50.77,82.86,384.00,141.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Performance of DRAM-TLBs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Relative translation latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Memory accesses on an LLT miss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Fig. 14. TLB miss rate for the last level TLB in the TLB hierarchy.</figDesc><graphic url="image-5.png" coords="15,144.44,272.83,197.16,134.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. GMMU extensions to support DRAM-TLBs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Performance summary (4KB page size).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fig. 17. Translation latency (4KB page size).</figDesc><graphic url="image-6.png" coords="17,74.94,83.02,336.36,157.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Performance sensitivity to page size.</figDesc><graphic url="image-7.png" coords="17,74.94,274.51,336.12,126.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>table commonly used in computing systems today. Each node in the page table is 4KB in size and contains 512 eight-byte entries that point to the next node in the page table. The leaf node contains the physical address mapping. Generally, the page table is sparsely populated and new nodes are created only when data is referenced. As such, the typical in-memory storage space for the application page table is roughly 0.2% of the total memory footprint (8 bytes of storage overhead when using 4KB pages).</figDesc><table /><note><p>Figure 2 illustrates a page walk for a four-level page table. The Memory Management Unit (MMU) (or page walker) first consults a predetermined register (e.g., CR3 register on ?86 systems) to determine the physical address for Page Map Level 4 (PML4) or root node of the page table. The MMU indexes PML4 to fetch the Page Directory Pointer (PDP) or third-level page table (first</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Baseline System Configuration We assume 5 bytes of tag storage per LLC entry (including valid, dirty, coherence state, and replacement bits). For the UCAT architecture, we assume an extra 40 cycle load-to-use latency to consult the UCAT on an LLT miss. The baseline LLC (and UCAT) uses the DRRIP replacement policy<ref type="bibr" target="#b23">(Jaleel et al. 2010)</ref> while the L1 cache and TLBs use pseudo-LRU replacement</figDesc><table><row><cell cols="2">Streaming Multiprocessor (SM)</cell></row><row><cell>Frequency</cell><cell>1 GHz</cell></row><row><cell>Number of SMs</cell><cell>128</cell></row><row><cell cols="2">Baseline TLB and Cache Hierarchy</cell></row><row><cell>L1 TLB (private)</cell><cell>32-entry, 4-way</cell></row><row><cell>LLT (shared)</cell><cell>1024-entry, 8-way</cell></row><row><cell>L1 cache (private)</cell><cell>128KB, 4-way</cell></row><row><cell>LLC (shared)</cell><cell>8MB, 16-way</cell></row><row><cell>MSHRs for Mem Reqs</cell><cell>128 per memory channel</cell></row><row><cell cols="2">Stacked Memory (HBM)</cell></row><row><cell>Capacity</cell><cell>16GB (4 stacks)</cell></row><row><cell>Bus Frequency</cell><cell>1GHz</cell></row><row><cell>Channels / Banks</cell><cell>32 / 16 per rank</cell></row><row><cell>Bus Width</cell><cell>128-bits per channel</cell></row><row><cell>Row Buffer Size</cell><cell>2048 Bytes</cell></row><row><cell>tCAS-tRCD-tRP-tRAS</cell><cell>14-14-14-33</cell></row><row><cell>Total Bandwidth</cell><cell>1024GB/s</cell></row><row><cell cols="2">Main Memory (DDR4)</cell></row><row><cell>Capacity</cell><cell>256GB</cell></row><row><cell>Bus Frequency</cell><cell>1600MHz (DDR 3.2GHz)</cell></row><row><cell>Channels / Banks</cell><cell>8 / 16 per rank</cell></row><row><cell>Bus Width</cell><cell>64 bits per channel</cell></row><row><cell>Row Buffer Size</cell><cell>2048 Bytes</cell></row><row><cell>tCAS-tRCD-tRP-tRAS</cell><cell>14-14-14-35</cell></row><row><cell>Total Bandwidth</cell><cell>204.8GB/s</cell></row><row><cell cols="2">Virtual Memory Configuration</cell></row><row><cell>Page Allocation</cell><cell>Bandwidth-aware Placement</cell></row><row><cell>Page Table (PT)</cell><cell>4-level hierarchical</cell></row><row><cell>Page Walk Caches</cell><cell>16 entries per PT level</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>TLB Behavior 4KB Page Size on Simulated 128 SM GPU</figDesc><table><row><cell>LLT Sensitivity</cell><cell>Name</cell><cell cols="3">LLT-mpki PT Access/LLT Miss Total Footprint</cell></row><row><cell></cell><cell>XSBench</cell><cell>90.9</cell><cell>1.59</cell><cell>3.2GB</cell></row><row><cell></cell><cell>dmr</cell><cell>81.3</cell><cell>1.81</cell><cell>1.5GB</cell></row><row><cell>High</cell><cell>GUPS</cell><cell>92.0</cell><cell>2.00</cell><cell>15GB</cell></row><row><cell></cell><cell>MaxFlow</cell><cell>86.8</cell><cell>1.83</cell><cell>1.5GB</cell></row><row><cell></cell><cell>MCB</cell><cell>69.5</cell><cell>1.00</cell><cell>100MB</cell></row><row><cell></cell><cell>bfs</cell><cell>1.45</cell><cell>1.06</cell><cell>572MB</cell></row><row><cell></cell><cell>LULESH</cell><cell>4.64</cell><cell>1.49</cell><cell>3.7GB</cell></row><row><cell>Medium</cell><cell>SNAP UMT</cell><cell>4.81 3.71</cell><cell>1.15 1.51</cell><cell>1.6GB 1.4GB</cell></row><row><cell></cell><cell>CoMD</cell><cell>4.95</cell><cell>1.18</cell><cell>780MB</cell></row><row><cell></cell><cell>HPGMG</cell><cell>6.78</cell><cell>1.54</cell><cell>4.4GB</cell></row><row><cell></cell><cell>MiniAMR</cell><cell>5.58</cell><cell>1.62</cell><cell>3.3GB</cell></row><row><cell>Low</cell><cell>AMG</cell><cell>2.30</cell><cell>1.38</cell><cell>2.9GB</cell></row><row><cell></cell><cell>Nekbone</cell><cell>4.36</cell><cell>1.29</cell><cell>1.1GB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>DUCATI Sensitivity to LLC Size (4KB Pages)</figDesc><table><row><cell cols="6">LLC Size 2 MB 4 MB 8 MB 16 MB 32 MB</cell></row><row><cell>Speedup</cell><cell>1.61</cell><cell>1.73</cell><cell>1.81</cell><cell>1.89</cell><cell>1.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance when GPU Walks Page Table</figDesc><table><row><cell cols="4">Page Size UCAT DRAM-TLB DUCATI</cell></row><row><cell>4KB</cell><cell>1.20</cell><cell>1.16</cell><cell>1.32</cell></row><row><cell>64KB</cell><cell>1.26</cell><cell>1.20</cell><cell>1.33</cell></row><row><cell>2MB</cell><cell>1.01</cell><cell>1.00</cell><cell>1.01</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACM Transactions on Architecture and Code Optimization, Vol. 16, No. 1, Article 6. Publication date: March 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Section 3 discusses workloads and simulation methodology.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Additional meta data (e.g., coherence state, replacement state, etc.) is also stored in the tag array.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>For the workloads in this study, on average 80% of the lines installed in the LLC are never re-referenced.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>In our baseline we assume a physically indexed, physically tagged cache. However, any other variant of virtual or physical indexing/tagging is equally possible with UCAT.ACM Transactions on Architecture and Code Optimization, Vol. 16, No. 1, Article 6. Publication date: March 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>We model a Perfect LLT by assuming all references to the LLT in the baseline system are hits. ACM Transactions on Architecture and Code Optimization, Vol. 16, No. 1, Article</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>Publication date: March 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>Received February 2018; revised January 2019; accepted January 2019 ACM Transactions on Architecture and Code Optimization, Vol. 16, No. 1, Article 6. Publication date: March 2019.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Page placement strategies for GPUs within heterogeneous memory systems</title>
		<author>
			<persName><forename type="first">Neha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike O'</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;15)</title>
		<meeting>the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Ats</surname></persName>
		</author>
		<ptr target="http://composter.com.ua/documents/ats_r1.1_26" />
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture (ISCA&apos;10)</title>
		<editor>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alan</forename><surname>Cox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</editor>
		<meeting>the 37th Annual International Symposium on Computer Architecture (ISCA&apos;10)</meeting>
		<imprint>
			<publisher>PCI Express, Address Translation Service</publisher>
			<date type="published" when="2009">2009. Jan09. 2010</date>
		</imprint>
	</monogr>
	<note>Translation caching: Skip, don&apos;t walk (the page table</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SpecTLB: A mechanism for speculative address translation</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="307" to="318" />
			<date type="published" when="2011">2011</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient virtual memory for big memory servers</title>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerating two-dimensional page walks for virtualized systems</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Serebrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Spadini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srilatha</forename><surname>Manne</surname></persName>
		</author>
		<idno type="DOI">10.1145/1346281.1346286</idno>
		<ptr target="https://doi.org/10.1145/1346281.1346286" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XIII)</title>
		<meeting>the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XIII)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-reach memory management unit caches</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<idno type="DOI">10.1145/2540708.2540741</idno>
		<ptr target="https://doi.org/10.1145/2540708.2540741" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-46)</title>
		<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-46)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="383" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shared last-level TLBs for chip multiprocessors</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE 17th International Symposium on High Performance Computer Architecture (HPCA&apos;11)</title>
		<meeting>the 2011 IEEE 17th International Symposium on High Performance Computer Architecture (HPCA&apos;11)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="62" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inter-core cooperative TLB for chip multiprocessors</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<idno type="DOI">10.1145/1736020.1736060</idno>
		<ptr target="https://doi.org/10.1145/1736020.1736060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XV)</title>
		<meeting>the 15th Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XV)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple but effective techniques for NUMA memory management</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bolosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<idno type="DOI">10.1145/74850.74854</idno>
		<ptr target="https://doi.org/10.1145/74850.74854" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Symposium on Operating Systems Principles (SOSP&apos;89)</title>
		<meeting>the 12th ACM Symposium on Operating Systems Principles (SOSP&apos;89)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="19" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simulation based study of TLB performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bradley Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anita</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno type="DOI">10.1145/139669.139708</idno>
		<ptr target="https://doi.org/10.1145/139669.139708" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA&apos;92)</title>
		<meeting>the 19th Annual International Symposium on Computer Architecture (ISCA&apos;92)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BEAR: Techniques for mitigating bandwidth bloat in gigascale DRAM caches</title>
		<author>
			<persName><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CAMEO: A two-level memory organization with capacity of main memory and flexibility of hardware-managed cache</title>
		<author>
			<persName><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">BATMAN: Maximizing bandwidth utilization for hybrid memory systems</title>
		<author>
			<persName><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M K</forename><surname>Qureshi</surname></persName>
		</author>
		<idno>TR-CARET-2015-01</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Computer ARchitecture and Emerging Technologies (CARET) Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report for</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Coral</surname></persName>
		</author>
		<ptr target="https://asc.llnl.gov/CORAL-benchmarks/" />
		<title level="m">CORAL Procurement Benchmarks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Five-level page tables</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/717293/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Traffic management: A holistic approach to memory placement on NUMA systems</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Dashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Quema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1145/2451116.2451157</idno>
		<ptr target="https://doi.org/10.1145/2451116.2451157" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;13)</title>
		<meeting>the 18th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;13)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="381" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The intel? many integrated core architecture</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Klemm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 International Conference on High Performance Computing and Simulation (HPCS&apos;12)</title>
		<meeting>the 2012 International Conference on High Performance Computing and Simulation (HPCS&apos;12)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="365" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The oracle sparc T5 16-core processor scales to eight sockets</title>
		<author>
			<persName><forename type="first">John</forename><surname>Feehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumti</forename><surname>Jairath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Loewenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Sivaramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Smentek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Turullols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Vahidsafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="48" to="57" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large pages may be harmful on NUMA systems</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Decouchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Qu?ma</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2643634.2643659NVIDIAGP100.2016.P100GPUAccelerator" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC&apos;14). USENIX Association</title>
		<meeting>the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC&apos;14). USENIX Association<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving performance via mini-applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Heroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Doerfler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Crozier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Carter</forename><surname>Willenbring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahesh</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">R</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidi</forename><forename type="middle">K</forename><surname>Keiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Thornquist</surname></persName>
		</author>
		<author>
			<persName><surname>Numrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep. SAND</title>
		<imprint>
			<biblScope unit="volume">5574</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009">2009. 2009. 2009</date>
		</imprint>
		<respStmt>
			<orgName>Sandia National Laboratories</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">HSA Platform System Architecture Specification. HSA Foundation</title>
		<author>
			<orgName type="collaboration">HMC Specification</orgName>
		</author>
		<ptr target="http://www.slideshare.net/hsafoundation/hsa-platform-system-architecture-specification-provisional-verl-10-ratifed" />
		<imprint>
			<date type="published" when="2013">2013. 2014</date>
		</imprint>
	</monogr>
	<note>HSA Foundation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 Architectures Optimization Reference Manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtual memory: Issues of implementation</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="1998-06">1998. Jun. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High performance cache replacement using rereference interval prediction (RRIP)</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture</title>
		<meeting>the 37th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Doi</surname></persName>
		</author>
		<idno type="DOI">10.1145/1815961.1815971</idno>
		<ptr target="https://doi.org/10.1145/1815961.1815971" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Jedec</surname></persName>
		</author>
		<title level="m">DDR4 SPEC (JESD79-4). JEDEC. JEDEC. 2013b. High Bandwidth Memory (HBM) DRAM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>JESD235</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Intel Xeon Phi Processor High Performance Programming: Knights Landing Edition</title>
		<author>
			<persName><forename type="first">James</forename><surname>Jeffers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Sodani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unison cache: A scalable and effective die-stacked DRAM cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.51</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2014.51" />
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A parallel ford-fulkerson algorithm for maximum flow problem</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suixiang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications</title>
		<meeting>the International Conference on Parallel and Distributed Processing Techniques and Applications</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>PDPTA&apos;13</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Insertion and promotion for tree-based pseudolru last-level caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Jim?nez</surname></persName>
		</author>
		<idno type="DOI">10.1145/2540708.2540733</idno>
		<ptr target="https://doi.org/10.1145/2540708.2540733" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual International Symposium on Microarchitecture. 13</title>
		<meeting>the 46th Annual International Symposium on Microarchitecture. 13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The compute architecture of intel processor graphics gen9</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Junkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Whitepaper v</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going the distance for TLB prefetching: An application-driven study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Kandiraju</surname></persName>
		</author>
		<author>
			<persName><surname>Sivasubramaniam</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=545215.545237" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Symposium on Computer Architecture (ISCA&apos;02)</title>
		<meeting>the 29th Annual International Symposium on Computer Architecture (ISCA&apos;02)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Minimalist open-page: A DRAM page-mode scheduling policy for the many-core era</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kaseridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stuecheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename></persName>
		</author>
		<idno type="DOI">10.1145/2155620.2155624</idno>
		<ptr target="https://doi.org/10.1145/2155620.2155624" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using dead blocks as a virtual victim cache</title>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim??nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burgerand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TAP: A TLP-aware cache management policy for a CPU-GPU heterogeneous architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calin</forename><surname>Burtscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Cascaval</surname></persName>
		</author>
		<author>
			<persName><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE 18th International Symposium on High Performance Computer Architecture (HPCA&apos;12)</title>
		<meeting>the 2012 IEEE 18th International Symposium on High Performance Computer Architecture (HPCA&apos;12)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2012</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Lonestar: A Suite of Parallel Irregular Programs? Jaekyu Lee and Hyesoon Kim</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficiently enabling conventional block sizes for very large die-stacked DRAM caches</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<idno type="DOI">10.1145/2155620.2155673</idno>
		<ptr target="https://doi.org/10.1145/2155620.2155673" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Microarchitecture. 11</title>
		<meeting>the 44th Annual International Symposium on Microarchitecture. 11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The HPC challenge (HPCC) benchmark suite</title>
		<author>
			<persName><forename type="first">Piotr</forename><forename type="middle">R</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kepner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">F</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Rabenseifner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM/IEEE Conference on Supercomputing</title>
		<meeting>the 2006 ACM/IEEE Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">213</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">AMD&apos;s next generation GPU and high bandwidth memory architecture: FURY</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Macri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Hot Chips 27 Symposium (HCS&apos;15)</title>
		<meeting>the 2015 IEEE Hot Chips 27 Symposium (HCS&apos;15)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Moammer</surname></persName>
		</author>
		<title level="m">AMD Zen Raven Ridge APU Features HBM, 128GB/s of Bandwidth and Large GPU</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unified memory in cuda 6.0. a brief overview of related data access and transfer issues</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Negrut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Seidl</surname></persName>
		</author>
		<idno>TR-2014-09</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Increasing TLB reach by exploiting clustering in page translations</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arup</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuko</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE 20th International Symposium on High Performance Computer Architecture</title>
		<meeting>the 2014 IEEE 20th International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aamer Jaleel, and Abhishek Bhattacharjee</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viswanathan</forename><surname>Vaidyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="258" to="269" />
		</imprint>
	</monogr>
	<note>-45)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large pages and lightweight memory management in virtualized systems: Can you have it both ways?</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture (MICRO)</title>
		<meeting>the International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Architectural support for address translation on GPUs: Designing memory management units for CPU/GPUs with unified address spaces</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Pichai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;14)</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;14)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Supporting x86-64 address translation for 100s of GPU lanes</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA&apos;14</title>
		<meeting>the 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA&apos;14</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1145/1250662.1250709</idno>
		<ptr target="https://doi.org/10.1145/1250662.1250709" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International Symposium on Computer Architecture. 11</title>
		<meeting>the 34th Annual International Symposium on Computer Architecture. 11</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fundamental latency trade-off in architecting DRAM caches: Outperforming impractical SRAM-Tags with a simple and practical design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabe</forename><forename type="middle">H</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><surname>Loh</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2012.30</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2012.30" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 45th Annual International Symposium on Microarchitecture. 12</title>
		<meeting>the 2012 45th Annual International Symposium on Microarchitecture. 12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avadis</forename><surname>Tevanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bolosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="896" to="908" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking TLB designs in virtualized environments: A very large part-of-memory TLB</title>
		<author>
			<persName><forename type="first">Jee</forename><surname>Ho Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recency-based TLB preloading</title>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Saulsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International Symposium on Computer Architecture (ISCA&apos;00)</title>
		<meeting>the 27th Annual International Symposium on Computer Architecture (ISCA&apos;00)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A mostly-clean DRAM cache for effective hit speculation and self-balancing dispatch</title>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike O'</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mithuna</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Thottethodi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2012.31</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2012.31" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 45th Annual International Symposium on Microarchitecture. 11</title>
		<meeting>the 2012 45th Annual International Symposium on Microarchitecture. 11</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Resilient die-stacked DRAM caches</title>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike O'</forename><surname>Vilas Sridharan</surname></persName>
		</author>
		<author>
			<persName><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA&apos;13)</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA&apos;13)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="416" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Knights landing: Second-generation intel xeon phi product</title>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Sodani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Gramunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename><surname>Corbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ho-Seop</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sundaram</forename><surname>Chinthamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hutsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Surpassing the TLB performance of superpages with less operating system support</title>
		<author>
			<persName><forename type="first">Madhusudhan</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<idno type="DOI">10.1145/195473.195531</idno>
		<ptr target="https://doi.org/10.1145/195473.195531" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI)</title>
		<meeting>the 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tradeoffs in supporting two page sizes</title>
		<author>
			<persName><forename type="first">Madhusudhan</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1145/139669.140406</idno>
		<ptr target="https://doi.org/10.1145/139669.140406" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA&apos;92)</title>
		<meeting>the 19th Annual International Symposium on Computer Architecture (ISCA&apos;92)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Operating system support for improving data locality on CC-NUMA compute servers</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Verghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Devine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII)</title>
		<meeting>the 7th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="279" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Observations and opportunities in architecting shared virtual memory for heterogeneous systems</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;16)</title>
		<meeting>the 2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;16)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="161" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An in-cache address translation mechanism</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garth</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><forename type="middle">M</forename><surname>Pendleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="358" to="365" />
			<date type="published" when="1986">1986</date>
			<publisher>IEEE Computer Society Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SHiP: Signature-based hit predictor for high performance caching</title>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 45th Annual International Symposium on Microarchitecture</title>
		<meeting>the 2012 45th Annual International Symposium on Microarchitecture<address><addrLine>Micro-44</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">ACCORD: Enabling associativity for gigascale DRAM caches by coordinating way-install and way-prediction</title>
		<author>
			<persName><forename type="first">Vinson</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddin</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA&apos;18)</title>
		<meeting>the 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA&apos;18)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Combining HW/SW mechanisms to improve NUMA performance of multi-GPU systems</title>
		<author>
			<persName><forename type="first">Vinson</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Bolotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE 51st International Symposium on Microarchitecture</title>
		<meeting>the 2018 IEEE 51st International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Eiman Ebrahimi, David Nellans, and Oreste Villa</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<title level="m">Towards high performance paged memory for GPUs</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
