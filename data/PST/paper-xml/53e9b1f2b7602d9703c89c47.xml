<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Tech-nion-Israel Institute of Technology</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">531881D0451149DC29B618FEFACD5513</idno>
					<idno type="DOI">10.1109/TSP.2011.2159211</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensing Matrix Optimization for Block-Sparse Decoding</head><p>Lihi Zelnik-Manor, Member, IEEE, Kevin Rosenblum, and Yonina C. Eldar, Senior Member, IEEE</p><p>Abstract-Recent work has demonstrated that using a carefully designed sensing matrix rather than a random one, can improve the performance of compressed sensing. In particular, a well-designed sensing matrix can reduce the coherence between the atoms of the equivalent dictionary, and as a consequence, reduce the reconstruction error. In some applications, the signals of interest can be well approximated by a union of a small number of subspaces (e.g., face recognition and motion segmentation). This implies the existence of a dictionary which leads to block-sparse representations. In this work, we propose a framework for sensing matrix design that improves the ability of block-sparse approximation techniques to reconstruct and classify signals. This method is based on minimizing a weighted sum of the interblock coherence and the subblock coherence of the equivalent dictionary. Our experiments show that the proposed algorithm significantly improves signal recovery and classification ability of the Block-OMP algorithm compared to sensing matrix optimization methods that do not employ block structure.</p><p>Index Terms-Block-sparsity, compressed sensing, sensing matrix design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE framework of compressed sensing aims at recovering an unknown vector from an underdetermined system of linear equations , where is a sensing matrix, and is an observation vector with . Since the system is under-determined, can not be recovered without additional information. In <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref>, it was shown that when is known to have a sufficiently sparse representation, and when is randomly generated, can be recovered uniquely with high probability from the measurements . More specifically, the assumption is that can be represented as for some orthogonal dictionary , where is sufficiently sparse. The vector can then be recovered regardless of and irrespective of the locations of the nonzero entries of . This can be achieved by approximating the sparsest representation using methods such as Basis Pursuit (BP) <ref type="bibr" target="#b2">[3]</ref> and Orthogonal Matching Pursuit (OMP) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In practice, overcomplete dictionaries with lead to improved sparse representations and are better suited for most applications. Therefore, we treat the more general case of overcomplete dictionaries in this paper.</p><p>A simple way to characterize the recovery ability of sparse approximation algorithms was presented in <ref type="bibr" target="#b3">[4]</ref>, using the coherence between the columns of the equivalent dictionary . When the coherence is sufficiently low, OMP and BP are guaranteed to recover the sparse vector . Accordingly, recent work <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> has demonstrated that designing a sensing matrix such that the coherence of is low improves the ability to recover . The proposed methods yield good results for general sparse vectors.</p><p>In some applications, however, the representations have a unique sparsity structure that can be exploited. Our interest is in the case of signals that are drawn from a union of a small number of subspaces <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. This occurs naturally, for example, in face recognition <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, motion segmentation <ref type="bibr" target="#b13">[14]</ref>, multiband signals <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, measurements of gene expression levels <ref type="bibr" target="#b17">[18]</ref>, and more. For such signals, sorting the dictionary atoms according to the underlying subspaces leads to sparse representations which exhibit a block-sparse structure, i.e., the nonzero coefficients in occur in clusters of varying sizes. Several methods, such as Block-BP (BBP) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, also referred to as Group lasso <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>, and Block-OMP (BOMP) <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> have been proposed to take advantage of this block structure in recovering the block-sparse representations . Bounds on the recovery performance were presented in <ref type="bibr" target="#b10">[11]</ref> based on the block restricted isometry property (RIP), and in <ref type="bibr" target="#b23">[24]</ref> using appropriate coherence measures. In particular, it was shown in <ref type="bibr" target="#b23">[24]</ref> that under conditions on the interblock coherence (i.e., the maximal coherence between two blocks) and the subblock coherence (i.e., the maximal coherence between two atoms in the same block) of the equivalent dictionary , Block-OMP is guaranteed to recover the block-sparse vector .</p><p>In this paper we propose a method for designing a sensing matrix, assuming that a block-sparsifying dictionary is provided. Our approach improves the recovery ability of block-sparse approximation algorithms by targeting the Gram matrix of the equivalent dictionary, an approach similar in spirit to that of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. While <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref> targeted minimization of the coherence between atoms, our method, which will be referred to as Weighted Coherence Minimization (WCM), aims at reducing a weighted sum of the interblock coherence and the subblock coherence.</p><p>It turns out that the weighted coherence objective is hard to minimize directly. To derive an efficient algorithm, we use the bound-optimization method, and replace our objective with an easier to minimize surrogate function that is updated in each optimization step <ref type="bibr" target="#b25">[26]</ref>. We develop a closed form solution for minimizing the surrogate function in each step, and prove that its iterative minimization is guaranteed to converge to a local solution of the original problem.</p><p>Our experiments reveal that minimizing the subblock coherence is more important than minimizing the interblock coherence. By giving more weight to minimizing the subblock coherence, the proposed algorithm yields sensing matrices that lead to equivalent dictionaries with nearly orthonormal blocks. Simulations show that such sensing matrices significantly improve signal reconstruction and signal classification results compared to previous approaches that do not employ block structure.</p><p>We begin by reviewing previous work on sensing matrix design in Section II. In Section III we introduce our definitions of total interblock coherence and total subblock coherence. We then present the objective for sensing matrix design, and show that it can be considered as a direct extension of the one used in <ref type="bibr" target="#b7">[8]</ref> to the case of blocks. We present the WCM algorithm for minimizing the proposed objective in Section IV and prove its convergence in Appendix A. We evaluate the performance of the proposed algorithm and compare it to previous work in Section V.</p><p>Throughout the paper, we denote vectors by lowercase letters, e.g., , and matrices by uppercase letters, e.g., .</p><p>is the transpose of . The column of the matrix is , and the row is . The entry of in the row with index and the column with index is . We define the Frobenius norm by , and the -norm of a vector by . The -norm counts the number of non-zero entries in . We denote the identity matrix by or when the dimension is not clear from the context. The spectral norm of is the square root of the largest eigenvalue of the positive-semidefinite matrix .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRIOR WORK ON SENSING MATRIX DESIGN</head><p>The goal of sensing matrix design is to construct a sensing matrix with that improves the recovery ability for a given sparsifying dictionary with . In other words, is designed to improve the ability of sparse approximation algorithms such as BP and OMP to recover the sparsest representation from <ref type="bibr" target="#b0">(1)</ref> where is the equivalent dictionary. In this section we briefly review the sensing matrix design method introduced by Duarte-Carvajalino and Sapiro <ref type="bibr" target="#b7">[8]</ref>. Their algorithm was shown to provide significant improvement in reconstruction success.</p><p>The motivation to design sensing matrices stems from the theoretical work of <ref type="bibr" target="#b3">[4]</ref>, where it was shown that BP and OMP succeed in recovering when the following condition holds:</p><p>(2)</p><p>Here is the coherence defined by <ref type="bibr" target="#b2">(3)</ref> The smaller , the higher the bound on the sparsity of . Since is overcomplete, and as a consequence not orthogonal, will always be strictly positive. Condition ( <ref type="formula">2</ref>) is a worst-case bound and does not reflect the average recovery ability of sparse approximation methods. However, it does suggest that recovery may be improved when is as orthogonal as possible.</p><p>Motivated by these observations, Duarte-Carvajalino and Sapiro <ref type="bibr" target="#b7">[8]</ref> proposed designing a sensing matrix by minimizing . This problem can be written as</p><formula xml:id="formula_0">(4)</formula><p>It is important to note that rather than minimizing , (4) minimizes the sum of the squared inner products of all pairs of atoms in , referred to as the total coherence</p><p>(5)</p><p>At the same time, solving (4) keeps the norms of the atoms close to 1.</p><p>While an approximate solution to (4) has already been presented in <ref type="bibr" target="#b7">[8]</ref>, we provide an exact solution that will be of use in the next sections. To solve (4), we rewrite its objective using the well-known relation between the Frobenius norm and the trace, <ref type="bibr" target="#b5">(6)</ref> Since the first term in ( <ref type="formula">6</ref>) is always positive, the objective of( <ref type="formula">4</ref>) is lower bounded by . From (6) it follows that minimizing (4) is equivalent to the minimization of . A solution to this problem can be achieved in closed form as follows. Let be the eigenvalue decomposition of , and let . Then, ( <ref type="formula">4</ref>) is equivalent to <ref type="bibr" target="#b6">(7)</ref> This problem is solved by choosing to be any matrix with orthonormal rows, such as , leading to . The optimal sensing matrix is then given by . Here, and throughout the paper, we assume that has full row rank, guaranteeing that is invertible. Note that the global minimum of the objective in (4) equals . The benefits of using such a sensing matrix were shown empirically in <ref type="bibr" target="#b7">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SENSING MATRIX DESIGN FOR BLOCK-SPARSE DECODING</head><p>The design of a sensing matrix according to <ref type="bibr" target="#b7">[8]</ref> does not take advantage of block structure in the sparse representations of the data. In this section we formulate the problem of sensing matrix design for block-sparse decoding. We first introduce the basic concepts of block-sparsity, and then present an objective which can be seen as an extension of (4) to the case of block-sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Block-Sparse Decoding</head><p>The framework of block-sparse decoding aims at recovering an unknown vector from an under-determined system of linear equations , where is a sensing matrix, and is an observation vector with . The difference with sparse recovery lies in the assumption that has a sufficiently block-sparse representation with respect to some block-sparsifying dictionary . The vector can then be recovered by approximating the blocksparsest representation corresponding to the measurements using methods such as Block-BP (BBP) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and Block-OMP (BOMP) <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>A block-sparsifying dictionary is a dictionary whose atoms are sorted in blocks which enable block-sparse representations for a set of signals. We can represent as a concatenation of column-blocks of size , where is the number of atoms belonging to the th block Similarly, we view the representation as a concatenation of blocks of length</p><p>We say that a representation is -block-sparse if its nonzero values are concentrated in blocks only. This is denoted by , where</p><p>The indicator function counts the number of blocks in with nonzero Euclidean norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Problem Definition</head><p>For a given block-sparsifying dictionary with , we wish to design a sensing matrix that improves the recovery ability of block-sparse approximation algorithms. Note that we allow to be overcomplete. A performance bound on the recovery success of block-sparse signals has been developed in <ref type="bibr" target="#b23">[24]</ref> for the case of a dictionary with blocks of a fixed size (i.e., ) and an equivalent dictionary with normalized columns. The bound is a function of the Gram matrix of the equivalent dictionary, defined as . The block of , , is denoted by</p><p>. The block of any other matrix will be denoted similarly. It was shown in <ref type="bibr" target="#b23">[24]</ref> that BBP and BOMP succeed in recovering the block sparsest representation corresponding to the measurements when the following condition holds: <ref type="bibr" target="#b7">(8)</ref> Here is the interblock coherence and is the subblock coherence. The interblock coherence is a generalization of the coherence , and describes the global properties of the equivalent dictionary. More specifically, measures the cosine of the minimal angle between two blocks in . The subblock coherence describes the local properties of the dictionary, by measuring the cosine of the minimal angle between two atoms in the same block in . Note, that when , (8) reduces to the bound in the sparse case <ref type="bibr" target="#b1">(2)</ref>. The term in <ref type="bibr" target="#b7">(8)</ref> suggests that needs to be reduced in order to loosen the bound. On the other hand, the term implies that the ratio should be small. This leads to a tradeoff between minimizing and minimizing to loosen the bound, which is reflected in the sensing matrix design objective presented later in this section.</p><p>Condition ( <ref type="formula">8</ref>) is a worst case bound and does not represent the average recovery ability of block-sparse approximation methods. It does suggest, however, that in order to improve the average recovery, all pairs of blocks in should be as orthogonal as possible and also all pairs of atoms within each block should be as orthogonal as possible. Inspired by <ref type="bibr" target="#b7">[8]</ref>, rather than minimizing the interblock coherence and the subblock coherence , we aim at minimizing the total interblock coherence and the total subblock coherence of the equivalent dictionary . We define the total interblock coherence as <ref type="bibr" target="#b8">(9)</ref> and the total subblock coherence by <ref type="bibr" target="#b9">(10)</ref> where are the diagonal entries of . The total interblock coherence equals the sum of the squared entries in belonging to different blocks (the green entries in Fig. <ref type="figure" target="#fig_0">1</ref>). Since this is the sum of Frobenius norms, also equals the sum of the squared singular values of the cross-correlation blocks in . When is normalized, is equivalent to the sum of the squared cosines of all the principal angles between all pairs of different blocks. The total subblock coherence measures the sum of the squared off-diagonal entries belonging to the same block (the red entries in Fig. <ref type="figure" target="#fig_0">1</ref>). When is normalized, equals the sum of the squared cosines of all the angles between atoms within the same block. Note that when the size of the blocks equals one, we get . Alternatively, one could define the total interblock coherence as the sum of the squared spectral norms (i.e., the largest singular values) of the cross-correlation blocks in , and the total subblock coherence as the sum of the squared maximal off-diagonal entries of the autocorrelation blocks in . These definitions are closer to the ones used in <ref type="bibr" target="#b7">(8)</ref>. The WCM algorithm presented in the next section can be slightly modified in order to minimize those measures as well. However, besides the increased complexity of the algorithm, the results appear to be inferior compared to minimizing the definitions ( <ref type="formula">9</ref>) and( <ref type="formula">10</ref>) of and . This can be explained by the fact that maximizing only the smallest principal angle between pairs of different blocks in and maximizing the smallest angle between atoms within the same block, creates a bulk of relatively high singular values and coherence values. While this may improve the worst-case bound in (8), it does not necessarily improve the average recovery ability of block-sparse approximation methods.</p><p>When minimizing the total interblock coherence and the total subblock coherence, we need to verify that the columns of are normalized, to avoid the tendency of columns with small norm values to be underused. Rather than enforcing normalization strongly, we penalize for columns with norms that deviate from 1 by defining the normalization penalty <ref type="bibr" target="#b10">(11)</ref> This penalty measures the sum of the squared distances between the diagonal entries in (the yellow entries in Fig. <ref type="figure" target="#fig_0">1</ref>) and 1.</p><p>While <ref type="bibr" target="#b7">[8]</ref> did not deal with the block-sparse case, it is straightforward to see that solving (4) is equivalent to minimizing the sum of the normalization penalty, the total interblock coherence and the total subblock coherence We have shown in the previous section that the objective in ( <ref type="formula">4</ref>) is bounded below by . Therefore <ref type="bibr" target="#b11">(12)</ref> This bound implies a tradeoff, and as a consequence, one cannot minimize , , and freely. Instead, we propose designing a sensing matrix that minimizes the normalization penalty and a weighted sum of the total interblock coherence and the total subblock coherence <ref type="bibr" target="#b12">(13)</ref> where is a parameter controlling the weight given to the total interblock coherence and the total subblock coherence. Note that alternative objectives can be formulated. For example, one could add an additional weighting parameter to the normalization penalty term. While this would allow us to better control the normalization of the atoms in , we prefer to deal with a single parameter only.</p><p>When , more weight is given to minimizing , and therefore solving (13) leads to lower total interblock coherence, which is made possible by aligning the atoms within each block [Fig. <ref type="figure" target="#fig_1">2(a)</ref>]. On the other hand, choosing gives more weight to minimizing . In this case, solving (13) leads to more orthonormal blocks in at the expense of higher [Fig. <ref type="figure" target="#fig_1">2(c)</ref>]. Finally, setting in <ref type="bibr" target="#b12">(13)</ref> gives equal weights to , , and , and reduces it to (4) [Fig. <ref type="figure" target="#fig_1">2(b)</ref>]. Therefore, the objective becomes independent of the block structure, which makes the correct choice when the signals do not have an underlying block structure. Choosing to ignore the block structure leads to the same conclusion. When an underlying block structure exists, we need to select a value for . We do that via empirical evaluation in Section V.</p><p>In the next section we present an iterative algorithm that converges to a local solution of ( <ref type="formula">13</ref>). Although we can only guarantee a local minimum, in our simulations we have noticed that independent executions of the algorithm tend to reach the same solution. Our empirical observations are demonstrated in the histograms in Fig. <ref type="figure" target="#fig_2">3</ref> (square dictionary) and Fig. <ref type="figure" target="#fig_3">4</ref> (a highly overcomplete dictionary), for both and .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. WCM-WEIGHTED COHERENCE MINIMIZATION</head><p>In this section, we present the WCM algorithm for minimizing <ref type="bibr" target="#b12">(13)</ref>, based on the bound-optimization method <ref type="bibr" target="#b25">[26]</ref>. This algorithm substitutes the original objective with an easier to minimize surrogate objective that is updated in each optimization step. After defining a surrogate function and showing it can be minimized in closed form, we prove that its iterative minimization is guaranteed to converge to a local solution of the original problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The WCM Algorithm</head><p>To obtain a surrogate function we rewrite the objective of ( <ref type="formula">13</ref>), which we denote by , as a function of the Gram matrix of the equivalent dictionary where the matrix operators , , and are defined as ; else, ; else, ; else with denoting the entry of . This equation follows directly from the definitions of , , and . We can now write <ref type="bibr" target="#b13">(14)</ref> where the matrix operators , , and are defined as ; else, ; else, ; else.</p><p>Based on ( <ref type="formula">14</ref>), we define a surrogate objective at the iteration as <ref type="bibr" target="#b14">(15)</ref> where is the Gram matrix of the equivalent dictionary from the previous iteration. In Appendix B, we prove that satisfies the conditions of a surrogate objective for the bound-optimization method. Therefore, iteratively minimizing is guaranteed to converge to a local minimum of the original objective , i.e., solve <ref type="bibr" target="#b12">(13)</ref>. The following proposition describes the closed form solution to minimizing at each iteration. Proof: See Appendix B A summary of the proposed WCM algorithm is given below. The computation complexity of this algorithm is . This is since every iteration includes finding the principal components of an matrix (the sensing matrix is in ), i.e., SVD of complexity . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we evaluate the contribution of the proposed sensing matrix design framework empirically. We compare the recovery and classification abilities of BOMP <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> when using sensing matrices designed by the proposed WCM to the outcome of ( <ref type="formula">4</ref>), which will be referred to as "Duarte-Sapiro" (DS) <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on Synthetic Data</head><p>For each simulation, we repeat the following procedure 100 times. We randomly generate a dictionary with normally distributed entries and normalize its columns. In order to evaluate WCM on structured dictionaries as well, we repeat the simulations using a dictionary containing randomly selected rows of the Discrete Cosine Transform (DCT) matrix. The dictionary is divided into blocks of size . We then generate test signals of dimension that have block-sparse representations with respect to . The generating blocks are chosen randomly and independently and the coefficients are i.i.d. uniformly distributed. We compare three options for designing : (i) random; (ii) the outcome of DS; and (iii) the proposed WCM which is initialized as the outcome of DS. Having found we calculate the equivalent dictionary and the measurements . Next, we obtain the block-sparsest representations of the measurements, , by applying BOMP with a fixed number of nonzero blocks.</p><p>We use two measures to evaluate the success of the simulations based on their outputs and .</p><p>• The percentage of recognized generating subspaces of (i.e., successful classification): where denotes element-wise multiplication. , , and measurements. To show that the results remain consistent for higher values of , we add an experiment with , , , and [Fig. <ref type="figure" target="#fig_9">8(a)</ref> and<ref type="figure">(b)</ref>]. We compare the obtained results to randomly set sensing matrices and to the outputs of DS <ref type="bibr" target="#b7">[8]</ref>, based on the normalized representation error , the classification success , and the ratio between the total subblock coherence and the total interblock coherence . The that WCM and DS coincide at for all the three measures, as expected. Note that for we get that is high, is high, and is low. On the other hand, when , i.e., when giving more weight to and less to , the signal reconstruction as well as the signal classification are improved compared to DS. This is in line with our earlier observation based on the bound in ( <ref type="formula">8</ref>) that the ratio should be small. Indeed our experiments show that when we get that is low. The simulation results show a dramatic change in performance around the value . This is since at the importance of becomes higher than that of , which changes the behavior of the algorithm.</p><p>While the improvement for is more significant, it is maintained for higher values of as well. Remarkably, for structured dictionaries and for higher values of , we see that leads to an improvement of . However, is compromised in this case. We can conclude that when designing sensing matrices for block sparse decoding, the best results are obtained by choosing close enough to 1. In other words, the best recovery results are obtained when the equivalent dictionary has nearly orthonormal blocks. This holds for dictionaries containing normally distributed entries as well as for dictionaries containing randomly selected rows of the DCT matrix. When the dictionary is overcomplete the blocks cannot be truly orthonormal. Based on these observations we conclude that is the best value. In our experiments on real data (next section) setting led to the same results as values close to 1. However, we did not want to completely ignore the interblock coherence thus we typically set . As mentioned before, WCM converges but could get trapped in local minima. As was shown in Fig. <ref type="figure" target="#fig_2">3</ref>(b), we observed empirically that for , every local minimum reached had the same objective value. We observed this empirical behavior also in the exhaustive experiments presented in this section. This means that the WCM algorithm converges consistently to the same solution of ( <ref type="formula">13</ref>) when , for all the experiments presented in this section. We emphasize however, that this may not be the case for other sets of parameters. Dictionary dimension: Fig. <ref type="figure" target="#fig_10">9</ref>(a) and (b) shows that when using WCM with on dictionaries with normally distributed entries and on structured dictionaries, the improvement in signal recovery is maintained for a wide range of , starting from square dictionaries, i.e., , to highly overcomplete dictionaries. For this experiment, we chose , , and . We note that for both types of dictionaries, the improvement of WCM over DS increases as the dictionary becomes more over-complete.</p><p>Varying block sizes: Finally, we show that WCM improves the results of block-sparse decoding for dictionaries with blocks of varying sizes as well. The generated dictionaries contain 15 blocks of size 4 and 20 blocks of size 3, with and . In this example, we set and . The results are shown as a function of in Fig. <ref type="figure" target="#fig_11">10</ref>(a) for dictionaries with normally distributed entries and in Fig. <ref type="figure" target="#fig_11">10(b</ref>) for structured dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Real Image Data</head><p>To evaluate the applicability of the proposed approach in practical scenarios we further perform experiments on real image data. Similar to <ref type="bibr" target="#b7">[8]</ref> we experimented with compression of image patches. We begin by hand-crafting a block-sparse dictionary . We select random locations in a given training image [Fig. <ref type="figure" target="#fig_13">11(a)</ref>] and extract around each location 9 overlapping patches of size 7 7, which are reshaped into 49 dimensional column vectors. We then fit a linear subspace of dimension to each group of overlapping patches. The computed 50 subspaces are used to construct our dictionary (i.e.,</p><p>). We then take 500 different testing images [see an example in Fig. <ref type="figure" target="#fig_13">11(b)</ref>] and extract from each image all the nonoverlapping patches of size 7 7, which are reshaped into 49 dimensional column vectors. They are compressed into dimensions using a sensing matrix . Three sensing matrices are tested: (i) random; (ii) designed according to DS <ref type="bibr" target="#b7">[8]</ref>; and (iii) designed according to our WCM algorithm with . Next, we obtain the block-sparse representations of the signals by applying BOMP with a fixed number of nonzero blocks. The sparse representations are used to recover the original signals of size 49. We then compute the mean representation over all image patches. For presentation purposes, we further reshape the estimated signals into 7 7 patches and reconstruct the image.</p><p>The above procedure was repeated 10 times, for 10 different dictionaries (each obtained using a different random selection of the training subspaces). Fig. <ref type="figure" target="#fig_13">11(c</ref>) presents the mean representation error over the 10 trials for each of the 500 testing images. WCM consistently leads to smaller errors.</p><p>Next, we repeat the same experiment but replacing the handcrafted dictionary with one trained using the approach proposed in <ref type="bibr" target="#b26">[27]</ref>. To train the dictionary we extract all 7 7 patches from the training image (total of 294). We then use SAC+BKSVD (see <ref type="bibr" target="#b26">[27]</ref>) to train the dictionary and recover the block structure. The rest of the experiment remains the same. As shown in Fig. <ref type="figure" target="#fig_13">11(g</ref>)-(i), using the trained dictionary instead of the designed one has little effect on the results. Finally, we further evaluate the results as a function of . Fig. <ref type="figure" target="#fig_14">12</ref> demonstrates that the results on image compression. When the reconstruction error is smaller for WCM than for DS or for a random . Note, that this is true for both and , however, the improvement for is more significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a framework for the design of a sensing matrix, assuming that a block-sparsifying dictionary is provided. We minimize a weighted sum of the total interblock coherence and the total subblock coherence, while attempting to keep the atoms in the equivalent dictionary as normalized as possible [see <ref type="bibr" target="#b12">(13)</ref>]. This objective can be seen as an intuitive extension of (4) to the case of blocks.</p><p>While it might be possible to derive a closed form solution to (13), we have presented the Weighted Coherence Minimization algorithm, an elegant iterative solution which is based on the bound-optimization method. In this method, the original objective is replaced with an easier to solve surrogate objective in each step. This algorithm eventually converges to a local solution of <ref type="bibr" target="#b12">(13)</ref>.</p><p>Simulations have shown that the best results are obtained when minimizing mostly the total subblock coherence. This leads to equivalent dictionaries with nearly orthonormal blocks, at the price of a slightly increased total interblock coherence. The obtained sensing matrix outperforms the one obtained when using the DS algorithm <ref type="bibr" target="#b7">[8]</ref> to solve(4). This improvement manifests itself in lower signal reconstruction errors and higher rates of successful signal classification. When giving equal weight to the total interblock coherence and to the total subblock coherence, the results are identical to solving(4). Moreover, both objectives coincide for this specific choice of , which ignores the existence of a block structure in the sparse representations of the signal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF CONVERGENCE</head><p>The surrogate function has been chosen in such a way as to bound the original objective from above for every , and to coincide at . Minimizing will then necessarily decrease the value of Formally, according to <ref type="bibr" target="#b25">[26]</ref>, the sequence of solutions generated by iteratively solving <ref type="bibr" target="#b16">(17)</ref> is guaranteed to converge to a local minimum of the original objective when the surrogate objective satisfies the following three constraints:</p><p>1) Equality at :</p><p>2) Upper-bounding the original function:</p><p>3) Equal gradient at :</p><p>We next prove that the three conditions hold.</p><p>Proof  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equal gradient at</head><p>: We calculate the gradient of and When substituting we obtain Therefore, the gradients of both objectives coincide at . This completes the convergence proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B PROOF OF PROPOSITION 1</head><p>Proof: In order to minimize , we rewrite the problem in an alternative form <ref type="bibr" target="#b17">(18)</ref> where is defined in <ref type="bibr" target="#b15">(16)</ref>. Let be the eigenvalue decomposition of and define . Substituting into (18) yields   <ref type="bibr" target="#b26">[27]</ref>. (h) A trained by DS D trained by <ref type="bibr" target="#b26">[27]</ref>. (i) A trained by WCM D trained by <ref type="bibr" target="#b26">[27]</ref>. where . According to <ref type="bibr" target="#b18">(19)</ref>, the surrogate objective can be minimized in closed form by finding the top components of . Let be the top eigenvalues of and the corresponding eigenvectors. Then, <ref type="bibr" target="#b18">(19)</ref> is solved by setting . Note that this solution is not unique, since can be multiplied on the left by any unitary matrix. This, however, does not affect the WCM algorithm since we only care about updating the Gram matrix , which is not influenced by the multiplication of on the left by a unitary matrix. Finally, the optimal sensing matrix is given by .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A graphical depiction of the Gram matrix G of an equivalent dictionary E with 6 blocks of size 3. The entries belonging to different blocks are in green, the off-diagonal entries belonging to the same block are in red, and the diagonal entries are in yellow.</figDesc><graphic coords="3,122.88,66.66,84.00,84.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of the absolute value of the Gram matrix of an equivalent dictionary for = 0:01 (a) = 0:5 (b), and = 0:99 (c) where the sensing matrix of size 12 2 18 was found by solving (13) given a randomly selected square dictionary composed of 6 blocks of size 3. The subblock entries are highlighted by red squares.</figDesc><graphic coords="3,305.76,67.02,244.00,81.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Histograms of the objective values obtained when solving (13) 100times with = 0:01 (a) and = 0:99 (b), for a given randomly generated square dictionary composed of 6 blocks of size 3. The sensing matrices of size12 2 18 are initialized as matrices with random entries. Note that the distribution is insignificant in (b), indicating that all the local minimum that our algorithm detected had the same objective value.</figDesc><graphic coords="4,44.46,66.54,238.00,201.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Histograms of the objective values obtained when solving (13) 100 times with = 0:01 (a) and = 0:99 (b), for a given randomly generated overcomplete dictionary composed of 24 blocks of size 3. The sensing matrices of size 12 2 18 are initialized as matrices with random entries.</figDesc><graphic coords="4,309.78,67.86,233.00,201.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Simulation results of sensing matrix design using the WCM algorithm with k = 1 and M = 6. The graphs show the normalized representation error e, the classification success r, and the ratio between the total subblock coherence and the total interblock coherence as a function of . In (a) the dictionary contains normally distributed entries, and in (b) randomly selected rows of the DCT matrix.</figDesc><graphic coords="5,144.60,67.62,304.00,267.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 :</head><label>1</label><figDesc>Weighted Coherence MinimizationTask: Solve for a given block-sparsifying dictionary where . Initialization: Calculate the eigenvalue decomposition of . Set as the outcome of (4), i.e., , and . Repeat until convergence:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Simulation results of sensing matrix design using the WCM algorithm with k = 2 and M = 14. The graphs show the normalized representation error e, the classification success r, and the ratio between the total subblock coherence and the total interblock coherence as a function of . In (a) the dictionary contains normally distributed entries, and in (b) randomly selected rows of the DCT matrix.</figDesc><graphic coords="7,117.12,66.18,359.00,318.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Simulation results of sensing matrix design using the WCM algorithm with k = 3 and M = 20. The graphs show the normalized representation errore, the classification success r, and the ratio between the total subblock coherence and the total interblock coherence as a function of . In (a) the dictionary contains normally distributed entries, and in (b) randomly selected rows of the DCT matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Simulation results of sensing matrix design using the WCM algorithm with k = 6 and M = 35. The graphs show the normalized representation errore, the classification success r, and the ratio between the total subblock coherence and the total interblock coherence as a function of . In (a) the dictionary contains normally distributed entries, and in (b) randomly selected rows of the DCT matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Simulation results of sensing matrix design using the WCM algorithm with k = 2 and M = 14. The graphs show the normalized representation error e and the classification success r as a function of K. In (a) the dictionary contains normally distributed entries, and in (b) randomly selected rows of the DCT matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Simulation results of sensing matrix design using the WCM algorithm on a dictionary containing 15 blocks of size 4 and 20 blocks of size 3, with k = 2 and M = 14. The graphs show the normalized representation error e, the classification success r, and the ratio between the total subblock coherence and the total interblock coherence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Image compression experiment (a) An image used to train sensing matrices using DS and WCM (b) An example of a query image (500 total) that was decomposed into 7 2 7 nonoverlapping patches which were compressed into 15 dimensions using each of the sensing matrices. We then use BOMP to recover the sparse representation of the original patches and reconstruct them. (d,g) Representation errors for each of the 500 different query images. WCM leads to smallest errors on almost all the images. (c,e,f,h,i) The obtained reconstructions (a) Training image. (b) An example query image. (c) Random A. (d) Representation error D designed. (e) A trained by DS D designed. (f) A trained by WCM D designed. (g) Representation error, D trained by<ref type="bibr" target="#b26">[27]</ref>. (h) A trained by DS D trained by<ref type="bibr" target="#b26">[27]</ref>. (i) A trained by WCM D trained by<ref type="bibr" target="#b26">[27]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Repeating the experiment of Fig. 11 for varying values of . When &gt; 0:5 WCM results in lower errors compared to DS and random A.</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Manuscript received September 17, 2010; revised February 03, 2011; accepted May 23, 2011. Date of publication June 09, 2011; date of current version August 10, 2011. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Sofia C. Olhede. The work of L. Zelnik-Manor was supported by Marie Curie IRG-208529 and by the Ollendorf foundation. The work of Y. Eldar was supported in part by a Magneton grant from the Israel Ministry of Industry and Trade and by the Israel Science Foundation by Grant 170/10.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently an algorithm engineer with Mobileye's Vehicle Detection division, Jerusalem, and is pursuing the M.B.A. degree at the Tel-Aviv University, Tel-Aviv, Israel. His main areas of interests include signal processing algorithms, image processing, and computer vision. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Greed is good: Algorithmic results for sparse approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2231" to="2242" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Matching pursuits and time-frequency dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993-12">Dec. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning compressed sensing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Allerton Conf</title>
		<meeting>Allerton Conf</meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimized projections for compressed sensing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5695" to="5702" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to sense sparse signals: Simultaneous sensing matrix and sparsifying dictionary optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Duarte-Carvajalino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA Preprint Ser</title>
		<imprint>
			<biblScope unit="page">2211</biblScope>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A theory for sampling signals from a union of subspaces</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2334" to="2345" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Time delay estimation from low rate samples: A union of subspaces approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gedalyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3017" to="3031" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust recovery of signals from a structured union of subspaces</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mishali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5302" to="5316" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lambertian refelectances and linear subspaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="383" to="390" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature Selection in Face Recognition: A Sparse Representation Perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. of California Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified algebraic approach to 2-D and 3-D motion segmentation and estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imag. Vision</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="421" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blind multiband signal reconstruction: Compressed sensing for analog signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mishali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="993" to="1009" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From theory to practice: Sub-Nyquist sampling of sparse wideband analog signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mishali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="375" to="391" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Necessary density conditions for sampling and interpolation of certain entire functions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Landau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Math</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recovering sparse signals using sparse measurement matrices in compressed DNA microarrays</title>
		<author>
			<persName><forename type="first">F</forename><surname>Parvaresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vikalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="285" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the reconstruction of block-sparse signals with an optimal number of measurements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Parvaresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3075" to="3085" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Average case analysis of multichannel sparse recovery using convex relaxation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rauhut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="505" to="519" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Consistency of the group lasso and multiple kernel learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1179" to="1225" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statist. Soc.: Ser. B (Statist. Methodol.)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The benefit of group sparsity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1978" to="2004" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Block-sparse signals: Uncertainty relations and efficient recovery</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuppinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bölcskei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3042" to="3054" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Block-sparsity: Coherence and efficient recovery</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bölcskei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2885" to="2888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Majorization-minimization algorithms for wavelet-based image restoration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2980" to="2991" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dictionary optimization for block-sparse representations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">08) received the B.Sc. degree in mechanical engineering from the Technion-Israel Institute of Technology, Haifa, in 1995, where she graduated summa cum laude, and the M.Sc. degree (with honors) and the Ph.D. degrees in computer science from the Weizmann Institute of Science in</title>
		<author>
			<persName><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 2004</date>
		</imprint>
	</monogr>
	<note>respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
