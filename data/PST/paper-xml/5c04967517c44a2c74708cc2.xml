<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EXCESSIVE INVARIANCE CAUSES ADVERSARIAL VULNERABILITY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
							<email>j.jacobsen@vectorinstitute.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">Vector Institute and University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vector Institute and University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Industrial Mathematics</orgName>
								<orgName type="institution">University of Bremen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vector Institute and University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EXCESSIVE INVARIANCE CAUSES ADVERSARIAL VULNERABILITY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from -adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an informationtheoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Figure <ref type="figure" target="#fig_10">1</ref>: All images shown cause a competitive ImageNet-trained network to output the exact same probabilities over all 1000 classes (logits shown above each image). The leftmost image is from the ImageNet validation set; all other images are constructed such that they match the non-class related information of images taken from other classes (for details see section 2.1). The excessive invariance revealed by this set of adversarial examples demonstrates that the logits contain only a small fraction of the information perceptually relevant to humans for discrimination between the classes.</p><p>Adversarial vulnerability is one of the most iconic failure cases of modern machine learning models <ref type="bibr" target="#b44">(Szegedy et al., 2013)</ref> and a prime example of their weakness in out-of-distribution generalization. It is particularly striking that under i.i.d. settings deep networks show superhuman performance on many tasks <ref type="bibr" target="#b32">(LeCun et al., 2015)</ref>, while tiny targeted shifts of the input distribution can cause them to make unintuitive mistakes. The reason for these failures and how they may be avoided or at least mitigated is an active research area <ref type="bibr" target="#b40">(Schmidt et al., 2018;</ref><ref type="bibr" target="#b20">Gilmer et al., 2018b;</ref><ref type="bibr" target="#b11">Bubeck et al., 2018)</ref>.</p><p>So far, the study of adversarial examples has mostly been concerned with the setting of small perturbation, or -adversaries <ref type="bibr" target="#b23">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b34">Madry et al., 2017;</ref><ref type="bibr" target="#b37">Raghunathan et al., 2018)</ref>.</p><p>Perturbation-based adversarial examples are appealing because they allow to quantitatively measure notions of adversarial robustness <ref type="bibr" target="#b9">(Brendel et al., 2018)</ref>. However, recent work argued that the perturbation-based approach is unrealistically restrictive and called for the need of generalizing the concept of adversarial examples to the unrestricted case, including any input crafted to be misinterpreted by the learned model <ref type="bibr" target="#b43">(Song et al., 2018;</ref><ref type="bibr" target="#b10">Brown et al., 2018)</ref>. Yet, settings beyond -robustness are hard to formalize <ref type="bibr" target="#b19">(Gilmer et al., 2018a)</ref>.</p><p>We argue here for an alternative, complementary viewpoint on the problem of adversarial examples. Instead of focusing on transformations erroneously crossing the decision-boundary of classifiers, we focus on excessive invariance as a major cause for adversarial vulnerability. To this end, we introduce the concept of invariance-based adversarial examples and show that class-specific content of almost any input can be changed arbitrarily without changing activations of the network, as illustrated in figure <ref type="figure" target="#fig_10">1</ref> for ImageNet. This viewpoint opens up new directions to analyze and control crucial aspects underlying vulnerability to unrestricted adversarial examples.</p><p>The invariance perspective suggests that adversarial vulnerability is a consequence of narrow learning, yielding classifiers that rely only on few highly predictive features in their decisions. This has also been supported by the observation that deep networks strongly rely on spectral statistical regularities <ref type="bibr" target="#b28">(Jo &amp; Bengio, 2017)</ref>, or stationary statistics <ref type="bibr" target="#b17">(Gatys et al., 2017)</ref> to make their decisions, rather than more abstract features like shape and appearance. We hypothesize that a major reason for this excessive invariance can be understood from an information-theoretic viewpoint of crossentropy, which maximizes a bound on the mutual information between labels and representation, giving no incentive to explain all class-dependent aspects of the input. This may be desirable in some cases, but to achieve truly general understanding of a scene or an object, machine learning models have to learn to successfully separate essence from nuisance and subsequently generalize even under shifted input distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions:</head><p>• We identify excessive invariance underlying striking failures in deep networks and formalize the connection to adversarial examples.</p><p>• We show invariance-based adversarial examples can be observed across various tasks and types of deep network architectures.</p><p>• We propose an invertible network architecture that gives explicit access to its decision space, enabling class-specific manipulations to images while leaving all dimensions of the representation seen by the final classifier invariant.</p><p>• From an information-theoretic viewpoint, we identify the cross-entropy objective as a major reason for the observed failures. Leveraging invertible networks, we propose an alternative objective that provably reduces excessive invariance and works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TWO COMPLEMENTARY APPROACHES TO ADVERSARIAL EXAMPLES</head><p>In this section, we define pre-images and establish a link to adversarial examples.</p><formula xml:id="formula_0">Definition 1 (Pre-images / Invariance). Let F : R d → R C be a neural network, F = f L • • • • • f 1</formula><p>with layers f i and let F i denote the network up to layer i. Further, let D : R d → {1, . . . , C} be a classifier with D = arg max k=1,...,C sof tmax(F (x)) k . Then, for input x ∈ R d , we define the following pre-images</p><formula xml:id="formula_1">(i) i-th Layer pre-image: {x * ∈ R d | F i (x * ) = F i (x)} (ii) Logit pre-image: {x * ∈ R d | F (x * ) = F (x)} (iii) Argmax pre-image: {x * ∈ R d | D(x * ) = D(x)},</formula><p>where (i) ⊂ (ii) ⊂ (iii) by the compositional nature of D. Moreover, the (sub-)network is invariant to perturbations ∆x which satisfy x * = x + ∆x.</p><p>→ Invariance-based: Non-trivial pre-images (pre-images containing more elements than input x) after the i-th layer occur if the chain</p><formula xml:id="formula_2">F (x) = F (x) y = ỹ (1) → Perturbation-based: F (x) = F (x) y = ỹ (2)</formula><formula xml:id="formula_3">f i • • • • • f 1 is not injective,</formula><p>for instance due to subsampling or non-injective activation functions like ReLU <ref type="bibr" target="#b6">(Behrmann et al., 2018a)</ref>. This accumulated invariance can become problematic if not controlled properly, as we will show in the following.</p><p>We Usually, such examples are constructed as -bounded adversarial examples <ref type="bibr" target="#b23">(Goodfellow et al., 2015)</ref>. However, as our goal is to characterize general invariances of the network, we do not restrict ourselves to bounded perturbations. Definition 3 (Invariance-based Adversarial Examples). Let G denote the i-th layer, logits or the classifier (Definition 1) and let x * = x be in the G pre-image of x and and o an oracle (Definition 2). Then, an invariance-based adversarial example fulfills o(x) = o(x * ), while G(x) = G(x * ) (and hence D(x) = D(x * )).</p><p>Intuitively, adversarial perturbations cause the output of the classifier to change while the oracle would still consider the new input x * as being from the original class. Hence in the context ofbounded perturbations, the classifier is too sensitive to task-irrelevant changes. On the other hand, movements in the pre-image leave the classifier invariant. If those movements induce a change in class as judged by the oracle, we call these invariance-based adversarial examples. In this case, however, the classifier is too insensitive to task-relevant changes. In conclusion, these two modes are complementary to each other, whereas both constitute failure modes of the learned classifier.</p><p>When not restricting to -perturbations, perturbation-based and invariance-based adversarial examples yield the same input x * via</p><formula xml:id="formula_4">x * = x 1 + ∆x 1 , D(x * ) = D(x 1 ), o(x * ) = o(x 1 ) (3) x * = x 2 + ∆x 2 , D(x * ) = D(x 2 ), o(x * ) = o(x 2 ),<label>(4)</label></formula><p>with different reference points x 1 and x 2 , see Figure <ref type="figure" target="#fig_0">2</ref>. Hence, the key difference is the change of reference, which allows us to approach these failure modes from different directions. To connect these failure modes with an intuitive understanding of variations in the data, we now introduce the notion of invariance to nuisance and semantic variations, see also <ref type="bibr" target="#b1">(Achille &amp; Soatto, 2018)</ref>. For example, such a nuisance perturbation could be a translation or occlusion in image classification. Further in Appendix A, we discuss the synthetic example called Adversarial Spheres from <ref type="bibr" target="#b20">(Gilmer et al., 2018b)</ref>, where nuisance and semantics can be explicitly formalized as rotation and norm scaling.</p><p>2 Bijective classifiers with simplified readout.</p><p>We build deep networks that give access to their decision space by removing the final linear mapping onto the class probes in invertible RevNet-classifiers and call these networks fully invertible RevNets. The fully invertible RevNet classifier can be written as D θ = arg max k=1,...,C sof tmax(F θ (x) k ), where F θ represents the bijective network. We denote z = F θ (x), z s = z 1,...,C as the logits (semantic variables) and z n = z C+1,...,d as the nuisance variables (z n is not used for classification). In practice we choose the first C indices of the final z tensor or apply a more sophiscticated DCT scheme (see appendix D) to set the subspace z s , but other choices work as well. The architecture of the network is similar to iRevNets <ref type="bibr">(Jacobsen et al., 2018)</ref> with some additional Glow components like actnorm <ref type="bibr" target="#b30">(Kingma &amp; Dhariwal, 2018)</ref>, squeezing, dimension splitting and affine block structure <ref type="bibr" target="#b15">(Dinh et al., 2017)</ref>, see Figure <ref type="figure">3</ref> for a graphical description. As all components are common in the bijective network literature, we refer the reader to Appendix D for exact training and architecture details. Due to its simple readout structure, the resulting invertible network allows to qualitatively and quantitatively investigate the task-specific content in nuisance and logit variables. Despite this restriction, we achieve performance on par with commonly-used baselines on MNIST and ImageNet, see Table <ref type="table" target="#tab_1">1 and</ref>  Analytic attack. To analyze the trained models, we can sample elements from the logit pre-image by computing x met = F −1 (z s , zn ), where z s and zn are taken from two different inputs. We term this heuristic metameric sampling. The samples would be from the true data distribution if the subspaces would be factorized as P (z s , z n ) = P (z s )P (z n ). Experimentally we find that logit metamers are revealing adversarial subspaces and are visually close to natural images on ImageNet. Thus, metameric sampling gives us an analytic tool to inspect dependencies between semantic and nuisance variables without the need for expensive and approximate optimization procedures.</p><p>Attack on adversarial spheres. First, we evaluate our analytic attack on the synthetic spheres dataset, where the task is to classify samples as belonging to one out of two spheres with different radii. We choose the sphere dimensionality to be d = 100 and the radii: Here, it is possible to move any point from the inner sphere to the outer sphere without changing the classifiers predictions. However, this is not possible for the classifier trained on z n . Most notably, the visualized failure is not due to a lack of data seen during training, but rather due to excessive invariance of the original classifier D on z s . Thus, the nuisance classifier on z n does not exhibit the same adversarial vulnerability in its subspace. Attack on MNIST and ImageNet. After validating its potential to uncover adversarial subspaces, we apply metameric sampling to fully invertible RevNets trained on MNIST and Imagenet, see Figure <ref type="figure" target="#fig_5">5</ref>. The result is striking, as the nuisance variables z n are dominating the visual appearance of the logit metamers, making it possible to attach any semantic content to any logit activation pattern. Note that the entire 1000-dimensional feature vector containing probabilities over all ImageNet classses remains unchanged by any of the transformations we apply. To show our findings are not a particular property of bijective networks, we attack an ImageNet trained ResNet152 with a gradientbased version of our metameric attack, also known as feature adversaries <ref type="bibr" target="#b38">(Sabour et al., 2016)</ref>. The attack minimizes the mean squared error between a given set of logits from one image to another image (see appendix B for details). The attack shows the same failures for non-bijective models. This result highlights the general relevance of our finding and poses the question of the origin of this excessive invariance, which we will analyze in the following section.</p><formula xml:id="formula_5">R 1 = 1, R 2 =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERCOMING INSUFFICIENCY OF CROSSENTROPY-BASED INFORMATION-MAXIMIZATION</head><p>In this section we identify why the cross-entropy objective does not necessarily encourage to explain all task-dependent variations of the data and propose a way to fix this. As shown in figure <ref type="figure" target="#fig_2">4</ref>, the nuisance classifier on z n uses task-relevant information not captured by the logit classifier D θ on z s (evident by its superior performance in the adversarial subspace).</p><p>We leverage the simple readout-structure of our invertible network and turn this observation into a formal explanation framework using information theory: Let (x, y) ∼ D with labels y ∈ {0, 1} C . Then the goal of a classifier can be stated as maximizing the mutual information <ref type="bibr" target="#b14">(Cover &amp; Thomas, 2006)</ref> between semantic features z s (logits) extracted by network F θ and labels y, denoted by I(y; z s ).</p><p>Adversarial distribution shift. As the previously discussed failures required to modify input data from distribution D, we introduce the concept of an adversarial distribution shift D Adv = D to formalize these modifications. Our first assumptions for D Adv is</p><formula xml:id="formula_6">I D Adv (z n ; y) ≤ I D (z n ; y).</formula><p>Intuitively, the nuisance variables z n of our network do not become more informative about y. Thus, the distribution shift may reduce the predictiveness of features encoded in z s , but does not introduce or increase the predictive value of variations captured in z n . Second, we assume I D Adv (y; z s |z n ) ≤ I D Adv (y; z s ), which corresponds to positive or zero interaction information, see e.g. <ref type="bibr" target="#b18">(Ghassami &amp; Kiyavash, 2017)</ref>. While the information in z s and z n can be redundant in this assumption, synergetic effects where conditioning on z n increase the mutual information between y and z s are excluded.</p><p>Bijective networks F θ capture all variations by design which translates to information preservation I(y; x) = I(y; F θ (x)), see <ref type="bibr" target="#b31">(Kraskov et al., 2004)</ref>. Consider the reformulation</p><formula xml:id="formula_7">I(y; x) = I(y; F θ (x)) = I(y; z s , z n ) = I(y; z s ) + I(y; z n |z s ) = I(y; z n ) + I(y; z s |z n )<label>(5)</label></formula><p>by the chain rule of mutual information <ref type="bibr" target="#b14">(Cover &amp; Thomas, 2006)</ref>, where I(y; z n |z s ) denotes the conditional mutual information. Most strikingly, equation 5 offers two ways forward:</p><p>1. Direct increase of I(y; z s )</p><p>2. Indirect increase of I(y; z s |z n ) via decreasing I(y; z n ).</p><p>Usually in a classification task, only I(y; z s ) is increased actively via training a classifier. While this approach is sufficient in most cases, expressed via high accuracies on training and test data, it may fail under D Adv . This highlights why cross-entropy training may not be sufficient to overcome excessive semantic invariance. However, by leveraging the bijection F θ we can minimize the unused information I(y; z n ) using the intuition of a nuisance classifier.</p><p>Definition 5 (Independence cross-entropy loss). Let F θ : R d → R d a bijective network with parameters θ ∈ R p1 and Fθ (x) = sof tmax(F θ (x) 1,...,C ). Furthermore, let D θnc : R d−C → [0, 1] C be the nuisance classifier with θ nc ∈ R p2 . Then, the independence cross-entropy loss is defined as:</p><formula xml:id="formula_8">min θ max θnc L iCE (θ, θ nc ) = C i=1 −y i log F zs θ (x) i =:L sCE (θ) + C i=1 y i log D θnc (F zn θ (x)) i =:L nCE (θ,θnc)</formula><p>.</p><p>The underlying principles of the nuisance classification loss L nCE can be understood using a variational lower bound on mutual information from <ref type="bibr" target="#b5">Barber &amp; Agakov (2003)</ref>. In summary, the minimization is with respect to a lower bound on I D (y; z n ), while the maximization aims to tighten the bound (see Lemma 10 in Appendix C). By using these results, we now state the main result under the assumed distribution shift and successful minimization (proof in Appendix C.   Thus, incorporating the nuisance classifier allows for the discussed indirect increase of I D Adv (y; z s ) under an adversarial distribution shift, visualized in Figure <ref type="figure" target="#fig_7">6</ref>.</p><p>To aid stability and further encourage factorization of z s and z n in practice, we add a maximum likelihood term to our independence cross-entropy objective as</p><formula xml:id="formula_9">min θ max θnc L(θ, θ nc ) = L iCE (θ, θ nc ) − d−C k=1 log (p k (F zn θ (x) k )|det(J x θ )|) =:L M LEn (θ) ,<label>(6)</label></formula><p>where det(J x θ ) denotes the determinant of the Jacobian of F θ (x) and p k ∼ N (β k , γ k ) with β k , γ k learned parameter. The log-determinant can be computed exactly in our model with negligible additional cost. Note, that optimizing L M LEn on the nuisance variables together with L sCE amounts to maximum-likelihood under a factorial prior (see Lemma 11 in Appendix C).</p><p>Just as in GANs the quality of the result relies on a tight bound provided by the nuisance classifier and convergence of the MLE term. Thus, it is important to analyze the success of the objective after training. We do this by applying our metameric sampling attack, but there are also other ways like evaluating a more powerful nuisance classifier after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLYING INDEPENDENCE CROSS-ENTROPY</head><p>In this section, we show that our proposed independence cross-entropy loss is effective in reducing invariance-based vulnerability in practice by comparing it to vanilla cross-entropy training in four aspects: (1) error on train and test set, (2) effect under distribution shift, perturbing nuisances via metameric sampling, (3) evaluate accuracy of a classifier on the nuisance variables to quantify the class-specific information in them and (4) on our newly introduced shiftMNIST, an augmented version of MNIST to benchmark adversarial distribution shifts according to Theorem 6.</p><p>For all experiments we use the same network architecture and settings, the only difference being the two additional loss terms as explained in Definition 5 and equation 6. In terms of test error of the logit classifier, both losses perform approximately on par, whereas the gap between train and test error vanishes for our proposed loss function, indicating less overfitting. For classification errors see Table <ref type="table" target="#tab_3">2</ref> in appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness under metameric sampling attack.</head><p>To analyze if our proposed loss indeed leads to independence between z n and labels y, we attack it with our metameric sampling procedure. As we are only looking on data samples and not on samples from the model (factorized gaussian on nuisances), this attack should reveal if the network learned to trick the objective. In Figure <ref type="figure" target="#fig_8">7</ref> we show interpolations between original images and logit metamers in CE-and iCE-trained fully invertible RevNets. In particular, we are holding the activations z s constant, while linearly interpolating nuisances z n down the column. The CE-trained network allows us to transform any image into any class without changing the logits. However, when training with our proposed iCE, the picture changes fundamentally and interpolations in the pre-image only change the style of a digit, but not its semantic content. This shows our loss has the ability to overcome excessive task-related invariance and encourages the model to explain and separate all task-related variability of the input from the nuisances of the task. A classifier trained on the nuisance variables of the cross-entropy trained model performs even better than the logit classifier. Yet, a classifier on the nuisances of the independence cross-entropy trained model is performing poorly (Table <ref type="table" target="#tab_3">2</ref> in appendix D). This indicates little class-specific information in the nuisances z n , as intended by our objective function. Note also that this inability of the nuisance classifier to decode class-specific information is not due to it being hard to read out from z n , as this would be revealed by the metameric sampling attack (see Figure <ref type="figure" target="#fig_8">7</ref>). shiftMNIST: Benchmarking adversarial distribution shift. To further test the efficacy of our proposed independence cross-entropy, we introduce a simple, but challenging new dataset termed shiftMNIST to test classifiers under adversarial distribution shifts D Adv . The dataset is based on vanilla MNIST, augmented by introducing additional, highly predictive features at train time that are randomized or removed at test time. Randomization or removal ensures that there are no synergy effects between digits and planted features under D Adv . This setup allows us to reduce mutual information between category and the newly introduced feature in a targeted manner. (a) Binary shiftMNIST is vanilla MNIST augmented by coding the category for each digit into a single binary pixel scheme. The location of the binary pixel reveals the category of each image unambigiously, while only minimally altering the image's appearance.</p><p>At test time, the binary code is not present and the network can not rely on it anymore. (b) Textured shiftMNIST introduces textured backgrounds for each digit category which are patches sampled from the describable texture dataset <ref type="bibr" target="#b13">(Cimpoi et al., 2014)</ref>.</p><p>At train time the same type of texture is underlayed each digit of the same category, while texture types across categories differ. At test time, the relationship is broken and texture backgrounds are paired with digits randomly, again minimizing the mutual information between background and label in a targeted manner. See Figure <ref type="figure" target="#fig_9">8</ref> for examples<ref type="foot" target="#foot_0">1</ref> .</p><p>It turns out that this task is indeed very hard for standard classifiers and their tendency to become excessively invariant to semantically meaningful features, as predicted by our theoretical analysis. When trained with cross-entropy, ResNets and fi-RevNets make zero errors on the train set, while having error rates of up to 87% on the shifted test set. This is striking, given that e.g. in binary shiftMNIST, only one single pixel is removed under D Adv , leaving the whole image almost unchanged. When applying our independence cross-entropy, the picture changes again. The errors made by the network improve by up to almost 38% on binary shiftMNIST and around 28% on textured shiftMNIST. This highlights the effectiveness of our proposed loss function and its ability to minimize catastrophic failure under severe distribution shifts exploiting excessive invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Adversarial examples. Adversarial examples often include -norm restrictions <ref type="bibr" target="#b44">(Szegedy et al., 2013)</ref>, while <ref type="bibr" target="#b19">(Gilmer et al., 2018a)</ref> argue for a broader definition to fully capture the implications for security. The -adversarial examples have also been extended to -feature adversaries <ref type="bibr" target="#b38">(Sabour et al., 2016)</ref>, which are equivalent to our approximate metameric sampling attack. Some works <ref type="bibr" target="#b43">(Song et al., 2018;</ref><ref type="bibr" target="#b16">Fawzi et al., 2018</ref>) consider unrestricted adversarial examples, which are closely related to invariance-based adversarial vulnerability. The difference to human perception revealed by adversarial examples fundamentally questions which statistics deep networks use to base their decisions <ref type="bibr" target="#b28">(Jo &amp; Bengio, 2017;</ref><ref type="bibr" target="#b46">Tsipras et al., 2019)</ref>.</p><p>Relationship between standard and bijective networks. We leverage recent advances in reversible <ref type="bibr" target="#b21">(Gomez et al., 2017)</ref> and bijective networks <ref type="bibr">(Jacobsen et al., 2018;</ref><ref type="bibr" target="#b4">Ardizzone et al., 2019;</ref><ref type="bibr" target="#b30">Kingma &amp; Dhariwal, 2018)</ref> for our analysis. It has been shown that ResNets and iRevNets behave similarly on various levels of their representation on challenging tasks <ref type="bibr">(Jacobsen et al., 2018)</ref> and that iRevNets as well as Glow-type networks are related to ResNets by the choice of dimension splitting applied in their residual blocks <ref type="bibr" target="#b24">(Grathwohl et al., 2019)</ref>. Perhaps unsurprisingly, given so many similarities, ResNets themselves have been shown to be provably bijective under mild conditions <ref type="bibr" target="#b7">(Behrmann et al., 2018b)</ref>. Further, excessive invariance of the type we discuss here has been shown to occur in non residual-type architectures as well <ref type="bibr" target="#b20">(Gilmer et al., 2018b;</ref><ref type="bibr" target="#b6">Behrmann et al., 2018a)</ref>. For instance, it has been observed that up to 60% of semantically meaningful input dimensions on the adversarial spheres problem are learned to be ignored, while retaining virtually perfect performance <ref type="bibr" target="#b20">(Gilmer et al., 2018b)</ref>. In summary, there is ample evidence that RevNet-type networks are closely related to ResNets, while providing a principled framework to study widely observed issues related to excessive invariance in deep learning in general and adversarial robustness in particular.</p><p>Information theory. The information-theoretic view has gained recent interest in machine learning due to the information bottleneck <ref type="bibr" target="#b45">(Tishby &amp; Zaslavsky, 2015;</ref><ref type="bibr" target="#b41">Shwartz-Ziv &amp; Tishby, 2017;</ref><ref type="bibr" target="#b2">Alemi et al., 2017)</ref> and usage in generative modelling <ref type="bibr" target="#b12">(Chen et al., 2016;</ref><ref type="bibr" target="#b26">Hjelm et al., 2019)</ref>. As a consequence, the estimation of mutual information <ref type="bibr" target="#b5">(Barber &amp; Agakov, 2003;</ref><ref type="bibr" target="#b3">Alemi et al., 2018;</ref><ref type="bibr" target="#b1">Achille &amp; Soatto, 2018;</ref><ref type="bibr" target="#b8">Belghazi et al., 2018)</ref> has attracted growing attention. The concept of group-wise independence between latent variables goes back to classical independent subspace analysis <ref type="bibr" target="#b27">(Hyvärinen &amp; Hoyer, 2000)</ref> and received attention in learning unbiased representations, e.g. see the Fair Variational Autoencoder <ref type="bibr" target="#b33">(Louizos et al., 2015)</ref>. Furthermore, extended cross-entropy losses via entropy terms <ref type="bibr" target="#b36">(Pereyra et al., 2017)</ref> or minimizing predictability of variables <ref type="bibr" target="#b39">(Schmidhuber, 1991)</ref> has been introduced for other applications. Our proposed loss also shows similarity to the GAN loss <ref type="bibr" target="#b22">(Goodfellow et al., 2014)</ref>. However, in our case there is no notion of real or fake samples, but exploring similarities in the optimization are a promising avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Failures of deep networks under distribution shift and their difficulty in out-of-distribution generalization are prime examples of the limitations in current machine learning models. The field of adversarial example research aims to close this gap from a robustness point of view. While a lot of work has studied -adversarial examples, recent trends extend the efforts towards the unrestricted case. However, adversarial examples with no restriction are hard to formalize beyond testing error. We introduce a reverse view on the problem to: (1) show that a major cause for adversarial vulnerability is excessive invariance to semantically meaningful variations, (2) demonstrate that this issue persists across tasks and architectures; and (3) make the control of invariance tractable via fully-invertible networks.</p><p>In summary, we demonstrated how a bijective network architecture enables us to identify large adversarial subspaces on multiple datasets like the adversarial spheres, MNIST and ImageNet. Afterwards, we formalized the distribution shifts causing such undesirable behavior via information theory. Using this framework, we find one of the major reasons is the insufficiency of the vanilla cross-entropy loss to learn semantic representations that capture all task-dependent variations in the input. We extend the loss function by components that explicitly encourage a split between semantically meaningful and nuisance features. Finally, we empirically show that this split can remove unwanted invariances by performing a set of targeted invariance-based distribution shift experiments. Proof. To proof above result, we need to draw the connection to the variational lower bound on mutual information from Lemma 9. Let the nuisance classifier D θnc (z n ) model the variational posterior q θnc (y|z n ). Then we have the lower bound</p><formula xml:id="formula_10">I(y; z n ) ≥ h(y) + E zn E y|zn log D θnc (z n ) =: I θnc (y; z n ).<label>(9)</label></formula><p>From Lemma 9 follows, that if D θnc (z n ) = p(y|z n ), it holds I(y; z n ) = I θnc (y; z n ). Hence, the nuisance classifier needs to model the conditional density perfectly.</p><p>Estimating this bound via Monte Carlo simulation requires sampling from the conditional density p(y|z n ). Following <ref type="bibr" target="#b2">(Alemi et al., 2017)</ref>, we have the Markov property y ↔ x ↔ z n as labels y interact with inputs x and representation z n interacts with inputs x. Hence,</p><formula xml:id="formula_11">p(y|z n )p(z n ) = p(y, z n ) = X p(x, y, z n )dx = X p(z n |x, y)p(y|x)p(x)dx = X p(z n |x)p(y|x)p(x)dx = E x [p(z n |x)p(y|x)].</formula><p>Including above and assuming F θ (x) = z n to be a deterministic function, we have</p><formula xml:id="formula_12">E zn E y|zn log D θnc (z n ) = E x E y|x E zn|x log D θnc (z n ) = E x E y|x log D θnc (z n ).</formula><p>Lemma 11 (Effect of MLE-term). Define semantics as z s = F θ (x) 1,...,C and nuisances as z n = F θ (x) C+1,...,d , where (x, y) ∼ D. Then, the MLE-term in equation 6 together with cross-entropy on the semantics</p><formula xml:id="formula_13">θ * = arg min θ L sCE (θ) + L M LEn (θ)</formula><p>minimizes the mutual information I(z s ; z n ).</p><p>Proof. Let zs = sof tmax(z s ). Then minimizing the loss terms L sCE and L M LEn is a maximum likelihood estimation under the factorial prior</p><formula xml:id="formula_14">p(z s , z n ) = p(z s )p(z n ) (10) = Cat((z s ) 1 , . . . , (z s ) C ) d−C k=1 p k (z n ) k , (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>where Cat is a categorical distribution. As sof tmax is shift-invariant, sof tmax(x + c) = sof tmax(x), above factorial prior for zs and z n yields independence between logits z s and z n up to a constant c. Finally note, the log term and summation in L M LEn and L CE is re-formulation for computational ease but does not change its minimizer as the logarithm is monotone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 PROOF OF THEOREM 6</head><p>From the assumptions follows I D Adv (y; z n ) = 0. Furthermore, we have the assumption</p><formula xml:id="formula_16">I D Adv (y; z s |z n ) ≤ I D Adv (z s ; y),</formula><p>excluding synergetic effects in the interaction information <ref type="bibr" target="#b18">(Ghassami &amp; Kiyavash, 2017)</ref>. By information preservation under homeomorphisms <ref type="bibr" target="#b31">(Kraskov et al., 2004)</ref> and the chain rule of mutual information <ref type="bibr" target="#b14">(Cover &amp; Thomas, 2006)</ref>, we have</p><formula xml:id="formula_17">I D Adv (y; x) = I D Adv (y; z s , z n ) = I D Adv (y; z n ) + I D Adv (y; z s |z n ) ≤ I D Adv (y; z s ).</formula><p>As z s = F (x) 1,...,C is obtained by the deterministic transform F , by the data processing inequality <ref type="bibr" target="#b14">(Cover &amp; Thomas, 2006)</ref> we have the inequality I D Adv (y; x) ≥ I D Adv (y; z s ). Thus, the claimed equality must hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 MUTUAL INFORMATION BOUNDED</head><p>Remark 12. Since our goal is to maximize the mutual information I(y; z s ) while minimizing I(y; z n ), we need to ensure that this objective is well defined as mutual information can be unbounded from above for continuous random variables. However, due to the data processing inequality <ref type="bibr" target="#b14">(Cover &amp; Thomas, 2006)</ref> we have I(y; z n ) = I(y; F θ (x)) ≤ I(y; x). Hence, we have a fixed upper bound given by our data (x, y). Compared to <ref type="bibr" target="#b8">(Belghazi et al., 2018)</ref> there is thus no need for gradient clipping or a switch to the bounded Jensen-Shannon divergence as in <ref type="bibr" target="#b26">(Hjelm et al., 2019)</ref> is not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D TRAINING AND ARCHITECTURAL DETAILS</head><p>All experiments were based on a fully invertible RevNet model with different hyperparameters for each dataset. For the spheres experiment we used Pytorch <ref type="bibr" target="#b35">(Paszke et al., 2017)</ref> and for MNIST, as well as Imagenet Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 SPHERES EXPERIMENTS</head><p>The network is a fully connected fully invertible RevNet. It has 4 RevNet-type ReLU bottleneck blocks with additive couplings and uses no batchnorm. We train it via cross-entropy and use the Adam optimizer <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2014)</ref> with a learning rate of 0.0001 and otherwise default Pytorch settings. The nuisance classifier is a 3 layer ReLU network with 1000 hidden units per layer.</p><p>We choose the spheres to be 100-dimensional, with R 1 = 1 and R 2 = 10, train on 500k samples for 10 epochs and then validate on another 100k holdout set. We achieve 100% train and validation accuracy for logit and nuisance classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 MNIST EXPERIMENTS</head><p>We use a convolutional fully invertible RevNet with additional actnorm and invertible 1x1 convolutions between each layer as introduced in Kingma &amp; Dhariwal (2018). The network has 3 stages, after which half of the variables are factored out and an invertible downsampling, or squeezing <ref type="bibr" target="#b15">(Dinh et al., 2017;</ref><ref type="bibr">Jacobsen et al., 2018)</ref> is applied. The network has 16 RevNet blocks with batch norm per stage and 128 filters per layer. We also dequantize the inputs as is typically done in flow-based generative models.</p><p>The network is trained via Adamax (Kingma &amp; Ba, 2014) with a base learning rate of 0.001 for 100 epochs and we multiply the it with a factor of 0.2 every 30 epochs and use a batch size of 64 and l2 weight decay of 1e-4. For training we compare vanilla cross-entropy training with our proposed independence cross-entropy loss. To have a more balanced loss signal, we normalize L nCE by the number of input dimensions it receives for the maximization step. The nuisance classifier is a fullyconnected 3 layer ReLU network with 512 units. As data-augmentation we use random shifts of 3 pixels. For classification errors of the different architectures we compare, see network. The first three stages consist of additive and the last of affine coupling layers. After the final layer we apply an orthogonal 2D DCT type-II to all feature maps and read out the classes in the low-pass components of the transformation. This effectively gives us an invertible global average pooling and makes our network even more similar to ResNets, that always apply global average pooling on their final feature maps. We train the network with momentum SGD for 128 epochs, a batch size of 480 (distributed to 6 GPUs), a base learning rate of 0.1, which is reduced by a factor of 0.1 every 32 epochs. We apply momentum of 0.9 and l2 weight decay of 1e-4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Connection between (1) invariance-based (long pink arrow) and (2) perturbation-based adversarial examples (short orange arrow). Class distributions are shown in green and blue; dashed line is the decision-boundary of a classifier. All adversarial examples can be reached either by crossing the decision-boundary of the classifier via perturbations, or by moving within the pre-image of the classifier to mis-classified regions. The two viewpoints are complementary to one another and highlight that adversarial vulnerability is not only caused by excessive sensitivity to semantically meaningless perturbations, but also by excessive insensitivity to semantically meaningful transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>define perturbation-based adversarial examples by introducing the notion of an oracle (e.g., a human decision-maker or the unknown input-output function considered in learning theory): Definition 2 (Perturbation-based Adversarial Examples). A Perturbation-based adversarial example x * ∈ R d of x ∈ R d fulfills: (i) Perturbation of decision: D(x * ) = o(x * ) and D(x) = D(x * ), where D : R d → {1, . . . , C} is the classifier and o : R d → {1, . . . , C} is the oracle.(ii) Created by adversary: x * ∈ R d is created by an algorithm A : R d → R d with x → x * . Further, -bounded adversarial ex. x * of x fulfill x − x * &lt; , • a norm on R d and &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 4 (</head><label>4</label><figDesc>Semantic/ Nuisance perturbation of an input). Let o be an oracle (Definition 2) and x ∈ R d . Then, a perturbation ∆x of an input x ∈ R d is called semantic, if o(x) = o(x + ∆x) and nuisance if o(x) = o(x + ∆x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Decision-boundaries in 2D subspace spanned by two random data points x 1 , x 2 . Right: Decision-boundaries in 2D subspace spanned by random datapoint x and metamer x met .</figDesc><graphic url="image-4.png" coords="5,146.35,56.35,316.78,92.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>10. By training a fully-connected fully invertible RevNet, we obtain 100% accuracy. After training we visualize the decision-boundaries of the original classifier D and a posthoc trained classifier on z n (nuisance classifier), see Figure 4. We densely sample points in a 2D subspace, following Gilmer et al. (2018b), to visualize two cases: 1) the decision-boundary on a 2D plane spanned by two randomly chosen data points, 2) the decision-boundary spanned by metameric sample x met and reference point x. In the metameric sample subspace we identify excessive invariance of the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Each column shows three images belonging together. Top row are source images from which we sample the logits, middle row are logit metamers and bottom row images from which we sample the nuisances. Top row and middle row have the same (approximately for ResNets, exactly for fully invertible RevNets) logit activations. Thus, it is possible to change the image content completely without changing the 10-and 1000-dimensional logit vectors respectively. This highlights a striking failure of classifiers to capture all task-dependent variability.</figDesc><graphic url="image-5.png" coords="5,108.00,395.43,395.92,111.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1): Theorem 6 (Information I DAdv (y; z s ) maximal after distribution shift). Let D Adv denote the adversarial distribution and D the training distribution. Assume I D (y; z n ) = 0 by minimizing L iCE and the distribution shift satisfies I D Adv (z n ; y) ≤ I D (z n ; y) and I D Adv (y; z s |z n ) ≤ I D Adv (y; z s ). Then, I D Adv (y; z s ) = I D (y; x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Left: Mutual information under distribution D train , Right: Effect of distributional shift to D Adv . Each case under training with cross-entropy (CE) and independence cross-entropy (iCE).Under distribution D, the iCE-loss minimizes I(y; z n ) (Lemma 10, Appendix C), but has no effect as the CE-loss already maximizes I(y; z s ). However under the shift to D Adv , the information I(y; z s ) decreases when training only under the CE-loss (orange arrow), while the iCE-loss induces I(y; z n ) = 0 and thus leaves I(y; z s ) unchanged (Theorem 6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Samples x = F −1 (z s , zn ) with logit activations z s taken from original image and zn obtained by linearly interpolating from the original nuisance z n (first row) to the nuisance of a target example z * n (last row upper block). The used target example is shown at the bottom. When training with cross-entropy, virtually any image can be turned into any class without changing the logits z s , illustrating strong vulnerability to invariance-based adversaries. Yet, training with independence cross-entropy solves the problem and interpolations between nuisances z n and z * n preserve the semantic content of the image.</figDesc><graphic url="image-6.png" coords="8,108.00,81.86,395.97,81.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: shiftMNIST experiments. (a): Binary shiftMNIST, where the class is additionally encoded with a location-based binary code on the left border of the image (highlighted with red circles). The shifted adversarial test distribution does not have the binary class encoding. (b): Texture shiftM-NIST, where the class is additionally encoded in background texture type. The texture-class coupling is randomized in the shifted adversarial test distribution. Right: Results of CE-trained ResNet, fully invertible RevNet and iCE-trained fully invertible RevNet. The CE-based models build excessive invariance with respect to the digit identity on D train and fail on D Adv . Difference denotes the largest improvement between CE-trained and iCE-trained model. The iCE model is more resilient to removing informative features, and reduces the error on D Adv up to 38%.</figDesc><graphic url="image-7.png" coords="8,413.86,353.38,100.10,100.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>B. 1</head><label>1</label><figDesc>Figure 9: Here we show a batch of randomly sampled metamers from our ImageNet-trained fully invertible RevNet-48. The quality is generally similar, sometimes colored artifacts appear.</figDesc><graphic url="image-12.png" coords="14,108.00,107.61,395.98,360.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.1 USING BIJECTIVE NETWORKS TO ANALYZE EXCESSIVE INVARIANCE</figDesc><table><row><cell>As invariance-based adversarial examples manifest themselves in changes which do not affect the output of the network F , we need</cell><cell></cell><cell>z s</cell><cell>z n</cell></row><row><cell>a generic approach that gives us access to the discarded nuisance vari-</cell><cell></cell><cell></cell></row><row><cell>ability. While feature nuisances are intractable to access for general ar-chitectures (see comment after Definition 1), invertible classifiers only</cell><cell></cell><cell cols="2">RevNet block x 16</cell></row><row><cell>remove nuisance variability in their final projection (Jacobsen et al.,</cell><cell></cell><cell cols="2">squeeze</cell></row><row><cell>2018). For C &lt; d, we denote the classifier as D : R d → {1, ..., C}.</cell><cell></cell><cell></cell></row><row><cell>Our contributions in this section are: (1) Introduce an invertible archi-</cell><cell></cell><cell>split</cell><cell>(optional)</cell></row><row><cell>tecture with a simplified readout structure, allowing to exactly visual-</cell><cell></cell><cell></cell></row><row><cell>ize manipulations in the hidden-space, (2) Propose an analytic attack</cell><cell>2 x</cell><cell cols="2">RevNet block x 16</cell></row><row><cell>based on this architecture allowing to analyze its decision-making, (3) Reveal striking invariance-based vulnerability in competitive classi-</cell><cell></cell><cell cols="2">squeeze</cell></row><row><cell>fiers.</cell><cell></cell><cell>x</cell></row><row><cell></cell><cell cols="3">Figure 3: The fully in-</cell></row><row><cell></cell><cell cols="3">vertible RevNet, a hybrid</cell></row><row><cell></cell><cell cols="3">of Glow and iRevNet with</cell></row><row><cell></cell><cell cols="3">simple readout structure.</cell></row><row><cell></cell><cell cols="3">z s represents the logits and</cell></row><row><cell></cell><cell cols="2">z n the nuisance.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The table shows error rates on the ILSVRC-2012 validation set of our proposed fully invertible RevNet compared to a VGG<ref type="bibr" target="#b42">(Simonyan &amp; Zisserman, 2014)</ref> and two ResNet<ref type="bibr" target="#b25">(He et al., 2016)</ref> variants, as well as an iRevNet(Jacobsen et al., 2018)  with a non-invertible final projection onto the logits. Our proposed fully invertible RevNet performs roughly on par with others.</figDesc><table><row><cell>Appendix</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>D.3 IMAGENET EXPERIMENTSWe use a convolutional fully invertible RevNet with 4 stages, 4 RevNet blocks per stage and invertible downsampling after each stage, as well as two invertible downsamplings on the input of the</figDesc><table><row><cell>MNIST</cell><cell cols="2">SOTA LeNet</cell><cell>CE</cell><cell>iCE (ours)</cell><cell>CE</cell><cell>iCE (ours)</cell></row><row><cell>Readout</cell><cell>Logit</cell><cell cols="2">Logit Logit</cell><cell>Logit</cell><cell cols="2">Nuisance Nuisance</cell></row><row><cell>% Test Error</cell><cell>0.21</cell><cell>1.70</cell><cell>0.39</cell><cell>0.38</cell><cell>0.34</cell><cell>27.70</cell></row><row><cell>% Train Error</cell><cell>-</cell><cell>-</cell><cell>0.00</cell><cell>0.37</cell><cell>0.00</cell><cell>40.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results comparing cross-entropy training (CE) with independence cross-entropy training (iCE) from Definition 5 and two architectures from the literature. The accuracy of the logit classifiers is on par for the CE and iCE networks, but the train error is higher for CE compared to test error, indicating less overfitting for iCE. Further, a classifier independently trained on the nuisance variables is able to reach even smaller error than on the logits for CE, but just 27.70% error for iCE, indicating that we have successfully removed most of the information of the label from the nuisance variables and fixed the problem of excessive invariance to semantically meaningful variability with no cost in test error.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Link to code and dataset: https://github.com/jhjacobsen/fully-invertible-revnet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head><p>We thank Ryota Tomioka for spotting a mistake in the proof for Theorem 6. We thank thank the anonymous reviewers, Ricky Chen, Will Grathwohl and Jesse Bettencourt for helpful comments on the manuscript. We gratefully acknowledge the financial support from the German Science Foundation for the CRC 1233 on "Robust Vision" and RTG 2224 "π 3 : Parameter Identification -Analysis, Algorithms, Applications"</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SEMANTIC AND NUISANCE VARIATION ON ADVERSARIAL SPHERES</head><p>Example 7 (Semantic and nuisance on Adversarial Spheres <ref type="bibr" target="#b20">(Gilmer et al., 2018b)</ref>). Consider classifying inputs x from two classes given by radii R 1 or R 2 . Further, let (r, φ) denote the spherical coordinates of x. Then, any perturbation ∆x, x * = x + ∆x with r * = r is semantic. On the other hand, if r * = r the perturbation is a nuisance with respect to the task of discriminating two spheres.</p><p>In this example, the max-margin classifier D(x) = sign x − R1+R2 2 is invariant to any nuisance perturbation, while being only sensitive to semantic perturbations. In summary, the transform to spherical coordinates allows to linearize semantic and nuisance perturbations. Using this notion, invariance-based adversarial examples can be attributed to perturbations of x * = x + ∆x with following two properties</p><p>Thus, the failure of the classifier D can be thought of a mis-alignment between its invariance (expressed through the pre-image) and the semantics of the data and task (expressed by the oracle).</p><p>Example 8 (Mis-aligned classifier on Adversarial Spheres). Consider the classifier</p><p>which computes the norm of x from its first d − 1 cartesian-coordinates. Then, D is invariant to a semantic perturbation with ∆r = R 2 − R 1 if only changes in the last coordinate x d are made.</p><p>We empirically evaluate the classifier in equation 7 on the spheres problem (10M/2M samples setting <ref type="bibr" target="#b20">(Gilmer et al., 2018b)</ref>) and validate that it can reach perfect classification accuracy. However, by construction, perturbing the invariant dimension x * d = x d + ∆x d allows us to move all samples from the inner sphere to the outer sphere. Thus, the accuracy of the classifier drops to chance level when evaluating its performance under such a distributional shift. To conclude, this underlines how classifiers with optimal performance on finite samples can exhibit non-intuitive failure modes due to excessive invariance with respect to semantic variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B APPROXIMATE GRADIENT-BASED METAMERIC SAMPLES</head><p>We use a standard Imagenet pre-trained Resnet-154 as provided by the torchvision package <ref type="bibr" target="#b35">(Paszke et al., 2017)</ref> and choose a logit percept y = G(x) that can be based on any seed image. Then we optimize various images x to be metameric to x by simply minimizing a mean squared error loss of the form:</p><p>in the 1000-dimensional semantic logit space via stochastic gradient descent. We optimize with Adam in Pytorch default settings and a learning rate of 0.01 for 3000 iterations. The optimization thus takes the form of an adversarial attack targeting all logit entries and with no norm restriction on the input distance. Note that our metameric sampling attack in bijective networks is the analytic reverse equivalent of this attack. It leads to the exact solution at the cost of one inverse pass instead of an approximate solution here at the cost of thousands of gradient steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C INFORMATION THEORY</head><p>Computing mutual information is often intractable as it requires the joint probability p(x, y), see <ref type="bibr" target="#b14">(Cover &amp; Thomas, 2006)</ref> for an extensive treatment of information theory. However, following variational lower bound can be used for approximation, see <ref type="bibr" target="#b5">(Barber &amp; Agakov, 2003)</ref>.</p><p>Lemma 9 (Variational lower bound on mutual information). Let X, Y be random variables with conditional density p(y|x). Further, let q θ (y|x) be a variational density depending on parameter θ.</p><p>Then, the lower bound</p><p>holds with equality if p(y|x) = q θ (y|x).</p><p>While above lower bound removes the need for the computation of p(y|x), estimating the expectation E Y |X still requires sampling from it. Using this bound, we can now state the effect of the nuisance classifiation loss.</p><p>Lemma 10 (Effect of nuisance classifier). Define semantics as z s = F θ (x) 1,...,C and nuisances as z n = F θ (x) C+1,...,d , where (x, y) ∼ D. Then, the nuisance classification loss yields</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for largescale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Deep variational information bottleneck. International Conference on Lerning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An information-theoretic analysis of deep latent-variable models</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analyzing inverse problems with invertible neural networks</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Lynton Ardizzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ullrich</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><surname>Kthe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The im algorithm: A variational approach to information maximization</title>
		<author>
			<persName><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systemss</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Dittmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fernsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Maaß</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09730</idno>
		<title level="m">Analysis of invariance and robustness via invertibility of relu-networks</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00995</idno>
		<title level="m">Invertible residual networks</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behar</forename><surname>Veliqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Sharada P Mohanty</surname></persName>
		</author>
		<author>
			<persName><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01976</idno>
		<title level="m">Adversarial vision challenge</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08352</idno>
		<title level="m">Catherine Olsson, Paul Christiano, and Ian Goodfellow. Unrestricted adversarial examples</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Razenshteyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10204</idno>
		<title level="m">Adversarial examples from computational constraints</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Wiley Series in Telecommunications and Signal Processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
	<note>ISBN 0471241954</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial vulnerability for any classifier</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Texture and art with deep neural networks</title>
		<author>
			<persName><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="178" to="186" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Interaction information for causal inference: The case of directed triangle</title>
		<author>
			<persName><forename type="first">Amiremad</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
		<idno>arXiv:abs/1701.08868</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06732</idno>
		<title level="m">Motivating the rules of the game for adversarial example research</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02774</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial spheres. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable reversible generative models with free-form continuous dynamics</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jörn-Henrik Jacobsen, Arnold W.M. Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks</title>
				<imprint>
			<date type="published" when="2000">2000. 2018</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1705" to="1720" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Measuring the tendency of cnns to learn surface statistical regularities</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11561</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating information</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>arXiv:abs/1511.00830</idno>
		<title level="m">The variational fair autoencoder</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Regularizing neural networks by penalizing confident output distributions. International Conference on Lerning Representations (workshop)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<title level="m">Adversarial manipulation of deep representations. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning factorial codes by predictability minimization</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. of Comp. Sci</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>University of Colorado at Boulder</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName><forename type="first">Ravid</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Ziv</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Constructing unrestricted adversarial examples with generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory Workshop (ITW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
