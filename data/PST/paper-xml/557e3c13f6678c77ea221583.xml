<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COOCCURRENCE SMOOTHING FOR STOCHASTIC LANGUAGE MODELING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ute</forename><surname>Essen</surname></persName>
							<email>essen@pfa.philips.de</email>
							<affiliation key="aff0">
								<orgName type="department">Philips GmbH Forschungslaboratorien</orgName>
								<address>
									<postBox>P. 0. Box 1980</postBox>
									<postCode>D-5100</postCode>
									<settlement>Aachen, Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Volker</forename><surname>Steinbiss</surname></persName>
							<email>steinbiss@pfa.philips.de</email>
							<affiliation key="aff0">
								<orgName type="department">Philips GmbH Forschungslaboratorien</orgName>
								<address>
									<postBox>P. 0. Box 1980</postBox>
									<postCode>D-5100</postCode>
									<settlement>Aachen, Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">COOCCURRENCE SMOOTHING FOR STOCHASTIC LANGUAGE MODELING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">566DD246ED5A7CD879B5F7DE866301A2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training corpora for stochastic language models are virtually always too small for maximum-likelihood estimation, so smoothing the models is of great importance. This paper derives the cooccurrence smoothing technique for stochastic language modeling and gives experimental evidence for its validity. Using word-bigram language models, cooccurrence smoothing improved the test-set perplexity by 14% on a German 100,000-word text corpus and by 10% on an English 1-million word corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Good stochastic language models are important for achieving high performance in large-vocabulary speech recognition. Although large text corpora are used to train these language models, the number of training observations is typically small as compared to the number of free model parameters. Many events are not observed in training and thus would be assigned zero probabilities by maximum-likelihood estimation. As on principle no word sequence should be excluded from recognition, zero probabilities must be avoided. This is achieved by smoothing the language model parameters.</p><p>We propose a novel smoothing technique for language modeling which is motivated by the cooccurrence smoothing method used for acoustic modeling <ref type="bibr" target="#b1">[2]</ref>, which was introduced by Sugawara [l]. A central point is the estimation of confusion probabilities of word pairs. The resulting confusion matrixthe cooccurrence matrixis used for smoothing the conditional word probabilities of the language model. We start with a general derivation of the cooccurrence smoothing technique for stochastic language modeling. Then, explicit formulas for the bigram model case are given. Experiments on a 100,000-word German and a 1 -million-word English corpus show the validity of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THEORY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Estimation of the Cooccurrence</head><p>In estimating the parameters of a stochastic language model on a training corpus, smoothing becomes an essential technique as training corpora are virtually always too small for maximum-likelihood estimation (if they are not, better refine the model). One possibility to obtain a more reliable estimate of the conditional probabilities of a word given some context is to take advantage of observations of other words that behave 'similarly' to this word. Let Wn be a random variable denoting word at position n and W = ..., Wn-1, Wn+l, Wn+2, ... the random variable for the Complementary word sequence. Let us assume for the moment that not one but two experiments are performed, one being marked with a prime. Thus, Wn=wn and WI;=wI; means that word w; was observed in experiment 1 and word wn in experiment 2. Assuming the context to be unknown but fixed, the probability We feel that it is not obvious how to choose the type of context and how to estimate the conditional probabilities P(w,=~,l[W]=k) in order to optimally estimate the cooccurrence matrix. A straightforward approach is to specify the context via the m-1 words directly This amounts to using a stochastic m-gram language model for calculating the cooccurrence matrix.</p><p>preceding wn , i. e. [w] = (wn-m+l, ..., wn-1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Smoothing of the Language Model</head><p>We start from a basic stochastic m-gram language model PB . In order to obtain a more robust estimate for the conditional probability of word Wn  Note that, as the m-gram model can be used to estimate the cooccurrence probabilities, in the end the model is used for smoothing itself!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Special Case: Bigram Language Model</head><p>For the bigram language model case, the smoothing formula of section The main result is summarized in Table <ref type="table" target="#tab_0">1</ref>. Method 1-A is smoothing over the words Wn (fonnula (1)) with a cooccurrence matrix _which has been estimated based on the predecessor words (A), i. e. using a standard cooccurrence matrices derived from the standard model, method l-A in a and a 'reversed' word</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>bigram context). As 1-A corpus</head><p>The improvement is larger on the and 2-B three smaller corpus I (14.4%) which is due to the fact that the word bigram probabilities were language models had to be evaluated. In comparison with our contexts (A) and (B) Of section 2*1 (a 10.3% reduction of test+et on be shown to be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Corpora</head><p>Experiments were run on two text corpora, which both were separated into a training (3/4) and an evaluation part (1/4). we only achieved minor improvements (0% -5%). This might be explained by the fact that only in case 1-A (or 2-B, resp.) exactly the relation between Wn and wml is modelled. In</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. Interpolation parameters as estimated with the leaving-one-out method flom the respective training portions (method I -A).</head><p>consequence of these results we did not try method (3), which in addition leads to time / memory problems.</p><p>Going back to method 1-A, Table <ref type="table">2</ref> shows the relative contributions of the partial language models. The cooccurrence-smoothed bigram almost replaces the unigram part, but 1 the zerogram partthe constant value (V = vocabulary size)remains important: both Pc(w, I . ) andPg (wn I . ) are zero for words w, not observed in the training section. Table <ref type="table" target="#tab_1">3</ref> indicates that the gain of the interpolated cooccurrence-smoothed model 1-A lies in the bigrams that have not been observed in the training data while both words have been observed in other contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS AND FUTURE WORK</head><p>Cooccurrence smoothing has so far successfully been applied in acoustic modeling for the smoothing of discrete probabilities of hidden Markov models. We have derived the cooccurrence smoothing technique for stochastic language modeling and have shown </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>IW</head><label></label><figDesc>-0532-9192 $3.00 0 1992 IEEE rr ~ Our measure of similarity is the probability of word w' being substituted by word w, so we estimate how likely w is to be observed in the same contexts in which w' has been seen. Here, a context k is an equivalence class [w] of the complementary word sequence preceding and following the word Wn under consideration. Two simple examples are the contexts defied by = ..., Wn-2, Wn-1, Wn+l, Wn+2, ... the predecessor word: [w] = Wn-1 , or the successor word: [w] = wn+l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>of this pair of observations is modelled by P (Wn=wn, WA-3 = P (Wn=wn, W;=W~ I [W]=[W'l) = c P ( w * w &amp; I m = m P m contexts k = c p W e n I I Y W P W n t Y IYWOPOYM k The confusion probabilities, forming the cooccurrence matrix, then are PC (WnWn I WAWA) k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>following the word sequence Wn-m+1 9 9Wn-I 9 PB (wn I Wn-m+l? --wn-1) we take account of conditional probabilities of words w; that behave similarly to Wn . Using the confusion probabilities derived above, the cooccurrence-smoothed probabilities Ps are defied as PC (Wn-n I W + W ~ =: PC (wn I ~2 PS (wn I Wn-m+l, C PC (wn I ~2 PB (w; I Wn-m+l, a -. 9Wn-1) 9Wn-1) = w;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 . 2 3</head><label>223</label><figDesc>tums into W W n I wn-1) = C PC(wn I WJ MWA I wn-1) (1) WA Instead of smoothing over the observations Wn, an interesting variant in the bigram case is to Results We compared three variants of cooccurrence smoothing with each other and with a standard model. The standard model is the linear interpolation of a bigram, a unigram and a zerogram (or floor) model; the three variants to be compared were obtained by additionally interpolating with a cooccurrence-smoothed bigram component. All interpolation parameters were exclusively estimated on the training section of the corpus using the leaving-one-out method [3].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>I is a German text corpus of section 3.2. IMethod !Standard1 1-A I 1-B I 2-A I I Corpus1 1 1 696 I 596 I 696 I 674 I newspaper articles comprising 100,000 words. Corpus II is an English corpus (the LOB Corpus) comprising 1 million words from a more heterogeneous text collection. With the two other methods (1-B and 2-A)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Evaluation of test-set perplexities for probabilities using linear interpolation. three variants of cooccurrence smoothing on two text corpora, which are described in</head><label>1</label><figDesc></figDesc><table><row><cell>In Our experiments we combined the</cell><cell></cell></row><row><cell>(l) and (2) with two</cell><cell>bigram</cell></row><row><cell>We used non-smoothed maximum-likeli-</cell><cell>less reliably estimated: The fraction of word</cell></row><row><cell>hood estimates for the bigram and unigram</cell><cell>bigrams observed in the test partition that were</cell></row><row><cell>probabilities (i. e. relative frequencies) in</cell><cell>not in the training partition is 50% for corpus I</cell></row><row><cell>order to separate the effects of different</cell><cell>and 12% for corpus II.</cell></row><row><cell>smoothing methods. In the experiments we</cell><cell></cell></row><row><cell>combined the cooccurrence-smoothed bigram</cell><cell></cell></row><row><cell>with the bigram, unigram and zerogram</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The dzTerences in test-set perplexities between the standard method and method I-A, evaluated on the following (overlapping) subsets: Bigrams where a) the predecessor Wn-1 b) the word W n itself c) the bigram (wn-l , W,) were not observed in training. (Negative figures indicate improvements.) its validity by experiments on two corpora:The 1-million word English LOB corpus and a 100,000-word German newspaper text corpus. Test-set perplexities were improved by 10.3% and 14.4%. We plan to further evaluate the new method by speech recognition experiments.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Isolated Word Recognition Using Hidden Markov Models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okochi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing<address><addrLine>Tampa, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985-03">March 1985</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust Smoothing Methods for Discrete Hidden Markov Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Kubala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing<address><addrLine>Glasgow</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-04">April 1989</date>
			<biblScope unit="page" from="548" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On Smoothing Techniques for Bigram-Based Natural Language Modelling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing<address><addrLine>Toronto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="825" to="828" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
