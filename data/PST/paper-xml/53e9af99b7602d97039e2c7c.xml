<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correlation Clustering with a Fixed Number of Clusters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-04-06">6 Apr 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ioannis</forename><surname>Giotis</surname></persName>
							<email>fgiotis@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Venkatesan</forename><surname>Guruswami</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Correlation Clustering with a Fixed Number of Clusters</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-04-06">6 Apr 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">587641CE3B5A5408B3AE1105D0A2453B</idno>
					<idno type="arXiv">arXiv:cs/0504023v1[cs.DS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We continue the investigation of problems concerning correlation clustering or clustering with qualitative information, which is a clustering formulation that has been studied recently <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2]</ref>. The basic setup here is that we are given as input a complete graph on n nodes (which correspond to nodes to be clustered) whose edges are labeled + (for similar pairs of items) and (for dissimilar pairs of items). Thus we have only as input qualitative information on similarity and no quantitative distance measure between items. The quality of a clustering is measured in terms of its number of agreements, which is simply the number of edges it correctly classifies, that is the sum of number of edges whose endpoints it places in different clusters plus the number of + edges both of whose endpoints it places within the same cluster.</p><p>In this paper, we study the problem of finding clusterings that maximize the number of agreements, and the complementary minimization version where we seek clusterings that minimize the number of disagreements. We focus on the situation when the number of clusters is stipulated to be a small constant k. Our main result is that for every k, there is a polynomial time approximation scheme for both maximizing agreements and minimizing disagreements. (The problems are NP-hard for every k 2.) The main technical work is for the minimization version, as the PTAS for maximizing agreements follows along the lines of the property tester for Max k-CUT from <ref type="bibr" target="#b10">[11]</ref>.</p><p>In contrast, when the number of clusters is not specified, the problem of minimizing disagreements was shown to be APX-hard <ref type="bibr" target="#b4">[5]</ref>, even though the maximization version admits a PTAS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this work, we continue the investigation of problems concerning an appealing formulation of clustering called correlation clustering or clustering using qualitative information that has been studied recently in several works, including <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2]</ref>. The basic setup here is to cluster a collection of n items given as input only qualitative information concerning similarity between pairs of items; specifically for every pair of items, we are given a (Boolean) label as to whether those items are similar or dissimilar. We are not provided with any quantitative information on how different pairs of elements are, as is typically assumed in most clustering formulations. These formulations take as input a metric on the items and then aim to optimize some function of the pairwise distances of the items within and across clusters. The objective in our formulation is to produce a partitioning into clusters that places similar objects in the same cluster and dissimilar objects in different clusters, to the extent possible.</p><p>An obvious graph-theoretic formulation of the problem is the following: given a complete graph on n nodes with each edge labeled either "+" (similar) or " " (dissimilar), find a partitioning of the vertices into clusters that agrees as much as possible with the edge labels. The maximization version, call it MAXAGREE seeks to maximize the number of agreements: the number of + edges inside clusters plus the number of edges across clusters. The minimization version, denoted MINDISAGREE, aims to minimize the number of disagreements: the number of edges within clusters plus the number of + edges between clusters.</p><p>In this paper, we study the above problems when the maximum number of clusters that we are allowed to use is stipulated to be a fixed constant k. We denote the variants of the above problems that have this constraint as MAXAGREE <ref type="bibr">[k]</ref> and MINDISAGREE <ref type="bibr">[k]</ref>. We note that, unlike most clustering formulations, the MAXAGREE and MINDISAGREE problems are not trivialized if we do not specify the number of clusters k as a parameter -whether the best clustering uses few or many clusters is automatically dictated by the edge labels. However, the variants we study are also interesting formulations, which are well-motivated in settings where the number of clusters might be an external constraint that has to be met, even if there are "better" clusterings (i.e., one with more agreements) with a different number of clusters. Moreover, the existing algorithms for, say MINDISAGREE, cannot be modified in any easy way to output a quality solution with at most k clusters. Therefore k-clustering variants pose new, non-trivial challenges that require different techniques for their solutions.</p><p>In the above description, we have assumed that every pair of items is labeled as + or in the input. In a more general variant, intended to capture situations where the classifier providing the input might be unable to label certain pairs of elements are similar or dissimilar, the input is an arbitrary graph G together with labels on its edges. We can again study the above problems MAXAGREE[k] (resp. MINDISAGREE <ref type="bibr">[k]</ref>) with the objective being to maximize (resp. minimize) the number of agreements (resp. disagreements) on edges of E (that is, we do not count non-edges of G as either agreements or disagreements). In situations where we study this more general variant, we will refer to these problems as MAXAGREE[k] on general graphs and MINDISAGREE[k] on general graphs. When we don't qualify with the phrase "on general graphs", we will always mean the problems on complete graphs.</p><p>Our main result in this paper is a polynomial time approximation scheme (PTAS) for MAXAGREE <ref type="bibr">[k]</ref> as well as MINDISAGREE[k] for k 2. We now discuss prior work on these problems, followed by a more detailed description of results in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous and related work</head><p>The above problem seems to have been first considered by Ben-Dor et al. <ref type="bibr" target="#b3">[4]</ref> motivated by some computational biology questions. Later, Shamir et al. <ref type="bibr" target="#b13">[14]</ref> studied the computational complexity of the problem and showed that MAXAGREE (and hence also MINDISAGREE), as well as MAXAGREE[k] (and hence also MINDISAGREE[k]) for each k 2 is NP-hard. They, however, used the term "Cluster Editing" to refer to this problem.</p><p>Partially motivated by some machine learning problems concerning document classification, Bansal, Blum, and Chawla <ref type="bibr" target="#b2">[3]</ref> also independently formulated and considered this problem. In particular, they initiated the study of approximate solutions to MINDISAGREE and MAXAGREE, and presented a PTAS for MAXAGREE and a constant factor approximation algorithm for MINDIS-AGREE (the approximation guarantee was a rather large constant, though). They also noted a simple factor 3 approximation algorithm for MINDISAGREE <ref type="bibr" target="#b1">[2]</ref>. Charikar, Guruswami and Wirth <ref type="bibr" target="#b4">[5]</ref> proved that MINDISAGREE is APX-hard, and thus one cannot expect a PTAS for the minimization problem similar to the PTAS for MAXAGREE. They also gave a factor 4 approximation algorithm for MINDISAGREE by rounding a natural linear programming relaxation using the region growing technique.</p><p>The problems on general graphs have also received attention. It is known that both MAXA-GREE and MINDISAGREE are APX-hard <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Using a connection to minimum multicut, several groups <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> presented an O(log n) approximation algorithm for MINDISAGREE. In fact, it was noted in <ref type="bibr" target="#b9">[10]</ref> that the problem is as hard to approximate as minimum multicut (and so this log n factor seems very hard to improve). For the maximization version, algorithms with performance ratio better than 0:766 are known for MAXAGREE <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. The latter work by Swamy <ref type="bibr" target="#b14">[15]</ref> shows that a factor 0:7666 approximation can also be achieved when the number of clusters is specified (i.e., for MAXAGREE[k] for k 2).</p><p>Another problem that has been considered, let us call it MAXCORR, is that of maximizing correlation, defined to be the difference between the number of agreements and disagreements. A factor O(log n) approximation for MAXCORR on complete graphs is presented in <ref type="bibr" target="#b5">[6]</ref>. Recently <ref type="bibr" target="#b1">[2]</ref> showed an integrality gap of (log n) for the underlying semidefinite program used in <ref type="bibr" target="#b5">[6]</ref>. They also prove that an approximation of O(log (G)) can be achieved on general graphs G, where ( ) is the Lov√°sz Theta Function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our results</head><p>The only previous approximation for MINDISAGREE <ref type="bibr">[k]</ref> was a factor 3 approximation algorithm for the case k = 2 <ref type="bibr" target="#b2">[3]</ref>. The problems were shown to be NP-hard for every k 2 in [14] using a rather complicated reduction. In this paper, we will provide a much simpler NP-hardness proof and prove that both MAXAGREE[k] and MINDISAGREE[k] admit a polynomial time approxima-tion scheme for every k 2. <ref type="foot" target="#foot_0">1</ref> These approximation schemes are presented in Section 3 and 4 respectively. The existence of a PTAS for MINDISAGREE[k] is perhaps surprising in light of the APX-hardness of MINDISAGREE when the number of clusters is not specified to be a constant (recall that the maximization version does admit a PTAS even when k is not specified).</p><p>It is often the case that minimization versions of problems are harder to solve compared to their complentary maximization versions. The APX-hardness of MINDISAGREE despite the existence of a PTAS for MAXAGREE is a notable example. The difficulty in these cases is when the optimum value of the minimization version is very small, since then even a PTAS for the complementary maximization problem need not provide a good approximation for the minimization problem. In this work, we first give a PTAS for MAXAGREE <ref type="bibr">[k]</ref>. This algorithm uses random sampling and follows closely along the lines of the property testing algorithm for Max k-Cut due to <ref type="bibr" target="#b10">[11]</ref>. We then develop a PTAS for MINDISAGREE <ref type="bibr">[k]</ref>, which is our main result. This requires more work and the algorithm returns the better of two solutions, one of which is obtained using the PTAS for MAXAGREE <ref type="bibr">[k]</ref>.</p><p>The difficulty in getting a PTAS for the minimization version is similar to that faced in the problem of Min k-sum clustering, which has the complementary objective function to Metric Max k-Cut. We remark that while an elegant PTAS for Metric Max k-Cut due to de la Vega and Kenyon <ref type="bibr" target="#b7">[8]</ref> has been known for several years, only recently has a PTAS for Min k-sum clustering been obtained <ref type="bibr" target="#b6">[7]</ref>. We note that the case of Min 2-sum clustering though was solved in <ref type="bibr" target="#b11">[12]</ref> soon after the Metric Max Cut algorithm of <ref type="bibr" target="#b7">[8]</ref>, but the case k &gt; 2 appeared harder. Similarly to this, for MINDISAGREE[k], we are able to quite easily give a PTAS for the 2-clustering version using the algorithm for MAXAGREE <ref type="bibr" target="#b1">[2]</ref>, but we have to work harder for the case of k &gt; 2 clusters. Some of the difficulty that surfaces when k &gt; 2 is detailed in Section 4.1.</p><p>In Section 5, we also note some results on the complexity of MAXAGREE[k] and MINDISAGREE[k] on general graphs -these are easy consequences of connections to problems like Max CUT and graph colorability.</p><p>Our work seems to nicely complete the understanding of the complexity of problems related to correlation clustering. Our algorithms not only achieve excellent approximation guarantees but are also sampling-based and are thus simple, combinatorial, and quite easy to implement. To compare with the situation for the case when k is not specified, the algorithm for MINDISAGREE in <ref type="bibr" target="#b2">[3]</ref> achieves a very large approximation factor. On the other hand, the algorithm in <ref type="bibr" target="#b4">[5]</ref> achieves a good factor of 4 but needs to solve a linear programming relaxation. In fact it could well be that on some instances we can find a better solution compared to what the algorithm of <ref type="bibr" target="#b4">[5]</ref> produces even when k isn't specified by trying our algorithm for some small values of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NP-hardness of MINDISAGREE and MAXAGREE</head><p>In this section we show that the exact versions of problems we are trying to solve are NP-hard. An NP-hardness result for MAXAGREE on complete graphs was shown in <ref type="bibr" target="#b2">[3]</ref>; however their reduction crucially relies on the number of clusters growing with the input size, and thus does not yield any hardness when the number of clusters is a fixed constant k. It was shown by Shamir, Sharan, and Tsur <ref type="bibr" target="#b13">[14]</ref>, using a rather complicated reduction, that these problems are NP-hard for each fixed number k 2 of clusters. We will provide a short and intuitive proof that MINDISAGREE[k] and MAXAGREE[k] are NP-hard.</p><p>Clearly it suffices to establish the NP-hardness of MINDISAGREE[k] since MAXAGREE[k] can be easily reduced on a complimentary graph. We will first establish NP-hardness for k = 2, the case for general k will follow by a simple "padding" with (k 2) large collection of nodes with + edges between nodes in each collection and edges to everywhere else.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1 MINDISAGREE[2] on complete graphs is NP-hard.</head><p>Proof : We know that Graph Min Bisection, namely partitioning the vertex set of a graph into two equal halves so that the number of edges connecting vertices in different halves is minimized, is NP-hard. From an instance G of Min Bisection with n(even) vertices we obtain a complete graph G 0 using the following polynomial time construction.</p><p>Start with G and label all existing edges of G as + edges in G 0 and non-existing edges as edges. For each vertex v create an additional set of n vertices. Let's call these vertices together with v, a "group" V v . Connect with + edges all pairs of vertices within V v . All other edges with one endpoint in V v as labeled as edges (except those already labeled).</p><p>We will now show that any 2-clustering of G 0 with the minimum number of disagreements, has 2 clusters of equal size with all vertices of any group in the same cluster. Consider some optimal 2-clustering W with 2 clusters W 1 and W 2 such that jW 1 j 6 = jW 2 j or not all vertices of some group are in the same cluster. Pick some group V v such that not all its vertices are assigned in the same cluster. If such a group cannot be found, pick a group V v from the larger cluster. Place all the vertices of the group in the same cluster obtaining W 0 such that jjW 0</p><formula xml:id="formula_0">1 j jW 0 2 jj is minimized. Let's assume that V 1 v vertices of group V v were in W 1 and V 2 v in W 2 . Wlog, let's assume that W 0 is obtained by moving the V 1 v group vertices in cluster W 2 . W 0 1 = W 1 n V 1 v ; W 0 2 = W 2 [ V 1 v</formula><p>We now observe the following facts about the difference in the number of disagreements between W 0 and W .</p><p>Clearly the number of disagreements between vertices not in V v and between one vertex in</p><formula xml:id="formula_1">V 2</formula><p>v with one in W 0 1 remains the same. The number of disagreements is decreased by jV 1 v j jV 2 v j based on the fact that all edges within V v are + edges.</p><p>It is also decreased by at least jV 1 v j jW 0 1 j (n 1) based on the fact that all but at most n 1 edges connecting vertices of V v to the rest of the graph are edges.</p><p>The number of disagreements increases at most jV 1 v j jW 2 n V 2 v j because (possibly) all of the vertices in V 1 v are connected with edges with vertices in W 2 outside their group.</p><p>Overall, the difference in the number of disagreements is at most jV 1 v j jW 2 n V<ref type="foot" target="#foot_1">2</ref> v j jV 1 v j jV 2 v j jV 1 v j jW 0 1 j + (n 1). Notice that since jjW 0 1 j jW 0 2 jj was minimized it must be the case that jW 0 1 j jW 2 n V 2 v j. Moreover since a group has an odd number of vertices and the total number of vertices of G 0 is even, it follows that jW 0 1 j 6 = jW 2 nV 2 v j and jW 0 1 j jW 2 nV 2 v j 1. Therefore the total number of disagreements increases at most (n 1)</p><formula xml:id="formula_2">jV 1 v j (jV 2 v j + 1). Since jV 1 v j + jV 2 v j = n + 1 and V 1 v cannot be empty, it follows that jV 1 v j (jV 2 v j + 1)</formula><p>n and the number of disagreements strictly decreases contradicting the optimality of W .</p><p>Therefore the optimal solution to the MINDISAGREE <ref type="bibr" target="#b1">[2]</ref> instance has 2 clusters of equal size and all vertices of any group are contained in a single cluster. It is now trivial to see that an optimal solution to the Min Bisection problem can be easily derived from the MINDISAGREE <ref type="bibr" target="#b1">[2]</ref> solution which completes the reduction.</p><p>We are now able to easily derive the following NP-hardness result. Proof : Consider an instance of the MINDISAGREE <ref type="bibr" target="#b1">[2]</ref> problem on a graph G with n vertices. Create a graph G 0 by adding to G, k 2 "groups" of n + 1 vertices each. All edges within a group are marked as + edges, while the remaining edges are marked as edges.</p><p>Consider now a k-clustering of G 0 such that the number of disagreements is minimized. It is easy to see that all the vertices of a group must make up one cluster. Also observe that any of the original vertices cannot end up in one group's cluster since that would induce n + 1 disagreements, stricly more than it could possibly induce in any of the 2 remaining clusters. Therefore the 2 non-group clusters are an optimal 2-clustering of G. The theorem easily follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PTAS for maximizing agreement with k clusters</head><p>In this section we will present a PTAS for MAXAGREE[k] for every fixed constant k. Our algorithm follows closely the PTAS for Max k-CUT by Goldreich et al. <ref type="bibr" target="#b10">[11]</ref>. In the next section, we will present our main result, namely a PTAS for MINDISAGREE[k], using the PTAS for MAXAGREE[k] together with additional ideas.  Proof : We first note that for every k 2, and every instance of MAXAGREE[k], the optimum number OPT of agreements is at least n 2 =16. Let n + be the number of positive edges, and n = n 2 n + be the number of negative edges. By placing all vertices in a single cluster, we get n + agreements. By placing vertices randomly in one of k clusters, we get an expected (1 1=k)n agreements just on the negative edges. Therefore OPT maxfn</p><formula xml:id="formula_3">+ ; (1 1=k)n g (1 1=k) n 2 =2 n 2 =16.</formula><p>The proof now follows from Theorem 4 which guarantees a solution within additive "n 2 of OPT for arbitrary " &gt; 0.</p><p>Theorem 4 On input ", and a labeling L of the edges of a complete graph G with n vertices, with probability at least 1 , algorithm MaxAg outputs a k-clustering of the graph such that the number of agreements induced by this k-clustering is at least OPT "n 2 =2, where OPT is the optimal number of agreements induced by any k-clustering of G. The running time of the algorithm is n " k O(" 2 log k log(1=" )) . The proof of this theorem is presented in Section 3.2, and we now proceed to describe the algorithm.</p><p>Algorithm MaxAg(k; "): Input: A labeling L : n 2 ! f+; g of the edges of the complete graph on vertex set V . Output: A k-clustering of the graph, i.e., a partition of V into (at most) k parts V 1 ; V 2 ; : : : ; V k .</p><p>1. Construct an arbitrary partition of the graph (V 1 ; V 2 ; : : : ; V m ); m = d 4 " e. 2. For i = 1 : : : m, choose uniformly at random with replacement from V n V i , a subset S i of size r = 1 " 2 log 1 " log k . 3. For i = 1 : : : m do the following (a) For each clustering of S i into (S i 1 ; : : :</p><formula xml:id="formula_4">; S i k ) do the following (i) For each vertex v 2 V i do (1) For j = 1 : : : k, let j (v) = j + (v) \ S i j j + P l6 =j j (v) \ S i l j. (2) Place v in W i j for which j (v) is maximized. (ii)</formula><p>If the clustering on the subgraph induced by the edges between V j and S i has more agreements than the currently stored one, store this clustering as W i (W i 1 ; : : :</p><formula xml:id="formula_5">; W i k ) 4. For j = 1 : : : k, let W j [ i W i j .</formula><p>Output clustering (W 1 ; : : : ; W k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Our algorithm is given a complete graph G(V; E) on n vertices. All the edges are marked as + or , denoting whether adjacent vertices are on agreement or disagreement respectively. For a vertex v, let + (v) be the set of vertices adjacent to v via + edges, and (v) the set of vertices adjacent to v via edges. The algorithm works in m = O(1=") steps. At each step we are placing ("n) vertices into clusters. We will show that with constant probability our choices of S i 's will allow us to place the vertices in such a way that the decrease in the number of agreements with respect to an optimal clustering is O(" 2 n 2 ) per step, thus the algorithm outputs a solution that has O("n 2 ) less agreements than any optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance analysis of MaxAg(k; ") algorithm</head><p>Consider an arbitrary optimal k-clustering of the graph D (D 1 ; : : : ; D k ). We consider the subsets of each cluster over our partition of vertices, defined as for j = 1; : :</p><formula xml:id="formula_6">: k; D i j D j \ V i D i (D i 1 ; : : : ; D i k )</formula><p>We will now define a sequence of hybrid clusterings, such that hybrid clustering H i , for i = 1; 2; : : : ; m + 1, consists of the vertices as clustered by our algorithm up to (not including) the i'th step and the rest of the vertices as clustered by D.</p><formula xml:id="formula_7">H i (H i 1 ; : : : ; H i k ) H i (H i 1 ; : : : ; H i k ) for j = 1; : : : k; H i j ([ i 1 l=1 W l j ) [ ([ m l=i D l j ) for j = 1; : : : k; H i j H i</formula><p>j n V i Although we will go through all possible clusterings of S i , for the rest of the analysis consider the particular clustering that matches the hybrid clusterings, for j = 1 : : : k; S i j</p><formula xml:id="formula_8">S i \ H i j</formula><p>The following theorem captures the fact that our random sample with high probability gives us a good estimate on the number of agreements towards each cluster for most of the vertices considered.</p><p>Lemma 5 For i = 1 : : : m, with probability at least 1 ( =4m) on the choice of S i , for all but at most an "=8 fraction of the vertices v 2 V i , the following holds for j = 1; : : : k;</p><formula xml:id="formula_9">j + (v) \ S i j j r j + (v) \ H i j j jV n V i j " 32 :<label>(1)</label></formula><p>(Note that if <ref type="bibr" target="#b0">(1)</ref> above holds, then it also holds with (v) in place of + (v).)</p><p>Proof : Consider an arbitrary vertex v 2 V i and the randomly chosen set S i = fu 1 ; : : : ; u r g. For each j 2 f1; : : : ; kg, we define the random variables for l = 1; : : : r; l j = 1; if u l 2 + (v) \ S i j ; 0; otherwise:</p><formula xml:id="formula_10">Clearly P r l=1 l j = j + (v) \ S i j j and P r[ l j = 1] = j + (v)\H i j j</formula><p>jV nV i j . Using an additive Chernoff bound we get that</p><formula xml:id="formula_11">P r j + (v) \ S i j j r j + (v) \ H i j j jV n V i j &gt; " 32 &lt; 2 exp( 2( " 32 ) 2 r) &lt; " 32mk<label>(2)</label></formula><p>Defining a random variable to count the number of vertices not satisfying inequality(1) and using Markov's inequality we get that for that particular j, inequality(1) holds for all but a fraction "=8 of vertices v 2 V i , with probability at least 1 ( =4mk). Using a probability union bound the lemma easily follows.</p><p>We define agree(A) to be equal to the number of agreements induced by k-clustering A. Now consider the placement of V i vertices in clusters W i 1 ; : : : ; W i k as performed by the algorithm during step i. We will examine the number of agreements compared to the placement of the same vertices under H i (placement under the optimal clustering), more specifically we will bound the difference in the number of agreements induced by placing vertices differently than H i . The following lemma formalizes this concept. Lemma 6 For i = 0; : : : m, we have agree(H i+1 ) agree(D) i 1 8 " 2 n 2 Proof : Observe that H 1 D and H m+1 W . The only vertices placed differently between H i+1 and H i are the vertices in V i . Suppose that our algorithm places v 2 V i in cluster x and v is placed in cluster x 0 under H i . For each vertex v the number of agreements towards clusters other than x; x 0 remains the same, therefore we will focus on the number of agreements towards these two clusters and the number of agreements within V i .</p><p>The number of agreements we could lose by thus misplacing v is</p><formula xml:id="formula_12">di xx 0 (v) = j + (v) \ H i x 0 j j + (v) \ H i x j + j (v) \ H i x j j (v) \ H i</formula><p>x 0 j Since our algorithm chose cluster x, by construction</p><formula xml:id="formula_13">j + (v) \ S i x j + j (v) \ S i x 0 j j + (v) \ S i x 0 j + j (v) \ S i x j<label>(3)</label></formula><p>If inequality (1) holds for vertex v, using it for + (v) and (v) in both clusters x,x 0 , we obtain bounds on the difference of agreements between our random sample's clusters S i x ; S i x 0 and the hybrid clusters H i x ; H i x 0 . Combining with inequality (3) we get that di xx 0 (v) is at most 1 8 "n. Therefore the total decrease in the number of agreements by this type of vertices is at most</p><formula xml:id="formula_14">1 8 "njV i j 1 8 " n 2 m</formula><p>. By Lemma 5 there are at most ("=8)jV i j vertices in V i for which inequality (1) doesn't hold. The total number of agreements originating from these vertices is at most 1  8 "jV i jn 1 8 " n 2 m . Finally, the total number of agreements from within V i is at most jV i j 2 1  4 " n 2 m . Overall the number of agreements that we could lose in one step of the algorithm is at most</p><formula xml:id="formula_15">1 2 " n 2 m 1 8 " 2 n 2 .</formula><p>The lemma follows by induction. The approximation guarantee of Theorem 4 easily follows from Lemma 6. At each step of the algorithm we need to go over all k r k-clusterings of our random sample S i and there are O(n=") steps. All other operations within a step can be easily implemented to run in constant time and the running time bound of our algorithm follows as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PTAS for minimizing disagreements with k clusters</head><p>This section is devoted to the proof of the following theorem, which is our main result in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 7 (Main) For every k 2, there is a PTAS for MINDISAGREE[k].</head><p>The algorithm for MINDISAGREE[k] will use the approximation scheme for MAXAGREE[k] as a subroutine. The latter already provides a very good approximation for the number of disagreements unless this number is very small. So in the analysis, the main work is for the case when the optimum clustering is right on most of the edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Idea behind the algorithm</head><p>The case of 2-clusters turns out to be lot simpler and we use it to first illustrate the basic idea. By the PTAS for maximization, we only need to focus on the case when the optimum clustering has only OPT = n 2 disagreements for some small &gt; 0. We draw a random sample S and try all partitions of it, and focus on the run when we guess the right partition S = S 1 [ S 2 , namely the way some fixed optimal clustering D partitions S. Since the optimum has very large number of agreements, each node in a set A of size at least (1 O( ))n will have a clear choice of which side they prefer to be on and we can find this out with high probability based on edges into S. Therefore, we can find a clustering which agrees with D on a set A of at least 1 O( ) fraction of the nodes. We can then go through this clustering and for each node in parallel switch it to the other side if that improves the solution to produce the final clustering. Nodes in A won't get switched and will remain clustered exactly as in the optimum D. The number of extra disagreements compared to D on edges amongst nodes in V</p><p>A is obviously at most the number of those edges which is O( 2 n 2 ). For edges connecting a node u 2 V A to nodes in A, since we placed u on the "better" side, and A is placed exactly as in D in the final clustering, we can have at most O( n) extra disagreements per node compared to D, simply the error introduced by the edges towards the misplaced nodes in V A. Therefore we get a clustering with at most</p><formula xml:id="formula_16">OPT + O( 2 n 2 ) (1 + O( ))OPT disagreements.</formula><p>Our algorithm for k-clustering for k &gt; 2 uses a similar high-level approach, but is more complicated. The main thing which breaks down compared to the k = 2 case is the following. For two clusters, if D has agreements on a large, i.e. (1 O( )), fraction of edges incident on a node u (i.e. if u 2 A in the above notation), then we are guaranteed to place u exactly as in D based on the sample S (when we guess its correct clustering), since the other option will have much poorer agreement. This is not the case when k &gt; 2, and one can get a large number of agreements by placing a node in say one of two possible clusters. It therefore does not seem possible to argue that each node in A is correctly placed, and then to use this to finish off the clustering.</p><p>However, what we can show is that nodes in A that are incorrectly placed, call this set B, must be in small clusters of D, and thus are few (at most O(n=k)) in number. Moreover, every node in A that falls in one of the large clusters that we produce, is guaranteed to be correctly placed. (These facts are the content of Lemma 10.) The nodes in B still need to be clustered, and since they could be of size (n=k), even a small number of mistakes per node in clustering them is more than we can afford. We get around this predicament by noting that nodes in B and A B are in different sets of clusters in D. It follows that we can cluster B recursively in new clusters (and we are making progress because B is clustered using fewer than k clusters). The actual algorithm must also deal with nodes outside A, and in particular decide which of these nodes are recursively clustered along with B. With this intuition in place, we now proceed to the formal specification of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithm for k-clustering to minimize disagreements</head><p>The following is the algorithm that gives a factor (1 + ") approximation for MINDISAGREE[k]. We will use a small enough absolute constant c 1 in the algorithm; the choice c 1 = 1=20 will work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm MinDisAg(k; "):</head><p>Input: A labeling L : n 2 ! f+; g of the edges of the complete graph on vertex set V = f1; 2; : : : ; ng Output: A k-clustering of the graph, i.e., a partition of V into (at most) k parts V 1 ; V 2 ; : : : ; V k . 0. If k = 1, return the obvious 1-clustering. 1. Run the PTAS for MAXAGREE[k] from previous section on input L with accuracy </p><formula xml:id="formula_17">C i = S i for 1 i k. (b) For each u 2 V S (i)</formula><p>For each i = 1; 2; : : : ; k, compute pval S (u; i), defined to be 1=jSj times the number of agreements on edges connecting u to nodes in S if u is placed in cluster i along with S i . (ii) Let j u = arg max i pval S (u; i), and val S (u) </p><formula xml:id="formula_18">= W 0 1 [ W 0 2 [ [ W 0 s</formula><p>(where some of the W 0 i 's may be empty) (e) Let C be the clustering comprising of the k clusters fC j g j2Large and fW 0 i g 1 i s . If the number of agreements of C is at least ClusVal, update ClusVal to this value, and update ClusMin C. 5. Output the better of the two clusterings ClusMax and ClusMin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance analysis of the algorithm</head><p>We now analyze the approximation guarantee of the above algorithm. We need some notation. Let</p><formula xml:id="formula_19">A = A 1 [ A 2 [</formula><p>A k be any k-clustering of the nodes in V . Define the function val A : V ! [0; 1] as follows: val A (u) equals the fraction of edges incident upon node u whose labels agree with clustering A (i.e., we count negative edges that are cut by A and positive edges that lie within the same A i for some i). Also define disagr(A) to be the number of disagreements of A w.r.t labeling L. (Clearly disagr(A) = n 1 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P</head><p>u2V (1 val A (u)).) For a node u 2 V and 1 i k, let A (u;i) denote the clustering obtained from A by moving u to A i and leaving all other nodes untouched. We define the function pval A : V f1; 2; : : : ; kg ! [0; 1] as follows: pval A (u; i) equals the fraction of edges incident upon u that agree with the clustering A (u;i) .</p><p>In the following, we fix D to be any optimal k-clustering that partitions V as</p><formula xml:id="formula_20">V = D 1 [ D 2 [ [ D k .</formula><p>Let be defined to be so that disagr(D)=n 2 , i.e., the clustering D has n 2 disagreements w.r.t. the input labeling L.</p><p>Call a sample S of nodes, each drawn uniformly at random with replacement, to be -good if the nodes in S are distinct<ref type="foot" target="#foot_2">3</ref> and for each u 2 V and i 2 f1; 2; : : : ; kg, jpval S (u; i) pval D (u; i)j (4)</p><p>for the partition S of S as [ k i=1 S i with S i = S \ D i (where pval S ( ; ) is as defined in the algorithm). The following lemma follows by a standard Chernoff and union bound argument similar to Lemma 5.  Therefore, in what follows we assume that the sample S is -good. In the rest of the discussion, we focus on the run of the algorithm for the partition S of S that agrees with the optimal partition D, i.e., S i = S \ D i . (All lemmas stated apply for this run of the algorithm, though we don't make this explicit in the statement.) Let (C 1 ; C 2 ; : : : ; C k ) be the clusters produced by the algorithm at end of Step 4(c) on this run. Let's begin with the following simple observation. Proof : Note that since u 2 D s , val D (u) = pval D (u; s). By the -goodness of S (recall Equation ( <ref type="formula">4</ref>)), pval S (u; s) pval D (u; s)</p><p>. Since we chose to place u in C r instead of C s , we must have pval S (u; r) pval S (u; s). By the -goodness of S again, we have pval D (u; r) pval S (u; r) . Combining these three inequalities gives us the claim of the lemma.</p><p>Define the set of nodes of low value in the optimal clustering D as T low def = fu j val D (u) 1 c 1 =k 2 g. The total number of disagreements is at least the number of disagreements induced by these low valued nodes, therefore</p><formula xml:id="formula_21">jT low j 2k 2 disagr(D) (n 1)c 1 = 2k 2 n 2 (n 1)c 1 4k 2 n c 1 :<label>(5)</label></formula><p>The following key lemma asserts that the large clusters produced in Step 4(c) are basically correct.</p><p>Lemma 10 Suppose c 1 16k 3 . Let Large f1; 2; : : : ; kg be the set of large clusters as in Step 4(c) of the algorithm. Then for each i 2 Large, C i T low = D i T low , that is w.r.t nodes of large value, C i precisely agrees with the optimal cluster D i .</p><p>Proof : Let i 2 Large be arbitrary. We will first prove the inclusion C i T low D i T low . Suppose this is not the case and there exists u 2 C i (D i [T low ). Let u 2 D j for some j 6 = i. Since u = 2 T low , we have val D (u) 1 c 1 =k 2 , which implies pval D (u; j) 1 c 1 =k 2 . By Lemma 9, this gives pval S (u; i) 1 c 1 =k 2 2 . Therefore we have 2(1 c 1 =k 2 ) pval D (u; i) + pval D (u; j) 2 jD i j + jD j j 1 n where the last step follows from the simple but powerful observation that each edge connecting u to a vertex in D i [ D j is correctly classified in exactly one of the two placements of u in the i'th and j'th clusters (when leaving every other vertex as in clustering D). We conclude that both</p><formula xml:id="formula_22">jD i j; jD j j 2( c 1 k 2 + )n + 1 :<label>(6)</label></formula><p>What we have shown is that if u 2 C i (D i [ T low ), then u 2 D j for some j with jD j j</p><formula xml:id="formula_23">2(c 1 =k 2 + )n + 1. It follows that jC i (D i [ T low )j 2(c 1 =k + k)n + k. Therefore, jD i j jC i j jT low j 2( c 1 k + k)n k n 2k 4k 2 n c 1 2( c 1 k + k)n k &gt; 2( c 1 k 2 + )n + 1 ;</formula><p>where the last step follows since c 1 16k 3 , k 2, c 1 = 1=20, and is tiny. This contradicts <ref type="bibr" target="#b5">( 6)</ref>, and so we conclude</p><formula xml:id="formula_24">C i T low D i T low . Now for the other inclusion D i T low C i T low . If a node v 2 D i (C i [ T low ) is placed in</formula><p>C q for q 6 = i, then a similar argument to how we concluded (6) establishes jD i j 2( c 1 k 2 + )n + 1, which is impossible since we have shown D i C i T low , and hence jD i j jC i j jT low j</p><formula xml:id="formula_25">n 2k 4k 2 n c 1 &gt; 2( c 1 k 2 + )n + 1</formula><p>, where the last step follows using The next lemma states that there is a clustering which is very close to optimum which agrees exactly with our large clusters. This will enable us to find a near-optimal clustering by recursing on the small clusters to recluster them as needed, exactly as our algorithm does. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 11 Assume</head><formula xml:id="formula_26">c 1 16k 3 . There exists a clustering F that partitions V as V = F 1 [F 2 [ F k that satisfies the following: (i) F i = C i for every i 2 Large (ii) The number of disagreements of the clustering F is at most disagr(F ) n 2 1 + 4k 2 c 1 ( + 2k 2 c 1 ) Proof : Suppose w 2 T low is such that w 2 C r ,</formula><p>Combining ( <ref type="formula">7</ref>), ( <ref type="formula">8</ref>) and ( <ref type="formula" target="#formula_27">9</ref>), we can conclude disagr(F ) disagr(D) (n 1)jT low j(2 + jT low j n 1 ). The claim now follows using the upper bound on jT low j from (5) (and using n 2 =(n 1) 2 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 12</head><p>If the optimal clustering D has n 2 disagreements for c 1 16k 3 , then the clustering ClusMin found by the algorithm makes at most n 2 (1 + "=10)(1 + 4k 2 =c 1 + 8k 4 =c 2 1 ) disagreements.</p><p>Proof : We note that when restricted to the set of all edges except those entirely within W , the set of agreements of the clustering C in Step 4(e) coincides precisely with that of F . Let n 1 be the number of disagreements of F on edges that lie within W and let n 2 be the number of disagreements on all other edges. Since W is clustered recursively, we have the number of disagreements in C is at most n 2 + n 1 (1 + "=10) (n 1 + n 2 )(1 + "=10). The claim follows from the bound on n 1 + n 2 from Lemma 11, Part (ii).</p><p>Theorem 13 For every " &gt; 0, algorithm MinDisAg(k; ") delivers a clustering with number of disagreements within a factor (1 + ") of the optimum.</p><p>Proof : Let OPT = n 2 be the number of disagreements of an optimal clustering. The solution ClusMax returned by the maximization algorithm has at most OPT + disagreements. The solution ClusMin has at most n 2 (1+"=10)(1+4k 2 =c 1 +8k 4 =c 2 1 )) disagreements. If &gt; "c 2 1 32k 4 , the former is within (1 + ") of the optimal. If "c 2 1 32k 4 (which also satisfies the requirement c 1 =16k 3 we had in Lemma 12), the latter clustering ClusMin achieves approximation ratio (1 + "=10)(1 + "=2) (1 + ") (recall that "c 1 16k 2 ). Thus the better of these two solutions is always an (1 + ") approximation.</p><p>To conclude Theorem 7, we examine the running time of MinDisAg. Step 4 will be run for k jSj = n O(k 4 =" 2 ) iterations. During each iteration, the placement of vertices is done in O(n log n). Finally, observe that there is always at least one large cluster, therefore the recursive call is always done on strictly less clusters. It follows that the running time of MinDisAg(k; ") can be described from the recurrence T (k; ") n O(k 4 =" 2 ) (n log n + T (k 1; "=10)) from which we derive that the total running time is bounded by n O(100 k =" 2 ) log n.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 3</head><label>3</label><figDesc>For every k 2, there is a polynomial time approximation scheme for MAXAGREE[k].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>" 2 c 2 1 32k 4 .</head><label>24</label><figDesc>Let ClusMax be the k-clustering returned. 2. Set = c 1 " 16k 2 . Pick a sample S V by drawing 5 log n 2 vertices u.a.r with replacement. 3. ClusVal 0; /* Keeps track of value of best clustering found so far*/ 4. For each partition S of S as S 1 [ S 2 [ [ S k , perform the following steps: (a) Initialize the clusters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>def = pval S (u; j u ). (iii) Place u in cluster C ju , i.e., C ju C ju [ fug. (c) Compute the set of large and small clusters as Large = fj j 1 j k; jC j j n 2k g, and Small = f1; 2; : : : ; kg Large. Let l = jLargej and s = k l = jSmallj. /* Note that s &lt; k. */ (d) Cluster W def = S j2Small C j into s clusters using recursive call to algorithm MinDisAg(s; "=10). Let the clustering output by the recursive call be W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 8</head><label>8</label><figDesc>The sample S picked in Step 2 is -good with high probability (at least 1 O(1= p n)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 9</head><label>9</label><figDesc>Suppose a node u 2 D s is placed in cluster C r at the end of Step 4(b) for r 6 = s, 1 r; s k. Then pval D (u; r) pval D (u; s) 2 = val D (u) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>c 1 16k 3</head><label>13</label><figDesc>and k 2 for the choice c 1 = 1=20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>w 2 D 7 ) 8 ) 2 .</head><label>2782</label><figDesc>s with r 6 = s. Consider the clustering formed from D by performing the following in parallel for each w 2 T low : If w 2 C r and w 2 D s for some r 6 = s, move w to D r . Let F = F 1 [ [ F k be the resulting clustering. By construction F i \ T low = C i \ T low for all i, 1 i k. Since we only move nodes in T low , clearly F i T low = D i T low for 1 i k. By Lemma 10, C i T low = D i T low for i 2 Large. Combining all these equalities we conclude that F i = C i for each i 2 Large. Now the only extra edges that the clustering F can get wrong compared to D are those incident upon nodes in T low , and therefore disagr(F ) disagr(D) (n 1) X w2T low (val D (w) val F (w)) (If a node w belongs to the same cluster in F and D (i.e., we did not move it), then since no node outside T low is moved in obtaining F from D, we have val F (w) val D (w) jT low j=(n 1) : (If we moved a node w 2 T low from D s to D r , then by Lemma 9 we have pval D (w; r) val D (w) Therefore for such a node w val F (w) pval D (w; r) jT low j=(n 1) val D (w) 2 jT low j=(n 1) :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>" 2 c 2 1 n 2 32k 4 = n 2 1 +" 2 c 2 1 32k 4</head><label>24114</label><figDesc></figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our approximation schemes will be randomized and deliver a solution with the claimed approximation guarantee with high probability. For simplicity, we do not explicitly mention this from now on.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This is also similar in spirit, for example, to the PTAS for Min 2-sum clustering based on the PTAS for Metric Max CUT<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that in the algorithm we draw elements of the sample with replacement, but for the analysis, we can pretend that S consists of distinct elements, since this happens with high probability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Since our sample size is (log n) as opposed to O(1) that was used in Lemma</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p><ref type="bibr" target="#b4">5</ref>, we can actually ensure (4) holds for every vertex w.h.p.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research supported in part by NSF grant CCF-0343672.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Complexity on general graphs</head><p>So far, we have discussed the MAXAGREE[k] and MINDISAGREE[k] problems on complete graphs. In this section, we note some results on the complexity of these graphs when the graph can be arbitrary. As we will see, the problems become much harder in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 14</head><p>There is a polynomial time factor 0:878 approximation algorithm for MAXAGREE <ref type="bibr" target="#b1">[2]</ref> on general graphs. For every k 3, there is a polynomial time factor 0:7666 approximation algorithm for MAXAGREE[k] on general graphs.</p><p>Proof : The bound for 2-clusters case follows from the Goemans-Williamson algorithm for Max CUT modified in the obvious way to account for the positive edges. The bound for k 3 is obtained by Swamy <ref type="bibr" target="#b14">[15]</ref> who also notes that slightly better bounds are possible for 3 k 5. We note that in light of the recent hardness result for Max CUT <ref type="bibr" target="#b12">[13]</ref>, the above guarantee for MAXAGREE <ref type="bibr" target="#b1">[2]</ref> is likely the best possible. Proof : The bound for 2-clustering follows by the simple observation that MINDISAGREE <ref type="bibr" target="#b1">[2]</ref> on general graphs reduces to Min 2CNF Deletion, i.e., given an instance of 2SAT, determining the minimum number of clauses that have to be deleted to make it satisfiable. The latter problem admits an O( p log n) approximation algorithm <ref type="bibr" target="#b0">[1]</ref>. The result on MINDISAGREE[k] for k 3 follows by a reduction from k-coloring. When k 3, it is NP-hard to tell if a graph is k-colorable, and thus even given an instance of MINDISAGREE[k] with only negative edges, it is NP-hard to determine if the optimum number of disagreements is zero or positive.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">O( p log n) approximation algorithms for Min Uncut, Min 2CNF deletion, and directed cut problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Makarychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the 37th ACM Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quadratic forms on graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the 37th ACM Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlation clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Special Issue on Clustering</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="89" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering gene expression patterns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Dor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yakhini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comp. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="281" to="297" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clustering with qualitative information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Guruswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>the 44th IEEE Symposium on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximizing quadratic programs: extending Grothendieck&apos;s inequality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>the 45th IEEE Symposium on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation schemes for clustering problems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fernandez De La Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karpinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rabani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the 35th Annual ACM Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A randomized approximation scheme for metric max-cut</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Fernandez</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kenyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>the 39th IEEE Symposium on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="468" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Correlation clustering with partial information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Demaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 6th APPROX</title>
		<meeting>of 6th APPROX</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Correlation clustering-minimizing disagreements on arbitrary weighted graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fiat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 11th ESA</title>
		<meeting>of 11th ESA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="208" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Property testing and its connection to learning and approximation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="653" to="750" />
			<date type="published" when="1998-07">July 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A sublinear-time approximation scheme for clustering in metric spaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>the 40th IEEE Symposium on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal inapproximability results for Max Cut and other 2-variable CSPs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>O'donnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>the 45th IEEE Symposium on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cluster graph modification problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 28th Workshop on Graph Theory (WG)</title>
		<meeting>of 28th Workshop on Graph Theory (WG)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="379" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Correlation Clustering: Maximizing agreements via semidefinite programming</title>
		<author>
			<persName><forename type="first">C</forename><surname>Swamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 15th SODA</title>
		<meeting>of 15th SODA</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="519" to="520" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
