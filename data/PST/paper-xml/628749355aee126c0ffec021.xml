<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lukas</forename><surname>Galke</surname></persName>
							<email>lukas.galke@mpi.nl</email>
						</author>
						<author>
							<persName><forename type="first">Ansgar</forename><surname>Scherp</surname></persName>
							<email>ansgar.scherp@uni-ulm.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Kiel</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">MPI for Psycholinguistics</orgName>
								<address>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Ulm</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today's state of the art. We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and Hete-GCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models. These results question the importance of synthetic graphs used in modern text classifiers. In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an O(N 2 ) graph, where N is the vocabulary plus corpus size. Finally, since Transformers need to compute O(L 2 ) attention weights with sequence length L, the MLP models show higher training and inference speeds on datasets with long sequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text categorization is the task of assigning topical categories to text units such as documents, social media postings, or news articles. Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows <ref type="bibr" target="#b0">(Bayer et al., 2021;</ref><ref type="bibr" target="#b26">Li et al., 2020;</ref><ref type="bibr" target="#b67">Zhou et al., 2020;</ref><ref type="bibr" target="#b24">Kowsari et al., 2019;</ref><ref type="bibr" target="#b20">Kadhim, 2019)</ref>.</p><p>There are approaches based on a Bag of Words (BoW) that perform text categorization purely on the basis of a multiset of tokens. Among them are Deep Averaging Networks (DAN) <ref type="bibr" target="#b17">(Iyyer et al., 2015)</ref>, a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) <ref type="bibr" target="#b45">(Shen et al., 2018)</ref> that explores different pooling strategies for pretrained word embeddings, and fastText <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref>, which uses a linear layer on top of pretrained word embed-dings. These models count the occurrence of all tokens in the input sequence, while disregarding word position and order, and then rely on word embeddings and fully connected feedforward layer(s). We call these BoW-based models.</p><p>Among the most popular recent methods for text categorization are graph-based models such as TextGCN <ref type="bibr" target="#b62">(Yao et al., 2019)</ref> that first induce a synthetic word-document co-occurence graph over the corpus and subsequently apply a graph neural network (GNN) to perform the classification task. Besides TextGCN, there are follow-up works like HeteGCN <ref type="bibr" target="#b41">(Ragesh et al., 2021)</ref>, TensorGCN <ref type="bibr" target="#b28">(Liu et al., 2020)</ref>, and HyperGAT <ref type="bibr" target="#b10">(Ding et al., 2020)</ref>, which we collectively call graph-based models.</p><p>Finally, there is the well-known Transformer <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref> universe with models such as BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and its sizereduced variants such as DistilBERT <ref type="bibr" target="#b43">(Sanh et al., 2019)</ref>. Here, the input is a (fixed-length) sequence of tokens, which is then fed into multiple layers of self-attention. Lightweight versions such as Distil-BERT and others <ref type="bibr" target="#b49">(Tay et al., 2020;</ref><ref type="bibr" target="#b13">Fournier et al., 2021)</ref> use less parameters but operate on the same type of input. Together with recurrent models such as LSTMs, we call these sequence-based models.</p><p>In this paper, we hypothesize that text categorization can be very well conducted by simple but effective BoW-based models. We investigate this research question in three steps: First, we conduct an in-depth analysis of the literature. We review the key research in the field of text categorization. From this analysis, we derive the different families of methods, the established benchmark datasets, and identify the top performing methods. We decide for which models we report numbers from the literature and which models we run on our own. Overall, we compare 16 different methods from the families of BoW-based models (8 methods), sequence-based models (3 methods), and graphbased models (5 methods). We run our own experi-ments for 7 of these methods on 5 text categorization datasets, while we report the results from the literature for the remaining methods.</p><p>The result is surprising: Our own BoW-based MLP, called the WideMLP, with only one wide hidden layer, outperforms many of the recent graphbased models for inductive text categorization <ref type="bibr" target="#b62">(Yao et al., 2019;</ref><ref type="bibr" target="#b28">Liu et al., 2020;</ref><ref type="bibr" target="#b41">Ragesh et al., 2021)</ref>. Moreover, we did not find any reported scores for BERT-based methods from the sequence-based family. Thus, we fine-tuned our own BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and DistilBERT <ref type="bibr" target="#b43">(Sanh et al., 2019)</ref>. These models set a new state of the art. On a metalevel, our study shows that MLPs have largely been ignored as competitor methods in experiments. It seems as if MLPs have been forgotten as baseline in the literature, which instead is focusing mostly on other advanced Deep Learning architectures. Considering strong baselines is, however, an important means to argue about true scientific advancement <ref type="bibr" target="#b45">(Shen et al., 2018;</ref><ref type="bibr" target="#b7">Dacrema et al., 2019)</ref>. Simple models are also often preferred in industry due to lower operational and maintenance costs.</p><p>Below, we introduce our methodology and results from the literature study. Subsequently, we introduce the families of models in Section 3. Thereafter, we describe the experimental procedure in Section 4. We present the results of our experiments in Section 5 and discuss our findings in Section 6, before we conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature on Text Categorization</head><p>Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies <ref type="bibr">(Minaee et al., 2021;</ref><ref type="bibr" target="#b0">Bayer et al., 2021;</ref><ref type="bibr" target="#b26">Li et al., 2020;</ref><ref type="bibr" target="#b67">Zhou et al., 2020;</ref><ref type="bibr" target="#b24">Kowsari et al., 2019;</ref><ref type="bibr" target="#b20">Kadhim, 2019;</ref><ref type="bibr" target="#b14">Galke et al., 2017;</ref><ref type="bibr" target="#b64">Zhang et al., 2016)</ref>. These cover the range from shallow to deep classification models. Second, we have screened for literature in key NLP and AI venues. Finally, we have complemented our search by checking results and papers on paperswithcode.com. On the basis of this input, we have determined three families of methods and benchmark datasets (see Table <ref type="table" target="#tab_1">2</ref>). We focus our analysis on identifying models per family showing strong performance and select the methods to include in our study. For all models, we have verified that the same train-test split is used. We check whether modified versions of the datasets have been used (e. g., fewer classes), to avoid bias and wrongfully giving advantages.</p><p>BoW-based Models Classical machine learning models that operate on a BoW-based input are extensively discussed in two surveys <ref type="bibr" target="#b24">(Kowsari et al., 2019;</ref><ref type="bibr" target="#b20">Kadhim, 2019)</ref> and other comparison studies <ref type="bibr" target="#b14">(Galke et al., 2017)</ref>. <ref type="bibr" target="#b17">Iyyer et al. (2015)</ref> proposed DAN, which combine word embeddings and deep feedforward networks. It is an MLP with 1-6 hidden layers, non-linear activation, dropout, and Ada-Grad as optimization method. The results suggest to use pretrained embeddings such as GloVe <ref type="bibr" target="#b40">(Pennington et al., 2014)</ref> over a randomly initialized neural bag of-words <ref type="bibr" target="#b21">(Kalchbrenner et al., 2014)</ref> as input. In fastText <ref type="bibr" target="#b3">(Bojanowski et al., 2017;</ref><ref type="bibr" target="#b19">Joulin et al., 2017)</ref> a linear layer on top of pretrained embeddings is used for classification. Furthermore, <ref type="bibr" target="#b45">Shen et al. (2018)</ref> explore embedding pooling variants and find that SWEM can rival approaches based on recurrent (RNN) and convolutional neural networks (CNN). We consider fastText, SWEM, and a DAN-like deeper MLP in our comparison.</p><p>Note that those approaches that rely on logistic regression on top of pretrained word embeddings, e. g., fastText, share a similar architecture as an MLP with one hidden layer. However, the standard training protocol involves pretraining the word embedding on large amounts of unlabeled text and then freezing the word embeddings while training the logistic regression <ref type="bibr" target="#b33">(Mikolov et al., 2013)</ref>.</p><p>Graph-based Models Using graphs induced from text for the task of text categorization has a long history in the community. An early work is the term co-occurrence graph of the KeyGraph algorithm <ref type="bibr" target="#b37">(Ohsawa et al., 1998)</ref>. The graph is split into segments, representing the key concepts in the document. Co-occurence graphs have also been used for automatic keyword extraction such as in RAKE <ref type="bibr">(Rose et al., 2010)</ref> and can be also used for classification <ref type="bibr" target="#b65">(Zhang et al., 2021)</ref>.</p><p>Modern approaches exploit this idea in combination with graph neural networks (GNN) <ref type="bibr" target="#b16">(Hamilton, 2020)</ref>. Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN <ref type="bibr" target="#b62">(Yao et al., 2019)</ref> and its successor TensorGCN <ref type="bibr" target="#b28">(Liu et al., 2020)</ref> as well as Hete-GCN <ref type="bibr" target="#b41">(Ragesh et al., 2021)</ref>, HyperGAT <ref type="bibr" target="#b10">(Ding et al., 2020)</ref>, and DADGNN <ref type="bibr" target="#b28">(Liu et al., 2020)</ref>. We briefly discuss these models: In TextGCN, the authors set up a graph based on word-word connections given by window-based pointwise mutual information and word-document TF-IDF scores. They use a one-hot encoding as node features and apply a two-layer graph convolutional network <ref type="bibr" target="#b23">(Kipf and Welling, 2017)</ref> on the graph to carry out the node classification task. HeteGCN combines ideas from Predictive Text Embedding <ref type="bibr" target="#b48">(Tang et al., 2015)</ref> and TextGCN and split the adjacency matrix into its word-document and word-word sub-matrices and fuse the different layers' representations when required. TensorGCN uses multiple ways of converting text data into graph data including a semantic graph created with an LSTM, a syntactic graph created by dependency parsing, and a sequential graph based on word co-occurrence. HyperGAT extended the idea of text-induced graphs for text classification to hypergraphs. The model uses graph attention and two kinds of hyperedges. Sequential hyperedges represent the relation between sentences and their words. Semantic hyperedges for word-word connections are derived from topic models <ref type="bibr" target="#b2">(Blei et al., 2001)</ref>. Finally, DADGNN is a graph-based approach that uses attention diffusion and decoupling techniques to tackle oversmoothing of the GNN and to be able to stack more layers.</p><p>In TextGCN's original transductive formulation, the entire graph including the test set needs to be known for training. This may be prohibitive in practical applications as each batch of new documents would require retraining the model. When these methods are adapted for inductive learning, where the test set is unseen, they achieve notably lower scores <ref type="bibr" target="#b41">(Ragesh et al., 2021)</ref>. GNNs for text classification use corpus statistics, e. g., pointwise mutual information (PMI), to connect related words in a graph <ref type="bibr" target="#b62">(Yao et al., 2019)</ref>. When these were omitted, the GNNs would collapse to bag-of-words MLPs. Thus, GNNs have access to more information than BoW-MLPs. GloVe <ref type="bibr" target="#b40">(Pennington et al., 2014</ref>) also captures PMI corpus statistics, which is why we include an MLP on GloVe input representations.</p><p>Sequence models: RNN and CNN Recurrent neural networks (RNN) are a natural choice for any NLP task. However, it turned out to be challenging to find numbers reported on text categorization in the literature that can be used as references. The bidirectional LSTM with two-dimensional max pooling BLSTM-2DCNN <ref type="bibr" target="#b66">(Zhou et al., 2016)</ref> has been applied on a stripped-down to 4 classes version of the 20ng dataset. Thus, the high score of 96.5 reported for 4ng cannot be compared with papers applied on the full 20ng dataset. Also Text-RCNN <ref type="bibr" target="#b25">(Lai et al., 2015)</ref>, a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset. The results of Text-RCNN are identical with BLSTM-2DCNN. For the MR dataset, BLSTM-2DCNN provides no information on the specific split of the dataset. RNN-Capsule <ref type="bibr" target="#b57">(Wang et al., 2018)</ref> is a sentiment analysis method reaching an accuracy of 83.8 on the MR dataset, but with a different train-test split. <ref type="bibr" target="#b31">Lyu and Liu (2020)</ref> combine a 2D-CNN with bidirectional RNN. Another work applying a combination of a convolutional layer and an LSTM layer is by <ref type="bibr">Wang et al. (2019b)</ref>. The authors experiment with five English and two Chinese datasets, which are not in the set of representative datasets we identified. The authors report that their approach outperforms existing models like fastText on two of the five English datasets and both Chinese datasets.</p><p>Sequence models: Transformers Surprisingly, only few works consider Transformer models for text categorization. A recent work shows that BERT outperforms classic TF-IDF BoW approaches on English, Chinese, and Portuguese text classification datasets <ref type="bibr" target="#b15">(Gonz?lez-Carvajal and Garrido-Merch?n, 2020)</ref>. We have not found any results of transformer-based models reported on those text categorization datasets that are commonly used in the graph-based approaches.</p><p>Therefore, we fine-tune BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and DistilBERT <ref type="bibr" target="#b43">(Sanh et al., 2019)</ref> on those datasets ourselves. BERT is a large pretrained language model on the basis of Transformers. Dis-tilBERT <ref type="bibr" target="#b43">(Sanh et al., 2019)</ref> is a distilled version of BERT with 40% reduced parameters while retaining 97% of BERT's language understanding capabilities. TinyBERT <ref type="bibr" target="#b18">(Jiao et al., 2020)</ref> and Mo-bileBERT <ref type="bibr" target="#b47">(Sun et al., 2020)</ref> would be similarly suitable alternatives, among others. We chose Dis-tilBERT because it can be fine-tuned independently from the BERT teacher. Its inference times are 60% faster than BERT, which makes it more likely to be reusable by labs with limited resources.</p><p>Summary From our literature survey, we see that all recent methods are based on graphs. BoW-based methods are hardly found in experiments, while, likewise surprisingly, Transformer-based sequence models are extremely scarce in the literature on topical text categorization. The recent surveys on text categorization include both classical and Deep Learning models, but none considered a simple MLP except for the inclusion of DAN <ref type="bibr" target="#b17">(Iyyer et al., 2015)</ref> in <ref type="bibr" target="#b26">Li et al. (2020)</ref>. (x) with parameters ? such that arg max( ?) preferably equals the true label y for input x.</p><p>As BoW-based model, we consider a one hidden layer WideMLP (i. e., two layers in total). We experiment with pure BoW, TF-IDF weighted, and averaged GloVe input representations. We also use a two hidden layers WideMLP-2. We list the numbers for fastText, SWEM, and logistic regression from <ref type="bibr" target="#b10">Ding et al. (2020)</ref> in our comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph-Based Text Categorization</head><p>Graph-based text categorization approaches first set up a synthetic graph on the basis of the text corpus D in the form of an adjacency matrix ? := make-graph(D). For instance, in TextGCN the graph is set up in two parts: word-word connections are modeled by pointwise mutual information and word-document edges resemble that the word occurs in the document. Then, a parameterized function f (graph) ? (X, ?) is learned that uses the graph as input, where X are the node features. The graph is composed of word and document nodes, each receiving its own embedding (by set-ting X = I). In inductive learning, however, there is no embedding of the test documents. Note that the graph-based approaches from the current literature such as TextGCN also disregard word order, similar to the BoW-based models described above. A detailed discussion of the connection between TextGCN and MLP is provided in Appendix B.</p><p>We consider top performing graph-based models from the literature, namely TextGCN along with its successors HeteGCN, TensorGCN, HyperGAT, DADGNN, as well as simplified GCN (SGC) <ref type="bibr" target="#b58">(Wu et al., 2019)</ref>. We do not run our own experiments for the graph-based models but rely on the original work and extensive studies by <ref type="bibr" target="#b10">Ding et al. (2020)</ref> and <ref type="bibr" target="#b41">Ragesh et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sequence-Based Text Categorization</head><p>We consider RNNs, LSTMs, and Transformers as sequence-based models. These models are aware of the order of the words in the input text in the sense that they are able to exploit word order information. Thus, the key difference to the BoW-based and graph-based families is that the word order is reflected by sequence-based model. The model sig-</p><formula xml:id="formula_0">nature is ? = f (sequence) ? ( x 1 , x 2 , . . . , x k ),</formula><p>where k is the (maximum) sequence length. Word position is modeled by a dedicated positional encoding. For instance, in BERT each position is associated with an embedding vector that is added to the word embedding at input level.</p><p>For the sequence-based models, we run our own experiments with BERT and DistilBERT, while reporting the scores of a pretrained LSTM from <ref type="bibr" target="#b10">Ding et al. (2020)</ref> for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Apparatus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use the same datasets and train-test split as in TextGCN <ref type="bibr" target="#b62">(Yao et al., 2019)</ref>. Those datasets are 20ng, R8, R52, ohsumed, and MR. Twenty Newsgroups (20ng)<ref type="foot" target="#foot_0">1</ref> (bydate version) contains long posts categorized into 20 newsgroups. The mean sequence length is 551 words with a standard deviation (SD) of 2,047. R8 and R52 are subsets of the Reuters 21578 news dataset with 8 and 52 classes, respectively. The mean sequence length and SD is 119 ? 128 words for R8, and 126 ? 133 words for R52. Ohsumed<ref type="foot" target="#foot_1">2</ref> is a corpus of medical abstracts from the MEDLINE database that are categorized into diseases (one per abstract). The mean sequence length is 285 ? 123 words. Movie Reviews (MR)<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b38">(Pang and Lee, 2005)</ref>, split by <ref type="bibr" target="#b48">Tang et al. (2015)</ref>, is a binary sentiment analysis dataset on sentence level (mean sequence length and SD: 25 ? 11). Table <ref type="table" target="#tab_1">2</ref> shows the dataset characteristics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inductive and Transductive Setups</head><p>We distinguish between a transductive and an inductive setup for text categorization. In the transductive setup, as used in TextGCN, the test documents are visible and actually used for the preprocessing step. In the inductive setting, the test documents remain unseen until test time (i. e., they are not available for preprocessing). We report the scores of the graph-based models for both setups from the literature, where available. BoW-based and sequence-based models are inherently inductive. <ref type="bibr" target="#b41">Ragesh et al. (2021)</ref> have evaluated a variant of TextGCN that is capable of inductive learning, which we include in our results, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Procedure and Hyperparameter Settings</head><p>We have extracted accuracy scores from the literature according to our systematic selection from Section 2. Below, we provide a detailed description of the procedure for the models that we have run ourselves. We borrow the tokenization strategy from BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> along with its uncased vocabulary. The tokenizer relies primarily on WordPiece <ref type="bibr" target="#b60">(Wu et al., 2016)</ref> for a high coverage while maintaining a small vocabulary.</p><p>Training our BoW-Models. Our WideMLP has one hidden layer with 1,024 rectified linear units (one input-to-hidden and one hidden-to-output layer). We apply dropout after each hidden layer, notably also after the initial embedding layer. Only for GloVe+WideMLP, neither dropout nor ReLU is applied to the frozen pretrained embeddings but only on subsequent layers. The variant WideMLP-2 has two ReLU-activated hidden layers (three layers in total) with 1, 024 hidden units each. While this might be overparameterized for single-label text classification tasks with few classes, we rely on recent findings that overparameterization leads to better generalization <ref type="bibr" target="#b36">(Neyshabur et al., 2018;</ref><ref type="bibr" target="#b35">Nakkiran et al., 2020)</ref>. In pre-experiments, we realized that MLPs are not very sensitive to hyperparameter choices. Therefore, we optimize crossentropy with Adam (Kingma and Ba, 2015) and its default learning rate of 10 -3 , a linearly decaying learning rate schedule and train for a high amount of steps <ref type="bibr" target="#b35">(Nakkiran et al., 2020)</ref> (we use 100 epochs) with small batch sizes (we use 16) for sufficient stochasticity, along with a dropout ratio of 0.5.</p><p>Fine-tuning our BERT models. For BERT and DistilBERT, we fine-tune for 10 epochs with a linearly decaying learning rate of 5 ? 10 -5 and an effective batch size of 128 via gradient accumulation of 8 x 16 batches. We truncate all inputs to 512 tokens. To isolate the influence of word order on BERT's performance, we conduct two further ablations. First, we set all position embeddings to zero and disable their gradient (BERT w/o pos ids). By doing this, we force BERT to operate on a bag-ofwords without any notion of word order or position. Second, we shuffle each sequence to augment the training data. We use this augmentation strategy to increase the number of training examples by a factor of two (BERT w/ shuf. augm.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Measures</head><p>We report accuracy as evaluation metric, which is equivalent to Micro-F1 in single-label classification (see Appendix C). We repeat all experiments five times with different random initialization of the parameters and report the mean and standard deviation of these five runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effectiveness</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the accuracy scores for the text categorization models on the five datasets. All graphbased models in the transductive setting show similar accuracy scores (maximum difference is 2 points). As expected, the scores decrease in the inductive setting up to a point where they are matched or even outperformed by our WideMLP.</p><p>In the inductive setting, the WideMLP models perform best among the BoW models, in particular, TFIDF+WideMLP and WideMLP on an unweighted BoW. The best-performing graph-based model is HyperGAT, yet DADGNN has a slight advantage on R8, R52, and MR. For the sequencebased models, BERT attains the highest scores, closely followed by DistilBERT.</p><p>The strong performance of WideMLP rivals all graph-based techniques reported in the literature, in particular, the recently published graph-inducing methods. MLP only falls behind HyperGAT, which relies on topic models to set up the graph. Another observation is that 1 hidden layer (but wide) is sufficient for the tasks, as the scores for MLP variants with 2 hidden layers are lower. We further observe that both pure BoW and TF-IDF weighted BoW lead to better results than approaches that exploit pretrained word embeddings such as GloVe-MLP, fastText, and SWEM.</p><p>With its immense pretraining, BERT yields the overall highest scores, closely followed by Distil-BERT. DistilBERT outperforms HyperGAT by 7 points on the MR dataset while being on par on the others. BERT outperforms the strongest graphbased competitor, HyperGAT, by 8 points on MR, 1.5 points on ohsumed, 1 point on R52 and R8, and 0.5 points on 20ng.</p><p>Our results further confirm that position embeddings are important for BERT with a notable decrease when those are omitted. Augmenting the data with shuffled sequences has led to neither a consistent decrease nor increase in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Efficiency</head><p>Parameter Count of the Models Table <ref type="table" target="#tab_3">4</ref> lists the parameter counts of the models. Even though the MLP is fully-connected on top of a bag-ofwords with the dimensionality of the vocabulary size, it has only half of the parameters as Distil-BERT and a quarter of the parameters of BERT. Using TF-IDF does not change the number of model parameters. Due to the high vocabulary size, GloVe-based models have a high number of parameters, but the majority of those is frozen, i. e., does not get gradient updates during training.</p><p>Runtime Performance of the Models We provide the total running times in Table <ref type="table" target="#tab_4">5</ref> as observed while conducting the experiments on a single NVIDIA A100-SXM4-40GB card. All WideMLP variants are an order of magnitude faster than Dis-tilBERT when considering the average runtime per epoch. DistilBERT is twice as fast as the original BERT. The transformers are only faster than BoW models on the MR dataset. This is because the sequences in the MR dataset are much shorter and less O(L 2 ) attention weights have to be computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Key Insights Our experiments show that our MLP models using BoW outperform the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting. Furthermore, the MLP models are comparable to Hyper-GAT. Only transformer-based BERT and Distil-BERT models outperform our MLP and set a new state-of-the-art. This result is important for two reasons: First, the strong performance of a pure BoW-MLP questions the added value of synthetic graphs in models like TextGCN to the text categorization task. Only HyperGAT, which uses the expensive Latent Dirichlet Allocation for computing the graph, slightly outperforms our BoW-WideMLP in two out of five datasets. Thus, we argue that using strong baseline models for text classification is important to assess the true scientific advancement <ref type="bibr" target="#b7">(Dacrema et al., 2019)</ref>.</p><p>Second, in contrast to conventional wisdom <ref type="bibr" target="#b17">(Iyyer et al., 2015)</ref>, we find that pretrained embeddings, e. g., GloVe, can have a detrimental effect when compared to using an MLP with one wide hidden layer. Such an MLP circumvents the bottleneck of the small dimensionality of word embeddings and has a higher capacity. Furthermore, we experiment with more hidden layers (see WideMLP-2), but do not observe any improvement when the single hidden layer is sufficiently wide. A possible explanation is that already a single hidden layer is sufficient to approximate any compact function to an arbitrary degree of accuracy depending on the width of the hidden layer <ref type="bibr" target="#b6">(Cybenko, 1989)</ref>.</p><p>Finally, a new state-of-the-art is set by the transformer model BERT, which is not very surpris-  ing. However, as our efficiency analysis shows, the MLPs require only a fraction of the parameters and are faster in their combined training and inference time except for the MR dataset. The attention mechanism of (standard) Transformers is quadratic in the sequence length, which leads to slower processing of long sequences. With larger batches, the speed of the MLP could be increased even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Discussion of Results</head><p>Graph-based models come with high training costs, as not only the graph has to be first computed, but also a GNN has to be trained. For standard GNN methods, the whole graph has to fit into the GPU memory and mini-batching is nontrivial, but possible with dedicated sampling techniques for GNNs <ref type="bibr" target="#b12">(Fey et al., 2021)</ref>. Furthermore, the original TextGCN is inher-ently transductive, i. e., it has to be retrained whenever new documents appear. Strictly transductive models are effectively useless in practice <ref type="bibr" target="#b30">(Lu et al., 2019)</ref> except for applications, in which a partially labeled corpus needs to be fully annotated. However, recent extensions such as HeteGCN, Hyper-GAT, and DADGNN already relax this constraint and enable inductive learning. Nevertheless, worddocument graphs require O(N 2 ) space, where N is the number of documents plus the vocabulary size, which is a hurdle for large-scale applications.</p><p>There are also tasks where the natural structure of the graph data provides more information than the mere text, e. g., citations networks or connections in social graphs. In such cases, the performance of graph neural networks is the state of the art <ref type="bibr" target="#b23">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b52">Velickovic et al., 2018)</ref> and are superior to MLPs that use only the node features and not the graph structure <ref type="bibr" target="#b44">(Shchur et al., 2018)</ref>. GNNs also find application in various NLP tasks, other than classification <ref type="bibr" target="#b59">(Wu et al., 2021)</ref>.</p><p>An interesting factor is the ability of the models to capture word order. BoW models disregard word order entirely and yield good results, but still fall behind order-aware Transformer models. In an extensive study, <ref type="bibr" target="#b5">Conneau et al. (2018)</ref> have shown that memorizing the word content (which words appear at all) is most indicative of downstream task performance. <ref type="bibr" target="#b46">Sinha et al. (2021)</ref> have experimented with pretraining BERT by disabling word order during pretraining and show that it makes surprisingly little difference for fine-tuning. In their study, word order is preserved during fine-tuning.</p><p>In our experiments, we have conducted complementary experiments: we have used a BERT model that is pretrained with word order, but we have deactivated position encoding during fine-tuning.</p><p>Our results show that there is a notable drop in performance but the model does not fail completely.</p><p>Other NLP tasks such as question answering <ref type="bibr" target="#b42">(Rajpurkar et al., 2016)</ref> or natural language inference <ref type="bibr">(Wang et al., 2019a)</ref> can also be regarded as text classification on a technical level. Here, the positional information of the sequence is more important than for pure topical text categorization. One can expect that BoW-based models perform worse than sequence-based models.</p><p>Generalizability We expect that similar observations would be made on other text classification datasets because we have already covered a range of different characteristics: long and short texts, topical categorization (20ng, Reuters, and Ohsumed) and sentiment prediction (MR) in the domains of forum postings, news, movie reviews, and medical abstracts. Our results are in line with those from other fields, who have reported a resurgence of MLPs. For example, in business prediction, an MLP baseline outperforms various other Deep Learning models <ref type="bibr" target="#b53">(Venugopal et al., 2021;</ref><ref type="bibr" target="#b63">Yedida et al., 2021</ref><ref type="bibr">). In computer vision, Tolstikhin et al. (2021)</ref> and Melas-Kyriazi (2021) proposed attention-free MLP models that are on par with the Vision Transformer <ref type="bibr" target="#b11">(Dosovitskiy et al., 2021)</ref>. In natural language processing, <ref type="bibr">Liu et al. (2021a)</ref> show similar results, while acknowledging that a small attention module is necessary for some tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Threats to Validity</head><p>We acknowledge that the experimental datasets are limited to English. While word order is important in the English language, it is notable that methods that discard word order still work well for text categorization. Another possible bias is the comparability of the results. However, we carefully checked all relevant parameters such as the train/test split, the number of classes in the datasets, if datasets have been pre-processed in such a way that, e. g., makes a task easier like reducing the number of classes, the training procedure, and the reported evaluation metrics. Regarding our efficency analysis, we made sure to report numbers for the parameter count and a measure for the speed other than FLOPs, as recommended by <ref type="bibr" target="#b8">Dehghani et al. (2021)</ref>. Since runtime is heavily dependant on training parameters such as batch size, we complement this with asymptotic complexity.</p><p>Practical Impact and Future Work Our study has an immediate impact on practitioners who seek to employ robust text categorization models in research projects and in industrial operational environments. Furthermore, we advocate to use an MLP baseline in future text categorization research, for which we provide concrete guidelines in Appendix A. As future work, it would be interesting to analyze multi-label classification tasks and to compare with hierarchical text categorization methods <ref type="bibr" target="#b39">(Peng et al., 2018;</ref><ref type="bibr" target="#b61">Xiao et al., 2019)</ref>. Another interesting yet challenging setting would be few-shot classification <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We argue that a wide multi-layer perceptron enhanced with today's best practices should be considered as a strong baseline for text classification tasks. In fact, the experiments show that our WideMLP is oftentimes on-par or even better than recently proposed models that synthesize a graph structure from the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Practical Guidelines for Designing a WideMLP</head><p>On the basis of our results, we provide recommendations for designing a WideMLP baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tokenization</head><p>We recommend using modern subword tokenizers such as BERT-like WordPiece or SentencePiece that yield a high coverage while needing a relatively small vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Representation</head><p>In contrast to conventional wisdom <ref type="bibr" target="#b17">(Iyyer et al., 2015)</ref>, we find that pretrained embeddings, e. g., GloVe, can have a detrimental effect when compared to using an MLP with one wide hidden layer. Such an MLP circumvents the bottleneck of the small dimensionality of word embeddings and has a higher capacity.</p><p>Depth vs. Width In text classification, width seems more important than depth. We recommend to use a single, wide hidden layer, i. e., one input-tohidden and one hidden-to-output layer, e. g., with 1,024 hidden units and ReLU activation. While this might be overparameterized for single-label text classification tasks with few classes, we rely on recent findings that overparameterization leads to better generalization <ref type="bibr" target="#b36">(Neyshabur et al., 2018;</ref><ref type="bibr" target="#b35">Nakkiran et al., 2020)</ref>. We further motivate the choice of using wide layers with results from multi-label text classification <ref type="bibr" target="#b14">(Galke et al., 2017)</ref>, which has shown that a (wide) MLP outperforms all tested classical baselines such as SVMs, k-Nearest Neighbors, and logistic regression. Follow-up work <ref type="bibr" target="#b32">(Mai et al., 2018)</ref> then found that also CNN and LSTM do not substantially improve over the wide MLP.</p><p>Having a fully-connected layer on-top of a bagof-words leads to a high number of learnable parameters. Still, the wide first input-to-hidden layer can be implemented efficiently by using an embedding layer followed by aggregation, which avoids large matrix multiplications.</p><p>In our experiments, we did not observe any improvement with more hidden layers (WideMLP-2), as suggested by <ref type="bibr" target="#b17">Iyyer et al. (2015)</ref>, but it might be beneficial for other, more challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization and Regularization</head><p>We seek to find an optimization strategy that does not require dataset-specific hyperparameter tuning. This comprises optimizing cross-entropy with <ref type="bibr">Adam (Kingma and Ba, 2015)</ref> and default learning rate 10 -3 , a linearly decaying learning rate schedule and training for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufficient stochasticity.</p><p>For regularization during this prolonged training, we suggest to use a high dropout ratio of 0.5. Regarding initialization, we rely on framework defaults, i. e., N (0, 1) for the initial embedding layer and random uniform U(-d input , d output ) for subsequent layers' weight and bias parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Connection between BoW-MLP and TextGCN</head><p>TextGCN uses the PMI matrix to set up edge weights for word-word connections. A single layer Text-GCN is a BoW-MLP, except for the document embedding. The one-hop neighbors are words which are aggregated after a nonlinear transform.</p><p>The basic GCN equation H = ?( ?XW ) reveals that the order of transformation and neighborhood aggregation is irrelevant. The document embedding implies that TextGCN is a semisupervised technique. Truly new documents, as in inductive learning scenarios, would need a special treatment such as using an all zero embedding vector.</p><p>A two-layer MLP can be characterized by the equation ? = W (2) ?(W (1) x + b (1) ) + b (2) . On bag-of-words inputs, the first layer W (1) x + b (1) can be replaced by an equivalent embedding layer with weighting (e. g., TF-IDF or length normalization) being applied during aggregation of the embedding vectors.</p><p>The first layer of TextGCN is equivalent to aggregating embedding vectors. A standard GCN layer with shared weights has the form (assuming self-loops have been inserted)</p><formula xml:id="formula_1">h i = j?N (i) a ij W (1) x j + b (1)</formula><p>Now in TextGCN node features are given by the identity, such that x j is one-hot. Then we can rewrite the first layer of Text-GCN as an aggregation of embeddings E. We gain</p><formula xml:id="formula_2">h i = j?N (i)</formula><p>a ij E j as W x + b may again be replaced by an embedding matrix if applied to one hot vectors x. Now E contains two types of embedding vectors: word embeddings and document embeddings corresponding to word nodes and document nodes. We see that the first layer of TextGCN is essentially an aggregation of word embeddings plus the document embedding. Only with a second layer, TextGCN considers the embedding of other documents whose words are connected to the present documents' words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Equivalence of Micro-F1 and Accuracy in Multiclass Classification</head><p>In multiclass classification, we have a single true label for each instance and the predictions are constrained to a single prediction per instance. As a consequence, the measures accuracy and Micro-F1 coincide to the same formula. Micro-F1 aggregates true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) globally. It can be expressed as:</p><formula xml:id="formula_3">Micro-F 1 = 2 c TP c 2 c TP c + c FP c + c FN c ,</formula><p>where c iterates over all classes.</p><p>While the accuracy can be expressed as:</p><formula xml:id="formula_4">Acc = c TP c + c TN c c TP c + c TN c + c FP c + c FN c</formula><p>In multiclass classification, every true positive is also a true negative for all other classes. When summing those up over the entire dataset, we obtain</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Properties of text categorization approaches. Graph-based models that rely on having access to unlabeled test documents such as TextGCN and TensorGCN are not capable of inductive learning without modifications.</figDesc><table><row><cell>Model</cell><cell cols="4">Synthetic Graph Position-Aware Arbitrary Length Inductive</cell></row><row><cell>Bag-of-Words</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Graph: TextGCN</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Graph: TensorGCN</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Graph: HeteGCN/HyperGAT</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Sequence: RNN/CNN</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Sequence: BERT/DistilBERT</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell cols="2">3 Models for Text Categorization</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">We formally introduce the three families of mod-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">els for text categorization, namely the BoW-based,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">graph-based, and sequence-based models. Table 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">summarizes the key properties of the approaches:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">whether they require a synthetic graph, whether</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">word position is reflected in the model, whether</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">the model can deal with arbitrary length text, and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">whether the model is capable of inductive learning.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>3.1 BoW-Based Text Categorization</p>Under pure BoW-based text categorization, we denote approaches that are not order-aware and operate only on the multiset of words from the input document. Given paired training examples (x, y) ? D, each consisting of a bag-of-words x ? R n vocab and a class label y ? Y, the goal is to learn a generalizable function ? = f (BoW) ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Characteristics of text classification datasets</figDesc><table><row><cell>Dataset</cell><cell cols="3">N #Train #Test #Classes</cell></row><row><cell>20ng</cell><cell cols="2">18,846 11,314 7,532</cell><cell>20</cell></row><row><cell>R8</cell><cell>7,674</cell><cell>5,485 2,189</cell><cell>8</cell></row><row><cell>R52</cell><cell>9,100</cell><cell>6,532 2,568</cell><cell>52</cell></row><row><cell>ohsumed</cell><cell>7,400</cell><cell>3,357 4,043</cell><cell>23</cell></row><row><cell>MR</cell><cell>10,662</cell><cell>7,108 3,554</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy and standard deviation on text classification datasets. Column "Provenance" reports the source.</figDesc><table><row><cell>Inductive Setting</cell><cell>20ng</cell><cell>R8</cell><cell>R52</cell><cell>ohsumed</cell><cell>MR</cell><cell>Provenance</cell></row><row><cell>BoW-Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Log. Regression</cell><cell>83.70</cell><cell>93.33</cell><cell>90.65</cell><cell>61.14</cell><cell>76.28</cell><cell>Ragesh et al. (2021)</cell></row><row><cell>SWEM</cell><cell cols="5">85.16 (0.29) 95.32 (0.26) 92.94 (0.24) 63.12 (0.55) 76.65 (0.63)</cell><cell>Ding et al. (2020)</cell></row><row><cell>fastText</cell><cell cols="5">79.38 (0.30) 96.13 (0.21) 92.81 (0.09) 57.70 (0.49) 75.14 (0.20)</cell><cell>Ding et al. (2020)</cell></row><row><cell>TF-IDF + WideMLP</cell><cell cols="5">84.20 (0.16) 97.08 (0.16) 93.67 (0.23) 66.06 (0.29) 76.32 (0.17)</cell><cell>own experiment</cell></row><row><cell>WideMLP</cell><cell cols="5">83.31 (0.22) 97.27 (0.12) 93.89 (0.16) 63.95 (0.13) 76.72 (0.26)</cell><cell>own experiment</cell></row><row><cell>WideMLP-2</cell><cell cols="5">81.02 (0.23) 96.61 (1.22) 93.98 (0.23) 61.71 (0.33) 75.91 (0.51)</cell><cell>own experiment</cell></row><row><cell>GloVe+WideMLP</cell><cell cols="5">76.80 (0.11) 96.44 (0.08) 93.58 (0.06) 61.36 (0.22) 75.96 (0.17)</cell><cell>own experiment</cell></row><row><cell>GloVe+WideMLP-2</cell><cell cols="5">76.33 (0.18) 96.50 (0.14) 93.19 (0.11) 61.65 (0.27) 75.72 (0.45)</cell><cell>own experiment</cell></row><row><cell>Graph-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TextGCN</cell><cell cols="6">80.88 (0.54) 94.00 (0.40) 89.39 (0.38) 56.32 (1.36) 74.60 (0.43) Ragesh et al. (2021)</cell></row><row><cell>HeteGCN</cell><cell cols="6">84.59 (0.14) 97.17 (0.33) 93.89 (0.45) 63.79 (0.80) 75.62 (0.26) Ragesh et al. (2021)</cell></row><row><cell>HyperGAT</cell><cell cols="6">86.62 (0.16) 97.07 (0.23) 94.98 (0.27) 69.90 (0.34) 78.32 (0.27) Ragesh et al. (2021)</cell></row><row><cell>DADGNN</cell><cell>-</cell><cell cols="3">98.15 (0.16) 95.16 (0.22) -</cell><cell>78.64 (0.29)</cell><cell>Liu et al. (2021b)</cell></row><row><cell>Seq.-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM (pretrain)</cell><cell cols="5">75.43 (1.72) 96.09 (0.19) 90.48 (0.86) 51.10 (1.50) 77.33 (0.89)</cell><cell>Ding et al. (2020)</cell></row><row><cell>DistilBERT</cell><cell cols="5">86.24 (0.26) 97.89 (0.15) 95.34 (0.08) 69.08 (0.60) 85.10 (0.33)</cell><cell>own experiment</cell></row><row><cell>BERT</cell><cell cols="5">87.21 (0.18) 98.03 (0.24) 96.17 (0.33) 71.46 (0.54) 86.61 (0.38)</cell><cell>own experiment</cell></row><row><cell>BERT w/o pos emb</cell><cell cols="5">81.47 (0.49) 97.39 (0.20) 94.70 (0.27) 65.18 (1.53) 80.35 (0.20)</cell><cell>own experiment</cell></row><row><cell cols="6">BERT w/ shuf. augm. 86.46 (0.42) 98.07 (0.21) 96.48 (0.18) 70.94 (0.60) 86.23 (0.33)</cell><cell>own experiment</cell></row><row><cell cols="2">Transductive Setting 20ng</cell><cell>R8</cell><cell>R52</cell><cell>ohsumed</cell><cell>MR</cell><cell>Provenance</cell></row><row><cell>Graph-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TextGCN</cell><cell>86.34</cell><cell>97.07</cell><cell>93.56</cell><cell>68.36</cell><cell>76.74</cell><cell>Yao et al. (2019)</cell></row><row><cell>SGC</cell><cell>88.5 (0.1)</cell><cell>97.2 (0.1)</cell><cell>94.0 (0.2)</cell><cell>68.5 (0.3)</cell><cell>75.9 (0.3)</cell><cell>Wu et al. (2019)</cell></row><row><cell>TensorGCN</cell><cell>87.74</cell><cell>98.04</cell><cell>95.05</cell><cell>70.11</cell><cell>77.91</cell><cell>Liu et al. (2020)</cell></row><row><cell>HeteGCN</cell><cell cols="6">87.15 (0.15) 97.24 (0.51) 94.35 (0.25) 68.11 (0.70) 76.71 (0.33) Ragesh et al. (2021)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Parameter counts of the models</figDesc><table><row><cell>Model</cell><cell>#parameters</cell></row><row><cell>WideMLP</cell><cell>31.3M</cell></row><row><cell>WideMLP-2</cell><cell>32.3M</cell></row><row><cell>GloVe+WideMLP</cell><cell>575,2M (frozen) + 0.3M</cell></row><row><cell cols="2">GloVe+WideMLP-2 575,2M (frozen) + 1.3M</cell></row><row><cell>DistilBERT</cell><cell>66M</cell></row><row><cell>BERT</cell><cell>110M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Total runtime (training+inference). Average of five runs rounded to minutes.</figDesc><table><row><cell>Model</cell><cell>#epochs</cell><cell>20ng</cell><cell>R8</cell><cell>R52</cell><cell>ohsumed</cell><cell>MR</cell></row><row><cell>WideMLP</cell><cell>100</cell><cell>7min</cell><cell cols="2">3min 4min</cell><cell>3min</cell><cell>4min</cell></row><row><cell>TF-IDF+WideMLP</cell><cell>100</cell><cell>9min</cell><cell cols="2">4min 4min</cell><cell>3min</cell><cell>4min</cell></row><row><cell>WideMLP-2</cell><cell>100</cell><cell>9min</cell><cell cols="2">5min 5min</cell><cell>3min</cell><cell>6min</cell></row><row><cell>GloVe+WideMLP</cell><cell>100</cell><cell>6min</cell><cell cols="2">3min 4min</cell><cell>3min</cell><cell>4min</cell></row><row><cell cols="2">GloVe+WideMLP-2 100</cell><cell>6min</cell><cell cols="2">4min 4min</cell><cell>3min</cell><cell>4min</cell></row><row><cell>DistilBERT</cell><cell>10</cell><cell>8min</cell><cell cols="2">4min 5min</cell><cell>3min</cell><cell>1min</cell></row><row><cell>BERT</cell><cell>10</cell><cell cols="3">15min 7min 8min</cell><cell>5min</cell><cell>2min</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://qwone.com/~jason/20Newsgroups/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://disi.unitn.it/moschitti/ corpora.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.cs.cornell.edu/people/ pabo/movie-review-data/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The source code is available online: https://github.com/lgalke/ text-clf-baselines</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>The focus of this work is text classification. Potential risks that apply to text classification in general also apply to this work. Nonetheless, we present alternatives to commonly used pretrained language models, which suffer from various sources of bias due to the large and poorly data used for pretraining <ref type="bibr" target="#b1">(Bender et al., 2021)</ref>. In contrast, the presented alternatives render full control over the training data and, thus, contribute to circumvent the biases otherwise introduced during pretraining.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey on data augmentation for text classification</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc-Andr?</forename><surname>Kaufhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Reuter</surname></persName>
		</author>
		<idno>abs/2107.03158</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001-12-03">2001. 2001. December 3-8, 2001</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are fewshot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><surname>Subbiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2126" to="2136" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02551274</idno>
	</analytic>
	<monogr>
		<title level="j">Math. Control. Signals Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we really making much progress? A worrying analysis of recent neural recommendation approaches</title>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Ferrari Dacrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<idno type="DOI">10.1145/3298689.3347058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems<address><addrLine>RecSys; Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-09-16">2019. 2019. September 16-20, 2019</date>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno>abs/2110.12894</idno>
		<title level="m">The efficiency misnomer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Be more with less: Hypergraph attention networks for inductive text classification</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.399</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4927" to="4936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GNNAutoScale: Scalable and expressive graph neural networks via historical embeddings</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07">2021. 18-24 July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="3294" to="3304" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A practical survey on faster and lighter transformers</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?tan</forename><surname>Marceau Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Aloise</surname></persName>
		</author>
		<idno>abs/2103.14636</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using titles vs. fulltext as source for automated semantic document annotation</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Galke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Brunsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansgar</forename><surname>Scherp</surname></persName>
		</author>
		<idno type="DOI">10.1145/3148011.3148039</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Knowledge Capture Conference, K-CAP 2017</title>
		<meeting>the Knowledge Capture Conference, K-CAP 2017<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-12-04">2017. December 4-6, 2017</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Comparing BERT against traditional machine learning text classification</title>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Carvajal</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">C</forename><surname>Garrido-Merch?n</surname></persName>
		</author>
		<idno>abs/2005.13012</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Graph Representation Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="DOI">10.2200/S01045ED1V01Y202009AIM046</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.372</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Survey on supervised machine learning techniques for automatic text classification</title>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ismael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kadhim</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10462-018-09677-1</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="292" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text classification algorithms: A survey</title>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjana</forename><surname>Mendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.3390/info10040150</idno>
	</analytic>
	<monogr>
		<title level="j">Inf</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015-01-25">2015. January 25-30, 2015</date>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A survey on text classification: From shallow to deep learning</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2008.00364</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Le. 2021a. Pay attention to MLPs</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<idno>abs/2105.08050</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tensor graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xien</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxin</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8409" to="8416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yanchun Liang, and Xiaoyue Feng. 2021b. Deep attention diffusion graph neural networks for text classification</title>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renchu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fausto</forename><surname>Giunchiglia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.642</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<biblScope unit="page" from="8142" to="8152" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph star net for generalized multi-task learning</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyan</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/1906.12330</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Combine convolution with recurrent networks for text classification</title>
		<author>
			<persName><forename type="first">Shengfei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2020. 2006.15795</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Luke Melas-Kyriazi. 2021. Do you even need attention? A stack of feed-forward layers does surprisingly well on ImageNet</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Galke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansgar</forename><surname>Scherp</surname></persName>
		</author>
		<idno type="DOI">10.1145/3197026.3197039</idno>
		<idno>abs/2105.02723</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries</title>
		<meeting>the 18th ACM/IEEE on Joint Conference on Digital Libraries<address><addrLine>Fort Worth, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-06-03">2018. 2018. June 03-07, 2018</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
	<note>Using deep learning for title-based semantic subject indexing to reach competitive performance to fulltext</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05">2013. December 5-8, 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 2021. Deep learning-based text classification: A comprehensive review</title>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3439726</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep double descent: Where bigger models and more data hurt</title>
		<author>
			<persName><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Kaplun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yamini</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards understanding the role of over-parametrization in generalization of neural networks</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
		<idno>abs/1805.12076</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Keygraph: Automatic indexing by cooccurrence graph based on building construction metaphor</title>
		<author>
			<persName><forename type="first">Yukio</forename><surname>Ohsawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nels</forename><forename type="middle">E</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiko</forename><surname>Yachida</surname></persName>
		</author>
		<idno type="DOI">10.1109/ADL.1998.670375</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Forum on Research and Technology Advances in Digital Libraries, IEEE ADL &apos;98</title>
		<meeting>the IEEE Forum on Research and Technology Advances in Digital Libraries, IEEE ADL &apos;98<address><addrLine>Santa Barbara, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998-04-22">1998. April 22-24, 1998</date>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219855</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale hierarchical text classification with recursively regularized deep graph-cnn</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178876.3186005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-23">2018. 2018. April 23-27, 2018</date>
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">HeteGCN: Heterogeneous graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Ragesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sundararajan</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Bairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Lingam</surname></persName>
		</author>
		<idno type="DOI">10.1145/3437963.3441746</idno>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event</title>
		<meeting><address><addrLine>Israel</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-03-08">2021. March 8-12, 2021</date>
			<biblScope unit="page" from="860" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470689646.ch1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas; Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley</addrLine></address></meeting>
		<imprint>
			<publisher>Ltd</publisher>
			<date type="published" when="2010">2016. 2010</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
	<note>Text Mining</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno>abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno>abs/1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms</title>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Masked language modeling and the distributional hypothesis: Order word matters pre-training for little</title>
		<author>
			<persName><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2888" to="2913" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MobileBERT: a compact task-agnostic BERT for resource-limited devices</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PTE: predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783307</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-08-10">2015. August 10-13, 2015</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno>abs/2009.06732</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">MLP-Mixer: An all-MLP architecture for vision</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2105.01601</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A comparison of deeplearning methods for analysing and predicting business processes</title>
		<author>
			<persName><forename type="first">Ishwar</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>T?llich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fairbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansgar</forename><surname>Scherp</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN52387.2021.9533742</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks, IJCNN 2021</title>
		<meeting><address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-07-18">2021. July 18-22, 2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for text classification</title>
		<author>
			<persName><forename type="first">Ruishuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2019.8852406</idno>
		<idno>IJCNN 2019</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hungary</forename><surname>Budapest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">July 14-19, 2019</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sentiment analysis by capsules</title>
		<author>
			<persName><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178876.3186015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web, WWW 2018<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-23">2018. April 23-27, 2018</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019. 2019, 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Graph neural networks for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shucheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Long</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2106.06090</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Efficient path prediction for semi-supervised and weakly supervised hierarchical text classification</title>
		<author>
			<persName><forename type="first">Huiru</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313658</idno>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13">2019. 2019. May 13-17, 2019</date>
			<biblScope unit="page" from="3370" to="3376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33017370</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. 2019. January 27 -February 1, 2019</date>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">When SIMPLE is better than complex: A case study on deep learning for predicting Bugzilla issue close time</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Yedida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<idno>abs/2101.06319</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bayesian performance comparison of text classifiers</title>
		<author>
			<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911547</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-07-17">2016. July 17-21, 2016</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Weakly-supervised text classification based on keyword graph</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiandong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.222</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2803" to="2813" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3485" to="3495" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A survey on text classification and its applications</title>
		<author>
			<persName><forename type="first">Xujuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Gururajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Revathi</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghazal</forename><surname>Bargshady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabal</forename><surname>Datta Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Kondalsamy-Chennakesavan</surname></persName>
		</author>
		<idno type="DOI">10.3233/WEB-200442</idno>
	</analytic>
	<monogr>
		<title level="j">Web Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="205" to="216" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
