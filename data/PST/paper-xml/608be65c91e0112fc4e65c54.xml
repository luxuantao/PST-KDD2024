<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online certification of preference-based fairness for personalized recommender systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-29">29 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Virginie</forename><surname>Do</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Corbett-Davies</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jamal</forename><surname>Atif</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
						</author>
						<title level="a" type="main">Online certification of preference-based fairness for personalized recommender systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-29">29 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.14527v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to assess the fairness of personalized recommender systems in the sense of envyfreeness: every (group of) user(s) should prefer their recommendations to the recommendations of other (groups of) users. Auditing for envyfreeness requires probing user preferences to detect potential blind spots, which may deteriorate recommendation performance. To control the cost of exploration, we propose an auditing algorithm based on pure exploration and conservative constraints in multi-armed bandits. We study, both theoretically and empirically, the trade-offs achieved by this algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recommender systems shape the information available to us and the opportunities we have access to. They help us prioritize content from news outlets and social networks, sort job offers, or find new people to connect with -from professional contacts to potential life partners. Substantial work has been done to audit online ranking or recommender systems <ref type="bibr" target="#b41">(Sweeney, 2013;</ref><ref type="bibr" target="#b19">Hannak et al., 2014;</ref><ref type="bibr" target="#b32">Lambrecht &amp; Tucker, 2019)</ref>. The goal has mostly been to assess parity criteria, for instance, whether housing recommendations are independent of the declared gender of the user. These audits focus only on the distribution of recommended items without regard to individual users' preferences. Their application to personalized recommender systems is thus limited to sanity checks such as the one described above, i.e., assessing whether recommendations are sensitive to an (inferred) protected group membership.</p><p>In this paper, we study algorithms that audit recommender systems from the perspective of preference-based fairness <ref type="bibr" target="#b45">(Zafar et al., 2017;</ref><ref type="bibr" target="#b42">Ustun et al., 2019;</ref><ref type="bibr" target="#b30">Kim et al., 2019)</ref>. We focus on recommendations that are envy-free, which means that users prefer their recommendations to the recommendations of other users. We also propose an extension of envy-free recommendations to groups of users rather than individuals, which is non-trivial because there is no single "group preference" nor "group recommendation".</p><p>Envy-freeness was originally studied in the domain of fair allocation <ref type="bibr" target="#b15">(Foley, 1967)</ref>. For recommender systems, it considers personalization fair as long as it is faithful to the underlying preferences of users. Moreover, unlike approaches based on parity measures, or the requirement that groups enjoy similar recommendation performance, envy-freeness is always compatible with optimal personalized recommendations. Yet, envy-freeness is not a performance criterion since giving the same recommendation to everyone is envyfree. In practice, recommender systems do personalize their predictions, but they are necessarily suboptimal because of the scarcity of per-user data. In that context, envy-freeness leaves enough freedom to accommodate every users' preferences, while precluding the unfair situation where users looking for the same opportunities are given recommendations of different quality.</p><p>The main challenge of auditing for envy-freeness is that it requires access to user preferences, which are only partially observed since users only interact with recommended items. Thus, to identify the recommender system's potential blind spots, the auditor needs an active exploration process that recommends items which would not have been recommended otherwise. To make such an exploration possible, we consider a scenario where the auditor is allowed to replace a user's recommendations with the recommendations that another user would have received in the same context. Envy, or the absence thereof, is estimated by suitably choosing whose recommendations should be shown to whom.</p><p>We first provide a formal analysis of envy-free recommender systems, including a non-trivial definition for groups. Then, we cast the problem of auditing for (group) envy-freeness as a new variant of pure exploration problems in bandits <ref type="bibr" target="#b0">(Audibert &amp; Bubeck, 2010;</ref><ref type="bibr" target="#b6">Bubeck et al., 2009)</ref>. To alleviate the potential negative side effect of exploration, namely that users may be given suboptimal recommendations, we follow the literature on conservative exploration <ref type="bibr" target="#b44">(Wu et al., 2016;</ref><ref type="bibr" target="#b17">Garcelon et al., 2020)</ref> and propose an algorithm that provably maintains, throughout the course of the audit, a performance close to the audited system. We study, formally and experimentally, the trade-offs between the three main criteria of the audit: the desired statistical confidence, the duration of the audit, and the per-user cost of exploration.</p><p>We discuss the related work in Sec.2. Envy-free and group envy-free recommender systems are studied in Sec. 3 and 4. The auditing algorithm and its theoretical guarantees are presented in Sec. 5, and the experimental analysis in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Fair recommendation The domain of fairness in machine learning is organized along two orthogonal axes. The first axis is whether fairness is oriented towards groups defined by protected attributes <ref type="bibr" target="#b2">(Barocas &amp; Selbst, 2016)</ref>, or rather oriented towards individuals <ref type="bibr" target="#b12">(Dwork et al., 2012)</ref>. The second axis is whether fairness is a question of parity (predictions [or prediction errors] should be the same between groups or individuals) <ref type="bibr" target="#b11">(Corbett-Davies &amp; Goel, 2018)</ref>, or preference-based (predictions are allowed to be different as long as they faithfully reflect the preferences of all parties) <ref type="bibr" target="#b45">(Zafar et al., 2017;</ref><ref type="bibr" target="#b30">Kim et al., 2019;</ref><ref type="bibr" target="#b42">Ustun et al., 2019)</ref>. Our work starts from the perspective of envy-freeness, which follows the preference-based approach and is oriented towards individuals. We also consider an extension to groups, though these need not be defined by protected attributes.</p><p>The literature on fair recommender systems covers two problems: auditing existing systems, and designing fair recommendation algorithms. Most of the auditing literature focused on group parity in recommendations <ref type="bibr" target="#b19">(Hannak et al., 2014;</ref><ref type="bibr" target="#b32">Lambrecht &amp; Tucker, 2019)</ref>, and equal user satisfaction <ref type="bibr" target="#b36">(Mehrotra et al., 2017;</ref><ref type="bibr" target="#b13">Ekstrand et al., 2018)</ref>, while our audit for envy-freeness focuses on whether personalized results are aligned with (unknown) user preferences. On the designing side, <ref type="bibr" target="#b37">Patro et al. (2020)</ref>; <ref type="bibr" target="#b23">Ilvento et al. (2020)</ref> cast fair recommendation as an allocation problem, with criteria akin to envy-freeness. They do not address the partial observability of preferences, so they cannot guarantee user-side fairness without an additional certificate that the estimated preferences effectively represent the true user preferences. Our work is thus complementary to theirs.</p><p>While we emphasized fairness for users, recommender systems are multi-sided <ref type="bibr" target="#b8">(Burke, 2017;</ref><ref type="bibr" target="#b37">Patro et al., 2020)</ref>, thus fairness can also be oriented towards recommended items <ref type="bibr" target="#b9">(Celis et al., 2017;</ref><ref type="bibr" target="#b5">Biega et al., 2018;</ref><ref type="bibr" target="#b18">Geyik et al., 2019)</ref>.</p><p>Multi-arm bandits In pure exploration bandits <ref type="bibr" target="#b6">(Bubeck et al., 2009)</ref>, an agent has to identify a specific set of arms after exploring as quickly as possible, without performance constraints. Our setting is close to threshold bandits <ref type="bibr" target="#b34">(Locatelli et al., 2016)</ref> where the goal is to find arms with better performance than a given baseline. Outside pure exploration, in the regret minimization setting, conservative exploration <ref type="bibr" target="#b44">(Wu et al., 2016)</ref> enforces the anytime average performance to be not too far worse than that of a baseline arm.</p><p>In our work, the baseline is unknown -it is the current recommender system -and the other "arms" are other users' policies. The goal is to make the decision as to whether an arm is better than the baseline, while not deteriorating performance compared to the baseline. We thus combine pure exploration and conservative constraints.</p><p>Existing work on fairness in exploration/exploitation <ref type="bibr" target="#b28">(Joseph et al., 2016;</ref><ref type="bibr" target="#b24">Jabbari et al., 2017;</ref><ref type="bibr" target="#b33">Liu et al., 2017)</ref> is different from ours because unrelated to personalization.</p><p>Envy in fair division Envy-freeness was first studied in social choice (specifically, in fair resource allocation) <ref type="bibr" target="#b15">(Foley, 1967)</ref>. Our setting is different because: a) the same item can be given to an unrestricted number of users, and b) true user preferences are unknown. There are several notions of group envy-freeness in fair division, either considering every subset of individuals (e.g., <ref type="bibr" target="#b4">Berliant et al. (1992)</ref>), or pre-defined groups (e.g., <ref type="bibr" target="#b35">Manurangsi &amp; Suksompong (2017)</ref>), but they are conceptually different from ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Envy-free recommendations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>There are M users, and we identify the set of users with [M ] = {1, . . . , M }. A personalized recommender system has one stochastic recommendation policy π m per user m. We denote by π m (a|x) the probability of recommending item a ∈ A for user m ∈ [M ] in context x ∈ X . We assume that X and A are finite to simplify notation, but this has no impact on the results. We consider a synchronous setting where at each time step, the recommender system observes a context x m t ∼ q m for each user, selects an item a m t ∼ π m (.|x m t ) and observes reward r m t ∼ ν m (a m t |x m t ) ∈ [0, 1]. We denote by ρ m (a|x) the expected reward for user m and item a in context x, and, for any recommendation policy π, u m (π) is the utility of m for π:</p><formula xml:id="formula_0">u m (π) = E x∼q m E a∼π(.|x) E r∼ν m (a|x) [r] = x∈X a∈A q m (x)π(a|x)ρ m (a|x)<label>(1)</label></formula><p>We assume that the environment is stationary: the context and reward distributions q m and ν m , as well as the policies π m are fixed. Even though in practice policies evolve as they learn from user interactions and user needs change over time, we leave the study of non-stationarities for future work.</p><p>The stationary assumption approximately holds when these changes are slow compared to the time horizon of the audit, which is reasonable when significant changes in user needs or recommendation policies take e.g., weeks. Our approach applies when items a are single products as well as when items are structured objects such as rankings. Examples of (context x, item a) pairs include: x is a query to a search engine and a is a document or a ranking of documents, or x is a song chosen by the user and a a song to play next or an entire playlist. Remember, our goal is not to learn the user policies π m , but rather to audit existing π m s for fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">-envy-free recommendations</head><p>The preference-based approach to fair machine learning stipulates that it is fair to apply different policies to different individuals or groups as long as it is in everyone's interest. Following this principle, we consider that the personalization of recommendations is fair only if it better accommodates individuals' preferences. The fairness criterion associated to preference-based fairness is envy-freeness <ref type="bibr" target="#b15">(Foley, 1967;</ref><ref type="bibr" target="#b1">Balcan et al., 2018;</ref><ref type="bibr" target="#b42">Ustun et al., 2019;</ref><ref type="bibr" target="#b30">Kim et al., 2019)</ref>. Adapting the previous definitions to the context of personalized recommendation, we consider that a system is fair as long as each user (weakly) prefers their recommendations to the recommendations of other users:</p><formula xml:id="formula_1">Definition 3.1. Let ≥ 0. A recommender system is -envy- free if: ∀m, n ∈ [M ] : u m (π n ) ≤ + u m (π m ).</formula><p>Envy-freeness neither guarantees the parity of recommendations, nor equal user satisfaction. In particular, since a non-personalized recommender system 1 is always envy-free, a system that only follows the preferences of one (group of) users can be envy-free, even if it is unfair according to other standards. As usual in the assessment of fairness, several diagnostics should be applied for a full assessment. Parity and equal satisfaction are straightforward to estimate since they do not require any form of exploration, which is why we focus in this paper only on how to audit for envy-freeness.</p><p>Among these criteria, envy-freeness is the only one compatible with optimal recommendations (see below). For instance, equal user satisfaction is impossible to achieve when some users have intrinsically more noisy preferences.</p><p>Violating parity or equality of satisfaction is only an indication that the system may fail, while violating envy-freeness is an indication that the system does fail. It is thus a much stronger signal than the other diagnostics.</p><p>Envy-freeness vs. optimality certificates Let π m, * ∈ argmax π u m (π) denote an optimal recommendation policy for m. Then the optimal recommender system (π m, * ) m∈M is envy-free since:</p><formula xml:id="formula_2">u m (π m, * ) = max π u m (π) ≥ u m (π n, * ).</formula><p>To understand the differences between a certificate of envyfreeness and a certificate of optimality, let us denote by Π * = {π : ∃u satisfying (1) , π ∈ argmax π u(π )} the set 1 i.e., ∀m, n, π m = π n of potentially optimal policies. If the set of users policies approximately covers the set of potentially optimal policies Π * , then an envy-free system is also optimal. 2 In practice, the space of optimal policies is much larger than the number of users (for instance, there are |A| |X | optimal policies in our setting), so that auditing for envy is tractable in cases where auditing for optimality is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sources of envy</head><p>Real-world recommender systems cannot learn optimal recommendation policies due to the lack of per-user data. They must rely on strong modeling assumptions with methods such as low-rank matrix factorization <ref type="bibr" target="#b31">(Koren et al., 2009)</ref>. This leads to envy when the system misrepresents the preferences of some users while matching those of users with similar tastes. Another source of envy is when the measurement of relevant side information varies across groups <ref type="bibr" target="#b40">(Suresh &amp; Guttag, 2019)</ref>. Many hybrid recommender systems indeed rely on user-side data such as listening history on a music streaming platform. Systematic envy arises if this information is unevenly collected across users, for example if the platform's inventory does not cover niche music tastes. We show experimentally in Sec. 6.1 and App. B.2 how mispecified models (e.g. with a rank that is too low) and measurement bias can create envy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Compatibility with item-side fairness</head><p>Envy-freeness is a user-centric notion. Towards multisided fairness <ref type="bibr" target="#b8">(Burke, 2017)</ref>, we analyze the compatibility of envyfreeness with item-side fairness criteria for rankings from <ref type="bibr" target="#b39">Singh &amp; Joachims (2018)</ref>, based on sensitive categories of items (denoted A 1 , ..., A S ). Parity of exposure prescribes that for each user, the exposure of an item category should be proportional to the number of items in that category. In Equity of exposure 3 , the exposure of item categories should be proportional to their average relevance to the user.</p><p>The optimal policies under parity and equity of exposure constraints, denoted respectively by (π m,par ) M m=1 and (π m,eq ) M m=1 , are defined given user m and context x as:</p><formula xml:id="formula_3">(parity) π m,par (.|x) = argmax p:A→[0,1] a p(a)=1 a∈A p(a)ρ m (a|x) u.c. ∀s ∈ [S], a∈As p(a) = |A s | |A| .<label>(2)</label></formula><p>Optimal policies under equity of exposure are defined simi-  • the policies (π m,par ) M m=1 are envy-free, while</p><p>• the policies (π m,eq ) M m=1 are not envy-free in general.</p><p>Optimal recommendations under parity of exposure are envy-free because the parity constraint (2) is the same for all users. Given two users m and n, π m,par is optimal for m under (2) and π n,par satisfies the same constraint, so we have u m (π m,par ) ≥ u m (π n,par ).</p><p>In contrast, the optimal recommendations under equity of exposure are, in general, not envy-free. A first reason is that less relevant item categories reduce the exposure of more relevant categories: a user who prefers item a but who also likes item b from another category envies a user who only liked item is a. Note that amortized versions of the criterion and other variants considering constraint averages over user/contexts <ref type="bibr" target="#b5">(Biega et al., 2018;</ref><ref type="bibr" target="#b37">Patro et al., 2020)</ref> have similar pitfalls unless envy-freeness is explictly enforced, as in <ref type="bibr" target="#b37">Patro et al. (2020)</ref> who developed an envyfree algorithm assuming the true preferences are known. For completeness, we describe in App. A a second reason why equity of exposure constraints create envy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Auditing scenario</head><p>The auditing scenario is the following: at each time step t, the auditor chooses to either (a) give the user a "normal" recommendation, or (b) explore user preferences by giving the user a recommendation from another user. This scenario has the advantage of lightweight infrastructure requirements, since the auditor only needs to query another user's policy, rather than implementing a full recommender system within the operational constraints of the platform. Moreover, this interface is sufficient to estimate envy because envy is defined based on the performance of other user's policies. This type of internal audit <ref type="bibr" target="#b38">(Raji et al., 2020)</ref> requires more access than usual external audits that focus on recommendation parity, but this is necessary to explore user preferences.</p><p>We note the auditor must make sure that this approach follows the relevant ethical standard for randomized experiments in the context of the audited system. The auditor must also check that using other user's recommendation policies does not pose privacy problems. From now on, we assume these issues have been resolved.</p><p>p(a), which is equivalent to (2). A similar remark holds for the equity constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Group envy-free recommendations</head><p>In this section, we present an extension of envy-free recommendation to groups rather than individuals. The motivation for this extension is twofold. First, it generalizes the existing definitions of preference-based fairness (which are oriented towards protected groups rather than individuals <ref type="bibr" target="#b45">(Zafar et al., 2017;</ref><ref type="bibr" target="#b42">Ustun et al., 2019)</ref>) to personalized predictions. Second, as we shall see in the next section, assessing group-level envy requires less exploration per user as groups become larger. Thus, in practice, it is likely that only group-level certificates have manageable sample complexity.</p><p>From now on, we assume we are given a partition of the users g 1 , . . . , g G , i.e., i∈[G] g i = [M ] and g i ∩ g j = ∅ when i = j. We define the group utility µ i for g i as the average utility of users in g i :</p><formula xml:id="formula_4">µ i = 1 |g i | m∈gi u m (π m ) .<label>(3)</label></formula><p>There are two desiderata for a definition of group envy-free recommendations:</p><p>i) following our scenario, we can only query other users' policies to perform exploration actions.</p><p>ii) group envy-freeness must relax envy-freeness, in the sense that an -envy-free recommender system must be -group envy-free for every partition of users.</p><p>The notion of envy-free recommendations described in the previous section uses u m (π n ), the utility of user m for user n's policy. The challenge is to define an analogous quantity between groups, which we solve using what we call matching policies. Given two groups i and k, we compute new policies π m k for every user m in g i by "reallocating" them the recommendations made to the users of g k . To solve point i) above, this reallocation is done by considering matching policies that are mixtures of users' policies: For every groups g i , g k and every user m ∈ g i :</p><formula xml:id="formula_5">∃(γ m,n ) n∈g k s.t. π m k = n∈g k γ m,n π n (4)</formula><p>where ∀n ∈ g k , γ m,n ≥ 0 and</p><formula xml:id="formula_6">n∈g k γ m,n = 1</formula><p>Notice that our recommendation policies are stochastic, so γ m,n reweights the probabilities to recommend each item.</p><p>We study several possible definitions for γ m,n later in this section. The advantage of these matching policies is that the auditor can obtain a recommendation from π m k by first sampling n from (γ m,n ) n∈g k and then querying π n , which fits our auditing scenario (point i) above).</p><p>With this constraint in mind, we can define -group envyfreeness using matching policies: Definition 4.1. Given i, k ∈ [G] and matching policies (π m k ) m∈gi , the utility of g k with respect to g i is</p><formula xml:id="formula_7">µ i k = 1 |g i | m∈gi u m (π m k ) .<label>(5)</label></formula><p>Given ≥ 0, the recommender system is -group-envy-free if</p><formula xml:id="formula_8">∀i, k ∈ [G] : µ i k ≤ µ i i + .<label>(6)</label></formula><p>Group envy-freeness is equivalent to envy-freeness when each group is a singleton. We use µ i i instead of µ i in ( <ref type="formula" target="#formula_8">6</ref>) to make it comparable to µ i k . The difference between the two is that µ i i uses the matching policies π m i instead of the usual recommendation policies π m , and we may have π m i = π i depending on the matching rule. The important result is that point ii) is satisfied when</p><formula xml:id="formula_9">µ i i = µ i : Proposition 2. If the mixture weights in (4) are such that ∀i ∈ [G], µ i i = µ i</formula><p>,then an -envy-free recommender system is -group envy-free for every partition</p><formula xml:id="formula_10">(g i ) i∈[G] . Proof. Since (π m ) M m=1 are -envy-free, for every i, k ∈ [G] and users m ∈ g i , n ∈ g k , we have, u m (π n ) ≤ u m (π m )+ . Since u m is linear in the policy, we have u m (π m k ) = n∈g k γ m,n u m (π n ) ≤ n∈g k γ m,n (u m (π m ) + ) . Since n∈g k γ m,n = 1, we have u m (π m k ) ≤ u m (π m ) + . Summing over m ∈ g i , we obtain µ i k ≤ µ i + = µ i i + .</formula><p>We now give two examples of matching policies to illustrate the importance of Proposition 2.</p><p>Average group policy A first possible definition for a matching policy is to take the average policy over a group. <ref type="formula">4</ref>). Unfortunately, this simple matching policy does not satisfy desideratum ii): Proposition 3. With average group policies, there exist envyfree recommender systems that are not group envy-free.</p><formula xml:id="formula_11">Let i, k ∈ [G] and m ∈ g i . The average group policy of g k for user m is π m k = 1 |g k | n∈g k π n , i.e., γ m,n = 1 |g k | for all n ∈ g k in (</formula><p>The reason for Prop. 3 is that when a group contains users with different policies, the average policy represents no one's preferences and µ i i µ i , violating the condition of Prop. 2. In some cases, this leads to detecting group envy even though there is no individual envy. A minimal example of such cases is presented in Table <ref type="table">1</ref>.</p><p>Mixtures from optimal transport In order to guarantee µ i i = µ i , we propose to define the mixture weights γ m,n for m ∈ g i and n ∈ g k using optimal transport <ref type="bibr" target="#b43">(Villani, 2008)</ref>.</p><p>The distance used to match two users m, n is the supremum</p><formula xml:id="formula_12">item 1 item 2 µ i µ 1 k µ 2 k g 1 ρ 1 , π 1 1, 1 0, 0 1 ρ 2 , π 2 0.5, 0 1, 1 g 2 ρ 3 , π 3 1, 1 0, 0 1 ρ 4 , π 4 1, 1 0, 0 (k=1) π m 1 0.5 0.5 0.625 0.5 (k=2) π m 2 1 0 0.75 1 Table 1.</formula><p>Example where average group policies fail on desideratum ii). The policies π m are optimal for utilities ρ m , so the recommendations are envy-free. However, because users in g1 have different optimal policies, the average policy is bad for both (µ 1 1 µ 1 ). Using average group policies, group envy is undesirably detected.</p><p>(over contexts) of the variational between policies c m,n = sup x∈X π m (.|x) − π n (.|x) 1 . Without prior knowledge, policies are the only available user information to the auditor. The matching could also use a task-specific distance <ref type="bibr" target="#b12">(Dwork et al., 2012)</ref>, if additional user information from the platform or guidelines from a policymaker are accessible.</p><p>Let P be the transportation polytope between the uniform distributions over g i and g k , i.e., each γ ∈ P is an |g i | × |g k | matrix with non-negative entries, rows summing to </p><formula xml:id="formula_13">γi,k = argmin γ∈P m∈gi n∈g k γm,n c m,n ∀m ∈ g i , γ m,n = |g i |γ i,k m,n .</formula><p>For m ∈ g i , the transported policy is</p><formula xml:id="formula_14">π m k = n∈g k γ m,n π n .</formula><p>Notice that γ defines a proper mixture since</p><formula xml:id="formula_15">n∈g k γ m,n = 1.</formula><p>Proposition 4. With transported policies, an -envy-free recommender system is -group envy-free for every</p><formula xml:id="formula_16">(g i ) i∈[G] .</formula><p>Prop. 4 then follows from Prop. 2, since with our definition of c, matched users have the same policies, so that µ i = µ i i . In App. C, we describe ways to construct groups in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Certificates of envy-freeness</head><p>We now turn to our auditing algorithm for envy-freeness. We directly focus on groups and only consider transported policies. For every group g i , the auditor must estimate whether all other groups k satisfy µ i k ≤ µ i + . To estimate µ i k , we obtain samples by making recommendations using the transported policy π m k and observing the reward, for every m ∈ g i . The remaining challenge is to choose which group k to sample at each time step while not deteriorating the user experience too much.</p><p>Algorithm 1: OCEF algorithm. ξ t (line 4) evaluates the conservative exploration constraint and is defined in (8). Values for β k (t) and confidence bounds µ k and µ k are given in Lemma 3.</p><p>input :Confidence parameter δ, conservative exploration parameter α, envy parameter output :ENVY or −NO ENVY 1 S 0 ← [K] // all arms except 0 2 for t=1, . . . do 3 Choose t from S t−1 // e.g., unif. sample</p><formula xml:id="formula_17">4 if β 0 (t−1) &gt; min k∈St−1 β k (t−1) or ξ t &lt; 0 then k t ← 0 5 else k t ← t 6 r t ←PULL-ARM(k t ) // update conf. intervals with Lem. 3 7 S t ← k ∈ S t−1 : µ k (t) &gt; µ 0 (t) + 8 if ∃k ∈ S t , µ k (t) &gt; µ 0 (t) then return ENVY 9 if S t = ∅ then return -NO ENVY 10 end</formula><p>We first describe the auditing algorithm when estimating envy for a single target group g i . For a full audit of a recommender system, we apply the algorithm to all groups in parallel. To simplify notation, we drop all superscripts i: we use g and µ k instead of g i and µ i k . We also re-index groups from 0 to K = G − 1, where index 0 represents the target group (µ 0 is thus the utility of the group for the audited recommender system). Because the audit is a special form of bandit problem, following the bandit literature, an index of a group is called an arm, and arm 0 is the baseline.</p><p>Objectives and evaluation metrics We present our algorithm OCEF (Online Certification of Envy-Freeness) in the next subsection. Given &gt; 0 and α ≥ 0, OCEF returns either ENVY or -NO-ENVY and has two objectives:</p><formula xml:id="formula_18">1. Correctness: if OCEF returns ENVY, then ∃k, µ k &gt; µ 0 . If OCEF returns -NO-ENVY then max k∈[K] µ k ≤ µ 0 + .</formula><p>2. Recommendation performance: during the audit, OCEF must maintain a fraction 1−α of the baseline performance. Denoting by k s ∈ {0, . . . , K} the arm (group index) chosen at round s, this requirement is formalized as a conservative exploration constraint <ref type="bibr" target="#b44">(Wu et al., 2016)</ref>:</p><formula xml:id="formula_19">∀t, 1 t t s=1 µ ks ≥ (1 − α)µ 0 .<label>(7)</label></formula><p>We focus on the fixed confidence setting, where given a confidence parameter δ ∈ (0, 1) the algorithm provably satisfies both objectives with probability 1 − δ. In addition, there are two criteria to assess an online auditing algorithm:</p><p>1. Duration of the audit: the number of time-steps before the algorithm stops.</p><p>2. Cost of the audit: the cumulative loss of rewards incurred. Denoting the duration by τ , the cost is τ µ 0 − τ s=1 µ ks . It is possible that the cost is negative when there is envy. In that case, the audit increased recommendation performance by finding better recommendations for the group.</p><p>We note the asymmetry in the return statements of the algorithm: ENVY does not depend on . This asymmetry is necessary to obtain finite worst-case bounds on the duration and the cost of audit, as we see in Theorem 1. We now describe the algorithm, and then provide theoretical guarantess for the objectives and evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The OCEF algorithm</head><p>OCEF is described in Alg. 1. It maintains confidence intervals on arm performances (µ k ) K k=0 . Given the confidence parameter δ, the lower and upper bounds on µ k at time step t, denoted by µ k (t) and µ k (t), are chosen so that with probability at least <ref type="formula">2014</ref>), we use anytime bounds inspired by the law of the iterated logarithm. These are given in Lem. 3 in App. G. OCEF maintains an active set S t of all arms in [K] (i.e., excluding the baseline) whose performance are not confidently less than µ 0 + . It is initialized to S 0 = [K] (line 1). At each round t, the algorithm selects an arm t ∈ S t (line 3). Then, depending on the state of the conservative exploration constraint (described later), the algorithm pulls k t , which is either t or the baseline (lines 4-6). After observing the reward r t , the confidence interval of µ t is updated, and all active arms that are confidently worse than the baseline plus are de-activated (line 7). The algorithm returns ENVY if an arm k is confidently better than the baseline (line 8), returns -NO-ENVY if there are no more active arms, (line 9) or continues if neither of these conditions are met.</p><formula xml:id="formula_20">1 − δ, we have ∀k, t, µ k ∈ [µ k (t), µ k (t)]. In the algorithm, β k (t) = (µ k (t) − µ k (t))/2. As Jamieson et al. (</formula><p>Conservative exploration To deal with the conservative exploration constraint (7), we follow <ref type="bibr" target="#b17">(Garcelon et al., 2020)</ref>. Denoting A t = {s ≤ t : k s = 0} the time steps at which the baseline was not pulled, we maintain a confidence interval such that with probability ≥ 1 − δ, we have ∀t &gt; 0, s∈At (µ ks − r s ) ≤ Φ(t). The formula for Φ is given in Lem. 4 in App. G. This confidence interval is used to estimate whether the conservative constraint ( <ref type="formula" target="#formula_19">7</ref>) is met at round t as follows. First, let us denote by N k (t) the number of times arm k has been pulled until t, and notice that ( <ref type="formula" target="#formula_19">7</ref>) is equivalent to s∈At µ ks −((1−α)t−N 0 (t))µ 0 ≥ 0. After choosing t (line 3), we use the lower bound on s∈At µ ks and the upper bound for µ 0 to obtain a conservative estimate of (7). Using τ = t − 1, this leads to:</p><formula xml:id="formula_21">ξ t = s∈Aτ r s − Φ(t) + µ t (τ ) + (N 0 (τ ) − (1 − α)t)µ 0 (τ ) . (8)</formula><p>Then, as long as the confidence intervals hold, pulling t does not break the constraint (7) if ξ t ≥ 0. The algorithm thus pulls the baseline arm when ξ t &lt; 0. To simplify the theoretical analysis, OCEF also pulls the baseline if it does not have the tightest confidence interval (lines 4-6).</p><p>PULL-ARM OCEF is an exploration algorithm which only chooses a group index. Once a group has been chosen, PULL-ARM (line 6 of Alg. 1) chooses the effective recommendations to make to users. The PULL-ARM routine is described in Alg. 2 (App. F). If OCEF chooses the baseline arm, then every user m in the audited group receives a recommendation from π m as usual. If OCEF chooses k = 0, then for every user m in the audited group, the recommendation uses the transported policy π m k defined in 4.2. It first samples n ∼ (γ m,n ) n ∈g k , and then queries n's policy π n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis</head><p>The main theoretical result of the paper is the following:</p><formula xml:id="formula_22">Theorem 1. Let ∈ (0, 1], α ∈ (0, 1], δ ∈ (0, 1 2 ) and η k = max(µ k − µ 0 , µ 0 + − µ k ) and h k = max(1, (|g|η k ) −1 ).</formula><p>Using µ, µ and Φ given in Lemmas 3 and 4 (App. G), OCEF achieves the following guarantees with probability ≥ 1 − δ:</p><p>• OCEF is correct and satisfies the conservative constraint on the recommendation performance (7).</p><p>• The duration is in</p><formula xml:id="formula_23">O K k=1 h k log K log( Kh k/δη k ) δ min(αµ 0 , η k ) . • The cost is in O k:µ k &lt;µ0 (µ0−µ k )h k η k log K log( Kh k/δη k )) δ .</formula><p>The important problem-dependent quantity η k is the gap between the baseline and other arms k. It is asymmetric depending on whether the arm is better than the baseline (µ k −µ 0 ) or the converse (µ 0 −µ k + ) because the stopping condition for ENVY does not depend on . This leads to a worst case that only depends on , since η k = max(µ k − µ 0 , µ 0 − µ k + ) ≥ 2 , while if the condition was symmetric, we would have possibly unbounded duration when µ k = µ 0 + for some k = 0. Overall, ignoring log terms, we conclude that when αµ 0 is large, the duration is of order |g|η k when αµ 0 is small compared to η k . This means that the conservative constraint has an impact mostly when it is relatively strict. It also means that when either αµ 0 η k or η 2 k η k the cost can be small even when the duration is fairly high. In all cases, it is critical to note that both the cost and duration, which are per-user quantities, decrease linearly with the group size |g|. Thus assessing group envy-freeness becomes more sample efficient (per user) as groups become larger.</p><p>Full audit To audit for the full system, we apply OCEF to all groups simultaneously. By the union bound, using δ = δ G instead of δ in OCEF's confidence intervals, the guarantees of Theorem 1 hold simultaneously for all groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We present two experiments describing sources of envy (Sec. 6.1) and one experiment evaluating the auditing algorithm OCEF on a movie recommendation task (Sec. 6.2).</p><p>We consider two measures of the degree of envy for groups (with equivalent notions for individuals). Denoting ∆ i = max(max</p><formula xml:id="formula_24">k∈[G] µ i k − µ i i , 0</formula><p>), these are:</p><p>• the average envy experienced by groups:</p><formula xml:id="formula_25">1 G i∈[G] ∆ i ,</formula><p>• the proportion of -envious groups:</p><formula xml:id="formula_26">1 G i∈[G]</formula><p>1 {∆ i &gt; } .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Envy from model mispecification on MovieLens</head><p>We demonstrate that envy arises from a standard recommendation model when the modeling assumptions are too strong. We simulate a movie recommendation task using the MovieLens-1M dataset <ref type="bibr" target="#b20">(Harper &amp; Konstan, 2015)</ref>, which contains ratings of movies by real users together with demographic information. We extract a 2000 × 2000 user × items matrix, keeping users and items with the most ratings, and fill in missing entries using a popular matrix completion algorithm<ref type="foot" target="#foot_3">5</ref> . This completed matrix serves as ground truth user preferences. Recommendations use low-rank matrix completion <ref type="bibr" target="#b3">(Bell &amp; Sejnowski, 1995)</ref> on a training sample of 20% of the ratings, where the rated items are sampled  uniformly at random. We generate binary ratings using a Bernoulli distribution with expectation given by our ground truth. Recommendations use a softmax policy over the predicted ratings. We study other policies in App. B.1.</p><p>We define user groups by MovieLens demographic attributes and their intersection. In particular, there are 2 groups for gender, 7 for age, 21 for occupation, 14 for age-gender, 42 for gender-occupation, and 120 for age-occupation.</p><p>In Figure <ref type="figure" target="#fig_3">1</ref>, we consider individuals and age-occupation groups. We observe that with one latent factor there is no envy. This is because all users receive the same recommendations since matrix completion is then equivalent to a popularity-based recommender system. With enough latent factors, preferences are properly captured by the model and the degree of envy tends to zero. For intermediate number of latent factors however, envy is visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation of the audit algorithm on MovieLens</head><p>Our goal is now to answer the following for OCEF: (1) What is the interplay between the required number of samples per user, the cost of exploration and the conservative exploration parameter? (2) In practice, how does the audit duration change as the number of groups or groups' sizes changes?</p><p>The task is to certify the (absence of) envy of the argmax policy for different types of groups on MovieLens. We use 50 latent factors. There is envy for 9 age-occupation groups, and no envy for the other group configurations.</p><p>Fig. <ref type="figure" target="#fig_4">2</ref> shows the result of OCEF on the various types of groups, where the curves are averaged on all groups, except for age-occupation groups where the average is over envious groups only (and averaged over 20 trials, with δ = = 0.05). The curves show that for gender-occupation and ageoccupation, duration is minimal for a non-trivial α. This is because when α is large, all arms are pulled as much as the baseline, so their confidence intervals are similar. When α decreases, the baseline is pulled more, which reduces the length of the relevant confidence intervals β 0 (t) + β k (t) for all arms k. This, in turn, shortens the audit because nonbaseline arms are more rapidly discarded or declared better.</p><p>When α becomes too small, however, the additional pulls of the baseline have no effect on β 0 (t) + β k (t) because it is dominated by β k (t), so the duration only increases. This subtle phenomenon is not captured by our analysis (Th. 1), because the ratios β 0 (t)/β k (t) are difficult to track formally.</p><p>The sign of the cost of exploration depends on whether there is envy. For envious age-occupation groups, exploration is beneficial to the users and so the cost is negative. On all other partitions however, the cost is positive. The cost of exploration increases with α and is closest to 0 when α becomes small because then β 0 (t) + β k (t) is the smallest possible for a given number of pulls of k.</p><p>The actual orders of magnitude of the audit duration in Fig. <ref type="figure" target="#fig_4">2</ref> seem large with more granular groups (e.g., 10 3 − 10 4 time steps for gender-occupation), but this is because the groups are small since there are only 2000 users in total. However, as seen in Th. 1, larger groups allow for shorter audits and smaller cost of exploration per user. To give a more complete picture with larger user databases, we artificially generate larger groups by randomly duplicating users. Fig. <ref type="figure" target="#fig_5">3</ref> presents the duration of the experiments as a function of the average size of the group. We see that, as expected from the theory, durations decrease linearly with the absolute group size, while also increasing with the number of groups.</p><p>Looking at age-gender, we see that group sizes of 150 users already allow for audits of 150 time steps, which is promising for online services with large user databases.</p><p>We give more details in App. D.2 and present qualitatively similar results on another dataset in App. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed the audit of recommender systems for user-side fairness with the criterion of envy-freeness, and discussed a generalized definition of group envy-freeness. The auditing problem requires an explicit exploration of user preferences, which leads to a formulation as a bandit problem with conservative constraints. We presented an algorithm for this problem and analyzed its performance experimentally.</p><p>item cat. 1 item cat. 2 utilities</p><formula xml:id="formula_27">(item idx) 1 2 3 4 u 1 u 2 (rewards) ρ 1 1 0 0.8 0.7 ρ 2</formula><p>0.8 0.7 1 0 (policies) π 1,eq 0.4 0 0.6 0 0.88 0.92 π 2,eq 0.6 0 0.4 0 0.92 0.88</p><p>Table <ref type="table">2</ref>. Example where the optimal recommendations under itemside equity of exposure constraints are not user-side fair because both users envy each other. There are 4 items, 2 item categories and 2 users. User 1 envies user 2 since u 1 (π 2,eq ) &gt; u 1 (π 1,eq ). Also, u 2 (π 1,eq ) &gt; u 2 (π 2,eq ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More on equity of exposure</head><p>We remind the definition of optimal policies with equity of exposure constraints from Section 3.3:</p><p>(equity) π m,eq (.|x) = argmax The constraints should be ignored when a∈A ρ m (a|x) = 0.</p><p>Following Proposition 1 from Section 3.3, we describe here a second source of envy when using optimal policies with equity of exposure constraints. By the linearity of the optimization problem for π m,eq , the policy assigns to the best item in a category the exposure of the entire category. It implies that categories with high average relevance have more exposure than categories with few but highly relevant items. Table <ref type="table">2</ref> gives an example with two users and two categories of items where both users envy each other with the optimal recommendations under equity of exposure constraints.</p><p>In some degenerate cases though, equity of exposure policies are envy-free. Lemma 2. If for all contexts x ∈ X , each user m ∈ [M ] only likes a single item category A sm , i.e. ∀a ∈ A \ A sm , ρ m (a|x) = 0, then the policies (π m,eq ) M m=1 are envyfree.</p><p>Proof. We set contexts x aside to simplify notation, but the generalization is straightforward.</p><p>We actually prove a stronger result than the lemma: if each user m only likes a single item, then (π m,eq ) M m=1 = (π m, * ) M m=1 , where π m, * is the optimal unconstrained policy for m. Let a m s = argmax a∈As ρ m (a) be the favorite item in category A s for user m, then the optimal equity of exposure constrained policies has the following analytical expression:</p><formula xml:id="formula_28">∀s ∈ S, ∀a ∈ A s , π m,eq (a) = 1 {a=a m s } a∈As ρ m (a ) a ∈A ρ m (a )</formula><p>, and we thus have: Then u m (π m,eq ) = ρ m (a m sm ) = max a∈A ρ m (a). Then π m,eq is the optimal unconstrained policy for user m, meaning the whole system is envy-free (cf. Sec 3.2).</p><formula xml:id="formula_29">u m (π m,eq ) = s∈[S]</formula><p>From Eq. 9, we actually note that (π m,eq ) M m=1 = (π m, * ) M m=1 if and only if each user m equally values their favorite items in each category they like, i.e. ∀m, ∃κ &gt; 0, ∀s ∈ S, ρ m (a m s ) &gt; 0 ⇒ ρ m (a m s ) = κ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sources of envy</head><p>In this section, we first list a few possible sources of envy in recommender systems. Then we describe experiments<ref type="foot" target="#foot_4">6</ref> which showcase two of these sources, namely model mispecification (App. B.1) and measurement bias (App. B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of sources of envy</head><p>Model mispecification Recommender systems often rely on strong modeling assumptions and multi-task learning, with methods such as low-rank matrix factorization <ref type="bibr" target="#b31">(Koren et al., 2009)</ref>. The limited capacity of the models (e.g., a rank that is too low) or incorrect assumptions might leave aside users with less common preference patterns. Appendix B.1 gives a more detailed example than Sec. 6 on a simulated movie recommendation task.</p><p>Misaligned incentives A recommender system might have incentives to recommend some items to specific users, e.g., sponsored content. Envy appears when there is a mismatch between users who like these items and users to whom they are recommended.</p><p>Measurement bias Many hybrid recommender systems rely on user interactions together with user-side data <ref type="bibr" target="#b7">(Burke, 2002)</ref>. This includes side-information such as browsing history on third-party, partner websites. Envy arises in these settings if there is measurement bias <ref type="bibr" target="#b40">(Suresh &amp; Guttag, 2019)</ref>, e.g., if the side information is unevenly collected for all users. Group envy arises when the information is missing in correlation to relevant social groups (e.g., browsing patterns are different per groups and partners are aligned with the patterns of a few groups only). Appendix B.2 provides an example environment where group envy arises.</p><p>Operational constraints Regardless of incentives, recommendations might need to obey additional constraints.</p><p>As described in Proposition 1, the item-side fairness constraint of equity of exposure is an example of possible source of (user-side) envy.</p><p>In the following, we present examples of environments where envy holds at the individual or group level, based on two of the previous sources of envy. The experiments for model mispecification (App. B.1) extend our simulations from Section 6.1 to various types of policies, and demonstrates how envy is observed at the level of individuals and small groups, on a typical movie recommendation task. For measurement bias (App. B.2), we describe a synthetic controlled experiment where envy systematically arises for groups of any sizes.</p><p>In these experiments, we measure envy for groups based on the quantity:</p><formula xml:id="formula_30">∆ i = max(max k∈[G] µ i k − µ i i , 0).</formula><p>In line with <ref type="bibr" target="#b10">(Chevaleyre et al., 2017)</ref>, we consider two ways of measuring the degree of envy, stated in the language of groups (with equivalent notions for individuals):</p><p>• the average envy experienced by groups:</p><formula xml:id="formula_31">1 G i∈[G] ∆ i ,</formula><p>• the proportion of -envious groups:</p><formula xml:id="formula_32">1 G i∈[G]</formula><p>1 {∆ i &gt; } .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Experiments with envy from model mispecification</head><p>We simulate a movie recommendation task using the MovieLens-1M dataset <ref type="bibr" target="#b20">(Harper &amp; Konstan, 2015)</ref>, which contains 1 million ratings from approximately 6000 users and 4000 movies. We use demographic attributes to group users by their gender (2 groups), age (7 groups), occupation (21 groups), intersection of gender-age (14 groups) and gender-occupation (42 groups), age-occupation (121 groups). We use movie genres to create item categories, keeping only the first genre if several are reported (18 categories).</p><p>We extract a 2000 × 2000 user × items matrix, keeping users and items with the most ratings, and fill in missing entries using a popular matrix completion algorithm<ref type="foot" target="#foot_5">7</ref> . This completed matrix serves as ground truth user preferences.</p><p>We then simulate a recommender system's estimation of preferences using low-rank matrix completion <ref type="bibr" target="#b3">(Bell &amp; Sejnowski, 1995)</ref> on a training sample of 20% of the ratings, where the rated items are sampled uniformly at random for each user. We generate binary ratings using a Bernoulli distribution with expectation given by our ground truth. This procedure is the same as the one used in our experiments presented in Section 6.1. More details on hyperparameters and methods can be found in Appendix D.2.</p><p>The first type of recommendation policies we use is the unconstrainted optimal policy (see Section 3.2) computed on estimated preferences. It is called the argmax policy because it recommends the single estimated preferred item of a user.</p><p>The second type of policy, the PAR policy, is the optimal policy under parity of exposure constraints (see Section 3.3) computed on estimated preferences as well. In that case, we use movie genres to create item categories, keeping only the first genre if several are reported (18 categories).</p><p>For both policies above, the recommendation task is to recommend a single item. We also add a ranking task. The ranking policy returns the ordered list of top-10 items according to estimated preferences. We use a standard ranking metric to evaluate the utility of a ranking, the Discounted Cumulative Gain <ref type="bibr" target="#b27">(Järvelin &amp; Kekäläinen, 2002)</ref>. Given a user m and a policy π, it is computed by:</p><formula xml:id="formula_33">DCG(π) = 10 j=1 ρ m (a π j ) log(1 + j)</formula><p>where a π j is the j-th item in the ranking given by π, and ρ m (a π j ) is the ground truth value of that item for user m. In Figures <ref type="figure" target="#fig_10">4, 5, 6</ref>, the variation of envy with the number of latent factors is qualitatively similar to the experiment with softmax policy shown in Sec. 6.1, with a peak of the degree of envy for a nontrivial number of latent factors.</p><p>In the next experiment, we describe a controlled experiment where envy arises systematically for a disadvantaged group, regardless of its size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Experiments with group envy from measurement bias</head><p>We consider an instance of measurement bias <ref type="bibr" target="#b40">(Suresh &amp; Guttag, 2019)</ref>, where the observation of relevant information varies across groups. Our example is a streaming platform whose inventory barely covers the music tastes of some specific group. The system misses a large part of their true listening history, which results in systematically noisier recommendations of new items for the disadvantaged group.  Formal model Formally, we consider the item set A (e.g., new music tracks to recommend) and a set of documents D (e.g., tracks in the inventory) of size D that users visited in the past. There are no contexts in this model.</p><p>We first generate ground-truth user preferences ρ m (a) by taking the dot product between a user embedding v m and an item embedding e a in a latent space of dimension d. These embeddings are drawn uniformly at random from unit vectors with positive coordinates of dimension d. Ratings sampled from this ground truth follow a Bernoulli distribution with parameter σ(α v m , e a − t) where t is a threshold and σ is the sigmoid function. Therefore, the true preferences over new items in A follow the same distribution across groups.</p><p>We correlate the true listening history of m in group g i with its group membership as follows. Let H 1 = {1, ..., d 2 } and H 2 = { d 2 , ..., d}, and let τ 1 , τ 2 be group-specific parameters, with τ 1 &gt; τ 2 . For h ∈ D, let e h be a document embedding generated similarly to the item embeddings, and let</p><formula xml:id="formula_34">X m h ∼ Pois(τ i v m , e h )1 {h∈Hi} .</formula><p>This means that the observations X m ∈ R D of the listening history for each user m are groupwise orthogonal, and that the amount of available listening counts varies by group (as controlled by τ i ).</p><p>Based on these biased observations, the recommender system learns a simple rating model by linear regression in the document space, and the system computes a stochastic policy that is a softmax distribution over these estimated ratings,  with parameter β to control the softmax temperature:</p><formula xml:id="formula_35">rm (a) = X m X m 2 ÊT a , π m,softmax (a) = σ(β rm (.)) a</formula><p>Due to the lack of historical data for the disadvantaged group g 2 whose representation is orthogonal to that of g 1 , the system is likely to incorrectly predict the best item for any user m in g 2 , whereas their counterpart n with similar preferences in the advantaged group g 1 is accurately given their favorite item. Therefore, any m in g 2 is likely to envy their alter ego n in g 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation</head><p>We consider 2 groups of 1000 users each (i.e., M = 2000), |A| = 200 new items to recommend and an inventory of |D| = 200 documents. For the ground truth ratings, we use a latent space of dimension d = 10 and set the parameters t = 1 d , α = 30 d. For the correlation of the history with groups, we set parameter τ 1 = 50 and we vary τ 2 in {0. <ref type="bibr">1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 50}.</ref> For the softmax policy, we vary the thermodynamic β in {0.01, 0.025, 0.05, 0.1, 1}. Ground truth ratings for the items A are sampled uniformly random, such that the linear regression uses a 20%/20%/60% train/valid/test split. The softmax policy π m,softmax is restricted to items in the test subset.</p><p>Recall that true user preferences over new items follow the same distribution across groups, whereas groups have orthogonal listening histories. Therefore, group g 2 is disadvantaged since less data is available for them. Fig. <ref type="figure" target="#fig_11">7</ref> plots individual and group envy as a function of the proportion of observed items for the disadvantaged group. We observe that the proportion of -envious individual users in the disadvantaged group g 2 decreases when a larger proportion of the listening history is available.</p><p>We now analyze group envy for the disadvantaged group. We observe that when no history is observed for the disadvantaged group, there is no envy. This is because in the absence of data, the predictions are random, and so users are matched randomly from g 2 to g 1 . When some history is observed but in small proportion, group envy is highest, and it decreases as a larger proportion of the history is revealed. Moreover, the softmax parameter β has an effect on group envy: when β increases, the softmax policy is closer to an argmax policy. Therefore, the ground cost used when computing transported policies becomes uninformative: users are matched uniformly at random between g 2 and g 1 , hence our inability to observe envy when β = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Groups in practice</head><p>Here we describe a favorable property of homogeneous groups and its implication for constructing groups in practice.</p><p>When policies in groups are homogeneous, then the average group policy and the transported policy are similar, so -envy-freeness translates to -group envy-freeness for average group policies as well, with ≈ . More interestingly, when we have prior knowledge that users in all groups have homogeneous preferences and homogeneous policies, the reciprocal of Proposition 4 from Sec. 4 holds for both average and transported policies: Proposition 5. Let , ˜ &gt; 0, and assume that for all groups i ∈ [G] and all pairs of users m, n in the same group g i , we have</p><formula xml:id="formula_36">sup x∈X π m (.|x) − π n (.|x) 1 ≤ ˜ and sup x∈X ρ m (.|x) − ρ n (.|x) 1 ≤ ˜ .</formula><p>Then, with either average group policies or transported group policies, -group envy-freeness implies ( + 4˜ )-envyfreeness.</p><p>The result is natural since when all groups have users with homogeneous preferences and policies, groups and users are a similar entity as far as the assessment of envy-freeness is concerned. The proof is straightforward and omitted. The interest of this remark lies in a better understanding of how to construct groups in practice, since homogeneity in policies and preferences impact how much a certificate of -group envy-freeness should be interpreted as a certificate of -envy-freeness with ≈ .</p><p>In the favorable case where we have strong prior knowledge on user preferences, building groups by clustering by user preferences allows the conditions of Proposition 5 to hold (assuming the system meets the minimal requirement of similar policies given similar preferences). The number of users in each cluster controls the trade-off between the strength of the guarantee (in terms of +4˜ of Prop. 5 8 ) and the number of required exploration steps per user. In that case, the certification of envy-freeness aims at certifying that preferences are properly represented in the recommendations.</p><p>In many cases, however, the auditor does not have prior knowledge on user preferences. An audit for group envyfreeness in this case will likely only be able to detect envy when some groups receive systematically worse recommendations than other groups. In the literature on fairness in machine learning, groups are most often defined by identity of sensitive attributes such as gender, ethnicity, age <ref type="bibr" target="#b2">(Barocas &amp; Selbst, 2016)</ref>, as well as intersections of them <ref type="bibr" target="#b29">(Kearns et al., 2018)</ref>, because these are dimensions of frequent systematic undesirable bias. Thus, groups that only differ by a value of these attributes should be compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. OCEF experiments D.1. Bandit experiments</head><p>We performed experiments on toy bandit environments to assess the performance of our algorithm OCEF on various configurations, which were also considered in <ref type="bibr">(Jamieson &amp; Nowak, 2014)</ref>. The four bandits instances have 10 arms. They are Bernoulli variables with means equal to 1) µ 0 = 0.6 and µ k = 0.3 for k ∈ [9],</p><p>2) µ 0 = 0.3, µ 1 = 0.6 and µ k = 0.3 for k = 2..9, 3) µ k = 0.7 − 0.7 * k 10 0.6 , k = 0, ..., 9, and the baseline is µ 0 , 4) same as 3), but permuting µ 0 and µ 1 .</p><p>8 ˜ is typically larger as soon as groups contain more users. Fig. <ref type="figure" target="#fig_12">8</ref> shows the result of applying OCEF on the various configurations, where we set δ = = 0.05 and report results averaged over 100 trials. We observe clear tendencies similar to those of the MovieLens experiments presented in Section 6.2, although the exact sweet spots in terms of α depends on the specific configuration.</p><p>The cost of exploration follows similar patterns as in the MovieLens experiments. In Prob. 2, the baseline has the worst performance, so exploration is beneficial to the user and the cost is negative. On the other hand, for instance in Prob. 4, the cost is close to 0 when α is very small and increases with α. It is the case where the baseline is not the best arm but is close to it, and there are many bad arms. When the algorithm is very conservative, bad arms are discarded rapidly thanks to the good estimation of the baseline performance. In this "low-cost" regime however, the audit is significantly longer.</p><p>We show additional results when varying δ in Figure <ref type="figure" target="#fig_13">9</ref>. Results are averaged over 100 simulations and the conservative exploration parameter is set to α = 0.05. The duration decreases as δ increases, i.e. a lower confidence certificate requires fewer samples per user. The duration for Problem 1 is longer than for the other instances. This is because with α set to 0.05 and the baseline mean being much higher than non-baseline arms, the conservative constraint 7 enforces many pulls of the baseline, since each exploration round is very costly. As a consequence, too little data is collected on the non-baseline arms to conclude that they are below µ 0 + . Since all non-baseline arms have equal means, the size of the active set remains the same for a long time, while in Problem 3, where the baseline is also the best arm, arms are eliminated one at a time.</p><p>We show how OCEF scales with the number of arms in Figure <ref type="figure" target="#fig_14">10</ref>, for fixed values α = δ = = 0.05. We set K max = 100 and define 4 instances as in the list above, except that K = K max instead of K = 9. We run OCEF on the instances µ 0:K and vary the value of K ≤ K max . The duration increases for all problems, and the slope depends on the gaps between µ 0 and the µ k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. MovieLens experiments</head><p>We sample 20% of the entries uniformly at random for each user to be observed, and the observed value is drawn from a Bernoulli distribution with mean given by the ground truth value. Matrix factorization is run with 50 latent factors, using a Python implementation 9 of a popular matrix completion algorithm algorithm. Hyperparameter search is conducted for the regularization parameter chosen in <ref type="bibr">[0.01, 0.1, 1, 10]</ref>. Notice that we aim at estimating envy of the resulting recommender system, so the exact training 9 Using https://github.com/gbolmier/funk-svd  method is not important as long as it is reasonable. We use an argmax policy that recommends the best item according to the estimated preferences. The transported policies are computed using Python Optimal Transport Library <ref type="bibr" target="#b14">(Flamary &amp; Courty, 2017)</ref>.</p><p>For α = = 0.05, the effect of varying the confidence parameter δ is shown in Figure <ref type="figure" target="#fig_15">11</ref>. Consistently with our upper bound (Theorem 1), the number of samples collected per user (left) decreases as δ increases. The sample complexity required for the partition gender-occupation (42 groups) is orders larger than for the partition by gender (2 groups). This highlights again the trade-off between the granularity of the partition of users in group and the duration of the audit. The cost per user (right) follows the same decreasing trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional experiments</head><p>We present a batch of experiments with different settings than the main one presented in Section 6. Since all results are qualitatively similar, we did not include them in the core paper. The goal is to confirm the following claim: the sparsity of the training set only affects the quality of the recommender system, and not the auditing process.</p><p>Using the same protocol, we repeated the experiments Precisely, to study the effect of sparsity, we subsampled training sets with 10% and 5% of the MovieLens "ground truth" ratings (obtained with matrix completion). As in our main experiments, these training sets are given as input to the FunkSVD algorithm, and the recommender system uses a softmax policy over predicted ratings. We remind that in Sec. 6, the same protocole was used with a training set made of 20% of the ratings.</p><p>Then, to compare our results on MovieLens with another dataset, we use the Google Local ratings dataset from <ref type="bibr" target="#b21">(He et al., 2017)</ref>. It includes reviews and ratings from 11,4M users about 3,1M businesses. As in <ref type="bibr" target="#b37">(Patro et al., 2020)</ref>, we restrict the dataset to users and businesses located in New York and with at least 10 reviews. We draw a random sample of 2,000 users among the remaining ones. With the same protocole as for MovieLens, we generate "ground truth" ratings for each user-item pair with standard matrix completion. This results in a full rating matrix with 2,000 users and 855 items. As before, we simulate a recommender system by training Funk-SVD on a subsample of 20% of the ratings, and using a softmax policy over predicted ratings. In the absence of demographic attributes, we focus on individual envy-freeness.</p><p>For the experiments on envy from mispecification (as in Sec. 6.1), <ref type="bibr">Figures 12,</ref><ref type="bibr">13,</ref><ref type="bibr">14</ref> show that the degree of envy varies similarly w.r.t. the number of latent factors across experimental settings. The only difference is the amplitude of envy.</p><p>Then, we evaluate the auditing algorithm (as in Sec. 6.2) on each of these environments, and observe that the performance of OCEF is not affected. Fig. <ref type="figure" target="#fig_9">15</ref>, 16 show that the results are qualitatively similar when recommendations are generated from sparser rating matrices extracted from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. The PULL-ARM routine</head><p>The PULL-ARM routine used in Alg. 1 is described in Alg. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Theoretical results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1.1. USEFUL LEMMAS</head><p>Recall that OCEF considers a single audited group g.</p><p>The algorithm relies on valid confidence intervals. As in <ref type="bibr">(Jamieson et al., 2014)</ref>, we use anytime bounds inspired by the law of the iterated algorithm (LIL), and a union bound.</p><p>We say that a random variable is σ-subgaussian if it is subgaussian with variance proxy σ 2 . Since we assume the rewards for each user are bounded, more precisely r m t ∈ [0, 1], they are 1 2 -subgaussian. Throughout the paper, we assume that rewards for each user are independent conditionally to the arm played.</p><p>Lemma 3. Let δ ∈ (0, 1). Assume the rewards are σsubgaussian.</p><p>Let ω ∈ (0, 1), θ = log(1 + ω) ωδ 2(2+ω)</p><formula xml:id="formula_37">1 1+ω . Then P [E] ≥ 1 − δ. Proof. By Lemma 3, P [E 1 ] ≥ 1 − δ 2</formula><p>. By the lemma above, with probability 1 − δ 2 , we have for all t &gt; 0, s∈At−1 (µ ks − r s ) ≤ φ(t). Then, notice that s∈At−1</p><formula xml:id="formula_38">(µ ks − r s ) = K k=1 N k (t − 1)(µ k − µ k (t − 1)) .</formula><p>Hence under E 1 we also have:</p><formula xml:id="formula_39">s∈At−1 (µ ks − r s ) ≤ K k=1 N k (t − 1)β k (t − 1).</formula><p>Therefore,</p><formula xml:id="formula_40">E = E 1 ∩ E 2 = E 1 ∩ s∈At−1 (µ ks − r s ) ≤ φ(t) ,</formula><p>and thus, by a union bound, we have: P</p><formula xml:id="formula_41">[E] ≥ 1 − δ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1.2. THEOREMS</head><p>We now provide our complete theoretical guarantees for correctness (Theorem 5), duration (Theorem 6) and cost (Theorem 7), which we then prove in App. G.2 and G.3. From these results, we derive Theorem 1 in the main paper, which we prove in App. G.4.</p><p>Theorem 5 (Correctness). With probability at least 1 − δ:</p><p>1. OCEF satisfies the safety constraint (7) at every time step, 2. if OCEF outputs -NO ENVY then g is not -envious, and if it outputs ENVY, then g is envious.</p><p>We denote log + (.) = max(1, log(.)).</p><p>Theorem 6 (Duration).</p><formula xml:id="formula_42">Let η k = max(µ k −µ 0 , µ 0 + −µ k ), δ ∈ (0, 1), θ = log(2) δ 6 ,<label>and</label></formula><formula xml:id="formula_43">∀k = 0, H k = 1 + 64 |g|η 2 k log 2(K + 1) log + 128(K+1) θ|g|η 2 k θ , H 0 = max max k∈[K] H k , 6K + 2 αµ 0 + K k=1 256 log 2(K+1) log(2H k ) θ αµ 0 |g|η k .</formula><p>With probability at least 1 − δ, OCEF stops in at most τ steps, with</p><formula xml:id="formula_44">τ ≤ K k=0 H k .</formula><p>Finally, we define the cost of exploration as the potential reward lost because of exploration actions, in our case the cumulative reward lost, on average over users in the group:</p><formula xml:id="formula_45">C t = tµ 0 − t s=1 µ ks .<label>(10)</label></formula><p>In the worst case, the following bound holds:</p><p>Theorem 7 (Cost of exploration). Under the assumptions and notation of Theorem 6, let τ be the time step where OCEF stops. With probability 1 − δ, we have:</p><formula xml:id="formula_46">C τ ≤ k:µ k &lt;µ0 (µ 0 − µ k )H k (11)</formula><p>Certification for all groups The audit of the full system consists in running OCEF for every group. Since we are making multiple tests, we need to use a tighter confidence parameter for each group so that the confidence intervals simultaneously hold for all groups.</p><p>Corollary 7.1 (Online certification). With probability at least 1 − δ, running OCEF simultaneously for all G groups, each with confidence parameter δ = δ G , we have:</p><p>1. for all i ∈ [G] OCEF satisfies the constraints (7), 2. all groups for which OCEF returns -NO ENVY are not -envious of any other users, and all groups for which OCEF returns ENVY are envious of another group.</p><p>3. For every group, the bounds on the duration of the experiment and the cost of exploration given by Theorems 6 and 7 (using δ/G instead of δ) are simultaneously valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Proof of Theorem 5</head><p>Proof. We assume that event E holds true. Then all confidence intervals are valid, i.e., for all k = 0, ..., K, µ k (t) ≤ µ k ≤ µ k (t), and s∈At−1 µ ks ≥ s∈At−1 r s − Φ(t).</p><p>Let Z t be the safety budget, defined as Z t = t s=1 µ ks − (1 − α)µ 0 t, so that the conservative constraint ( <ref type="formula" target="#formula_19">7</ref>) is equivalent to ∀t, Z t ≥ 0. We have Z t = s∈At−1 µ ks + µ kt + (N 0 (t − 1) − (1 − α)t)µ 0 . Therefore, ξ t (eq. ( <ref type="formula">8</ref>)) is a lower bound on the safety budget Z t if t is played. By construction of the algorithm, the safety constraint ( <ref type="formula" target="#formula_19">7</ref>) is immediately satisfied since a pull that could violate it is not permitted.</p><p>By the validity of confidence intervals under E, if OCEF stops because of the first condition, then ∃k, µ k &gt; µ 0 . Therefore 0 is not -envious of k and OCEF is correct.</p><p>If OCEF stops because of the second condition, i.e., ∀k, µ k (t) ≤ µ 0 (t) + , then ∀k, µ k ≤ µ 0 + . Therefore 0 is not envious and OCEF is correct.</p><p>Since P [E] ≥ 1 − δ, OCEF satisfies the safety constraint and is correct with probability ≥ 1 − δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Proofs of Theorem 6 and Theorem 7</head><p>Notation For conciseness, we use K = K + 1, and</p><formula xml:id="formula_47">ψ k (t) = 2σ 2 (1 + √ ω) 2 (1 + ω) log 2 K θ log((1 + ω)N k (t)) |g| , so that β k (t) = ψ k (t) N k (t) .</formula><p>We shall also use</p><formula xml:id="formula_48">Γ ω = 2σ 2 (1+ √ ω) 2 (1+ω) |g|</formula><p>. We use the convention ψ k (t) = 0 when N k (t) = 0, and set β k (t) to some value strictly larger than when N k (t) = 1.</p><p>We remind that ω ∈ (0, 1), θ = log(1 + ω) ωδ 2(2+ω)</p><formula xml:id="formula_49">1 1+ω and k = max(µ k − µ 0 , µ 0 + − µ k ). We denote by η min = min k∈[K] η k .</formula><p>Finally, we notice that under event E (as defined in Sec. G.1.1), we have for all k ∈ {0, . . . , K} and all t:</p><formula xml:id="formula_50">µ k + 2β k (t) ≥ µ k (t) ≥ µ k ≥ µ k (t) ≥ µ k − 2β k (t). (12) Lemma 8. Under event E, for every k ∈ [K], if k is pulled at round t, then 4β k (t) ≥ η k .</formula><p>Proof of Lemma 8. Since k is pulled at t, the two following inequalities hold:</p><formula xml:id="formula_51">µ k (t − 1) &gt; µ 0 (t − 1) + (13) µ k (t − 1) ≤ µ 0 (t − 1)<label>(14)</label></formula><p>We prove them by contradiction. If (13) does not hold, then k should be discarded from the active set at time t − 1, and therefore cannot be pulled at t. Likewise, if (14) does not hold, then the algorithm stops at t − 1, so k cannot be pulled at t.</p><p>Using ( <ref type="formula">13</ref>) and ( <ref type="formula">12</ref>), we have:</p><formula xml:id="formula_52">µ k +2β k (t−1) ≥ µ k (t−1) &gt; µ 0 (t−1)+ ≥ µ 0 −2β 0 (t−1)+ .</formula><p>Since 0 was not pulled at time t, we also have</p><formula xml:id="formula_53">β 0 (t − 1) ≤ β k (t − 1), hence 4β k (t − 1) ≥ µ 0 + − µ k .</formula><p>Using ( <ref type="formula" target="#formula_51">14</ref>) and ( <ref type="formula">12</ref>) we have µ k − 2β k (t) ≤ µ 0 + 2β 0 (t) and since β 0 (t) ≤ β k (t), we obtain 4β k (t − 1) ≥ µ k − µ 0 .</p><p>In the following lemma, we recall that we denote log + (.) = max(1, log(.)).</p><p>Lemma 9. Under event E, ∀τ &gt; 0, ∀k ∈ [K], we have</p><formula xml:id="formula_54">N k (τ ) ≤ H k with H k = 1 + 32σ 2 (1 + √ ω) 2 (1 + ω) |g|η 2 k × log 2(K + 1) log + 64(K+1)σ 2 (1+ √ ω) 2 (1+ω) 2 θ|g|η 2 k θ Proof. Let τ &gt; 0, k ∈ [K]</formula><p>, and let t ≤ τ be last time step before τ at which k was pulled. If such a t does not exist, then N k (τ ) = 0 and the result holds. In all cases, we have</p><formula xml:id="formula_55">N k (t) = N k (τ ).</formula><p>We consider t &gt; 0 from now on.</p><p>By Lemma 8, we have 4β k (t − 1) ≥ η k , and thus</p><formula xml:id="formula_56">N k (t − 1) ≤ 16ψ k (t−1) η 2 k , which writes, if N k (t) &gt; 0: N k (t − 1) ≤ 16ψ k (t − 1) η 2 k ≤ 16Γ ω η 2 k log 2 K θ log ((1 + ω)N k (t − 1)) . (15) Using 1 t log log((1+ω)t) Ω ≥ c ⇒ t ≤ 1 c log log((1+ω)/cΩ) Ω (see Equation (1) in (Jamieson et al., 2014)) with Ω = θ 2 K and c = η 2 k 16Γω , we obtain N k (t−1) ≤ 16Γ ω η 2 k log 2 K θ log (1 + ω)32 KΓ ω θη 2 k (16)</formula><p>Since N k (t) = N k (t − 1) + 1, using log + instead of log inside to deal with the case N k (t − 1) = 0 gives the desired result.</p><p>Lemma 10. Under event E, at every time step τ , we have</p><formula xml:id="formula_57">N 0 (τ ) ≤ max max k∈[K] H k , 6K + 2 αµ 0 + K k=1 64σ 2 (1 + √ ω) 2 (1 + ω) log 2(K+1) log((1+ω)H k ) θ αµ 0 |g|η k Proof.</formula><p>Let τ &gt; 0 and t ≤ τ the last time 0 was pulled before τ . We assume t &gt; 0.</p><p>Case 1: 0 was pulled because β 0 (t − 1) &gt; min k∈[K] β k (t − 1).</p><p>Then N 0 (τ ) = N 0 (t − 1) + 1 ≤ 1 + max k =0 N k (t − 1).</p><p>By lemma 8, we thus have N 0 (τ ) ≤ max k∈[K] H k .</p><p>Case 2: 0 was pulled because ξ t &lt; 0. Here the proof follows similar steps as that of Theorem 5 in <ref type="bibr" target="#b44">(Wu et al., 2016)</ref>.     Recall that Φ(t) = min( K k=1 N k (t − 1)β k (t − 1), φ(t)), and therefore Φ(t) ≤ </p><formula xml:id="formula_58">f k ≤ 16ψ k (t − 1) η k + η k + 4 16ψ k (t − 1) 2 η 2 k + ψ k (t − 1)</formula><p>Using ( x z ) 2 + x ≤ x z + z 2 for x ≥ 0, z &gt; 0, with x = 4ψ k (t − 1) and z = η k , we obtain:</p><formula xml:id="formula_59">f k ≤ 16ψ k (t − 1) η k + 16ψ k (t − 1) η k + 3η k ≤ 32ψ k (t − 1) η k + 3η k .<label>(18)</label></formula><p>Using ψ k (t − 1) = Γ ω log 2 K θ log((1 + ω)N k (t − 1)) if N k (t) &gt; 0 and N k (t − 1) ≤ H k by Lemma 9, we obtain</p><formula xml:id="formula_60">f k ≤ 32Γ ω η k log 2 K θ log ((1 + ω)H k ) + 3η k .</formula><p>This bound is also valid when N k (t) &gt; 0.</p><p>Going back to (17), and since µ 0 ≤ µ 0 (t − 1) under E, we have (notice η k ≤ 2 since µ k ∈ [0, 1] and ∈ [0, 1]):</p><formula xml:id="formula_61">αN 0 (t − 1)µ 0 ≤(1 − α)µ 0 (t − 1) + 6K + K k=1 32Γ ω η k log 2 K θ log ((1 + ω)H k ) .<label>(19)</label></formula><p>To bound the first term of the right-hand side, let us first notice that the final result holds if N 0 (t − 1) ≤ max k∈[K] H k . So we can assume N 0 (t − 1) &gt; max k∈[K] H k from now on. By the definition of the H k s (see above (15)), this implies N 0 (t − 1) &gt; 16ψ0(t−1) η 2 min , which in turn implies 4β 0 (t − 1) ≤ η min .</p><p>We thus use µ 0 (t − 1) ≤ µ 0 + 2β 0 (t − 1) ≤ µ 0 + ηmin 2 ≤ 2, which gives the final result.</p><p>The result directly follows from (19).</p><p>The proof of Theorem 6 follows from τ = K k=1 N k (τ ) + N 0 (τ ), by setting ω = 1 for ease of reading, and σ = 1 2 since Bernoulli variables are 1 2 -subgaussian (using Hoeffding's inequality <ref type="bibr" target="#b22">(Hoeffding, 1963)</ref>).</p><p>We prove Corollary 7.1 from Theorem 5 and Theorem 6.</p><p>We now prove Theorem 7:</p><p>Proof. Since playing the baseline is neutral in the cost of exploration, it can be re-written as:</p><formula xml:id="formula_62">C τ = K k=1 (µ 0 − µ k )N k (τ ) ≤ k:µ k &lt;µ0 (µ 0 − µ k )N k (τ ),</formula><p>where τ is the time the algorithm stops. Using Lemma 9 to upper bound N k (τ ), we obtain the result.</p><p>Corollary 7.1 simply follows from the fact that by applying each algorithm with confidence δ/G, the confidence intervals are then simultaneously valid for all groups with probability 1 − δ, so all the correctness/duration/cost proofs holds for all groups simultaneously with probability 1 − δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Proof of Theorem 1</head><p>Theorems 5, 6, and 7 are summarized in Theorem 1 in the main paper. We restate with corrections Theorem 1 and prove it below:</p><p>Theorem. Let ∈ (0, 1], α ∈ (0, 1], δ ∈ (0, 1 2 ) and η k = max(µ k −µ 0 , µ 0 + −µ k ) and h k = max(1, 1 |g|η k</p><p>).</p><p>Using µ, µ and Φ given in Lemmas 3 and 4, OCEF achieves the following guarantees with probability at least 1 − δ:</p><p>• OCEF is correct and satisfies the conservative constraint on the recommendation performance (7).</p><p>• The duration is in</p><formula xml:id="formula_63">O K k=1 h k log K log( Kh k δη k</formula><p>) δ min(αµ 0 , η k ) .</p><p>• The cost is in</p><formula xml:id="formula_64">O k:µ k &lt;µ0 (µ0−µ k )h k η k log K log( Kh k δη k ) δ .</formula><p>Proof. With δ ∈ (0, 1 2 ), let θ = log(2) δ 6 . Then Theorems 6 and 7 hold for (δ, θ).</p><p>Duration We first show that: δ ≥ 3 as soon as K ≥ 2. We thus have</p><formula xml:id="formula_65">H k = O h k η k log Kh k δη k ,<label>(20)</label></formula><formula xml:id="formula_66">H k = 1 + O 1 |g|η 2 k log K δ log Kh k δη k =B ,<label>(22)</label></formula><p>Using log(x) ≤ x ⇒ x log(x) ≤ x 2 for x ≥ 0, and the fact that log We have:</p><formula xml:id="formula_67">Γ = O K αµ 0 + K k=1 h k αµ 0 log K log(H k ) δ = O K k=1 h k αµ 0 log K log(H k ) δ = O K k=1 h k αµ 0 log K log( Kh k δη k ) δ ,</formula><p>where the second equality is because K = K k=1 1 ≤ K k=1 h k , and the last equality uses eq. ( <ref type="formula">21</ref>). Combining this with eq. ( <ref type="formula" target="#formula_65">20</ref>) we have:</p><formula xml:id="formula_68">H 0 = O K k=1 h k min(αµ 0 , η k ) log K log( Kh k δη k ) δ .</formula><p>Using eq. ( <ref type="formula" target="#formula_65">20</ref>) again to bound τ = H 0 + K k=1 H k , , we get the desired bound for duration.</p><p>Cost For the cost, we remind the bound given in Th. 7:</p><formula xml:id="formula_69">C τ ≤ k:µ k &lt;µ0 (µ 0 − µ k )H k = O k:µ k &lt;µ0 (µ 0 − µ k )h k η k log K δ log Kh k δη k</formula><p>(26) using ( <ref type="formula" target="#formula_66">22</ref>) and 1 + 1</p><formula xml:id="formula_70">|g|η 2 k = O( h k η k ).</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>larly 4 , but the constraints are ∀s, a∈As p(a) = a∈As ρ m (a|x) a∈A ρ m (a|x) . We show their relation to envy-freeness: Proposition 1. With the above notation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 |gi| and columns summing to 1 |g k | . The transported policies are then defined as: Definition 4.2. Let i, k ∈ [G]. Define γi,k and γ as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>cost is of order k 1 |g|η k . This becomes k 1 αµ0|g|η k and k 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Effect of model mispecification on MovieLens, with 121 groups defined by intersecting age and occupation.</figDesc><graphic url="image-1.png" coords="7,307.44,67.06,234.00,79.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Scaling w.r.t. α on MovieLens experiments. There are 2 gender, 7 age, and 21 occupation groups.</figDesc><graphic url="image-2.png" coords="8,55.44,67.06,234.00,96.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Scaling w.r.t. group size on MovieLens experiments. Larger groups allow for shorter durations.</figDesc><graphic url="image-3.png" coords="8,55.44,213.75,234.01,96.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>m ∈ [M ] only likes a single item category s m ∈ [S], i.e. ∀a ∈ A \ A sm , ρ m (a) = 0, then a∈As ρ m (a) a∈A ρ m (a) = 1 {s=sm} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Model mispecification on MovieLens with Argmax policy. Groups are defined by age-occupation.</figDesc><graphic url="image-4.png" coords="13,55.44,67.06,234.00,83.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Model mispecification on MovieLens with Ranking policy. Groups are defined by age-occupation.</figDesc><graphic url="image-5.png" coords="13,55.44,197.29,234.01,82.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Model mispecification on MovieLens with Parity of exposure. Groups are defined by age-occupation.</figDesc><graphic url="image-7.png" coords="13,307.44,192.51,234.00,82.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Envy caused by measurement bias, as a function of the proportion of observed data for the disadvantaged groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Effect of the conservative exploration parameter α on the duration and cost of auditing on Bandit experiments.</figDesc><graphic url="image-8.png" coords="14,307.44,67.06,234.01,114.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Effect of the confidence parameter δ on the duration and cost on 4 different bandit instances.</figDesc><graphic url="image-9.png" coords="15,307.44,67.06,234.01,114.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Effect of the number of arms on the duration on 4 different bandit instances.</figDesc><graphic url="image-10.png" coords="15,365.94,231.40,117.00,117.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Scaling w.r.t. δ on MovieLens experiments.</figDesc><graphic url="image-11.png" coords="16,55.44,67.06,234.00,113.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Model mispecification on MovieLens with Softmax policy, trained on 5% of the ratings. Groups are defined by ageoccupation.</figDesc><graphic url="image-12.png" coords="16,307.44,67.06,234.00,77.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Model mispecification on MovieLens with Softmax policy, trained on 10% of the ratings. Groups are defined by age-occupation.</figDesc><graphic url="image-13.png" coords="16,307.44,218.19,234.01,77.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>0 (t−1) − (1 − α)t)µ 0 (t−1) &lt; 0We drop µ t (t−1), replace t by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>K</head><label></label><figDesc>k=0 N k (t − 1) + 1 and rearrange terms to obtain:αN 0 (t − 1)µ 0 (t − 1) ≤ (1 − α)µ 0 (t − 1) + (1 − α)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>N</head><label></label><figDesc>k (t − 1)µ 0 (t − 1) − s∈At−1 r s + Φ(t) (17)Since we have β 0 (t − 1) ≤ β k (t − 1) (otherwise we would be in case 1), and A t−1 = K k=1 N k (t − 1), we bound the the sum over arms in (17):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>t − 1)(µ 0 + 2β 0 (t − 1))≤ K k=1 N k (t − 1)(µ 0 + 2β k (t − 1t − 1)N k (t − 1).Using Lemma 4, we also bound− s∈At−1 r s ≥ s∈At−1 µ s + Φ(t) (under E).Plugging this into (17) gives:αN 0 (t − 1)µ 0 (t − 1) ≤ (1 − α)µ 0 (t − 1) + 2(1 − α)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>α)µ 0 − µ ks ) + 2Φ(t).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>K k=1 N k (t − 1)β k (t − 1). Using µ 0 − µ ks ≤ η ks and s∈At−1 η ks = K k=1 N k (t − 1)η k , we obtain: αN 0 (t − 1)µ 0 (t − 1) ≤ (1 − α)µ 0 (t − 1) + K k=1 (η k − αµ 0 )N k (t − 1) + 4 Ψ k (t − 1)N k (t − 1) . We bound f k := (η k − αµ 0 )N k (t − 1) + 4 Ψ k (t − 1)N k (t − 1). Since (15) N k (t − 1) ≤ 16ψ k (t−1) η 2 k + 1 , and η k − αµ 0 ≤ η k , we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>θ</head><label></label><figDesc>We replace the log + term from Th. 6 by log Kh k δη k &gt; 0, because Kh k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>so that H 0 = max(max k∈[K] H k , Γ).</figDesc><table><row><cell></cell><cell>Kh k δη k</cell><cell cols="4">≥ 0, we have:</cell><cell></cell></row><row><cell cols="2">B ≤ log</cell><cell cols="2">Kh k δη k</cell><cell>log</cell><cell cols="2">Kh k δη k</cell><cell>≤ 2 log</cell><cell>Kh k δη k</cell><cell>.</cell></row><row><cell cols="2">Since 1 + 1 |g|η 2 k</cell><cell cols="6">≤ 2 h k η k , eq. (20) holds.</cell></row><row><cell cols="5">We now bound log(H k ):</cell><cell></cell><cell></cell></row><row><cell cols="6">log(H k ) = O log</cell><cell>h k η k</cell><cell>log</cell><cell>Kh k δη k</cell><cell>(23)</cell></row><row><cell></cell><cell></cell><cell cols="4">= O log</cell><cell cols="2">Kh k δη k</cell><cell>log</cell><cell>Kh k δη k</cell><cell>(24)</cell></row><row><cell></cell><cell></cell><cell cols="4">= O log</cell><cell cols="2">Kh k δη k</cell><cell>(25)</cell></row><row><cell cols="8">where the last line comes from Kh k δη k log Kh k δη k</cell><cell>≤ Kh k δη k</cell><cell>2 .</cell></row><row><cell cols="5">Therefore, eq. (21) holds.</cell><cell></cell><cell></cell></row><row><cell>Now, let</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Γ =</cell><cell cols="2">6K + 2 αµ 0</cell><cell>+</cell><cell>K k=1</cell><cell cols="3">128 log 2(K+1) log(2H k ) θ αµ 0 |g|η k</cell><cell>,</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">LAMSADE, PSL, Université Paris Dauphine, France</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Facebook AI. Correspondence to: Virginie Do &lt;virginie.do@dauphine.eu&gt;.Preprint. Under review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">The original criterion(Singh &amp; Joachims, 2018, Eq. 4)   would be written in our case as ∀s, s ∈[S], 1 |As| a∈As p(a) = 1 |A s | a∈A s</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">Using http://github.com/gbolmier/funk-svd.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">For all our experiments, we used Python and a machine with Intel Xeon Gold 6230 CPUs, 2.10 GHz, 1.3 MiB of cache.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">Using http://github.com/gbolmier/funk-svd.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6">Available athttps://cseweb.ucsd.edu/ ˜jmcauley/datasets.html</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Let</p><p>Then,</p><p>subgaussian as an average of |g| independent σ-subgaussian variables. Notice that the choice of θ makes sure that β k is well defined as long as N k (t) &gt; 0. We use the convention that when N k (t) = 0, β k (t) is strictly larger than when N k (t) = 1 to ensure β g rarm is strictly decreasing with N k . Also, when N k (t) = 0, we set µ k (t) = 0.</p><p>Following <ref type="bibr" target="#b17">(Garcelon et al., 2020)</ref>, our lower bound on the conservative constraint relies on Freedman's martingale inequality <ref type="bibr" target="#b16">(Freedman, 1975)</ref>. Lemma. Assume all rewards are σ-subgaussian. Let A t = {s ≤ t : k s = 0} be the number of times a nonbaseline arm k = 0 has been pulled up to time t. Let</p><p>Then, ∀δ &gt; 0,</p><p>Notice that the first term in</p><p>As in Lemma 3, we use the convention</p><p>, with φ(t) defined in Lemma 4. Let E be the event under which all confidence intervals are valid, i.e.:</p><p>E = E 1 ∩ E 2 with E 1 = ∀k ∈ {0, . . . , K}, ∀t &gt; 0, µ k (t) ∈ [µ k (t); µ k (t)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Best arm identification in multi-armed bandits</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Envy-free classificatoion</title>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Noothigattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Procaccia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08700</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Big data&apos;s disparate impact</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calif. L. Rev</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="671" to="769" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An informationmaximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1159" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the fair division of a heterogeneous commodity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berliant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dunz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Economics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="216" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Equity of attention: Amortizing individual fairness in rankings</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st international acm sigir conference on research &amp; development in information retrieval</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pure exploration in multi-armed bandits problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stoltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Algorithmic learning theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hybrid recommender systems: Survey and experiments. User modeling and user-adapted interaction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="331" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multisided fairness for recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00093</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ranking with fairness constraints</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Celis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Straszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Vishnoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06840</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed fair allocation of indivisible goods</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chevaleyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Endriss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The measure and mismeasure of fairness: A critical review of fair machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00023</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</title>
				<meeting>the 3rd Innovations in Theoretical Computer Science Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">All the cool kids, how do they fit in?: Popularity and demographic biases in recommender evaluation and effectiveness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ekstrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Azpiazu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ekstrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Anuyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability and Transparency</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pot python optimal transport library</title>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<ptr target="https://github.com/rflamary/POT" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Resource allocation and the public sector</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Foley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On tail probabilities for martingales. the Annals of Probability</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="page" from="100" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conservative exploration in reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Garcelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1431" to="1441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fairnessaware ranking in search &amp; recommendation systems with application to linkedin talent search</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ambler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2221" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring price discrimination and steering on e-commerce web sites</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hannak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on internet measurement conference</title>
				<meeting>the 2014 conference on internet measurement conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="305" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis)</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Translation-based recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM conference on recommender systems</title>
				<meeting>the eleventh ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-category fairness in sponsored search auctions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ilvento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagadeesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="348" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fairness in reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1617" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 48th Annual Conference on Information Sciences and Systems (CISS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">lil&apos;ucb: An optimal exploration algorithm for multiarmed bandits</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="423" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fairness in learning: Classic and contextual bandits</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Preventing fairness gerrymandering: Auditing and learning for subgroup fairness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2564" to="2572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01793</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preference-informed fairness. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lambrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2966" to="2981" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Calibrated fairness in bandits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Radanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dimitrakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An optimal algorithm for the thresholding bandit problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Locatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gutzeit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carpentier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08671</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asymptotic existence of fair divisions for groups</title>
		<author>
			<persName><forename type="first">P</forename><surname>Manurangsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Suksompong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Social Sciences</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auditing search engines for differential satisfaction across demographics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on World Wide Web companion</title>
				<meeting>the 26th international conference on World Wide Web companion</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="626" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fairrec: Two-sided fairness for personalized recommendations in two-sided platforms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1194" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Closing the ai accountability gap: defining an endto-end framework for internal algorithmic auditing</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith-Loud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Theron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fairness of exposure in rankings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A framework for understanding unintended consequences of machine learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10002</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discrimination in online ad delivery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fairness without harm: Decoupled classifiers with preference guarantees</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ustun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conservative bandits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shariff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1254" to="1262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From parity to preference-based notions of fairness in classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="228" to="238" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
