<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Road Network Detection Using Probabilistic and Graph Theoretical Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-10-24">October 24, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cem</forename><surname>Ünsalan</surname></persName>
							<email>unsalan@yeditepe.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="laboratory">Computer Vision Research Laboratory</orgName>
								<orgName type="institution">Yeditepe University</orgName>
								<address>
									<postCode>34755</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Beril</forename><surname>Sirmacek</surname></persName>
							<email>beril.sirmacek@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="laboratory">Computer Vision Research Laboratory</orgName>
								<orgName type="institution">Yeditepe University</orgName>
								<address>
									<postCode>34755</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">C</forename><surname>Ünsalan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Photogrammetry and Image Anal-ysis</orgName>
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute</orgName>
								<orgName type="institution" key="instit2">German Aerospace Center</orgName>
								<address>
									<postCode>82234</postCode>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Road Network Detection Using Probabilistic and Graph Theoretical Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-10-24">October 24, 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">185F362D894CC2433DA559B01BFF51DA</idno>
					<idno type="DOI">10.1109/TGRS.2012.2190078</idno>
					<note type="submission">received May 20, 2011 revised October 24, 2011; accepted February 24, 2012. Date of publication April 17, 2012; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aerial images</term>
					<term>binary balloon algorithm</term>
					<term>edge detection</term>
					<term>graph representation</term>
					<term>kernel-based density estimation</term>
					<term>road network detection</term>
					<term>satellite images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Road network detection from very high resolution satellite and aerial images has diverse and important usage areas such as map generation and updating. Although an expert can label road pixels in a given image, this operation is prone to errors and quite time consuming. Therefore, an automated system is needed to detect the road network in a given satellite or aerial image in a robust manner. In this paper, we propose such a novel system. Our system has three main modules: probabilistic road center detection, road shape extraction, and graph-theory-based road network formation. These modules may be used sequentially or interchangeably depending on the application at hand. To show the strengths and weaknesses of our system, we tested it on several very high resolution satellite (Geoeye, Ikonos, and QuickBird) and aerial image sets. We compared our system with the ones existing in the literature. We also tested the sensitivity of our system to different parameter values. Obtained results indicate that our system can be used in detecting the road network on such images in a reliable and fast manner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R OAD network detection from a satellite or an aerial image is an important and challenging remote sensing problem. Its solution may lead to automated map generation and updating which may affect various applications in a positive manner. Also, the solution may help automated path planning for unmanned aerial vehicles. The resolution of recent satellite (like Geoeye, Ikonos, and QuickBird) and aerial images allows road network detection. However, well-known computer vision techniques are not suitable alone for automatically detecting the road network from a given image due to several constraints. Road segments in the image may have different intensity values. Their widths may change. Moreover, junctions of unknown number of roads and roundabouts may increase the difficulty of the problem. Roads can be occluded by other nearby objects like buildings and trees. Since the resolution of the recent satellite images is fairly high, even vehicles may partially occlude the roads. Therefore, advanced methods are needed to extract the road network from very high resolution satellite or aerial images.</p><p>In the literature, there are several methods to detect the road network from a given satellite or aerial image. Three excellent papers by different authors survey and classify road detection methods until 2003 <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b37">[38]</ref>. One set of recent studies focuses on straight-line-based methods for road detection. Katartzis et al. <ref type="bibr" target="#b17">[18]</ref> in their work first applied local analysis using morphological filters to detect straight lines. They also used line tracking methods for this purpose. Using global analysis and Markov random fields, they combined road segments. Shi and Zhu <ref type="bibr" target="#b28">[29]</ref> focused on detecting urban road segments. They obtained a binary image using thresholding to detect line segments. Their method depends on line segment matching and morphological operations. Stoica et al. <ref type="bibr" target="#b36">[37]</ref> used a Gibbs point process based method to detect road segments. They modeled the road network as a connection of line segments. Gamba et al. <ref type="bibr" target="#b13">[14]</ref> extracted linear features from optical and SAR images using multiple directional filters. They used perceptual grouping to detect road segments with these features. Yang and Wang <ref type="bibr" target="#b41">[42]</ref> developed a method to detect main roads from satellite images. First, they detected road primitives such as straight lines and homogenous regions for this purpose. Then, they linked these detected primitives. In the present study, we neither depend on linear features nor straight line segments.</p><p>Another set of studies focuses on graph representation in road network extraction. Steger et al. <ref type="bibr" target="#b35">[36]</ref> worked on complete road network extraction from aerial images. They emphasize that the low-level road extraction methods will be fragmented and will contain false alarms. Therefore, they benefit from graph theory to model the road network. To handle uncertainties, they used the fuzzy set theory. They used Steger's method <ref type="bibr" target="#b34">[35]</ref> to extract the road segments in their experiments. In a following study, Wiedemann and Hinz <ref type="bibr" target="#b40">[41]</ref> proposed to use multispectral images by extracting lines (again using Steger's method) from all available channels. They use the union operation to have a unique representation. In modeling the road network, they benefit from local, regional, and global information. Wiedemann and Ebner <ref type="bibr" target="#b39">[40]</ref> proposed a method to complete incompletely extracted road networks. To do so, they introduced a link hypothesis based on the network characteristics. Hinz and Wiedemann <ref type="bibr" target="#b14">[15]</ref> proposed a general idea on the internal evaluation of object extraction systems. They picked two road extraction methods as examples. They showed that, by introducing internal evaluation methodology, the performance of these systems increases. Mayer et al. <ref type="bibr" target="#b20">[21]</ref> compared six road extraction methods on six test images (three aerial and three multispectral Ikonos). They aimed to show the potential of these methods on road center extraction. In Section V-E, we compare our method with these methods using the same Ikonos images. All of these methods depend on the previously extracted road center pixels. Moreover, they do not extract the road shape. In this paper, we propose a novel road center extraction method. We also propose a novel road shape extraction method. Also, these two methods interact with each other. Therefore, although our method uses graph formalism in modeling the road network, it is different from the previous work in the literature.</p><p>Another set of road detection studies benefits from multispectral information. Ünsalan and Boyer <ref type="bibr" target="#b37">[38]</ref> proposed a method to detect the street network and buildings in residential regions using multispectral information in a novel way. Jin and Davis <ref type="bibr" target="#b16">[17]</ref> used two different methods to detect roads in urban and rural regions. Their first method depends on segmentation and grouping. Their second method is based on multiscale curvilinear structure extractors. They also benefit from directional morphological filters and multispectral information. Wang et al. <ref type="bibr" target="#b38">[39]</ref> detected road segments in panchromatic and multispectral QuickBird images. Mena and Malpica <ref type="bibr" target="#b23">[24]</ref> used color information in terms of RGB and HSI color spaces. First, they smoothed the image by median filtering. Then, they applied texture analysis to segment the image. They finally simplified the result by morphological filtering and thinning. Christophe and Inglada <ref type="bibr" target="#b10">[11]</ref> combined the multispectral information by spectral angle in their method. They obtained straight line segments by using the gradient direction. However, they need a seed pixel labeled by the user. Zhang and Couloigner <ref type="bibr" target="#b42">[43]</ref> benefit from the multispectral information to detect road segments and parks. In this paper, we do not benefit from color and multispectral information. Also, there are several studies attacking the road detection problem from different perspectives. Pandit et al. <ref type="bibr" target="#b25">[26]</ref> benefit from multitemporal images for road detection. Different from previous studies, they first detect vehicles on the road. Then, they take these as seed points and detect the road network. Unfortunately, their method depends on multitemporal information. Hu et al. <ref type="bibr" target="#b15">[16]</ref> defined the pixel footprint by homogeneous polygonal areas around each pixel. Using Fourier shape descriptors, they classified the road area. In the present study, we do not have a classifier which needs to be trained. Peng et al. <ref type="bibr" target="#b26">[27]</ref> worked on updating outdated road maps. They used a multiscale statistical method. They included the GIS data as prior information to their system. In our system, we do not need such a prior information. Peteri and Ranchin <ref type="bibr" target="#b27">[28]</ref> proposed a modular road shape extraction method. Their first module uses graph representation. The graph can come from a road database or can be extracted automatically. Their second module is on road shape extraction by active contours. Being modular, this work resembles our system. However, we propose novel road center extraction and shape extraction methods in this paper. Movaghati et al. <ref type="bibr" target="#b24">[25]</ref> used particle and Kalman filters interchangeably for road track-ing. Unfortunately, their method needs a starting point. In a previous study, we proposed a method to detect the road center pixels which may be taken as the starting point of the present study <ref type="bibr" target="#b31">[32]</ref>.</p><p>In this paper, we propose a novel modular system to detect the road network in very high resolution panchromatic satellite and aerial images. In our system, we do not benefit from prior information such as GIS data. Our focus here is extracting road networks with minimal prior information. This is a perfect example for road network extraction of another country, where the prior information is either limited or does not exist. Our system is composed of three main modules. In the first module, we extract edge pixels as primitives. Then, we use these to detect road centers by a novel probabilistic method. In the second module, we extract the road shape by a binary balloon algorithm. In the third and final module, we benefit from graph theory to represent the extracted road segments in a graph formation. This allows us to refine and improve the detection results. We applied our road network detection method on several satellite (Geoeye, Ikonos, and QuickBird) and aerial images. These represent different road formations and region properties. We provide the obtained results in the experiments section. In the same section, we compare our method with the ones in the literature from different perspectives. We also test the sensitivity of our system to the used parameters in the experiments section. Next, we explain our system in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBABILISTIC ROAD CENTER DETECTION</head><p>The first module in our system is road center detection. In this module, we pick edge pixels as observations of an unknown probability density function (pdf). Inspired from our previous study, we benefit from a modified form of variable kernelbased density estimation for this purpose <ref type="bibr" target="#b32">[33]</ref>. Different from our previous work, we embed the shape constraints of road segments in estimating the unknown pdf. We then apply novel methods to locate the road center pixels from the estimated pdf. In the following sections, we explain each step in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Finding Road Primitives</head><p>Our road network detection system depends on the edge information in the image. Unfortunately, very high resolution satellite and aerial images may contain undesired details besides road pixels. Hence, standard edge detection methods may not work properly. To overcome this problem, we first apply nonlinear median filtering to smooth the given image. In our previous studies, we also benefit from such nonlinear smoothing operations <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>. After smoothing, we ask the user to indicate whether to find "bright" or "dark" road segments. Unfortunately, edge detection does not work properly for dark road segments. Therefore, this is the only step that we ask the guidance of the user. If the road to be detected is dark, then we obtain the negated image and process it.</p><p>After median filtering and image negation (if necessary), we apply the Canny edge detector to extract edge pixels as primitives <ref type="bibr" target="#b7">[8]</ref>. We picked this edge detector due to its scalability and robustness to noise. We adjusted its parameters for an average test image and fixed them through our experiments.</p><p>In Section V, we discuss the effect of these parameters on the final road detection results. We also tested several other edge detectors in Section V to show that our system does not depend on a specific edge detector.</p><p>We then group the detected edge pixels by their connectedness using connected component analysis <ref type="bibr" target="#b33">[34]</ref>. Based on this operation, we first eliminate groups having closed or curved shapes (since they have low probability to represent a road segment). In doing this, we benefit from the shape information as the ratio of the area (sum of pixels for an open curve and sum of all pixels within a closed curve) and the bounding box (minimum rectangle covering the curve) of the group. If this ratio is greater than 0.1 for a group, we discard it. In Section V, we justify this constant threshold value experimentally. After this operation, we refine edge pixels as (x i , y i ) for i = 1, . . . , I, I being the total number of refined pixels. We also calculate the gradient direction θ i for each refined edge pixel.</p><p>In Fig. <ref type="figure" target="#fig_0">1</ref>, we provide the third Ikonos test image, having a size of 693 × 1051 pixels. We will use this image throughout this paper to visualize the computation steps. In this figure, we provide the Canny edge detection results as well as the refined edge pixels based on the shape criteria. As can be seen, by using the shape criteria, most of the edges belonging to nonroad regions are eliminated.</p><p>After refining edge pixels based on the shape criteria, we obtain the approximate road width for each edge pixel at (x i , y i ). To do so, we obtain the nearest road pixel from other groups in terms of the Euclidean distance. We assume this distance as the approximate road width (w i ) for the edge pixel (x i , y i ). To prevent wrong approximations, if w i is higher than 20 pixels, we assume that no nearest edge group is detected. In this case, we set w i value as the mean of the previously detected approximate road widths. In this way, we can also calculate an approximate road width for road segments represented by one side of their borders. Therefore, missing edges of a road segment are also taken into account by this method. At the end of the primitive extraction step, we have three sets of values: road edge pixel locations (x i , y i ), the gradient direction θ i , and the approximate road width w i associated with them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Kernel-Based Density Estimation</head><p>Edge pixels extracted in the previous section lead to road center pixel detection. We formulate this step with a probabilistic framework. To do so, we represent possible road center locations as discrete joint random variables. We then estimate their pdf by taking the extracted edge pixels as observations. Each observation may represent a nearby road center based on the road width at that location. This method is an extension of our previous study on building detection <ref type="bibr" target="#b32">[33]</ref>. Here, we expand it by embedding the generic road segment shape in estimating the pdf.</p><p>For completeness, we start explaining the variable kernelbased density estimation as explained in <ref type="bibr" target="#b29">[30]</ref>. To represent road segments, we define a fixed kernel which is an elongated Gaussian function as</p><formula xml:id="formula_0">N (x, y) = 1 κ exp -x 2 + y 5 2 (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where κ is the normalizing constant. Here, we obtain the elongated Gaussian by scaling the y coordinate by five. This kernel function satisfies the two constraints required by kernel-based density estimation as x y N (x, y) = 1 and N (x, y) ≥ 0 ∀(x, y). We specifically picked such a kernel function to model the road characteristics as an ensemble of elongated shapes. Therefore, we embed the generic road shape in density estimation.</p><p>In the next phase, we estimate the location of road center pixels based on our previous observations: the road edge pixels (x i , y i ), the road width w i (half of this value is used in estimating the road center), and the gradient direction θ i (the angle between the major axis of the elliptical Gaussian with the horizontal axis) associated with each edge pixel. Based on these, the kernel for the ith observation becomes</p><formula xml:id="formula_2">N i (x, y) = 1 κ exp(A T A)<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">A = w i /2 0 0 w i /10 cos(θ i ) -sin(θ i ) sin(θ i ) cos(θ i ) x -x i y -y i .<label>(3)</label></formula><p>In (3), the matrix A is composed of the multiplication of scaling, rotation, and shifting parts. Using the kernel given in (2) for i = 1, . . . , I (I being the total number of refined edge pixels, mentioned in Section II-A), we can estimate the bivariate pdf representing possible road centers as</p><formula xml:id="formula_4">p(x, y) = 1 κ I i=1 N i (x, y). (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>In Fig. <ref type="figure" target="#fig_1">2</ref>, we provide the estimated pdf for the third Ikonos test image. This image is color coded (red represents very high values, and blue represents low values). As can be seen, even  this result is very informative for a human operator since it highlights road segments fairly well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Road Center Detection</head><p>Based on the estimated pdf in (4), we can detect road center pixels. Therefore, we use a two-level (high and low) hysteresis thresholding inspired by Canny's method <ref type="bibr" target="#b7">[8]</ref>. To calculate the high threshold value in an adaptive manner, we first obtain the maximum value of the pdf as p m = max (x,y) {p(x, y)}. Then, high threshold will be adjusted as 0.3 × p m . We take pixel locations higher than this value as possible road centers. In Section V, we justify this multiplier in high threshold calculation. We then embed the road segment characteristics into the detection process using the following observation. Road segments are continuous, and they do not end suddenly. Therefore, some locations close to the detected road center pixels may also be road centers. To handle these, we set the low threshold value in an adaptive manner as 0.01 × p m . We label peak locations higher than this low threshold as possible road centers if their neighbor is also a road center. We provide the detected road center pixels for the third Ikonos test image in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. As can be seen, most road center pixels are successfully detected by this operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Initial Tracking</head><p>To obtain the maximum information from the estimated pdf, we finally apply a basic tracking operation on it. Therefore, we first obtain the endpoints (having just one neighbor) of the detected road center pixels. From each endpoint, we apply a basic tracking as follows. In each iteration, the next possible road pixel is selected from the next three neighbors (lying perpendicular to the endpoint and having the highest pdf value). We iterate until the next candidate pixel falls below a certain threshold. Although this is a very primitive method for tracking, it serves our purpose. We provide the detected road center pixels for the third Ikonos test image after initial tracking in Fig. <ref type="figure" target="#fig_2">3(b</ref>). As can be seen, most road center pixels are successfully detected. However, the road centers in the upper right portion of the image could not be detected since the road network there is not visible enough. In the next section, we will apply a more advanced tracking method based on the parametric representation of road center pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ROAD SHAPE EXTRACTION</head><p>The detected road center pixels may not be sufficient alone to the user. The next level of information to be added is the road shape. Therefore, in this section, we improve and generalize our previous binary balloon algorithm to extract the shape of road segments <ref type="bibr" target="#b37">[38]</ref>. To note here, the term "binary balloon" is used since the method is based on the edge information and is different from the method presented in <ref type="bibr" target="#b11">[12]</ref>. This shape information also helps us to refine road detection results. We explain this method next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Binary Balloons</head><p>In a previous study, we introduced a binary balloon algorithm to extract road segments and separate houses from binary images (obtained from thresholding multispectral image representations) <ref type="bibr" target="#b37">[38]</ref>. Our algorithm was based on initially detected straight line segments (as initial balloons) from binary images. Since we were dealing with a thresholded image, we extracted four set of balloons to represent it. Then, we refined these by a voting scheme. In this paper, we generalize and improve this approach in two ways. First, our new binary balloon algorithm only depends on the edge information. Hence, we do not need a thresholded image (based on multispectral information). Second, we do not assume any straight line segments as initial balloons. This leads to more flexible balloon shape extraction without using four set of balloons and the voting scheme. Therefore, this new method is more general and fast compared to the previous one.</p><p>To extract the road shape, we pick the road center pixels detected in the previous section as initial balloons. Therefore, we first represent them in a binary matrix form R such that the pixel value is one if there is a road center there. Otherwise, the entries will have a value of zero for this matrix. We discard the junction pixels (having more than two neighbors) in this matrix. We apply connected components analysis and group these road segments as R k for k = 1, . . . , K. These will serve as initial balloons. For each road segment (initial balloon), we also extract the endpoints as d 1 k and d 2 k . Then, we iteratively apply dilation and intersection operations to each road segment separately to obtain the corresponding road shape.</p><p>To explain this method in detail, let us start with the definition of the translation and dilation operations. The translation of a set R k by p is</p><formula xml:id="formula_6">(R k ) p = {c|c = a + p, a ∈ R k } . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>The dilation of set R k by set M (the structuring element) is</p><formula xml:id="formula_8">R k ⊕ M = b∈M (R k ) b . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>To extract the road shape, we assign a 3 × 3 mask composed of all ones to the set M . For each road segment R k , we apply the following iterative operation to extract its shape. We first dilate the corresponding endpoints as</p><formula xml:id="formula_10">d 1 k = k 1 k ⊕ M and d 2 k = d 2 k ⊕ M .</formula><p>Then, we dilate the initial road segment and exclude the endpoints as</p><formula xml:id="formula_11">S k = (R k ⊕ M ) ∩ d 1 k ∪ d 2 k ∪ R k . (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>Therefore, in the first iteration, we obtain the boundary pixels.</p><p>For the remaining iterations, we have</p><formula xml:id="formula_13">S k = (S k ⊕ M ) ∩ d 1 k ∪ d 2 k ∪ R k . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>At the end of each iteration, we obtain two separate sets representing the two sides of the road segment.</p><p>We apply this operation until we satisfy the stopping criterion as follows. We apply the intersection operation to check whether we have reached the boundary. The boundary is composed of the edge pixels at the beginning of the operation as D. Therefore, for each road side (S 1 k and S 2 k ), we check if the number of pixels D ∩ S 1 k and D ∩ S 2 k is above the 2% of the total number of pixels in R k . In Section V, we justify this constant threshold value experimentally. We continue the iteration until we satisfy this constraint. At the end of the iterations, we obtain the road shape S k for the road segment at hand. We apply this iterative operation for each road segment separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Shape-Based Refinement</head><p>As we apply the shape extraction operation, some balloons may expand above the limit. This means that they do not represent a reliable road shape, or the initial road segment extracted is not reliable. Therefore, we discard that segment and the balloon extracted from it. We perform this operation by checking the iteration number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Intensity-Based Refinement</head><p>The extracted shape also allows us to check whether the road segment belongs to the road network or not. We perform this by the help of the intensity information. If the intensity of the road segment (balloon) is not consistent with the general intensity value of the extracted road network, we discard it. For this purpose, we calculate the median intensity value on all extracted road segments. If the road segment at hand has the median intensity below 80% of this value, we assume it to be an outlier and discard it.</p><p>After eliminating the road segments based on the aforementioned two criteria, we combine the remaining segments. Finding their boundary, we form the road shape. For future operations, we also keep the road center pixels which have passed the shape-and intensity-based refinement tests. We provide the extracted road network shape before and after initial tracking in Fig. <ref type="figure" target="#fig_3">4</ref>. As can be seen, the extracted road shape for the road network in the third Ikonos test image is fairly good. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. GRAPH-THEORY-BASED ROAD NETWORK FORMATION</head><p>In previous sections, we detected possible road center pixels. Then, we extracted the corresponding road shape. Besides these, the road network has certain structural (neighborhood and shape) properties. To benefit from this information, we benefit from graph formalism in this section. Therefore, we first represent the detected road center pixels in a parametric curve form. This allows us to use a parametric tracking method. Then, we form a graph to represent the road network. This leads to refining some road segments due to their shape and neighborhood conditions. Therefore, we can obtain a more robust and correct road network representation. In forming the graph, we benefit from tools in our previous study <ref type="bibr" target="#b37">[38]</ref>. However, due to the character of the problem at hand, we extended and improved them substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Forming the Graph</head><p>In forming the graph, we start with labeling the junctions in road center pixels. We group the remaining pixels by their connectedness. Assume that we obtain C of such pixel groups. We fit a parametric curve for each group as <ref type="figure">n c</ref> ] stands for the parameter value. Polynomial coefficients a l and b l are obtained from least square fitting. In <ref type="bibr" target="#b8">(9)</ref>, L is the fit order of the parametric curve picked as L = 0.1 × n c to have an adaptive fit order. We represent the parametric curves and junctions in a graph form. A simple graph G consists of a finite nonempty set V and a set E of two-element subsets of V . The set V is called the vertex set of G; the set E is called the edge set of G. We write G = (V, E) to denote the graph G with vertex set V and edge set E. In forming the road network, we assign each junction (extracted in the previous step) as the vertex of the graph as v j . We assign the parametric curves (x c (t), y c (t)) for c = 1, . . . , C as the edges of the graph. We provide the graph formed from the third Ikonos test image in Fig. <ref type="figure" target="#fig_4">5(a)</ref>.</p><formula xml:id="formula_15">(x c (t), y c (t)) = L-1 l=0 a l t l , L-1 l=0 b l t l (9) for c = 1, . . . , C curves. t = [1, . . . ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parametric Tracking</head><p>In this tracking phase, we benefit from the parametric curves (edges of the graph) extracted in the previous section. Since  we have the parametric representation of a curve, we can estimate the next point on its free end (the junction of the edge which has no other edges connected to it). To do so, we first increase the parameter value of the curve above its limits. Then, by calculating the polynomial values there, we estimate the next coordinate of the curve. We also pick the two neighbors of this point lying in the orthogonal direction to the parametric curve. We calculate the pdf values of these three points from ( <ref type="formula" target="#formula_4">4</ref>) and pick the one having the maximum value. If this value is higher than 0.05 × p m , we update the parametric curve representation to include this new entry. In Section V, we justify this constant in thresholding experimentally. We iterate this process until there is no new point to be added. We apply this method to all vertices having free endpoints. At the end of this operation, we reform the graph since some edges and vertices may be changed. We provide the reformed graph for the third Ikonos test image after the parametric tracking operation in Fig. <ref type="figure" target="#fig_4">5(b)</ref>. We also apply the shape extraction module to the results obtained by the graph formalism. We provide the road detection results in Fig. <ref type="figure" target="#fig_5">6</ref>(a). As can be seen, some missing road segments in the initial graph have been extracted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Refining the Graph</head><p>Finally, we have the full road network at hand. However, some edges may not reliably represent a road segment. We eliminate these by embedding the structure information on the graph by unary and binary constraints. Unary constraints are based on the properties of each graph edge alone. Binary constraints take the neighborhood information in the graph into account.</p><p>To define unary and binary constraints, for each edge, we define three measures. The first is arclength, defined as</p><formula xml:id="formula_16">γ c = n c -v1 t=1 (x c (t+1)-x c (t)) 2 +(y c (t+1)-y c (t)) 2 . (10)</formula><p>The second measure is straightness, defined as</p><formula xml:id="formula_17">ρ c = (x c (n c ) -x c (1)) 2 + (y c (n c ) -y c (1)) 2 γ c . (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>This measure has a value of one for straight lines. It has a value of zero for closed shapes since the starting and end points are the same. Finally, the third measure is the angle values (for the two endpoints) α 1 c and α 2 c as</p><formula xml:id="formula_19">α 1 c = arctan y c (1) -y c (2) x c (1) -x c (2) (<label>12</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">α 2 c = arctan y c (n c ) -y c (n c -1) x c (n c ) -x c (n c -1) . (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>1) Unary Constraints: We have four unary constraints to refine the graph. If an edge is long and straight, we should keep it in the graph since it resembles a road segment. If the line is not straight, unary constraints will not work. Therefore, in implementation, we added tolerances. In other saying, if γ c &gt; 20 and ρ c &gt; 0.3 for the edge, we keep it. If an edge is short and curved, it may not represent a road segment reliably. Therefore, we should discard it. Based on this constraint, we discard edges with γ c &lt; 15 and ρ c &lt; 0.2. Moreover, since a road segment cannot be circular under normal conditions, we discard edges with ρ c &lt; 0.1 independent of their arclength. Finally, if an edge has only one neighbor and is short (γ c &lt; 15), probably it belongs to a house nearby. Therefore, we also discard it.</p><p>2) Binary Constraints: To refine the graph further, we apply two more binary constraints. Here, we apply these on edges not discarded by unary constraints. Assume that we have two edges e i and e j . If γ i + γ j &gt; 15, |ρ iρ j | &lt; 0.3, and |α 1 iα 2 j | &lt; π/3, these two edges may represent a longer road segment. Therefore, we keep them in the graph. We also apply this test to three edge pairs. Finally, if an edge lies between two long edges, we keep it since it connects these long edges.</p><p>Both unary and binary constraints refine the graph such that strong edges are kept in the graph and the remaining problematic edges are discarded. We provide the final graph after applying unary and binary constraints in Fig. <ref type="figure" target="#fig_4">5(c</ref>). As can be seen, some problematic edges are eliminated.</p><p>We finally apply the shape extraction module after refining the graph using unary and binary constraints. We provide the road detection results in Fig. <ref type="figure" target="#fig_5">6(b</ref>). As can be seen, the road network extraction in this step is far better than previous steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>To show the strengths and weaknesses of our road network detection system, we perform several tests in this section. Our test bed is composed of large and representative images from four different sensors such as Geoeye, Ikonos, QuickBird satellite, and aerial. These test images are obtained from different sources. Unfortunately, our Ikonos and aerial test images are licensed and cannot be shared. However, the Geoeye and Quick-Bird test images are publicly available and can be obtained from <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and <ref type="bibr" target="#b4">[5]</ref>. As for the quality, the Ikonos and Geoeye image sets are original. On the other hand, QuickBird and aerial image sets are jpeg compressed. Therefore, their quality is not as good as the original versions.</p><p>The resolution of our Geoeye, Ikonos, QuickBird, and aerial images are 0.4, 1, 0.6, and 0.6 m, respectively. To note here, due to the viewing angle of the satellite, while capturing the image, these values may fluctuate slightly. Since our test images have different resolutions, we downsampled Geoeye, QuickBird, and aerial images by 50% to have one set of parameters through our system. Decreasing the resolution helped us in two ways. First, the computation time of the algorithm is proportional to the size of the test image. Therefore, decreasing the image size also decreased the computation time of our algorithm. Second, although the details present in high-resolution images may be useful for object detection, they may act as noise for road extraction. Therefore, it is best to find an optimal image resolution for road extraction.</p><p>Our test images contain diverse road characteristics both in grayscale and shape. Some of our images represent a very complex road network close to city centers. Some other test images represent the road network in rural regions, where the land formation may be misleading. As a remainder, road characteristics do not change frequently in city centers. On the other hand, around rural regions and city borders, road networks expand and change most. Therefore, detecting the road network in these regions is more important.</p><p>In quantifying the performance values, we follow the criteria used by Peng et al. <ref type="bibr" target="#b26">[27]</ref> as</p><formula xml:id="formula_23">Completeness = T P T P + F N (<label>14</label></formula><formula xml:id="formula_24">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correctness = T P T P + F P (15)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality = T P T P +</head><formula xml:id="formula_25">F P + F N (<label>16</label></formula><formula xml:id="formula_26">)</formula><p>where T P , F N, and F P stand for true positive, false negative, and false positive, respectively. The three criteria given in ( <ref type="formula" target="#formula_23">14</ref>)-( <ref type="formula" target="#formula_25">16</ref>) have the following explanations. The completeness value indicates the percentage of the ground truth road pixels detected. The correctness value indicates the percentage of the correct road pixels. Finally, the quality value indicates the goodness of the result. In the following sections, we will call these as Comp, Corr, and Qual, respectively. In calculating these values, we used manually formed ground truth data (since any previously formed ground truth data are unavailable). To In the following sections, we provide our experimental results on the sensitivity of our system to the edge detector type, robustness to parameter variations, performance, and computation time of our system modules separately. We compare our road center detection and road shape extraction modules with the state-of-the-art methods in the literature. We also provide the comparison results of our system with a commercial one in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tests on Different Edge Detectors</head><p>Our road network detection system depends on the edge information. In tests, we picked the Canny edge detector since it has a better response. However, our system also works on different edge detectors such as Sobel, Prewitt, Roberts, Log, and Edison <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b33">[34]</ref>. We pick the third Ikonos test image and provide the performance of our system with different edge detectors in Table <ref type="table" target="#tab_0">I</ref>.</p><p>As can be seen, the detection results using different edge detectors (besides Canny) have similar completeness and correctness values. The Canny edge detector differs from the rest in terms of a balanced completeness and correctness value. Based on these results, we can first claim that our system also works on different edge detectors. Second, the Canny edge detector has a better performance in terms of joint completeness and correctness values. Therefore, we picked it in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tests on System Parameters</head><p>In our road network detection system, most parameters are adjusted automatically. However, there are also some manually adjusted parameters in the system. In this section, we justify these selections experimentally. We also test the sensitivity of our road network detection system to variations of these parameters. We pick the third Ikonos test image in the following tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Canny Edge Detector:</head><p>The first parameter test is on the Canny edge detector. We adjusted its variance parameter automatically. To detect weak edges, we also fixed the low threshold level (in hysteresis thresholding) to 0.01 (for normalized values between zero and one). In Fig. <ref type="figure" target="#fig_6">7</ref>(a), we provide the completeness (marked by solid squares), compactness (marked by solid circles), and quality (marked by solid triangles) values for the high threshold (in hysteresis thresholding) levels between 0.15 and 0.35. We also follow the same notations in the following parameter tests. As can be seen, our detection results are similar for this parameter range. Around 0.25, we obtain good results in terms of performance. Therefore, we fixed the high threshold value to 0.25 throughout the tests.</p><p>2) Shape-Based Refinement in Road Primitives: In finding road primitives, we eliminate some edge segments based on their shape values in Section II-A. Here, we change the threshold used in the refinement step (from 0.05 to 0.30) and observe its effect on the final road network detection performance. We provide the results in Fig. <ref type="figure" target="#fig_6">7(b)</ref>. In this figure, as the threshold value exceeds 0.1, the compactness value stabilizes. Therefore, we fixed this parameter to 0.1 in the experiments.</p><p>3) Constant in Thresholding the PDF in Road Center Detection: The third test is on the constant in thresholding the pdf value discussed in Section II-C. In Fig. <ref type="figure" target="#fig_6">7(c</ref>), we provide the final road network detection results as we change this constant between 0.2 and 0.4. As can be seen, the obtained results are fairly stable. However, the completeness and compactness values change slightly as we exceed 0.3. Therefore, we fixed the constant to 0.3 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Binary Balloon Expansion Threshold:</head><p>The fourth test is on the binary balloon expansion threshold as mentioned in Section III-A. We changed this parameter from 0.01 to 0.05 and provide the final road network detection results in Fig. <ref type="figure" target="#fig_6">7(d)</ref>.</p><p>As can be seen, the obtained performance values are almost the same. Therefore, we fixed this parameter to 0.02 throughout the experiments.</p><p>5) Parametric Tracking Threshold: The final test is on the constant in the adaptive threshold value used in parametric road  tracking module of our system explained in Section IV-B. We changed this value from 0.04 to 0.08 and provided the results in Fig. <ref type="figure" target="#fig_6">7</ref>(e). As can be seen, the performance values do not change much. Therefore, we fixed it to 0.05 throughout the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Tests</head><p>In this section, we first tabulate the performance of our system to each image set (obtained from a specific sensor). We also provide the best and worst results for each image set in separate figures. Then, we provide the overall performance value. Since we extract different outputs at different modules of our system, we tabulate their performances separately as the following: initial detection and tracking, graph representation (including parametric tracking), and graph refinement (using unary and binary constraints).</p><p>1) Results on Geoeye Satellite Images: We picked 12 Geoeye images having a total size of 9 454 319 pixels. In these images, the road network occupies 1 176 799 pixels in total. We provide the performance of our system for each module in Table <ref type="table" target="#tab_1">II</ref>. As can be seen, the average performance of our system at each module is fairly good.</p><p>We provide the extracted road network on two Geoeye test images such that we have the best and worst detection results in terms of the completeness value (using all modules) in Fig. <ref type="figure" target="#fig_7">8</ref>. As can be seen, the best result is obtained in a residential region (the fifth Geoeye image). In this image, although the houses are highly populated, the road network can be clearly seen. On the other hand, the variability of the road width in this image is apparent. Our method handled this variability fairly well.</p><p>The worst result is obtained on the junction of residential and industrial regions (the fourth Geoeye image). In this image, the cars on the road are visible. Although these act as noise, they are handled fairly well by our probabilistic detection method. As in the fifth Geoeye image, the road width varies in this image, and our method handled it fairly well. The possible reason for the poor performance on the fourth Geoeye image is as follows. In the industrial region, parts of large buildings are also labeled as road segments. Therefore, the false alarm rate increased. To handle such false alarms, the color information may be used in our system in the future.</p><p>2) Results on Ikonos Satellite Images: Next, we pick 11 Ikonos images having a total size of 11 681 756 pixels of which 707 811 belong to the road network. We provide the performance of our system on these images in Table <ref type="table" target="#tab_2">III</ref>. Similar to the Geoeye images, we obtain fairly good results on the Ikonos image set.</p><p>For the Ikonos images, we obtained best and worst results on a residential and rural image, respectively, as in Fig. <ref type="figure">9</ref>. On the residential region (the eighth Ikonos image), since the contrast between the road pixels and the environment is large, the detection results are better than the rest. Although there are nearby houses in this image, they did not affect the road detection results. For the rural region (the tenth Ikonos image), some road segments can be seen clearly. However, some other road segments have grayscale values close to the background. Although our system was able to detect main road segments, it was unable to detect the low contrast and narrow road segments in this image. Moreover, the tree formations and geological shapes in this image resemble road segments. These are also possible reasons for the poor performance in the tenth Ikonos image.</p><p>3) Results on QuickBird Satellite Images: We next pick nine QuickBird images having a total size of 4 353 905 pixels of which 195 450 are road network pixels. For each module, the performance of our system on these images is given in Table <ref type="table" target="#tab_0">IV</ref>. Unfortunately, the performance of our system on QuickBird images is not as good as the Goeye and Ikonos images. As we mentioned previously, these images were not original (jpeg compressed). This may have caused a decrease in performance.</p><p>As in the previous image sets, we provide the best and worst results on the QuickBird image set in Fig. <ref type="figure" target="#fig_0">10</ref>. The best result is obtained on a rural region (the fourth QuickBird image). In this image, the road characteristics are fairly dominant. Moreover, the land formations in this region are also fairly smooth. The worst performance is obtained on a rural region (the fifth QuickBird image). This image has characteristics similar to the tenth Ikonos image. Therefore, the same reasons for the poor performance also apply here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Results on Aerial Images:</head><p>We next pick ten aerial images. The total size of these images is 14 938 006 pixels, with 772 774 of them representing the road network. We provide the performance of our system on these test images in Table <ref type="table">V</ref>. Again, the results on these images are not as good as the Geoeye and Ikonos images. To note here, these images were also jpeg compressed.</p><p>In the aerial image set, the best performance is obtained on a residential region (the third aerial image), as shown in Fig. <ref type="figure" target="#fig_0">11</ref>. In this image, the contrast between the road pixels and the background is fairly high. Therefore, our system detected the road network reliably. Although there are nearby buildings in the region, they did not affect the road detection performance much. The only major missing road segment in this image is in the lower part. It seems that the Canny edge detector could not detect the borders of this road segment. By manually adjusting the edge detection parameter, this road segment can also be detected. The worst result is obtained on another residential region (the fourth aerial image), as shown in Fig. <ref type="figure" target="#fig_0">11</ref>. This image is similar to the third aerial image. Again, one branch of the road network could not be detected in this image due to its contrast with the background. Therefore, the performance decreased substantially on this image.</p><p>5) Overall Performance: Finally, we provide the overall performance of our system on all test images. On gross total, the size of our test images becomes 40 427 986 pixels. The road network in these images is represented by 2 852 834 pixels. For each module, the performance of our system is given in Table <ref type="table" target="#tab_3">VI</ref>. As can be seen, on such a large and diverse image set, the obtained average performance values are fairly good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computation Time</head><p>We analyze the computational load (in terms of CPU timings) of our system in this section. Therefore, we pick the third Ikonos test image and obtained the CPU timings of our system with all modules used. We used MATLAB as the coding environment on a PC having Intel Core2Quad processor with 3.0-GHz clock speed. Since we have four modules, we provide the CPU time needed for each separately as follows. Initial road detection module (including all preprocessing operations) needs 40.19 s. The initial tracking operation on this module adds an extra 18.94 s. The next module is graph representation (including parametric tracking), and it needs 52.51 s. Finally, the graph refinement module using unary and binary constraints needs 13.20 s. The total time needed for all operations sums up to 124.84 s. As can be seen, the most time-consuming module of our system is graph representation. Based on these timings and the corresponding performances, the user may select which modules to use in his or her applications. To note here, this timing may increase or decrease based on the image complexity at hand.</p><p>Although this timing depends on the computer used, we believe that this result is comparable with a human expert's performance. Since our system provides the road shape information, it may even be faster than an expert. Moreover, as the computer technology advances, the timing advantage will become clearer. It should also be mentioned that our system may work 24 h a day which is not the case for a human expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison With the Existing Methods</head><p>In this section, we compare our road network detection system with the similar methods in the literature. Since most existing methods either focus on road center detection or road shape extraction, we perform the comparison in two steps. First, we consider the road center detection module of our system. Then, we consider the road shape extraction module. In both comparisons, we quantify the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Road Center Detection Comparison:</head><p>In comparing the road detection module with the ones in the literature, we applied two tests. First, we tested our system on three multispectral Ikonos test images (kindly provided by Prof. H. Mayer) used in a previous work <ref type="bibr" target="#b20">[21]</ref>. In Mayer's work, the road center pixel detection results are tabulated using six different methods for the same images. Therefore, we were able to compare our road detection module with these quantitatively.</p><p>We obtained the grayscale version of the multispectral images and used our road center detection module on these. Although these images have 4-m resolution, we were able to run our method on them. We tabulated our results as well as the ones reported in Mayer's work (in terms of completeness and correctness) in Table <ref type="table" target="#tab_4">VII</ref>. In this table, Img-1, Img-2, and Img-3 correspond to images Ikonos1-Sub1, Ikonos3-Sub1, and Ikonos3-Sub2 (as called by Mayer), respectively. In this table, we call other methods in the same name with Mayer.</p><p>In providing our results (in the proposed line) in Table <ref type="table" target="#tab_4">VII</ref>, we had the same assumptions as Mayer et al. In forming the performance table, they mention that the values for completeness and correctness should be at least 0.6 and 0.75 so that the results become practical. In accordance with this constraint, our system passes both thresholds for the Ikonos3-Sub1 and Ikonos3-Sub2 images. This is also the case for most previous studies. Different from previous studies, our method also passes the completeness threshold for the Ikonos1-Sub1 image.</p><p>If we look more closely in Table <ref type="table" target="#tab_4">VII</ref>, we can summarize the following observations. For the Ikonos1-Sub1 image, our method has the highest completeness value with the least correctness. For the Ikonos3-Sub1 image, our method has the fourth best completeness value with the second best correctness. For the Ikonos3-Sub2 image, our method has the fourth best completeness value with the second best correctness. Therefore, our method is comparable (even better for some cases) with the existing methods. Please also remember that this comparison is only on road center detection. However, our method also provides road shape information which will be considered in the following section.</p><p>As a second road center detection test, we pick the commercial Halcon software to extract curvilinear structures from the third Ikonos test image (the benchmark image of this paper) <ref type="bibr" target="#b3">[4]</ref>. The Halcon software uses Steger's method <ref type="bibr" target="#b34">[35]</ref> in extracting  <ref type="bibr" target="#b19">[20]</ref>, Lankton and Tannenbaum <ref type="bibr" target="#b18">[19]</ref>, and Bernard et al. <ref type="bibr" target="#b6">[7]</ref>. We used the publicly available software prepared by Bernard <ref type="bibr" target="#b0">[1]</ref> for these methods. We also used the mean shift clustering method introduced by Comaniciu and Meer <ref type="bibr" target="#b12">[13]</ref> to label road segments. Using all of these methods, we provide the road detection (in terms of shape) results on the third Ikonos test image in Table VIII. To eliminate any bias, we provide both our road center detection result and the one provided by Halcon as initial balloons in this table. We used the initial balloons to refine clusters in the mean shift clustering method.</p><p>As can be seen in Table <ref type="table" target="#tab_5">VIII</ref>, the initial balloon (provided by either our method or Halcon) does not affect the road detection performance much for the six methods. Only our method is affected. This is reasonable since our system is optimized within itself. On the other hand, using another road center method in our system still gives fairly good results. This shows the modularity of our road network detection system. In terms of the completeness value, our system gives the third best result after Caselles and Lankton's methods. Among these three methods, our method gives the best completeness and quality values. In terms of the correctness value, our system gives the third best result after Bernard and Li's methods. Among these three methods, our method again gives the best correctness and quality values.</p><p>On the third Ikonos image, the computation times for the active contour models are as follows (excluding the initial balloon extraction step): Caselles 64.76 s., Chan 68.91 s., Li 515.43 s., Lankton 297.06 s., and Bernard 478.61 s. As we mentioned in the previous section, the time needed for our method is 84.65 s., excluding the initial road detection step (to be fair). Again, our method gives the third best timing after Caselles and Chan's methods. Among these three, our method gives the second best completeness and the best correctness values.</p><p>After these extensive comparisons, we can claim that our proposed method has some clear advantages compared to the existing methods in the literature. It also has some shortcomings, which we believe can be eliminated by using extra information (such as multispectral representations). To note here, some of our test images are open to public. They can be used as standard benchmark images for road detection studies in future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. FINAL COMMENTS</head><p>In this paper, we have proposed a novel modular system to detect the road network from a given panchromatic very high resolution satellite or aerial image. We have first detected the road center pixels using a novel probabilistic framework. We have benefited from the edge information as road primitives. By a variable kernel-based density estimation, we have located the possible road center pixels as the first output of our system. The probabilistic method eliminates the need for parallel edges to represent road segments. Therefore, it overcomes the missing data problem. The probabilistic method does not depend on a specific edge detector as shown in Section V-A. Probabilistic representation also simplifies tracking. Although there are various road center tracking approaches in the literature, they, most of the times, depend on grayscale or multispectral information. These values may change in the image due to intensity changes. However, the probabilistic representation is robust to intensity changes since it depends on the edge information.</p><p>Our method does not only extract road center pixels. It also extracts the road shape and road width information via a novel balloon algorithm. Therefore, we do not use any prior information about the road width. This may give information for road classification purposes (i.e., highways, streets, etc.) in future studies. Our balloon algorithm is local and does fitting for each part separately. Therefore, it is more robust. This comes from our probabilistic representation.</p><p>We have benefited from graph theory to represent the road network in a parametric and structural form. This helps us to refine and improve our road network detection results further. Unlike previous approaches, we do not use edge-based graph representation. Instead, the edge information leads to a probabilistic representation. This leads to a graph-based method. To note here, our graphs also model the road segments extracted by our novel balloon algorithm. Applying the road shape extraction module to this result, we have obtained the final output of our system.</p><p>These novel contributions lead to road network detection using only panchromatic images. Unlike the previous approaches, we do not benefit from color information (since it may not be available) or multispectral data which leads to easy extraction of man-made structures (however, its availability may also be limited). Moreover, we do not benefit from any prior information such as GIS data. However, we can embed this information to our method. Our focus here is extracting the road network with minimal prior information. This is a perfect example for road network extraction of another country, where the prior information is either limited or it does not exist.</p><p>We have tested our system on a diverse data set obtained from Geoeye, Ikonos, QuickBird satellite, and aerial images from different regions with fairly diverse road and surface characteristics. We believe that some of our images represent a fairly complex road network, especially close to city centers. The extraction of the road network from the city center is the candidate for the most difficult scenario. However, most road detection methods do not perform well on city centers. For these locations, extra DSM and DTM data are needed. Moreover, road characteristics do not change in city centers frequently. On the other hand, around rural regions and city borders, road networks expand and change most. Therefore, detecting the road network in these regions is more important. On all of the test images, we obtained fairly good detection results. Since we have tested our system on such a diverse data set, we can claim that our system does not specifically depend on a certain image type.</p><p>There are two main reasons for the errors in our system. First, some nearby buildings are detected as road segments. This error can be corrected if a building detection system is used in parallel. Second, there are some inevitable errors due to the road-like patterns in farms and rural regions. Using color information may be of help to overcome this problem. There are also some road segments missed by our system. The main reason for these is the low visibility of the segments. Finally, our system does not discriminate rivers since they resemble road segments. However, they can be discriminated by using color information. Besides these acceptable shortcomings, we have obtained very promising results on a large and diverse test set.</p><p>Our system has several advantages. It does not depend on the user interaction, such as manually labeling the starting point of the road network. It only needs the road intensity label ("bright" or "dark") from the user. Most parameters of our system are adjusted automatically. The road detection performance is also fairly insensitive to changes in the remaining parameters. The road detection performance is also fairly insensitive to the image resolution within its limits. In Section V, we have tested our system on 0.8-(after downsampling) to 4-m test images, and the system worked fine. However, if we have a test image with less than 5-m resolution, our edge detection module will not work reliably. Hence, all of the dependent modules will fail. One solution to this problem is adjusting the Canny edge detection parameter manually there. We have also compared our system with the state-of-the-art road detection methods on the same image set. The results show the usefulness of our system. Therefore, we can suggest that our system can be used in detecting the road network from very high resolution satellite and aerial images in a reliable and fast manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Third Ikonos test image, its Canny edges, and refined edges by the shape criteria. (a) Third Ikonos test image. (b) Canny edges. (c) Refined edges.</figDesc><graphic coords="3,39.73,70.42,246.12,200.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Estimated pdf for the third Ikonos test image.</figDesc><graphic coords="4,44.94,190.35,245.16,96.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detected road center pixels for the third Ikonos test image before and after initial tracking. (a) Before initial tracking. (b) After initial tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Third Ikonos test image and road network shape extraction results before and after initial tracking. (a) Before initial tracking. (b) After initial tracking.</figDesc><graphic coords="5,302.75,70.38,246.12,97.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Graph extraction steps for the third Ikonos test image. (a) Initial graph. (b) After parametric tracking. (c) After unary and binary constraints.</figDesc><graphic coords="6,44.94,69.90,246.12,199.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Third Ikonos test image, road network shape extraction results. (a) After parametric tracking. (b) After unary and binary constraints.</figDesc><graphic coords="6,44.94,311.20,246.12,96.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Parameter sensitivity tests. Solid squares-completeness, solid circles-compactness, and solid triangles-quality. (a) Canny edge detector. (b) Shape-based refinement. (c) Road center detection. (d) Binary balloon. (e) Parametric tracking.</figDesc><graphic coords="8,44.94,70.22,246.12,303.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Best and worst road network detection results on two Geoeye test images. (a) Fifth Geoeye test image. (b) Road network detected. (c) Fourth Geoeye test image. (d) Road network detected.</figDesc><graphic coords="8,324.95,166.21,211.32,232.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .Fig. 11 .</head><label>91011</label><figDesc>Fig. 9. Best and worst road network detection results on two Ikonos test images. (a) Eighth Ikonos test image. (b) Road network detected. (c) Tenth Ikonos test image. (d) Road network detected.TABLE IV ROAD NETWORK DETECTION PERFORMANCES ON NINE QUICKBIRD SATELLITE TEST IMAGES</figDesc><graphic coords="9,327.24,438.65,196.80,51.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,68.93,166.09,197.88,317.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ROAD</head><label>I</label><figDesc>NETWORK DETECTION PERFORMANCES ON THE THIRD IKONOS TEST IMAGE USING DIFFERENT EDGE DETECTORS</figDesc><table /><note><p>avoid any bias, two authors and a Ph.D. student from the German Aerospace Center prepared this ground truth data.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ROAD</head><label>II</label><figDesc>NETWORK DETECTION PERFORMANCES ON 12 GEOEYE SATELLITE TEST IMAGES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ROAD</head><label>III</label><figDesc>NETWORK DETECTION PERFORMANCES ON 11 IKONOS SATELLITE TEST IMAGES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI ROAD</head><label>VI</label><figDesc>NETWORK DETECTION PERFORMANCES ON ALL TEST IMAGES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII ROAD</head><label>VII</label><figDesc>CENTER DETECTION PERFORMANCES ON THREE TEST IMAGES FROM<ref type="bibr" target="#b20">[21]</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>OF ROAD SHAPE EXTRACTION METHODS ON THE THIRD IKONOS IMAGE lines. Based on Mayer's performance criteria, the completeness obtained from the third Ikonos image using Halcon is 0.5601 with the correctness value being 0.8201. On the same image, our system (only using the road detection module) has the completeness and correctness values as 0.7155 and 0.7712, respectively. Comparing both results, we can claim that our completeness value is fairly higher than Halcon's result, with comparable correctness values.2) Road Shape Extraction Comparisons: We next compare the road shape extraction module of our system with six different methods in the literature. Five of these methods are active contour and level set based ones introduced by Caselles et al.<ref type="bibr" target="#b8">[9]</ref>, Chan and Vese<ref type="bibr" target="#b9">[10]</ref>, Li et al.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Prof. H. Mayer from the Universität der Bundeswehr München, Neubiberg, Germany, for providing their test data and Ph.D. candidate S. Türmer from Technische Universität München, Munich, Germany, for obtaining road detection results on the Halcon software.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://www.creatis.insa-lyon.fr/~bernard/creaseg/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://www.digitalglobe.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://www.geoeye.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://www.mvtec.com/download/reference/linesgauss.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://www.satimagingcorp.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiresolution, semantic objects, and context for road extraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Modeling for the Acquisition of Topographic Information From Images and Maps</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Birkhauser-Verlag</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="140" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational B-spline level-set: A linear filtering approach for fast deformable model evolution</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Friboulet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thevenaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1179" to="1191" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic active contours</title>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="79" />
			<date type="published" when="1997-03">Feb./Mar. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust road extraction for high resolution satellite images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Christophe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Inglada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finite-element methods for active contour models and balloons for 2D and 3D images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1131" to="1147" />
			<date type="published" when="1993-11">Nov. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving urban road extraction in high-resolution images exploiting directional filtering, perceptual grouping, and simple topological concepts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dell'acqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lisini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="391" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Increasing efficiency of road extraction by self-diagnosis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1457" to="1466" />
			<date type="published" when="2004-12">Dec. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Road network extraction and intersection detection from aerial images by tracking road footprints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Femiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4144" to="4157" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An integrated system for automatic road mapping from high-resolution multi-spectral satellite imagery by information fusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="257" to="273" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A model-based approach to the automatic extraction of linear features from airborne images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katartzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pizurica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2073" to="2079" />
			<date type="published" when="2001-09">Sep. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Localizing region-based active contours</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lankton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tannenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2029" to="2039" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimization of regionscalable fitting energy for image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1940" to="1949" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A test of automatic road extraction approaches</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Baltsavias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Archives Photogramm., Remote Sens., Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="214" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Edge detection with embedded confidence</title>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1351" to="1365" />
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">State of the art on automatic road extraction for GIS update: A novel classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Mena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3037" to="3058" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An automatic method for road extraction in rural and semi-urban areas starting from high resolution satellite imagery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Malpica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1201" to="1220" />
			<date type="published" when="2005-07">Jul. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Road extraction from satellite images using particle filtering and extended Kalman filtering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Movaghati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moghaddamjoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2807" to="2817" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic road network extraction using high resolution multi-temporal satellite images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="272" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incorporating generic and specific prior knowledge in a multiscale phase field model for road extraction from VHR images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="146" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated road network extraction using collaborative linear and surface models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ranchin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MAPPS/ASPRS Conf</title>
		<meeting>MAPPS/ASPRS Conf<address><addrLine>San Antonio, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The line segment match method for extracting road network from high-resolution satellite images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="514" />
			<date type="published" when="2002-02">Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Density Estimation for Statistics and Data Analysis, 1st ed</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Urban area and building detection using SIFT keypoints and graph theory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sýrmaçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ünsalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1156" to="1167" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Road network extraction using edge detection and spatial voting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sýrmaçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ünsalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int. Conf. Pattern Recognit</title>
		<meeting>20th Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3113" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A probabilistic framework to detect buildings in aerial and satellite images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sýrmaçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ünsalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="221" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<title level="m">Image Processing, Analysis and Machine Vision</title>
		<meeting><address><addrLine>Ludhiana, India</addrLine></address></meeting>
		<imprint>
			<publisher>CL Engineering</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An unbiased detector of curvilinear structures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="125" />
			<date type="published" when="1998-02">Feb. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automatic Extraction of Man-Made Objects from Aerial and Space Images (II)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Radig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Birkhaeuser-Verlag</publisher>
			<biblScope unit="page" from="245" to="256" />
			<pubPlace>Basel, Switzerland</pubPlace>
		</imprint>
	</monogr>
	<note>The role of grouping for road extraction</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Gibbs point process for road extraction from remotely sensed images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="121" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A system to detect houses and residential street networks in multispectral satellite images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ünsalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="461" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extraction of road networks using pan-sharpened multispectral and panchromatic QuickBird images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="273" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic completion and evaluation of road networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ebner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Archives Photogramm., Remote Sens., Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">B3/2</biblScope>
			<biblScope unit="page" from="979" to="986" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic extraction and evaluation of road networks from satellite imagery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Archives Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3-2W5</biblScope>
			<biblScope unit="page" from="95" to="100" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Classified road detection from satellite images based on perceptual organization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="4653" to="4669" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Benefit of the angular texture signature for the separation of parking lots and roads on high resolution multispectral imagery</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Couloigner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="937" to="946" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
