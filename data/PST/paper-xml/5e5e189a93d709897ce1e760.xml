<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INFOGRAPH: UNSUPERVISED AND SEMI-SUPERVISED GRAPH-LEVEL REPRESENTATION LEARNING VIA MU-TUAL INFORMATION MAXIMIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila-Quebec Institute for Learning Algorithms</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
							<email>jhoffmann@g.harvard.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Mila-Quebec Institute for Learning Algorithms</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
							<email>vikas.verma@aalto.fi</email>
							<affiliation key="aff1">
								<orgName type="institution">Mila-Quebec Institute for Learning Algorithms</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Mila-Quebec Institute for Learning Algorithms</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">HEC Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">CIFAR AI Research Chair</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">INFOGRAPH: UNSUPERVISED AND SEMI-SUPERVISED GRAPH-LEVEL REPRESENTATION LEARNING VIA MU-TUAL INFORMATION MAXIMIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Representation learning for graphs has mainly dealt with supervised learning tasks. Recently, however, researchers have proposed algorithms that learn graph-level representations in an unsupervised manner <ref type="bibr" target="#b34">Narayanan et al. (2017);</ref><ref type="bibr" target="#b0">Adhikari et al. (2018)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs have proven to be an effective way to represent very diverse types of data including social networks <ref type="bibr" target="#b36">Newman &amp; Girvan (2004)</ref>, biological reaction networksPavlopoulos et al. <ref type="bibr">(2011)</ref>, proteinprotein interactions <ref type="bibr" target="#b26">Krogan et al. (2006)</ref>, the quantum mechanical properties of individual molecules <ref type="bibr" target="#b60">Xie &amp; Grossman (2018)</ref>; <ref type="bibr" target="#b19">Jin et al. (2018)</ref>, and many more. Graphs provide explicit information about the coupling between individual units in a larger part along with a well defined framework for assigning properties to the nodes and the edges connecting them. There has been a significant amount of previous work done studying many aspects of graphs including link prediction <ref type="bibr" target="#b11">Gao et al. (2011)</ref>; <ref type="bibr" target="#b58">Wang et al. (2011)</ref> and node prediction <ref type="bibr" target="#b1">Blei et al. (2003)</ref>. Due to its flexibility, graph-like data structures can capture rich information which is critical in many applications.</p><p>At the lowest level, much work has been done on learning node representations-low-dimensional vector embeddings of individual nodes <ref type="bibr" target="#b45">Perozzi et al. (2014)</ref>; <ref type="bibr" target="#b52">Tang et al. (2015)</ref>; <ref type="bibr" target="#b15">Grover &amp; Leskovec (2016)</ref>. Another field that has attracted a large amount of attention recently is learning representations of entire graphs. Such a problem is critical in a variety of applications such as predicting the properties of molecular graphs in both drug discovery and material science <ref type="bibr" target="#b5">Chen et al. (2019b;</ref><ref type="bibr">a)</ref>. There has been some recent progress based on neural message passing algorithms <ref type="bibr" target="#b13">Gilmer et al. (2017)</ref>; <ref type="bibr" target="#b60">Xie &amp; Grossman (2018)</ref>, which learn the representations of entire graphs in a supervised way. These methods have been shown achieving state-of-the-art results on a variety of different prediction tasks <ref type="bibr" target="#b22">Kipf et al. (2018)</ref>; <ref type="bibr" target="#b60">Xie &amp; Grossman (2018)</ref>; <ref type="bibr" target="#b13">Gilmer et al. (2017)</ref>; <ref type="bibr" target="#b5">Chen et al. (2019a)</ref>.</p><p>However, one of the most difficult obstacles for supervised learning on graphs is that it is often very costly or even impossible to collect annotated labels. For example, in the chemical domain labels are typically produced with a costly Density Functional Theory (DFT) calculation. One option is to use semi-supervised methods which combine a small handful of labels with a larger, unlabeled, dataset. In real-world applications, partially labeled datasets are common, making tools that are able to efficiently utilize the present labels particularly useful.</p><p>Coming up with methods that are able to learn unsupervised representations of an entire graph, as opposed to nodes, is an important step in working with unlabeled or partially labeled graphs <ref type="bibr" target="#b34">Narayanan et al. (2017)</ref>; <ref type="bibr" target="#b17">Hu et al. (2019)</ref>; <ref type="bibr" target="#b37">Nguyen et al. (2017)</ref>. For example, there exists work that explores pre-training techniques for graphs to improve generalization <ref type="bibr" target="#b17">Hu et al. (2019)</ref>. Another common approach to unsupervised representation learning on graphs is through graph kernels <ref type="bibr" target="#b46">Pr≈æulj (2007)</ref>; <ref type="bibr" target="#b20">Kashima et al. (2003)</ref>; <ref type="bibr" target="#b41">Orsini et al. (2015)</ref>. However, many of these methods do not provide explicit graph embeddings which many machine learning algorithms operate on. Furthermore, the handcrafted features of graph kernels lead to high dimensional, sparse or non-smooth representations and thus result in poor generalization performance, especially on large datasets <ref type="bibr" target="#b34">Narayanan et al. (2017)</ref>.</p><p>Unsupervised learning of latent representations is also an important problem in other domains, such as image generation <ref type="bibr" target="#b21">Kingma &amp; Welling (2013)</ref>; <ref type="bibr">Kim &amp; Mnih (2018)</ref> and natural language processing <ref type="bibr" target="#b30">Mikolov et al. (2013a)</ref>. A recent work introduced Deep Infomax, a method that maximizes the mutual information content between the input data and the learned representation <ref type="bibr" target="#b16">Hjelm et al. (2018)</ref>. This method outperforms other methods on many unsupervised learning tasks. Motivated by Deep InfoMax <ref type="bibr" target="#b16">Hjelm et al. (2018)</ref>, we aim to use mutual information maximization for unsupervised representation learning on the entire graph. Specifically, our objective is to maximize the mutual information between the representations of entire graphs and the representations of substructures of different granularity. We name our model InfoGraph.</p><p>We also propose a semi-supervised learning model which we name InfoGraph*. We employ a student-teacher framework similar to Mean-Teacher method <ref type="bibr" target="#b53">Tarvainen &amp; Valpola (2017)</ref>. We maximize the mutual information between intermediate representations of the two models so that the student model learns from the teacher model. The student model is trained on the labeled data using a supervised objective function while the teacher model is trained on unlabeled data with InfoGraph. Using InfoGraph*, we achieve performance competitive with state-of-the-art methods on molecular property prediction.</p><p>Concurrently to this work, information maximizing graph neural networks (IGNN) was introduced which uses mutual information maximization between edge states and transform parameters to achieve state-of-the-art predictions on a variety of supervised molecule property prediction tasks <ref type="bibr" target="#b5">Chen et al. (2019b)</ref>. In this work, our focus is on unsupervised and semi-supervised scenarios.</p><p>Graph Kernels. Constructing graph kernels is a common unsupervised task in learning graph representations. These kernels are typically evaluated on node classification tasks. In graph kernels, a graph G is decomposed into (possibly different) {G s } sub-structures. The graph kernel K(G 1 , G 2 ) is defined based on the frequency of each sub-structure appearing in G 1 and G 2 respectively. Namely, K(G 1 , G 2 ) = f Gs 1 , f Gs 2 , where f Gs is the vector containing frequencies of {G s } sub-structures, and , is an inner product in an appropriately normalized vector space. Much work has been devoted to deciding which sub-structures are more suitable than others (refer to appendix A.1). Instead of defining hand crafted similarity measures between substructures, InfoGraph adopts a more principled metric -mutual information.</p><p>Contrastive methods. An important approach for unsupervised representation learning is to train an encoder to be contrastive between representations that capture statistical dependencies of interest and those that do not. For example, a contrastive approach may employ a scoring function, training the encoder to increase the score on "real" input (a.k.a, positive examples) and decrease the score on "fake" input (a.k.a., negative samples). For more detailed discussion, refer to appendix A.2.</p><p>Deep Graph InfoMax (DGI) <ref type="bibr" target="#b54">Veliƒçkoviƒá et al. (2018)</ref> also belongs to this category, which aims to train a node encoder that maximizes mutual information between node representations and the pooled global graph representation. Although we built upon a similar methodology, our aim is different than theirs as our goal is to obtain embeddings at the whole graph level for unsupervised and semisupervised learning whereas DGI only evaluates node level embeddings. In order to differentiate our method with Deep Graph Infomax <ref type="bibr" target="#b54">(Veliƒçkoviƒá et al. (2018)</ref>), we term our model InfoGraph.</p><p>Semi-supervised Learning. A comprehensive overview of semi-supervised learning (SSL) methods is out of the scope of this paper. We refer readers to Appendix B for a short overview or <ref type="bibr">Zhu et al. (2003)</ref>; <ref type="bibr" target="#b4">Chapelle et al. (2006)</ref>; <ref type="bibr" target="#b40">Oliver et al. (2018)</ref> for more comprehensive discussions. Here, we discuss a state-of-the-art method applicable for regression tasks -Mean Teacher <ref type="bibr" target="#b53">Tarvainen &amp; Valpola (2017)</ref>.Mean Teacher adds a loss term which encourages the distance between the original network's output and the teacher's output to be small. The teacher's predictions are made using an exponential moving average of parameters from previous training steps. Inspired by the "studentteacher" framework in Mean Teacher model, our semi-supervised model (InfoGraph*) deploys two separate encoders but instead of explicitly encouraging the output of the student model to be similar to the teacher model's output, we enable the student model to learn from the teacher model by maximizing mutual information between intermediate representations learned by two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Most recent work on graphs focus on supervised learning tasks or learning node representations. However, many graph analytic tasks such as graph classification, regression, and clustering require representing entire graphs as fixed-length feature vectors. Though graph-level representations can be obtained through the node-level representations implicitly, explicitly extracting the graph can be more straightforward and optimal for graph-oriented tasks.</p><p>Another scenario that is important, yet attracts comparatively less attention in the graph related literature is semi-supervised learning. One of the biggest challenges in prediction tasks in biology <ref type="bibr" target="#b63">Yan et al. (2017)</ref>; <ref type="bibr" target="#b65">Yang et al. (2014)</ref>   <ref type="formula">2017</ref>) is the extreme scarcity of labeled data. Therefore, semi-supervised learning, in which a large number of unlabeled samples are incorporated with a small number of labeled samples to enhance accuracy of models, will play a key role in these areas.</p><p>In this section, we first formulate an unsupervised whole graph representation learning problem and a semi-supervised prediction task on graphs. Then, we present our method to learn graph-level representations. Afterwards we present our proposed model for the semi-supervised learning scenario. InfoGraph uses a batch-wise fashion to generate all possible positive and negative samples. For example, consider the toy example with 2 input graphs in the batch and 7 nodes (or patch representations) in total. For the global representation of the blue graph, there will be 7 input pairs to the discriminator and same for the red graph. Thus, the discriminator will take 14 (global representation, patch representation) pairs as input in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PROBLEM DEFINITION</head><p>Unsupervised Graph Representation Learning. Given a set of graphs G = {G 1 , G 2 , ...} and a positive integer Œ¥ (the expected embedding size), our goal is to learn a Œ¥-dimensional distributed representation of every graph G i ‚àà G. We denote the number of nodes in G i as |G i |. We denote the matrix of representations of all graphs as Œ¶ ‚àà R |G|√óŒ¥ .</p><p>Semi-supervied Graph Prediction Tasks. Given a set of labeled graphs</p><formula xml:id="formula_0">G L = {G 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , G |G L | } with corresponding output {o 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , o |G L | }, and a set of unlabeled samples G U = {G |G L |+1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , G |G L |+|G U |</formula><p>}, our goal is to learn a model that can make predictions for unseen graphs. Note that in most cases |G U | |G L |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INFOGRAPH</head><p>We focus on graph neural networks (GNNs)-a flexible class of embedding architectures which generate node representations by repeated aggregation over local node neighborhoods. The representations of nodes are learned by aggregating the features of their neighborhood nodes, so we refer to these as patch representations. GNNs utilize a READOUT function to summarize all the obtained patch representations into a fixed length graph-level representation.</p><p>Formally, the k-th layer of a GNN is</p><formula xml:id="formula_1">h (k) v = COMBINE (k) h (k‚àí1) v , AGGREGATE (k) h (k‚àí1) v , h (k‚àí1) u , e uv : u ‚àà N (v) ,<label>(1)</label></formula><p>where h</p><formula xml:id="formula_2">(k)</formula><p>v is the feature vector of node v at the k-th iteration/layer (or patch representation centered at node i), e uv is the feature vector of the edge between u and v, and N (v) are neighborhoods to node v. h (0) v is often initialized as node features. READOUT can be a simple permutation invariant function such as averaging or a more sophisticated graph-level pooling function <ref type="bibr" target="#b66">Ying et al. (2018)</ref>; <ref type="bibr" target="#b67">Zhang et al. (2018)</ref>.</p><p>We seek to obtain graph representations by maximizing the mutual information between graph-level and patch-level representations. By doing so, the graph representations can learn to encode aspects of the data that are shared across all substructures. Assume that we are given a set of training samples G := {G j ‚àà G} N j=1 with empirical probability distribution P on the input space. Let œÜ denote the set of parameters of a K-layer graph neural network. After the first k layers of the graph neural network, the input graph will be encoded into a set of patch representations {h</p><formula xml:id="formula_3">(k) i } N i=1 .</formula><p>Next, we summarize feature vectors at all depths of the graph neural network into a single feature vector that captures patch information at different scales centered at every node. Inspired by <ref type="bibr" target="#b62">Xu et al. (2018b)</ref>, we use concatenation. That is,</p><formula xml:id="formula_4">h i œÜ = CONCAT({h (k) i } K k=1 ) (2) H œÜ (G) = READOUT({h i œÜ } N i=1 ) (3)</formula><p>where h i œÜ is the summarized patch representation centered at node i and H œÜ (G) is the global representation after applying READOUT. Note that here we slightly abuse the notation of h.</p><p>We define our mutual information (MI) estimator on global/local pairs, maximizing the estimated MI over the given dataset G := {G j ‚àà G} N j=1 :</p><formula xml:id="formula_5">œÜ, œà = arg max œÜ,œà G‚ààG 1 |G| u‚ààG I œÜ,œà ( h u œÜ ; H œÜ (G)).<label>(4)</label></formula><p>I œÜ,œà is the mutual information estimator modeled by discriminator T œà and parameterized by a neural network with parameters œà. We use the Jensen-Shannon MI estimator (following the formulation of Nowozin et al. ( <ref type="formula">2016</ref>)),</p><formula xml:id="formula_6">I œÜ,œà (h i œÜ (G); H œÜ (G)) := E P [‚àísp(‚àíT œÜ,œà ( h i œÜ (x), H œÜ (x)))] ‚àí E P√ó P[sp(T œÜ,œà ( h i œÜ (x ), H œÜ (x)))] (5)</formula><p>where x is an input sample, x (negative sample) is an input sampled from P = P, a distribution identical to the empirical probability distribution of the input space, and sp(z) = log(1 + e z ) is the softplus function. In practice, we generate negative samples using all possible combinations of global and local patch representations across all graph instances in a batch.</p><p>Since H œÜ (G) is encouraged to have high MI with patches that contain information at all scales, this favours encoding aspects of the data that are shared across patches and aspects that are shared across scales. The algorithm is illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>.</p><p>It should be noted that our model is similar to Deep Graph Infomax (DGI) <ref type="bibr" target="#b54">Veliƒçkoviƒá et al. (2018)</ref>, a model proposed for learning unsupervised node embeddings. However, there are important design differences due to the different problems that we are focusing on. First, in DGI they use random sampling to obtain negative samples due to the fact that they are mainly focusing on learning node embeddings on a graph. However, contrastive methods require a large number of negative samples to be competitive <ref type="bibr" target="#b16">Hjelm et al. (2018)</ref>, thus the use of batch-wise generation of negative samples is crucial as we are trying to learn graph embeddings given many graph instances. Second, the choice of graph convolution encoders is also crucial. We use <ref type="bibr">GIN Xu et al. (2018a)</ref> while DGI uses <ref type="bibr">GCN Kipf &amp; Welling (2016)</ref> as GIN provides a better inductive bias for graph level applications. Graph neural network designs should be considered carefully so that graph representations can be discriminative towards other graph instances. For example, we use sum over mean for READOUT and that can provide important information regarding the size of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SEMI-SUPERVISED INFOGRAPH</head><p>Based on the previous unsupervised model, a straightforward way to do semi-supervised property prediction on graphs is to combine the purely supervised loss and the unsupervised objective function which acts as a regularization term. In doing so, the model is trained to predict properties for the labeled dataset while keeping a rich discriminative intermediate representation learned from both the labeled and the unlabeled dataset. That is, we try to minimize the following objective function:</p><formula xml:id="formula_7">L total = |G L | i=1 L supervised (y œÜ (G i ), o i ) + Œª |G L |+|G U | j=1 L unsupervised (h œÜ (G j ); H œÜ (G j ))<label>(6)</label></formula><p>where L supervised (y œÜ (G i ), o i ) is defined as the loss function of graph G i that measures the discrepancy between the classifier output y œÜ (G i ) and the true output There are two separate encoders with the same architecture, one for the supervised task and the other trained using both labeled and unlabeled data with an unsupervised objective (eq. equation 4). We encourage the mutual information of the two representations learned by the two encoders to be high by deploying a discriminator that takes a pair of representation as input and determines whether they are from the same input graph.</p><formula xml:id="formula_8">o i . L unsupervised (h œÜ (G j ); H œÜ (G j ) is the</formula><p>unsupervised InfoGraph loss term as defined in eq. equation 4 that can be optimized using both labeled and unlabeled data. The hyper-parameter Œª controls the relative weight between the purely supervised and the unsupervised loss. The intuition behind this is that the model will benefit from learning a good representation from the large amount of unlabeled data while learning to predict the corresponding supervised label.</p><p>However, supervised tasks and unsupervised tasks may favor different information or a different semantic space. Simply combining the two loss functions using the same encoder may lead to "negative transfer"<ref type="foot" target="#foot_0">1</ref> (Pan &amp; <ref type="bibr" target="#b42">Yang, 2009;</ref><ref type="bibr" target="#b48">Rosenstein et al., 2005)</ref>. We propose a simple way to alleviate this problem: we deploy two encoder models: the encoder on the labelled data (supervised encoder) and the encoder on the unlabelled data (unsupervised encoder). For transferring the learned representations from the unsupervised encoder to the supervised encoder, we define a loss term that encourages the representations learned by the two encoders to have high mutual information, at all levels of representations (third term of Eq. 8). Formally, let œï denote the set of parameters of another K-layered graph neural network, identical to the one parameterized by œÜ, and let Œª be a tunable hyper-parameter, H k œÜ (G), H k œï (G) be global encoder representations of the graph G at encoder layer k, then total loss function can be defined as follows:</p><formula xml:id="formula_9">L total = |G L | i=1 L supervised (y œÜ (G i ), o i ) + |G L |+|G U | j=1 L unsupervised (h œï (G j ); H œï (G j )) (7) ‚àí Œª |G L |+|G U | j=1 1 |G j | K k=1 I(H k œÜ (G j ); H k œï (G j ).<label>(8)</label></formula><p>Notice that this formulation can be seen as a special instance of the student-teacher framework. However, unlike the recent student-teacher methods for semi-supervised learning <ref type="bibr" target="#b27">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b53">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b57">Verma et al., 2019b)</ref>, which enforce the predictions of the student model to be similar to the teacher model, we enforce the transfer of knowledge from the teacher model to the student model via mutual-information maximization at various levels of representations.</p><p>In practice, to reduce the computation overhead introduced by the third term of Eq 8, instead of enforcing the mutual-information maximization over all the layers of the encoders, at each training update, we enforce mutual-information maximization on a randomly chosen layer of the encoder <ref type="bibr" target="#b56">(Verma et al., 2019a)</ref>.</p><p>In our semi-supervised experiments, we refer to the naive method using the objective function given in eq. equation 6 as InfoGraph. We refer to the method that uses two separate encoders and employ the objective function given in eq. equation 8 as InfoGraph*. InfoGraph* is fully summarized in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate the effectiveness of the graph-level representation learned by InfoGraph on downstream graph classification tasks and on semi-supervised molecular property prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>For graph classification, we conduct experiments on 6 well-known benchmark datasets: MUTAG, PTC, REDDIT-BINARY, REDDIT-MULTI-5K, IMDB-BINARY, and IMDB-MULTI <ref type="bibr" target="#b64">(Yanardag &amp; Vishwanathan (2015)</ref>). For semi-supervised learning tasks, we use the publicly available QM9 dataset <ref type="bibr" target="#b47">Ramakrishnan et al. (2014)</ref>. Additional details of the datasets can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BASELINES</head><p>For graph classification, we used 6 state-of-the-art graph kernels for comparison: Random Walk (RW) <ref type="bibr" target="#b12">G√§rtner et al. (2003)</ref> For semi-supervised tasks, aside from comparing the results with the fully supervised results, we also compare our results with a state-of-the-art semi-supervised method: Mean Teachers <ref type="bibr" target="#b53">Tarvainen &amp; Valpola (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EXPERIMENT CONFIGURATION</head><p>For graph classification tasks, we adopt the same procedure of previous works <ref type="bibr" target="#b38">Niepert et al. (2016)</ref>; <ref type="bibr" target="#b55">Verma &amp; Zhang (2017)</ref>; <ref type="bibr" target="#b64">Yanardag &amp; Vishwanathan (2015)</ref>; <ref type="bibr" target="#b67">Zhang et al. (2018)</ref> to make a fair comparison and used 10-fold cross validation accuracy to report the classification performance. Experiments are repeated 5 times. We report results from previous papers with the same experimental setup if available. If results are not previously reported, we implement them and conduct a hyperparameter search according to the original paper. For node2vec <ref type="bibr" target="#b15">Grover &amp; Leskovec (2016)</ref>, we took the result from Narayanan et al. ( <ref type="formula">2017</ref>) but we did not run it on all datasets as the implementation details are not clear in the paper. For Deep Graph Kernels, we report the best result out of Deep WL Kernels, Deep GK Kernels, and Deep RW Kernels. For sub2vec, we report the best result out of its two variants: sub2vec-N and sub2vec-S. For all methods, the embedding dimension is set to 512 and parameters of downstream classifiers are independently tuned using cross validation on training folds of data. The best average classification accuracy is reported for each method. The classification accuracies are computed using <ref type="bibr">LIBSVM Chang &amp; Lin (2011)</ref>, and the C parameter was selected from {10 ‚àí3 , 10 ‚àí2 , . . . , 10 2 , 10 3 }.</p><p>The QM9 dataset has 130462 molecules in it. We adopt similar experimental settings as traditional semi-supervised methods <ref type="bibr" target="#b53">Tarvainen &amp; Valpola (2017)</ref>; <ref type="bibr" target="#b27">Laine &amp; Aila (2016)</ref>; <ref type="bibr" target="#b32">Miyato et al. (2018)</ref>.</p><p>We randomly chose 5000 samples as labeled samples for training and another 10000 as validation samples, 10000 samples for testing, and use the rest as unlabeled training samples. Note that we use the exact same split when running the supervised model and the semi-supervised model. We use the validation set to do model selection and we report scores on the test set. All targets were normalized  The bottom half shows the error ratio (with respect to supervised result) of the semi-supervised models using the same underlying model. Lower scores are better and values less than 1.0 indicate better performance than the supervised baseline.</p><p>to have mean 0 and variance 1. We minimize the mean squared error between the model output and the target, although we evaluate mean absolute error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">MODEL CONFIGURATION</head><p>For the unsupervised experiments, we use the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b61">Xu et al. (2018a)</ref>.</p><p>For the semi-supervised experiments, we adopt the same model as in <ref type="bibr">Gilmer et al. (2017) (enn-s2s)</ref>.</p><p>As recommended in <ref type="bibr" target="#b40">Oliver et al. (2018)</ref>, we use the exact same underlying model architecture when comparing semi-supervised learning approaches as our goal is not to produce state-of-the-art results, but instead to provide a rigorous comparative analysis in a common framework. In both scenarios, models were trained using SGD with the Adam optimizer. We use Pytorch <ref type="bibr" target="#b43">Paszke et al. (2017)</ref> and the Pytorch Geometric <ref type="bibr" target="#b10">Fey &amp; Lenssen (2019)</ref> libraries for all our experiments. For detailed hyper-parameter settings and architecture detail of the discriminator, see Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>The results of evaluating unsupervised graph level representations using downstream graph classification tasks are presented in Table <ref type="table" target="#tab_1">1</ref>. We show results from six methods including three state-of-the-art graph kernel methods: WL <ref type="bibr" target="#b51">Shervashidze et al. (2011)</ref>, <ref type="bibr">DGK Yanardag &amp; Vishwanathan (2015)</ref>, and MLG <ref type="bibr" target="#b24">Kondor &amp; Pan (2016)</ref>. While these kernel methods perform well on individual datasets, none of them are competitive across all of the datasets. Additionally, MLG suffers from a long run time and take more than 24 hours to run on the two larger benchmark datasets. We find that InfoGraph outperforms all of these baselines on 4 out of 6 of the datasets. In the other 2 datasets, InfoGraph still has very competitive performance.</p><p>The results of the semi-supervised learning experiments on the molecular property prediction task are presented in Table <ref type="table" target="#tab_2">2</ref>. We observe that by simply combining the supervised objective with the unsupervised infomax objective (InfoGraph) obtains better performance compared to the purely supervised models on 7 out of 12 of the targets. However, in 1 out of 12 targets it does not obtain better performance and in 4 out of 12 targets, it results in poorer performance. This "negative transfer" effect may be caused by the fact that the supervised objective and the unsupervised objective favor different information or different latent semantic space. This effect is alleviated with InfoGraph*, our modified version of InfoGraph for semi-supervised learning. InfoGraph* improves over the supervised model in all the 12 targets. InfoGraph* obtains the best result on 11 targets while the Mean Teacher method obtains the best results on 2 targets (with one overlap). However, the Mean Teacher model yields worse performance on 2 targets when compared to the supervised result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose InfoGraph to learn unsupervised graph-level representations and InfoGraph* for semi-supervised learning. We conduct experiments on graph classification and molecular property prediction tasks to evaluate these two methods. Experimental results show that InfoGraph and InfoGraph* are both very competitive with state-of-the-art methods. There are many research works on semi-supervised learning on image data, but few of them focus on semi-supervised learning for graph structured data. In the future, we aim to explore semi-supervised frameworks designed specifically for graphs.</p><p>Mean Teacher: A difficulty with the Œ†-model approach is that it relies on a potentially unstable "target" prediction, namely the second stochastic network prediction which can rapidly change over the course of training. As a result, <ref type="bibr" target="#b53">Tarvainen &amp; Valpola (2017)</ref> proposed to obtain a more stable target output for unlabeled data by setting the target to predictions made using an exponential moving average of parameters from previous training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DATASETS</head><p>C.1 GRAPH CLASSIFICATION DATASETS MUTAG contains 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 different discrete labels. PTC is a dataset of 344 different chemical compounds that have been tested for carcinogenicity in male and female rats. This dataset has 19 discrete labels. IMDB-BINARY and IMDB-MULTI are movie collaboration datasets. Each graph corresponds to an ego-network for each actor/actress, where nodes correspond to actors/actresses and an edge is drawn between two actors/actresses if they appear in the same movie. Each graph is derived from a pre-specified genre of movies, and the task is to classify the genre graph it is derived from. REDDIT-BINARY and REDDIT-MULTI5K are balanced datasets where each graph corresponds to an online discussion thread and nodes correspond to users. An edge was drawn between two nodes if at least one of them responded to another's comment. The task is to classify each graph to the community or subreddit that it belongs to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 QM9</head><p>All molecules in the dataset consist of Hydrogen (H), Carbon (C), Oxygen (O), Nitrogen (N), and Flourine (F) atoms and contain up to 9 non-Hydrogen atoms. In all, this results in about 134,000 drug-like organic molecules that span a wide range of chemical compositions and properties. A total of 12 interesting and fundamental chemical properties are pre-computed for each molecule. For a detailed description of the properties in the QM9 dataset, see section 10.2 of <ref type="bibr" target="#b13">Gilmer et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MODEL CONFIGURATION</head><p>For the unsupervised experiments, we use the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b61">Xu et al. (2018a)</ref>. GNN layers are chosen from {4, 8, 12}. Initial learning rate is chosen from the set {10 ‚àí2 , 10 ‚àí3 , 10 ‚àí4 }. The number of epochs are chosen from {10, 20, 100}. The batch size is set to 128.</p><p>For the semi-supervised experiments, the number of set2set computations is set to 3. Model were trained with an initial learning rate 0.001 for 500 epochs with a batch size 20. For the supervised case, the weight decay is chosen from {0, 10 ‚àí3 , 10 ‚àí4 }. For InfoGraph and InfoGraph*, Œª is chosen from {10 ‚àí3 , 10 ‚àí4 , 10 ‚àí5 }.</p><p>The discriminator scores global-patch representation pairs by passing two representations to different non-linear transformations and then takes the dot product of the two transformed representations. Both non-linear transformations are parameterized by 3-layered feed-forward neural networks with jumping connections. Following each linear layer is a ReLU activation function.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>or molecular machine learning Duvenaud et al. (2015); Gilmer et al. (2017); Jia &amp; Liang (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of InfoGraph. N.A. denotes neighborhood aggregation. An input graph is encoded into a feature map by graph convolutions and jumping concatenation. The discriminator takes a (global representation, patch representation) pair as input and decides whether they are from the same graph.InfoGraph uses a batch-wise fashion to generate all possible positive and negative samples. For example, consider the toy example with 2 input graphs in the batch and 7 nodes (or patch representations) in total. For the global representation of the blue graph, there will be 7 input pairs to the discriminator and same for the red graph. Thus, the discriminator will take 14 (global representation, patch representation) pairs as input in this case.</figDesc><graphic url="image-1.png" coords="4,108.00,81.86,396.00,115.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the semi-supervised version of InfoGraph (InfoGraph*).There are two separate encoders with the same architecture, one for the supervised task and the other trained using both labeled and unlabeled data with an unsupervised objective (eq. equation 4). We encourage the mutual information of the two representations learned by the two encoders to be high by deploying a discriminator that takes a pair of representation as input and determines whether they are from the same input graph.</figDesc><graphic url="image-2.png" coords="6,108.00,81.86,396.00,151.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-3.png" coords="16,147.60,151.51,316.81,237.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy on 6 datasets. The result in bold indicates the best reported classification accuracy. The top half of the table compares results with various graph kernel approaches while bottom half compares results with other state-of-the-art unsupervised graph representation learning methods. '&gt; 1 day' represents that the computation exceeds 24 hours. 'OMR' is out of memory error.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>MUTAG</cell><cell cols="2">PTC-MR</cell><cell>RDT-B</cell><cell></cell><cell cols="2">RDT-M5K</cell><cell cols="2">IMDB-B</cell><cell cols="2">IMDB-M</cell></row><row><cell>(No. Graphs)</cell><cell></cell><cell>188</cell><cell>344</cell><cell></cell><cell>2000</cell><cell></cell><cell>4999</cell><cell></cell><cell>1000</cell><cell></cell><cell cols="2">1500</cell></row><row><cell>(No. classes)</cell><cell></cell><cell>2</cell><cell>2</cell><cell></cell><cell>2</cell><cell></cell><cell>5</cell><cell></cell><cell>2</cell><cell></cell><cell>3</cell><cell></cell></row><row><cell>(Avg. Graph Size)</cell><cell></cell><cell>17.93</cell><cell>14.29</cell><cell></cell><cell>429.63</cell><cell></cell><cell>508.52</cell><cell></cell><cell cols="2">19.77</cell><cell cols="2">13.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Graph Kernels</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RW</cell><cell cols="2">83.72 ¬± 1.50</cell><cell cols="2">57.85 ¬± 1.30</cell><cell>OMR</cell><cell></cell><cell>OMR</cell><cell></cell><cell cols="2">50.68 ¬± 0.26</cell><cell cols="2">34.65 ¬± 0.19</cell></row><row><cell>SP</cell><cell cols="2">85.22 ¬± 2.43</cell><cell cols="2">58.24 ¬± 2.44</cell><cell cols="2">64.11 ¬± 0.14</cell><cell cols="2">39.55 ¬± 0.22</cell><cell cols="2">55.60 ¬± 0.22</cell><cell cols="2">37.99 ¬± 0.30</cell></row><row><cell>GK</cell><cell cols="2">81.66 ¬± 2.11</cell><cell cols="2">57.26 ¬± 1.41</cell><cell cols="2">77.34 ¬± 0.18</cell><cell cols="2">41.01 ¬± 0.17</cell><cell cols="2">65.87 ¬± 0.98</cell><cell cols="2">43.89 ¬± 0.38</cell></row><row><cell>WL</cell><cell cols="2">80.72 ¬± 3.00</cell><cell cols="2">57.97 ¬± 0.49</cell><cell cols="2">68.82 ¬± 0.41</cell><cell cols="2">46.06 ¬± 0.21</cell><cell cols="2">72.30 ¬± 3.44</cell><cell cols="2">46.95 ¬± 0.46</cell></row><row><cell>DGK</cell><cell cols="2">87.44 ¬± 2.72</cell><cell cols="2">60.08 ¬± 2.55</cell><cell cols="2">78.04 ¬± 0.39</cell><cell cols="2">41.27 ¬± 0.18</cell><cell cols="2">66.96 ¬± 0.56</cell><cell cols="2">44.55 ¬± 0.52</cell></row><row><cell>MLG</cell><cell cols="2">87.94 ¬± 1.61</cell><cell cols="2">63.26 ¬± 1.48</cell><cell>&gt; 1 Day</cell><cell></cell><cell>&gt; 1 Day</cell><cell></cell><cell cols="2">66.55 ¬± 0.25</cell><cell cols="2">41.17 ¬± 0.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Other Unsupervised Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>node2vec</cell><cell cols="2">72.63 ¬± 10.20</cell><cell cols="2">58.58 ¬± 8.00</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell></row><row><cell>sub2vec</cell><cell cols="2">61.05 ¬± 15.80</cell><cell cols="2">59.99 ¬± 6.38</cell><cell cols="2">71.48 ¬± 0.41</cell><cell cols="2">36.68 ¬± 0.42</cell><cell cols="2">55.26 ¬± 1.54</cell><cell cols="2">36.67 ¬± 0.83</cell></row><row><cell>graph2vec</cell><cell cols="2">83.15 ¬± 9.25</cell><cell cols="2">60.17 ¬± 6.86</cell><cell cols="2">75.78 ¬± 1.03</cell><cell cols="2">47.86 ¬± 0.26</cell><cell cols="2">71.1 ¬± 0.54</cell><cell cols="2">50.44 ¬± 0.87</cell></row><row><cell>InfoGraph</cell><cell cols="2">89.01 ¬± 1.13</cell><cell cols="2">61.65 ¬± 1.43</cell><cell cols="2">82.50 ¬± 1.42</cell><cell cols="2">53.46 ¬± 1.03</cell><cell cols="2">73.03 ¬± 0.87</cell><cell cols="2">49.69 ¬± 0.53</cell></row><row><cell>Target</cell><cell cols="5">Mu (0) Alpha (1) HOMO (2) LUMO (3) Gap (4)</cell><cell>R2 (5)</cell><cell cols="2">ZPVE(6) U0 (7)</cell><cell>U (8)</cell><cell>H (9)</cell><cell cols="2">G(10) Cv (11)</cell></row><row><cell>MAE</cell><cell>0.3201</cell><cell>0.5792</cell><cell>0.0060</cell><cell>0.0062</cell><cell cols="2">0.0091 10.0469</cell><cell>0.0007</cell><cell cols="5">0.3204 0.2934 0.2722 0.2948 0.2368</cell></row><row><cell>Semi-Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Error Ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean-Teachers</cell><cell>1.09</cell><cell>1.00</cell><cell>0.99</cell><cell>1.00</cell><cell>0.97</cell><cell>0.52</cell><cell>0.77</cell><cell>1.16</cell><cell>0.93</cell><cell>0.79</cell><cell>0.86</cell><cell>0.86</cell></row><row><cell>InfoGraph</cell><cell>1.02</cell><cell>0.97</cell><cell>1.02</cell><cell>0.99</cell><cell>1.01</cell><cell>0.71</cell><cell>0.96</cell><cell>0.85</cell><cell>0.93</cell><cell>0.93</cell><cell>0.99</cell><cell>1.00</cell></row><row><cell>InfoGraph*</cell><cell>0.99</cell><cell>0.94</cell><cell>0.99</cell><cell>0.99</cell><cell>0.98</cell><cell>0.49</cell><cell>0.52</cell><cell>0.44</cell><cell>0.58</cell><cell>0.57</cell><cell>0.54</cell><cell>0.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of semi-supervised experiments on QM9 dataset. The result in bold indicates the best performance. The top half of the table shows the mean absolute error (MAE) of the supervised model.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We slightly abuse this term in this paper as it usually refers to transferring knowledge from a less related source and thus may hurt the target performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We summarize our contributions as follows:</p><p>‚Ä¢ We propose InfoGraph, an unsupervised graph representation learning method based on Deep InfoMax (DIM) <ref type="bibr" target="#b16">Hjelm et al. (2018)</ref>. ‚Ä¢ We show that InfoGraph can be extended to semi-supervised prediction tasks on graphs.</p><p>‚Ä¢ We empirically show that InfoGraph surpasses state-of-the-art performance on graph classification tasks with unsupervised learning and obtains performance comparable with state-ofart methods on molecular property prediction tasks using semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Shengchao Liu and Weihua Hu for the extremely helpful discussions and comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RELATED WORK</head><p>A.1 GRAPH KERNELS Popular graph kernels are graphlets <ref type="bibr" target="#b46">Pr≈æulj (2007)</ref>; <ref type="bibr" target="#b50">Shervashidze et al. (2009)</ref>, random walk and shortest path kernels <ref type="bibr" target="#b20">Kashima et al. (2003)</ref>; <ref type="bibr" target="#b2">Borgwardt &amp; Kriegel (2005)</ref>, and the Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b51">Shervashidze et al. (2011)</ref>. Furthermore, deep graph kernels <ref type="bibr" target="#b64">Yanardag &amp; Vishwanathan (2015)</ref>, graph invariant kernels <ref type="bibr" target="#b41">Orsini et al. (2015)</ref>, optimal assignment graph kernels <ref type="bibr" target="#b25">Kriege et al. (2016)</ref> and multiscale Laplacian graph kernels <ref type="bibr" target="#b24">Kondor &amp; Pan (2016)</ref> have been proposed with the goal to redefine kernel functions to appropriately capture sub-structural similarity at different levels. Another line of research in this area focuses on efficiently computing these kernels either through exploiting certain structural dependencies, or via approximations/randomization <ref type="bibr" target="#b9">Feragen et al. (2013)</ref>; de <ref type="bibr" target="#b7">Vries (2013)</ref>; <ref type="bibr" target="#b35">Neumann et al. (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CONTRASTIVE METHODS</head><p>Contrastive methods are central many popular word-embedding methods <ref type="bibr" target="#b6">Collobert &amp; Weston (2008)</ref>; <ref type="bibr" target="#b33">Mnih &amp; Kavukcuoglu (2013)</ref>; <ref type="bibr" target="#b31">Mikolov et al. (2013b)</ref>. Word2vec <ref type="bibr" target="#b30">Mikolov et al. (2013a)</ref> is an unsupervised algorithm which obtains word representations by using the representations to predict context words (the words that surround it). Doc2vec <ref type="bibr" target="#b28">Le &amp; Mikolov (2014)</ref> is an extension of the continuous Skip-gram model that predicts representations of words from that of a document containing them. Researchers extended many of these unsupervised language models to learn representations of graph-structured input <ref type="bibr" target="#b0">Adhikari et al. (2018)</ref>; <ref type="bibr" target="#b34">Narayanan et al. (2017)</ref>. For example, graph2vec Narayanan et al. ( <ref type="formula">2017</ref>) extends Doc2vec to arbitrary graphs. Intuitively, for graph2vec a graph and the rooted subgraphs in it correspond to a document and words in a paragraph vector, respectively. One of the technical contributions of the paper is using the Weisfeiler-Lehman relabelling algorithm <ref type="bibr" target="#b59">Weisfeiler &amp; Lehman (1968)</ref>; <ref type="bibr" target="#b51">Shervashidze et al. (2011)</ref> to enumerate all rooted subgraphs up to a specified depth. AWE (Anonymous Walk Embeddings) <ref type="bibr" target="#b18">Ivanov &amp; Burnaev (2018)</ref> is another method based on CBOW framework. instead of using rooted subgraphs as words like graph2vec, AWE considers anonymous walk embeddings for the same source node as co-occurring words. InfoGraph has the two advantages when compared with these methods. First, InfoGraph learns representations directly from data instead of utilizing hand-crafted procedures (i.e. Weisfeiler-Lehman relabelling algorithm in graph2vec and random walkw in AWE). Second, InfoGraph has a clear objective that can be easily combined with other objectives. For example, InfoGraph*, the semi-supervised method that we proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SEMI-SUPERVISED LEARNING</head><p>Here we discuss the most common class of SSL methods which involve adding an additional loss term to the training of a neural network as they are pragmatic and are currently the state-of-the-art on image classification datasets.</p><p>Entropy Minimization (EntMin): EntMin <ref type="bibr" target="#b14">Grandvalet &amp; Bengio (2005)</ref> adds a loss term applied that encourages the network to make "confident" (low-entropy) predictions for all unlabeled examples, regardless of their class.</p><p>Pseudo-Labeling: Pseudo-labeling <ref type="bibr" target="#b29">Lee (2013)</ref> proceeds by producing "pseudo-labels" for unlabeled input data points using the prediction function itself over the course of training. Pseudo-labels which have a corresponding class probability that is larger than a predefined threshold are used as targets for a the standard supervised loss function.</p><p>Œ†-Model: Neural networks can produce different outputs for the same input while common regularization techniques such as data augmentation, dropout, and adding noise are applied. Œ†-Model Laine &amp; Aila (2016); <ref type="bibr" target="#b49">Sajjadi et al. (2016)</ref> adds a loss term which encourages the distance between a network's output for different passes of unlabeled data through the network to be small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual Adversarial</head><p>Training: Instead of relying on the built-in stochasticity as in Œ†-Model, Virtual Adversarial Training (VAT) <ref type="bibr" target="#b32">Miyato et al. (2018)</ref> directly approximates a tiny perturbation to add to the input which would most significantly affect the output of the prediction function. This pertubation can be approximated with an extra back-propagation for each optimization step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E CONVERGENCE PLOT</head><p>To prove that the objective Eq.8 with multiple loss terms can be optimized, we provide a convergence plot of InfoGraph*. We can see that the three loss terms all converge after around 150 epochs of training. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sub2vec: Feature learning for subgraphs</title>
		<author>
			<persName><forename type="first">Bijaya</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naren</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="170" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE international conference on data mining (ICDM&apos;05)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on intelligent systems and technology (TIST)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation and sample bias correction theory and algorithm for regression</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">103126</biblScope>
			<date type="published" when="2006">2014. 2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Semi-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Utilizing edge features in graph neural networks via variational information maximization</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weike</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxing</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Shyue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwen</forename><surname>Pengfei Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyong</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05488</idno>
	</analytic>
	<monogr>
		<title level="j">Chemistry of Materials</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3564" to="3572" />
			<date type="published" when="2019">2019a. 2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph networks as a universal machine learning framework for molecules and crystals</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast approximation of the weisfeiler-lehman graph kernel for rdf data</title>
		<author>
			<persName><forename type="first">Gerben Kd De</forename><surname>Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="606" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al√°n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName><forename type="first">Aasa</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marleen</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal link prediction by integrating content and structure information</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
				<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1169" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>G√§rtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Pre-training graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11921</idno>
		<idno>arXiv:1707.07328</idno>
	</analytic>
	<monogr>
		<title level="m">Anonymous walk embeddings</title>
				<imprint>
			<date type="published" when="2017">2018. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05983</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on machine learning (ICML-03)</title>
				<meeting>the 20th international conference on machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2018</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hyunjik Kim and Andriy Mnih. Disentangling by factorising</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04687</idno>
		<title level="m">Neural relational inference for interacting systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The multiscale laplacian graph kernel</title>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2990" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName><forename type="first">Pierre-Louis</forename><surname>Nils M Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Global landscape of protein complexes in the yeast saccharomyces cerevisiae</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Nevan J Krogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyuan</forename><surname>Cagney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gouqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandr</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Ignatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nira</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">P</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><surname>Tikuisis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="issue">7084</biblScope>
			<biblScope unit="page">637</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013b</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Lihui Chen andvYang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs</title>
		<author>
			<persName><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajasekar</forename><surname>Venkatesan</surname></persName>
		</author>
		<idno>CoRR, abs/1707.05005</idno>
		<ptr target="http://arxiv.org/abs/1707.05005" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient graph kernels by randomization</title>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Novi</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="378" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finding and evaluating community structure in networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Girvan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26113</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Semi-supervised learning of hierarchical representations of molecules using neural message passing</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10168</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph invariant kernels</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raedt</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using graph theory to analyze biological networks</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Georgios A Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charalampos</forename><forename type="middle">N</forename><surname>Secrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><forename type="middle">G</forename><surname>Moschopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Soldatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kossida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Aerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pantelis</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><surname>Bagos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioData mining</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName><forename type="first">Nata≈°a</forename><surname>Pr≈æulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="e177" to="e183" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Pavlo O Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName><forename type="first">Zvika</forename><surname>Michael T Rosenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2005 workshop on transfer learning</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mutual exclusivity loss for semi-supervised deep learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1908" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
				<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<publisher>International World Wide Web Conferences Steering Committee</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veliƒçkoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/verma19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">Jun 2019a</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannala</forename><surname>Juho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
				<editor>
			<persName><forename type="first">Sarit</forename><surname>Kraus</surname></persName>
		</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">August 10-16, 2019. ijcai.org, 2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Community discovery using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="521" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Detecting subnetwork-level dynamic correlations</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuxuan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="265" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Egonet: identification of human disease ego-network modules</title>
		<author>
			<persName><forename type="first">Rendong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC genomics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">314</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
				<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2018. 2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
	<note>Thirty-Second AAAI Conference on Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
