<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CausalVAE: Structured Causal Disentanglement in Variational Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-18">18 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengyue</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Furui</forename><surname>Liu</surname></persName>
							<email>&lt;liufurui2@huawei.com&gt;.</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinwei</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CausalVAE: Structured Causal Disentanglement in Variational Autoencoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-18">18 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.08697v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning disentanglement aims at finding a low dimensional representation, which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder is commonly used to disentangle independent factors from observations. However, in real scenarios, the factors with semantic meanings are not necessarily independent. Instead, there might be an underlying causal structure due to physics laws. We thus propose a new VAE based framework named CausalVAE, which includes causal layers to transform independent factors into causal factors that correspond to causally related concepts in data. We analyze the model identifiabitily of CausalVAE, showing that the generative model learned from the observational data recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic datasets consisting of pictures with multiple causally related objects abstracted from physical world, and a benchmark face dataset CelebA. The results show that the causal representations by CausalVAE are semantically interpretable, and lead to better results on downstream tasks. The new framework allows causal intervention, by which we can intervene any causal concepts to generate artificial data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised disentangled representation learning is of importance in various applications such as speech, object recognition, natural language processing, recommender systems <ref type="bibr" target="#b8">(Hsu et al., 2017;</ref><ref type="bibr" target="#b16">Ma et al., 2019;</ref><ref type="bibr" target="#b7">Hsieh et al., 2018)</ref>. The reason is that it would help enhancing the performance of models, i.e. improving the generalizability, robustness against adversarial attacks as well as the explanability, by learning data's latent representation. One of the most common frameworks for disentangled representation learning is Variational Autoencoders (VAE), a deep generative model trained using backpropagation to disentangle the underlying explanatory factors. To achieve disentangling via VAE, one uses a penalty function to regularize the training of the model by reducing the gap between the distribution of the latent factors and a standard Multivatrate Gaussian. It is expected to recover the latent variables if the observations in real world are generated by countable independent factor. To further enhance the disentangement, a line of methods consider minimizing the mutual information between different latent factors. For example, <ref type="bibr" target="#b4">Higgins et al. (2017)</ref>; <ref type="bibr" target="#b1">Burgess et al. (2018)</ref> adjust the hyperparameter to force latent codes to be independent of each other. <ref type="bibr" target="#b12">Kim &amp; Mnih (2018)</ref>; <ref type="bibr" target="#b2">Chen et al. (2018)</ref> further improve the independent by reducing total correlation. The theory of disentangled representation learning is still at its early stage. We face problems such as the lack of a formal definition for disentangled representations and identifiability of disentanglement of generic models in unsupervised learning. To fill the gap, <ref type="bibr" target="#b5">Higgins et al. (2018)</ref> proposed a new formalization of alignment between real world and latent space, and it is the first work which gives a formal definition of disentanglement. <ref type="bibr" target="#b14">Locatello et al. (2018)</ref> challenged the common settings of state-of-the-arts, arguing that they can not find an identifiable model without inductive bias. Although they do consider the unreasonable aspect of disentanglement tasks, there are still unsolved problems like identifiability and explainability of the independent factors, or learnability of parameters from observations. Common disentangling methods make a general assumption that the observations of real world are generated by countable independent factors. The recovered independent factors are considered good representations of data. We challenge this assumption, as in many real world situations, meaningful factors are connected with causality. Let us consider an example of a swinging pendulum Fig. <ref type="figure" target="#fig_0">1</ref>, the direction of the light l and the pendulum p are causes of the location loc and length of shadow len. We aim at learning deep representations that correspond to the four concepts. Obviously, these concepts are not independent, i.e. the direction of the light and the pendulum determine the location and the length of the shadow. There exists various kinds of causal model which could measure this causal relationship i.e. Linear Structual Equation Models (SEM) <ref type="bibr" target="#b20">(Shimizu et al., 2006)</ref>. Existing methods for disentangled representation learning like β-VAE <ref type="bibr" target="#b4">(Higgins et al., 2017)</ref> might not work as they forces the learned latent code to be as independent as possible. We argue the necessity to learn the causal representation as it allows us to intervention. For example, if we manage to learn latent codes corresponding to those four concepts, we can control the shape of the shadow without interrupting the generation of the light and the pendulum. This corresponds to the do-calculus <ref type="bibr" target="#b19">(Pearl, 2009)</ref> in causality, where the system operates under the condition that certain variables are controlled by external forces.</p><p>In this paper, we develop a causal disentangled representation learning framework that recovers dependent factors by introduce Linear SEM into variation autoencoder framework. We enforce the structure to the learned latent code by designing a loss function that penalizes the deviation of the learned graph to a Directed Acyclic Graph (DAG). In addition, we analyze the identifiablilty of the proposed generative model, to guarantee an the learned disentangled codes are similar with the true one.</p><p>To verify the effectiveness of the proposed method, we conduct experiments on the dataset which consists of multiple causally related objects. We demonstrate empirically that The learned factors are with semantic meanings and can be intervened to generate artificial images that do not appear in training data.</p><p>We highlight our contributions of this paper as follows:</p><p>• We propose a new framework of generative model to achieve causal disentanglement learning.</p><p>• We develop a theory on identifiability of our generative models, which guarantees that the true generative model is recoverable up to certain degree.</p><p>• Experiments with synthetic and real world images are conducted to show the causal representations learned by proposed method have rich semantics and more effective for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works &amp; Preliminary</head><p>In this section, we firstly provide background knowledge on disentangled representation learning, and we shall focus on recent state-of-the-arts using variational autoencoders. We review some recent advance of causality in generative models.</p><p>In the rest of the paper, we denote the latent variables by z with factorized density p(z) = Π d i=1 p(z i ) where d &gt; 1, and p(z|x) the posterior of the latent variables given the observation x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Disentanglement &amp; Identifiability Problems</head><p>Disentanglement is a typical concept towards independent factorial representation of data. The classic method for identifying intrinsic independent factors is ICA <ref type="bibr" target="#b3">(Comon, 1994;</ref><ref type="bibr" target="#b10">Jutten &amp; Karhunen, 2003)</ref>. <ref type="bibr" target="#b3">Comon (1994)</ref> prove model identifiability of ICA in linear case. However, the identifiability of linear ICA model could not be extended to non-linear settings directly. <ref type="bibr" target="#b9">Hyvarinen &amp; Morioka (2016)</ref>; <ref type="bibr" target="#b0">Brakel &amp; Bengio (2017)</ref> proposed a general identidfiability result for nonliear ICA, which links to the ideas of disentanglement under variational autoencoder. The disentangled representation learning learns mutually independent latent factors by an encoder-decoder framework. In the process, a standard normal distribution is used as prior  , whose prior distribution is assumed to be standard Multivariate Gaussian. Then it is transformed by the causal layer to be causal representation z. The z are assumed to be with a conditional prior distribution p(z|u). z is taken as the input of the decoder to reconstruct observation x.</p><p>of latent code. They use complex neural functions q(z|x) to approximate parameterized conditional probability p(z|x). This framework was extended by various existing works. Those works often introduce new independence constraints on the original loss function, leading to various disentangling metrics. β-VAE <ref type="bibr" target="#b4">(Higgins et al., 2017)</ref> proposes an adaptation framework which adjusts the weight of KL term to balance between independence of disentangled factors and reconstruction performance. While factor VAE <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> proposes a new frame work which focuses solely on the independence of factors.</p><p>The aforementioned unsupervised algorithms do not perform well in some situations which content complex dependency among each factors, possibly because of lacking Inductive Bias and identidfiability of the generative model <ref type="bibr" target="#b14">(Locatello et al., 2018)</ref>.</p><p>The identidfiability problem in variational autoencoder are defined as follows: if the parameters θ learned from data leads to a marginal distribution that equals the true one produced by θ, i. e., p θ (x) = p θ (x), then the joint distribution also matches p θ (x, z) = p θ (x, z). It means that the learned parameters is identidfiability. <ref type="bibr" target="#b11">Khemakhem et al. (2019)</ref> prove that the unsupervised variational autoencoder training results in infinite numbers of distinct models inducing the same data distributions, which means that the underlying ground truth is non-identifiable via unsupervised learning. On the contrary, by leveraging a few labels for supervision, one is able to recover the true model <ref type="bibr" target="#b17">(Mathieu et al., 2018;</ref><ref type="bibr" target="#b14">Locatello et al., 2018)</ref>. <ref type="bibr" target="#b13">Kulkarni et al. (2015)</ref>; <ref type="bibr" target="#b15">Locatello et al. (2019)</ref> use few labels to guide model training to reduce the parameter uncertainty. <ref type="bibr" target="#b11">Khemakhem et al. (2019)</ref> gives an identifiability result of variational autoencoder, by utilizing the theory of nonlinear ICA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Causal Discovery from Pure Observational Data</head><p>We refer to causal representation as the representations that are structured by a causal graph. Discovering the causal graph from pure observational data has attracted large amount of attention in the past decades <ref type="bibr" target="#b6">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b24">Zhang &amp; Hyvarinen, 2012;</ref><ref type="bibr" target="#b20">Shimizu et al., 2006)</ref>. <ref type="bibr" target="#b19">Pearl (2009)</ref> introduce a probabilistic graphical model based framework to learn causality from data. <ref type="bibr" target="#b20">Shimizu et al. (2006)</ref> proposed an effective method called LiNGAM to learn the causal graph and they proved that the model is fully identifiable under the assumption that the causal relationship is linear and the noise is non-Gaussian distributed. <ref type="bibr" target="#b25">Zheng et al. (2018)</ref> introduces DAG constraints for graph learning under continuous optimization (NOTEARS). <ref type="bibr">Zhu &amp; Chen (2019)</ref>; <ref type="bibr" target="#b18">Ng et al. (2019)</ref> use autoencoder framework to learn causal graph from data. <ref type="bibr" target="#b22">Suter et al. (2018)</ref> use causality theories to explain disentangled latent representations. Furthermore, <ref type="bibr" target="#b24">Zhang &amp; Hyvarinen (2012)</ref> use more complex hypothesis function to represent a more sophisticated cause-effect relationships between two entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present our method by starting with a new definition of latent representation, and then give a framework of disentanglement using supervision. At last, we give theoretical analysis of the model identifiability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Causal Model</head><p>To formalize causal representation framework, we consider n concepts in real world which have specific physical meanings. The concepts in observations are causally mixed by the causal relationship causal graph A *elaborate it in introduction.</p><p>As we mentioned, meaningful concepts are mostly not independent factors. We thus introduce causal representation in this paper. The causal representation is a latent data representation with a joint distribution that can be described by a probabilistic graphical model, specially a Directed Acyclic Graph (DAG). We consider linear models in the paper, i.e. Linear Structural Equation models (SEM) on latent factors z as:</p><formula xml:id="formula_0">z = A T z + = (I − A T ) −1 ,<label>(1)</label></formula><p>∼ N (0, I).</p><p>(</p><formula xml:id="formula_1">)<label>2</label></formula><p>where z ∈ R n is structural representation of n concepts. The independent noise are assumed to be Multivariate Gaussian.</p><p>Once we are able to learn the causal representations from data, we are able to do intervention to the latent codes to generate artificial data which does not appear in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generative Model</head><p>Our model is under the framework of VAE-based disentanglement. In additional to the encoder and the decoder structures, we introduce a causal layer to learn causal representations. The causal layer exactly implements a Linear SEM as described in Eq. 1, where (I − A T ) −1 is the parameters to learn in this layer.</p><p>Unsupervised learning of the model might be infeasible due to the identifiability issue discussed in <ref type="bibr" target="#b14">(Locatello et al., 2018)</ref>. As a result, the learnability of the causal layer is in question, and predefined casual representation is not identifiable. To address this issue, similar to iVAE <ref type="bibr" target="#b11">(Khemakhem et al., 2019)</ref>, we use the additional information associated with the true causal concepts as supervising signals. The additional observations must include the information of real concepts like the label, pixel level observations. We build a causal conditional generative framework which uses the additional observations from causal concepts. We will discuss the identifiability of models given additional observations later.</p><p>We follow similar definition and notation to iVAE <ref type="bibr" target="#b11">(Khemakhem et al., 2019)</ref>. Denote by x ∈ R d the observed variables and u ∈ R n the additional information. u i corresponds to the i-th concept in real causal system. Let z ∈ R n be the latent substantive variables with semantics and ∈ R n be the latent independent variables where</p><formula xml:id="formula_2">z = A T z + = (I − A T ) −1 . For simplicity, we denote C = (I − A T ) −1 .</formula><p>We now clarify the model assumptions for generation and inference process. Note that we regard both z and as the latent variables. Consider the following conditional generative model parameterized by θ = (f , h, C, T, λ):</p><formula xml:id="formula_3">p θ (x, z, |u) = p θ (x|z, , u)p θ ( , z|u).<label>(3)</label></formula><p>Let f (z) denotes the decoder which is assumed to be an invertible function and h(x) denote the encoder. Let ∈ R n be independent noise variables, and z ∈ R n as the latent codes of n concepts.</p><p>We define the generation and inference process as follows:</p><formula xml:id="formula_4">p θ (x|z, , u) = p θ (x|z) = p ξ dec (x − f (z)), q φ ( |x, u) = p ξenc ( − h(x)).<label>(4)</label></formula><p>which is obtained by assuming the following decoding and encoding equations</p><formula xml:id="formula_5">x = f (z) + ξ dec , (5) = h(x) + ξ enc ,<label>(6)</label></formula><p>where ξ = {ξ dec , ξ enc } are the vectors of independent noise with probability density p ξ (ξ). When ξ is infinitesimal, the encoder and decoder distributions can be regarded as deterministic ones.</p><p>We define the joint prior p θ (z, |u) for latent variables z and as</p><formula xml:id="formula_6">p θ ( , z|u) = p ( )p θ (z|u). (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where p ( ) = N (0, I) and the prior of latent substantive variables p θ (z|u) is a factorized Gaussian distribution conditioning on the additional observation u, i.e.</p><formula xml:id="formula_8">p θ (z|u) = Π i p i (z i |pa(z i ), u i )p(u i |pa(u i )), ∼ N (µ i (z i )λ(u i ), σ i (z i )λ 2 (u i )). (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where λ is an arbitrary function (approximated by a neural network). In this paper, since each causal representation depend on the value of their parents node. We consider the case λ(u) = u where pa(u i ) denotes the parents node of u i .</p><p>The distribution has two sufficient statistics, the mean and variance of z, which are denoted by T(z) = (µ(z), σ(z)) = (T 1,1 (z 1 ), . . . , T n,2 (z n )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Method</head><p>We apply variational Bayes to learn a tractable distribution q φ ( , z|x, u) to approximate the true posterior p θ ( , z|x, u).</p><p>Given data set D, we obtain empirical data distribution q D (x, u). The parameters θ and φ are learned by optimizing the following evidence lower bound (ELBO) on the expected data log-likelihood log p θ (x|u) = p θ (x, z, |u)dzd :</p><formula xml:id="formula_10">E q D [log p θ (x|u)] ≥ ELBO = E q D [E ,u∼q φ [log p θ (x|z, , u)] − D(q φ ( , z|x, u)||p θ ( , z|u))].<label>(9)</label></formula><p>where D(• •) denotes KL divergence.</p><p>Noticing the one-to-one correspondence between and z, we simplify the variational posterior as follows:</p><formula xml:id="formula_11">q φ ( , z|x, u) = q φ ( |x, u)1 z=C (z), = q φ (z|x, u)1 =C −1 z ( ).</formula><p>Further according to the model assumptions introduced in Section 3.2, i.e., generation process (4) and prior ( <ref type="formula" target="#formula_6">7</ref>), the ELBO can be rewritten as:</p><formula xml:id="formula_12">ELBO = E q D [E q φ (z|x,u) [log p θ (x|z)] − D(q φ ( |x, u)||p ( )) − D(q φ (z|x, u)||p θ (z|u))].<label>(10)</label></formula><p>where the third term is the key to disentangling the latent codes.</p><p>The causal adjacency matrix A is constrained to be a DAG. We introduce the acyclicity constraint. Instead of using traditional DAG constraint that is combinatorial, we adopt a continuous constraint function <ref type="bibr" target="#b25">(Zheng et al., 2018;</ref><ref type="bibr">Zhu &amp; Chen, 2019;</ref><ref type="bibr" target="#b18">Ng et al., 2019;</ref><ref type="bibr" target="#b23">Yu et al., 2019)</ref> . The function achieves 0 if and only if the adjacency matrix A are directed acyclic graph <ref type="bibr" target="#b23">(Yu et al., 2019)</ref>.</p><formula xml:id="formula_13">H(A) ≡ tr((I + A • A) n ) − n = 0. (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>The decoder (generator) uses latent concept representation for reconstruction. To make learning process more smooth, we add the square term to the constraint. Thus the optimization of ELBO should be constrained by Eq. 11:</p><p>maximize ELBO.</p><p>subject to H(A) = 0,</p><formula xml:id="formula_15">H 2 (A) = 0.<label>(12)</label></formula><p>By lagrangian multiplier method, we have the new loss function</p><formula xml:id="formula_16">L = −ELBO + α(H(A) + H 2 (A)). (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>where α denotes regularization hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Identifiability Analysis</head><p>In this section, we present the identifiability of our proposed model. We adopt the ∼-identifiability <ref type="bibr" target="#b11">(Khemakhem et al., 2019)</ref> as follows:</p><p>Definition 1. Let ∼ be the binary relation on Θ defined as follows:</p><formula xml:id="formula_18">(f , h, C, T, λ) ∼ ( f , h, C, T, λ) ⇔ ∃B 1 , B 2 |T(h(x)) = B 1 T( h(x)), T(f −1 (x)) = B 2 T( f −1 (x)), ∀x ∈ X . (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>If B 1 is an invertible matrix and B 2 is an invertible diagonal matrix in which each elements on diagonal correspond to u i .</p><p>we say that the model parameter is ∼-identifiable.</p><p>By extending Theorem 1 in iVAE <ref type="bibr" target="#b11">(Khemakhem et al., 2019)</ref>, we obtain the identifiability theory of our causal generative model.</p><p>Theorem 1. Assume that the data we observed are generated according Eq. 3-4 and the following assumptions hold, 1. The set {x ∈ X |φ ξ (x) = 0} has measure zero, where φ ξ is the characteristic function of the density p ξ defined in Eq. 5.</p><p>2. The Jacobian matrix of decoder function f and encoder function h are full rank.</p><p>3. The sufficient statistics T i,s (z i ) = 0 almost everywhere for all 1 ≤ i ≤ n and 1 ≤ s ≤ 2, where T i,s (z i ) is the sth statistic of variable z i .</p><p>4. The additional observations u i = 0</p><p>. Then the parameters (f , h, C, T, λ) are ∼-identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sketch of proof:</head><p>Step 1: We analyze the identifiability of started by p θ (x|u) = p θ (x|u). Then we define a new invertible matrix L which contains additional observation u i in causal system, and use it to prove that the learned T is the transformation of T.</p><p>Step 2: We analyze the identifiability of z by replacing C in step 1 with z. Then we use the invertible matrix B 2 , a diagonal matrix containing u to finish the proof.</p><p>More details are in Appendix.</p><p>The parameters θ of true generative model are unknown during the learning process. The identifiablity of generative model is given by Theorem 1 which guarantees the parameters θ learned by hypothetical functions are in identifiable family.</p><p>In addition, all z i in z align to the additional observation of concept i and they are expected to inherent the causal relationship of causal system. That is why that it could guarantee that the z are causal representations.</p><p>Then, for the causal representation z learned by the causal layer parameterized by C, we here analyze the indentifiablity of A.</p><p>Let A denote true causal structure of z and Ã denote the matrix leanred by our model. The following corollary illustrates the non-dentifiable A.</p><p>Corollary 1. Suppose A and Ã are the true adjacency matrix and the adjacency matrix learned by our model, respectively. Then the following statement holds:</p><formula xml:id="formula_20">A ∼ Ã. (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>Or equivalently, the exists an invertible matrix B such that</p><formula xml:id="formula_22">T(Ch(x)) = B T( Ch (x)).<label>(16)</label></formula><p>Intuitively, the Ã learned in causal layer produces the p(z| ), which recovers the true one up to linear transformation.</p><p>We furthur discuss some intuitions of idetifiability. Existing works often learn latent representation in an unsupervised way. However, our method uses the supervised ways, including additional observations. This supervision brings benefit that we can get the identifiability result of model.</p><p>The identifiability of the model under supervision of additional observation is obtained by the conditional prior p θ (z|u) generated from u. The conditional prior guarantees that the sufficient statistics of p θ (z|u) are related to the value of u. In other words, the values of z are determined by the supervision signal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present the experimental results of our proposed method CausalVAE on datasets. Compared with those learned by the state-of-the-arts, the representation learned by our method performs well in both the synthetic causal image dataset and real world face data CelebA.</p><p>We test our CausalVAE on two tasks. The first task is factor interventions, and the second is downstream tasks, namely image classification.</p><p>In our experiments, the structure of the decoder largely influences the results. Thus, we use two designed decoders. The first one decodes the concepts separately and sum them up as the final output, and the second one decodes all concepts using a single neural network. The structures are in Fig. <ref type="figure">3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">SYNTHETIC DATA</head><p>We do experiments on the scenarios containing causally structured entities or concepts. We run models on a synthetic dataset, which include images consisting of causally related objects. A data generator is used to produce the images as model inputs. We will release our data generator soon.</p><p>Pendulum: We generate images with 3 entities (pendulum, light, shadow) which include 4 concepts (pendulum angle, light angle, shadow location, shadow length). The picture includes a pendulum. The angles of pendulum and the light are changing overtime. We use the projection laws to generate the shadows. The shadow are influenced by the light and angle of the pendulums. The causal graph of concepts is showed in Fig. <ref type="figure" target="#fig_4">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">BANCHMARK DATASET</head><p>In real world systems, cause and effect relationships commonly exist. To test our proposed method in these kinds of scenarios, we choose a banchmark CelebA<ref type="foot" target="#foot_0">1</ref> , which is widely used in computer vision tasks. In this dataset, there are in total 200k human images with labels on different concepts. We focus on 3 concepts (age, gender and beard) on human faces in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baselines</head><p>CausalVAE-unsup: CausalVAE-unsup is the method under unsupervised setting. The architecture of the model is the same as CausalVAE but the additional observations are not used. We adjust the loss function by removing the additional observation.</p><p>β-VAE: β-VAE is a common baseline for unsupervised disentanglement works. The dimensions of the latent representation are the same as that used in CausalVAE. The Standard Multivariate Gaussian distribution is adopted as the prior of latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DC-IGN:</head><p>This baseline model is the model under supervised setting. They generate priors of latent variables conditional on the labels. As the case of β-VAE, dimensions of latent variables are set in line with our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Intervention experiments</head><p>Intervention experiments aim at testing if certain dimension of the latent codes has understandable semantic meanings. We control the value of latent vector by do-calculus operation introduced before, and check the reconstructed images.</p><p>For the experiments, all images of the dataset are used to train our proposed model CausalVAE and other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">SYNTHETIC</head><p>For the experiments on synthetic dataset, we use different latent variable dimensions. We use 4 and 2 concepts on pendulum and water dataset, respectively. Then in all the experiments, we set the hyperparameter α = 1.</p><p>We use CausalVAE-a to represent the CausalVAE model with decoder (a), and CausalVAE-b to represent the CausalVAE model with decoder (b). The same rules apply to the DC-IGN model.</p><p>We intervened 4 concepts of pendulum, and the results are showed in Fig. <ref type="figure">4</ref>. The intervention strategy are illustrated in following step: 1) we learned a CausalVAE model; 2) we put a pendulum image into encoder and get the latent code z. 3) we change the value of z i as 0. For example, when we want to intervene gender, we will change the value of z i=light directly as 0 and keep other z i =light unchanged. 4) we put the total changed latent code z into decoder and got reconstruct image.</p><p>In implementation of CausalVAE, similar to β-VAE, we adjust the KL term in ELBO by multiplying a beta:</p><formula xml:id="formula_23">β 1 D(q φ ( |x)||p( )) + β 2 D(q φ (z|x, u)||p θ (z|u))</formula><p>The hyperparameters of CausalVAE β 1 = 0.1, β 2 = 0.3.</p><p>Since we set the latent value as constant 0, if we controlled concept successfully, the pattern of controlled concept in one image will be the same as other images in its line. For example, when we control pendulum angel in 4(a), the first line shows that the pendulum angle in each images are almost same. And the same with light angle, each lights in different images of the second line are in the middle of top of images. And other concepts in line 3 and line 4 show similar effect.</p><p>From the results of CausalVAE with decoder (a) showed by Fig. <ref type="figure">4</ref>(a), we find that the when we control the angle of light and pendulum, the location and length of shadows change correspondingly. But controlling the shadow factors, the light and pendulum are not affected. This result does not appear when we use decoder (b).</p><p>For experiments using decoder (b), controlling the two causes (pendulum angle and light angle), the two effects (shadow length and shadow location) do not change the reconstructed images in an expected way. In addition, controlling the effects factors in the latent representation does not influence the reconstructed images. The reason is that the decoder (b) itself may be an physical model which reasons out the effect factors based on the cause factors. The information contained in effect factors is hence not useful.</p><p>Then we analyze the results of DC-IGN. The intervention results are showed in Fig. <ref type="figure">4</ref>(c) (d). Results show that there exists a problem that the control of causes sometimes does not influence the effects. This is because they do not have a causal layer to model the factors so that the learned factors are not concepts we expect.</p><p>We also test CausalVAE on water dataset. This scenario has two concepts. The intervention on the ball size (cause) influences the water height (effect), but the intervention on the effect does not influence the causes. We also find that the results have some fluctuations. The control of the concepts is not as good as that in the pendulum experiments. It is possibly because two concepts are related by a bijective function (one-to-one mapping), and it brings difficulty for the model to understand casual relations between concepts.</p><p>In water experiments, we also find that the decoder (a) performs better than decoder (b). We do not use the unsupervised method in these experiments because it will not guarantee all the representations are aligned to the concepts well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">HUMAN FACE</head><p>We also executed the experiments on real world banchmark data CelebA. In this kind of scenarios, the causal system is often complex, which has heterogeneous causes and effects. It is hard to observe all the concepts in the causal systems. In this experiments, we focus on only 3 concepts (age, gender and beard). Other concepts will possibly be confounders in system. Decoder (a) is used in our experiments.</p><p>We conducted our intervention experiments by following step: 1) we learned a CausalVAE model; 2) we put a human picture into encoder and get the latent code z. 3) we change the value of z i from -0.5 to 0.5, in which each z i are correspond to the concept respectively. For example, when we want to intervene gender, we will change the value of z i=gender directly from -0.5 to 0.5 and keep other z i =gender unchanged. 4) we put the total changed latent code z into decoder and got reconstruct picture.</p><p>Different with synthetic data, we did not change the value of latent code as constant 0 but set the value in a range of number.</p><p>Thus the figures will show the concept changing clearly.</p><p>The Fig. <ref type="figure" target="#fig_7">7</ref> demonstrate the result of CausalVAE under the parameters β 1 = 0.1, β 2 = 0.2. And (a)(b)(c) show the intervention experiments on concepts of age, gender and beard respectively. The interventions perform well that when we intervened the cause concept gender, not only the appearance of gender but the beard changed. In contrast, when we intervened effect concept beard, the gender in figure Fig. <ref type="figure" target="#fig_7">7</ref>(c) are not changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Downstream Task</head><p>We also use the representation to do the downstream task on synthetic data. In this paper, we conduct tasks of image classification. We use the latent causal representation as the input of the classifier, and do experiments on predictions of causes and effects.</p><p>The 80% of dataset are used as the training data and the remaining are for testing. The cause conceptual vectors learned by our model are the inputs of a classifier, to predict either the cause labels or effect labels. The cause labels on pendulum dataset are produced by equally partitioning the angles 0 to 90 degree into 6 classes. The effect labels are constructed by dividing the original additional observations associated with the concepts into 3 classes. In water dataset, classifications on cause label and effect label are all binary classifications. The results are showed in table <ref type="table" target="#tab_0">1</ref>. It shows that using the latent codes learned by CausalVAE and DC-IGN, in general, leads to better classification performance than using that learned by other baselines. Our proposed method achieves the best performance. The choice of decoder does not have significant influences on the results when our model is used. However, it has a clear influence on the results of unsupervised baseline models like CausalVAE-unsup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a framework for latent representation learning. We argue that causal representation is good representation for machine learning tasks, and incorporate a causal layer to learn this representation under the framework of variational autoencoder. We give identifiability result of the model when additional observations are available for supervised learning. The method is tested on synthetic and real datasets, on both intervention experiments and downstream tasks. Our viewpoint is expected to bring new insights into the domain of representation learning. (18)</p><p>In Gaussian distribution, p θ (z|u) can be written as follow:</p><formula xml:id="formula_24">p θ (z|u) = Π i p θ (z i |pa(u i ), u i ) = Π i p θ (z i |u i ). (<label>19</label></formula><formula xml:id="formula_25">)</formula><p>where i is the concept index.</p><p>Adopting the definition of multivariate Gaussian distribution, we define</p><formula xml:id="formula_26">λ s (u) =    λ s 1 (u 1 ) . . . λ s n (u n )    .<label>(20)</label></formula><p>There exists the following equations:</p><formula xml:id="formula_27">log | det(C)| + log | det(J h (x))| − log Q(Ch(x)) + 2 s=1 T s (Ch(x))λ s (u), = log | det( C)| + log | det(J h (x))| − log Q( C h(x)) + 2 s=1 T s ( C h(x)) λ s (u). (<label>21</label></formula><formula xml:id="formula_28">)</formula><p>where Q denotes the base measure. In Gaussian distribution, it is σ(z).</p><p>In learning process, A is restricted as DAG. Thus, the C exists which is full rank matrix. The item which is not related to u in Eq. 21 are cancelled out <ref type="bibr" target="#b21">(Sorrenson et al., 2020)</ref>.</p><formula xml:id="formula_29">2 s=1 T s (Ch(x))λ s (u)) = 2 s=1 T s ( C h(x)) λ s (u)). (<label>22</label></formula><formula xml:id="formula_30">)</formula><p>where s denote the index of sufficient statistics of Gaussian distributions, indexing the mean (1) and the variance <ref type="bibr">(2)</ref>.</p><p>By assuming that the additional observation u i is different, it is guaranteed that coefficients of the observations for different concepts are distinct. Thus, there exists an invertible matrix corresponding to additional information u:</p><formula xml:id="formula_31">L = λ 1 (u) λ 2 (u) .<label>(23)</label></formula><p>Since the assumption that u i = 0 holds, L is 2n × 2n invertible and full rank diagonal matrix. We have:</p><formula xml:id="formula_32">B 3 LT(h(x)) = L T( h(x)) ⇒ T(h(x)) = B 1 T( h(x)). (<label>24</label></formula><formula xml:id="formula_33">)</formula><p>where B 3 is invertible matrix which corresponds to C and B 1 = L −1 B −1 3 L. The definition of L on learning model migrates the definition of L on ground truth.</p><p>Then we adopt the definitions following <ref type="bibr" target="#b11">(Khemakhem et al., 2019)</ref>. According to the Lemma 3 in <ref type="bibr" target="#b11">(Khemakhem et al., 2019)</ref>, we are able to pick out a pair ( i ,<ref type="foot" target="#foot_1">2</ref> i ) such that, (T i (z i ), T i (z 2 i )) are linearly independent. Then concat the two points into a vector, and denote the Jacobian matrix Q = [J T ( ), J T ( 2 )], and define Q on T( h−1 • h( )) in the same manner. By differentiating Eq. 24, we get</p><formula xml:id="formula_34">Q = B 1 Q. (<label>25</label></formula><formula xml:id="formula_35">)</formula><p>Since the assumptiom (2) that Jacobian of h is full rank holds, it can prove that both Q and Q are invertible matrix. Thus from Eq. 25, B 1 is invertible matrix. The details are shown in <ref type="bibr" target="#b11">(Khemakhem et al., 2019)</ref>.</p><p>Step 2: Under the assumption in Theorem 1, replace the Ch(x) with f −1 (x) in Eq. 17, then</p><formula xml:id="formula_36">2 s=1 T s (f −1 (x))λ s (u)) = 2 s=1 T s ( f −1 (x)) λ s (u)).<label>(26)</label></formula><p>Then use Eq. 23 to replace the λ matrix in Eq. 26, and we get:</p><formula xml:id="formula_37">L(f −1 (x)) = L h( f −1 (x)),<label>(27)</label></formula><formula xml:id="formula_38">T(f −1 (x)) = B 2 h( f −1 (x)). (<label>28</label></formula><formula xml:id="formula_39">)</formula><p>where</p><formula xml:id="formula_40">B 2 =    u −1 1 λ 1 1 (u 1 ) . . . u −2 n λ 2 n (u n )    .<label>(29)</label></formula><p>Using the same way as shown in Eq. 25, it can prove that B 2 is invertible matrix.</p><p>Eq. 24 and Eq. 28 both hold. Combining the two results supports the identifiability result in CausalVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We use one NVIDIA Tesla P40 GPU as our train and inference device.</p><p>For the implementation of CausalVAE and other baselines, we extend z to matrix z ∈ R n * k where n is the number of concepts and k is the latent dimension of each z i . The corresponding prior or conditional prior distributions of CausalVAE and other baselines are also adjusted (this means that we extend the multivariate Gaussian to the matrix Gaussian).</p><p>The subdiemnsions k for each synthetic (pendulum, water) experiments are set to be 4, and 16 for CelebA experiments. The implementation of continuous DAG constraint H(A) follows the code of <ref type="bibr" target="#b23">(Yu et al., 2019)</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A swinging pendulum.</figDesc><graphic url="image-1.png" coords="2,273.24,105.17,50.40,50.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The information flow of CausalVAE. The observation x is the input of a encoder, and the encoder generates latent variable, whose prior distribution is assumed to be standard Multivariate Gaussian. Then it is transformed by the causal layer to be causal representation z. The z are assumed to be with a conditional prior distribution p(z|u). z is taken as the input of the decoder to reconstruct observation x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Architectures of the two decoders used in experiments. (a) presents a structure that each concept is decoded separately by one network, and their results are assembled to be final output. (b) presents a structure that concepts are decoded by single neural network.</figDesc><graphic url="image-2.png" coords="7,57.93,206.95,116.64,116.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Causal graphs of three dataset. (a) shows the causal graph in pendulum dataset. The concepts are pendulum angle, light angle, shadow location and shadow length. (b) shows the causal graph in water dataset, on concepts water height and ball size. (c) shows the causal graph in CelebA, on concepts age, gender and beard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The results of DO-experiments on water. For each experiment we randomly choose 4 results. The first row presents results of controlling ball size (cause) and the second row controls water height (effect). The bottom one is the ground truth. Training epoch for models is set to be 100.</figDesc><graphic url="image-6.png" coords="8,57.93,72.04,116.64,87.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a). In our experiments, we generate about 7k images (6K for training and 1k for inference), the angle of light and pendulum are ranged in around [− π 4 , − π 4 ]. Water: We produce artificial images, consisting of a ball in a cup filled with water. There are 2 concepts (ball size, height of water bar). The height is effect of the ball size. The causal graph is ploted in Fig. 5 (b) and the dataset includes 7k images, 6k images for training the disentanglemet model and the classifier model, and the rest of dataset are used as the test data of classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Results of CausalVAE on CelebA, are results under hyperparameters (β1, β2, α) = (0.1, 0.2, 1). The controlled factors from top to bottom line are age, gender and beard, respectively. The first row shows the result of controlling gender, and the second row shows that of controlling age. The bottom is the result of controlling beard.</figDesc><graphic url="image-12.png" coords="9,153.88,444.16,291.58,116.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The accuracy of classifiers on test dataset. The training epoach is 300 for pendulum and 50 for water. Experiments are repeated 5 times, and the median are reported.</figDesc><table><row><cell>β-VAE</cell><cell>0.6801</cell><cell>0.1905</cell><cell>0.7867</cell><cell>0.7707</cell><cell>0.6685</cell><cell>0.6679</cell><cell>0.7629</cell><cell>0.7629</cell></row><row><cell>DC-IGN</cell><cell>0.8313</cell><cell>0.7634</cell><cell>0.8570</cell><cell>0.8662</cell><cell>0.7649</cell><cell>0.8626</cell><cell>0.7710</cell><cell>0.7972</cell></row><row><cell>CausalVAE-unsup</cell><cell>0.8039</cell><cell>0.8028</cell><cell>0.8667</cell><cell>0.8496</cell><cell>0.9362</cell><cell>0.6663</cell><cell>0.7924</cell><cell>0.7990</cell></row><row><cell>CausalVAE</cell><cell>0.8658</cell><cell>0.8587</cell><cell>0.8564</cell><cell>0.8656</cell><cell>0.8952</cell><cell>0.8874</cell><cell>0.8032</cell><cell>0.8038</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>2 .B.1. DO-ExperimentsIn DO-experiments, we train the model on synthetic data for 100 epochs, on CelebA for 500 epochs and use this model to generate latent code of representations. Network design of models trained on synthetic data.</figDesc><table><row><cell>encoder</cell><cell></cell><cell>decoder(a)</cell><cell>decoder(b)</cell></row><row><cell>4*96*96×900 fc. 1ELU</cell><cell cols="2">concepts×( 4× 300 fc. 1ELU )</cell><cell>concepts× (4× 300 fc. 1ELU)</cell></row><row><cell>900×300 fc. 1ELU</cell><cell cols="3">concepts× (300×300 fc. 1ELU) concepts×(300×300 fc. 1ELU)</cell></row><row><cell>300×2*concepts*k fc.</cell><cell cols="2">concepts×(300× 1024 fc. 1ELU)</cell><cell>concepts×(300× 1024 fc.)</cell></row><row><cell>-</cell><cell cols="2">concepts×(1024× 4*96*96 fc.)</cell><cell>concepts×(1024× 4*96*96 fc.)</cell></row><row><cell>encoder</cell><cell></cell><cell></cell><cell>decoder</cell></row><row><cell>-</cell><cell></cell><cell cols="2">concepts×(1×1 conv. 128 1LReLU(0.2), stride 1)</cell></row><row><cell cols="4">4×4 conv. 32 1LReLU (0.2), stride 2 concepts×(4×4 convtranspose. 64 1LReLU (0.2), stride 1)</cell></row><row><cell cols="4">4×4 conv. 64 1LReLU (0.2), stride 2 concepts×(4×4 convtranspose. 64 1LReLU (0.2), stride 2)</cell></row><row><cell cols="2">4×4 conv. 64 1LReLU(0.2), stride 2</cell><cell cols="2">concepts×(4×4 convtranspose. 32 1LReLU (0.2), stride 2)</cell></row><row><cell cols="4">4×4 conv. 64 1LReLU (0.2), stride 2 concepts×(4×4 convtranspose. 32 1LReLU (0.2), stride 2)</cell></row><row><cell cols="4">4×4 conv. 256 1LReLU (0.2), stride 2 concepts×(4×4 convtranspose. 32 1LReLU (0.2), stride 2)</cell></row><row><cell>1×1 conv. 3, stride 1</cell><cell></cell><cell cols="2">concepts×(4×4 convtranspose. 3 , stride 2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Network design of models trained on CelebA.</figDesc><table><row><cell>pendulum-cause</cell><cell>pendulum-effect</cell><cell>water-cause</cell><cell>water-effect</cell></row><row><cell>4×50 fc. 1ELU</cell><cell cols="3">2×(4×32 fc. 1ELU) 4×32 fc. 1ELU 4×32 fc. 1ELU</cell></row><row><cell>32×6 fc.</cell><cell>2×(32×32 fc. 1ELU)</cell><cell>32×2 fc.</cell><cell>32×2 fc.</cell></row><row><cell>-</cell><cell>2×(32×3 fc.)</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Network designs of models for downstream tasks.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/fishmoon1234/DAG-GNN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning independent features with adversarial nets for non-linear ica</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05050</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<title level="m">A. Understanding disentangling in beta-vae</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Independent component analysis, a new concept? Signal processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><surname>Beta-Vae</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Learning basic visual concepts with a constrained variational framework. Iclr</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02230</idno>
		<title level="m">Towards a definition of disentangled representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled and interpretable representations from sequential data</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1878" to="1889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised feature extraction by time-contrastive learning and nonlinear ica</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Morioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3765" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Advances in nonlinear blind source separation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th Int. Symp. on Independent Component Analysis and Blind Signal Separation (ICA2003)</title>
				<meeting>of the 4th Int. Symp. on Independent Component Analysis and Blind Signal Separation (ICA2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="245" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Variational autoencoders and nonlinear ICA: A unifying framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<idno>CoRR, abs/1907.04809</idno>
		<ptr target="http://arxiv.org/abs/1907.04809" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05983</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Disentangling by factorising. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2539" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12359</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Disentangling factors of variation using few labels</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01258</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5712" to="5723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02833</idno>
		<title level="m">Disentangling disentanglement in variational autoencoders</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A graph autoencoder approach to causal structure learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<idno>CoRR, abs/1911.07420</idno>
		<ptr target="http://arxiv.org/abs/1911.07420" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><surname>Causality</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Sorrenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04872</idno>
		<title level="m">Disentanglement by nonlinear ica with general incompressible-flow networks (gin)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miladinović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00007</idno>
		<title level="m">Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Dag-Gnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10098</idno>
		<title level="m">Dag structure learning with graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2599</idno>
		<title level="m">On the identifiability of the post-nonlinear causal model</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9472" to="9483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Causal discovery with reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/1906.04477</idno>
		<ptr target="http://arxiv.org/abs/1906.04477" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">A. Proof of Theorem</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">There exist following equations, B.1.1. SYNTHETIC We present the experiments of our proposed CausalVAE with two kinds of decoder, and experiments of other baselines with decoder (a)</title>
		<author>
			<persName><surname>Khemakhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Based on information flow of the model, we would analyze the identifiability of and z. The general logic of the proofing follows</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>For all the observational pairs (x, u), let J h denote the Jacobian matrix of the encoder function. The hyperparameters are defined as: 1. CausalVAE : β 1 = 0.1, β 2 = 0.3</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Causalvae-Unsup</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">DC-IGN : β 1 = 0.4. 4. β-VAE : β 1 = 0</title>
				<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The details of the neural networks are shown in Table 2. B.1.2. CELEBA The reconstruction errors during the training are shown in Fig</title>
		<imprint/>
	</monogr>
	<note>We only present the experiments with decode (a). The hyperparameters are: 1. CausalVAE : β 1 = 0.1, β 2 = 0.2</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Causalvae-Unsup</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The details of the neural networks are shown in Table 3. We also present the DO-experiments of CausalVAE and DC-IGN</title>
		<idno>VAE : β 1 = 0.3</idno>
		<imprint/>
	</monogr>
	<note>In the training of the models, we both use face labels (age, gender and beard</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">From the figures, we find that interventions on the latent variables constructed by CausalVAE, in general, show a better performance than on those constructed by DC-IGN, especially on cause concepts. The intervention on age in Fig</title>
		<imprint/>
	</monogr>
	<note>is a good example demonstrating the performance</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">This fact shows certain entanglement of the concepts learned by DC-IGN, and these concepts do not follow a cause-effect relationship. B.2. Downstream Task Here we show the loss curves during the training. We use 85% of the synthetic data for training. We present the experiments on two synthetic data and each one includes the experiments of identifying cause labels and effect labels. In addition, CausalVAE achieves better accuracy than most of the baselines. It shows evidence that our proposed method learns conceptual representations</title>
		<imprint/>
	</monogr>
	<note>When CausalVAE controls the effect latent variables beard, it will not change other concepts on the reconstructed images. However, in DO-experiments under DC-IGN, other conceptual parts like gender will change even though we only intervene on the beard dimension. The network designs of the classifiers are shown in Table 4</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
