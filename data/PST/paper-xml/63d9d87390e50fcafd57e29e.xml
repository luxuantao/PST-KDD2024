<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DepGraph: Towards Any Structural Pruning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-01-30">30 Jan 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gongfan</forename><surname>Fang</surname></persName>
							<email>gongfan@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
							<email>maxinyin@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Technologies Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">Bi</forename><surname>Mi</surname></persName>
							<email>michael.bi.mi@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
							<email>xinchao@nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DepGraph: Towards Any Structural Pruning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-01-30">30 Jan 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.12900v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecturespecific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this ambitious goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all parameters in a removed group to be consistently unimportant, thereby avoiding significant performance degradation after pruning. To address this problem, we propose a general and fully automatic method, Dependency Graph (DepGraph), to explicitly model the interdependency between layers and comprehensively group coupled parameters. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple L1 norm criterion, the proposed method consistently yields gratifying performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent emergence of edge computing applications calls for the necessity for compressing deep neural networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b56">57]</ref>, of which the favorable results often come at the cost of cumbersome network architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>. Among the many network compression paradigms, pruning has proven itself to be highly effective and practical <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b65">66]</ref>. The goal of network pruning *  is to remove redundant parameters from a given network to lighten its size and potentially speed up the inference. Mainstream pruning approaches can be roughly categorized into two schemes, structurual pruning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b62">63]</ref> and unstructurual pruning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41]</ref>. The core difference between the two lies in that, structural pruning changes the structure of neural networks by physically removing grouped parameters, while unstructural pruning conducts zeroing on partial weights without modification to the network structure. Compared to unstructural ones, structural pruning does not rely on specific AI accelerators or software to reduce memory consumption and computational costs, thereby finding a wider domain of applications in practice <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b60">61]</ref>. Nevertheless, the nature of structural pruning per se makes itself a challenging task, especially for modern deep neural networks with coupled and complex internal structures. The rationale lies in that, deep neural networks are built upon a large number of basic modules like convolu-tion, normalization, or activation, yet these modules, either parameterized or not, are intrinsically coupled through the intricate connections <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>. As a result, even when we seek to remove only one channel from a CNN as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), we have to inevitably take care of its interdependencies to all layers simultaneously, otherwise we will eventually get a broken network. To be exact, the residual connection requires the output of two convolutional layers to share the same number of channels and thus forces them to be pruned together <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b62">63]</ref>. The same goes for structural pruning on other network architectures depicted in other architectures like Transformers, RNNs and GNNs as illustrated in Figs. <ref type="figure" target="#fig_0">1(b-d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corresponding author</head><p>Unfortunately, dependency does not only emerge in residual structures, which can be infinitely complex in modern models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>. Existing structural approaches have largely relied on case-by-case analyses to handle dependencies in networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref>. Despite the promising results achieved, such a network-specific pruning approach is effort-consuming. Moreover, these methods are not directly generalizable, meaning that the manually-designed grouping scheme is not transferable to other network families or even the network architectures in the same family, which in turn, greatly limit their industrial applications under in-thewild conditions.</p><p>In this paper, we strive for a generic scheme towards any structural pruning, where structural pruning over arbitrary network architectures is executed in an automatic fashion, At the heart of our approach is to estimate the Dependency Graph (DepGraph), which explicitly models the interdependency between paired layers in neural networks. Our motivation to introduce DepGraph for structural pruning stems from the observation that, structural pruning at one layer effectively "triggers" pruning at adjacent layers, which further leads to a chain effect {BN 2 ?Conv 2 ?BN 1 ?Conv 1 } shown in Figure <ref type="figure" target="#fig_0">1</ref>(a). As such, to trace the inter-dependencies across different layers, we may decompose the dependency chain into a recursive process, which naturally boils down to the process of finding the maximum connected components in graph, and can be solved in O(N ) complexity via graph traversal. Specifically, for the to-be-pruned layer in the network, we can treat it as the root to trigger the pruning on adjacent coupled layers, and then continue to take the triggered layer as the starting point to repeat the triggering process recursively. By doing so, all coupled layers can be comprehensively collected for pruning.</p><p>It is also worth noting that in structural pruning, grouped layers are pruned simultaneously, which expects the parameters in the removed group to be consistently unimportant, which brings certain difficulties to existing importance criteria designed for a single layer <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>. To be exact, the parameter importance in a single layer no longer reveals correct importance due to its entanglement with other parameterized layers. The importance estimated on different layers is likely to be non-additive and sometimes even selfcontradictory, making it difficult to choose truly unimportant groups for pruning. To address this problem, we fully leverage the comprehensive ability of dependency modeling powered by DepGraph to learn consistent sparsity within groups, so that those zeroized ones can be safely removed without too much performance degradation. With the dependency modeling, we show in the experiments that a simple L1 norm criterion can achieve comparable performance to modern approaches.</p><p>To validate the effectiveness of DepGraph, we apply the proposed method to several popular architectures including CNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>, Transformers <ref type="bibr" target="#b9">[10]</ref>, RNNs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b49">50]</ref>, and GNNs <ref type="bibr" target="#b51">[52]</ref>, where competitive performance is achieved compared to state-of-the-art methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b62">63]</ref>. For CNN pruning, our method obtains a 2.57? accelerated ResNet-56 model with 93.64% accuracy on CIFAR, which is better than the original one with 93.53% accuracy. And on ImageNet-1k, our algorithm achieves more than 2? speed-up on ResNet-50, with only 0.32% performance lost. More importantly, our method can be readily transferred to various popular networks, including ResNe(X)t <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b58">59]</ref>, DenseNet <ref type="bibr" target="#b22">[23]</ref>, VGG <ref type="bibr" target="#b47">[48]</ref>, MobileNet <ref type="bibr" target="#b44">[45]</ref>, GoogleNet <ref type="bibr" target="#b50">[51]</ref> and Vision Transformer <ref type="bibr" target="#b9">[10]</ref>, and demonstrate gratifying results. Besides, we also conduct further experiments on nonimage neural networks, including LSTM <ref type="bibr" target="#b11">[12]</ref> for text classification, DGCNN <ref type="bibr" target="#b55">[56]</ref> for 3D point cloud, and GAT <ref type="bibr" target="#b51">[52]</ref> for graph data, where our method achieves from 8? to 16? acceleration without a significant performance drop.</p><p>In sum, our contribution is a generic pruning scheme towards any structural pruning, termed as Dependency Graph (DepGraph), which allows for automatic parameter grouping and effectively improves the generalizability of structural pruning over various network architectures, including CNNs, RNNs, GNNs and Vision Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Structural and Unstructural Pruning. Pruning has made tremendous progress in the field of network acceleration <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>. Based on pruning schemes, mainstream pruning approaches can be divided into two types: structural pruning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b62">63]</ref> and unstructural pruning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref>. Structural pruning removes structurallygrouped parameters to reduce the dimension of neural networks, while unstructural pruning conducts zeroing on partial weights without modification to the network structure. Specifically, unstructural pruning is easy to implement and naturally generalizable to various networks. But it usually relies on specific AI accelerators or softwares to enable model acceleration <ref type="bibr" target="#b14">[15]</ref>. The structural pruning technique, on the other hand, reduces model size and inference </p><formula xml:id="formula_0">? ? ! ? !"# ? !"$ ? ! ? !"# ? !"$ ? ! ? "# ? "$ ? ! Figure 2.</formula><p>Grouped parameters with inter-dependency in different structures. All highlighted parameters must be pruned simultaneously.</p><p>costs by removing structural parameters from networks, but is limited by structural constraints <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. In the literature, the design space of pruning algorithms includes but not is limited to pruning schemes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref>, parameter selection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, layer sparsity <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46]</ref> and training protocols <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b53">54]</ref>. Of these, parameter selection is one of the most important topics. In the past few years, a large number of technically-sound criteria have been proposed, like magnitude-based criteria <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b61">62]</ref> or gradient-based criteria <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>. Another type of method distinguish unimportant parameters through sparse training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref>, which pushes some parameters to zero for pruning. Compared to those static criteria, sparse training is more likely to find unimportant parameters but costs more computational resources since it requires network training.</p><p>Pruning Grouped Parameters. Dependency modeling is a key and prerequisite step for any structural pruning as it involves the simultaneous removal of parameters that are structurally coupled to one another due to complicated network architectures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b67">68]</ref>. The concept of pruning grouped parameters has been studied since the early days of structural pruning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>. As an example, when pruning two consecutive convolutional layers, pruning a filter within the first layer results in the removal of kernels related to that filter in the subsequent layer <ref type="bibr" target="#b27">[28]</ref>. The grouping of parameters is deterministic once the network architectures are presented, and it is possible to analyze each parameter individually, as has been done in most previous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b62">63]</ref>. However, such manually designed schemes are inevitably non-transferable to new architectures, which limits the application of structural pruning under in-the-wild conditions. Recently, some pilot works have been proposed to resolve the complicated relationship between layers and leverage the grouping property in order to improve structural pruning performance <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b62">63]</ref>. Unfortunately, existing techniques still rely on empirical rules or strong architecture assumptions, which is not general enough for any structural pruning. As a part of this study, we present a general framework for addressing this problem and demonstrate that addressing parameter grouping can bring significant benefits to pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dependency in Neural Networks</head><p>In this work, we focus on structural pruning of any neural networks under the restriction of parameter dependency. Without loss of generality, we develop our method upon fully-connected (FC) layers. Let's begin with a linear neural network composed of three consecutive layers as illustrated in Figure <ref type="figure">2</ref> (a), parameterized by 2-D weight matrices w l , w l+1 and w l+2 respectively. This simple neural network can be made slim by structural pruning via the removal of neurons. In this case, it is easy to find that some dependencies emerge between parameters, denoted as w l ? w l+1 , which forces w l and w l+1 to be simultaneously pruned. Specifically, to prune the k-th neuron that bridges w l and w l+1 , both w l [k, :] and w l+1 [:, k] will be pruned.</p><p>In the literature, researchers handle layer dependencies and enable structural pruning on deep neural networks with manually-designed and model-specific schemes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>. Nevertheless, there are many kinds of dependencies just as illustrated in Figure <ref type="figure">2 (b-d</ref>). It is intractable, to some extent, to manually analyze all those dependencies in a caseby-case manner, let alone that simple dependencies can be nested or composed to form more arbitrarily-complex patterns. To address the dependency issue in structural pruning, we introduce Dependency Graph in this work, which provides a general and fully-automatic mechanism for dependency modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dependency Graph</head><p>Grouping. To enable structural pruning, we firstly need to group different layers according to their inter-dependency. Formally, our goal is to find a grouping matrix G ? R L?L where L refers to the depth of a to-be-pruned network, and G ij = 1 indicates the presence of dependency between i-th layer and j-th layer. We let Diag(G) = 1 1?L to enable selfdependency for convenience. With the grouping matrix, it is easy to find those coupled layers with inter-dependency to the i-th layer, i.e., finding the group g(i):</p><formula xml:id="formula_1">g(i) = {j|G ij = 1}</formula><p>(1) Nevertheless, It is typically non-trivial to estimate the group from a neural network due to the fact that modern deep networks may consist of thousands of layers with complicated connections, resulting in a large and dense grouping matrix G. In this matrix, G ij is not only determined by the i-th and j-th layers, but also affected by those intermediate layers that bridge them. Such non-local relation is not explicit and can not be handled with simple rules in most cases. In this regard, we do not directly estimate the grouping matrix G, and propose to develop an equivalent but easy-to-estimate method, i.e., the Dependency Graph, from which G can be efficiently derived.</p><formula xml:id="formula_2">Conv ? ! BN ? " ReLU ? # + Conv ? $ BN ? % ReLU ? &amp;<label>(</label></formula><p>Dependency Graph. To begin, let's consider a group g = {w 1 , w 2 , w 3 }, with dependencies w 1 ? w 2 , w 2 ? w 3 and w 1 ? w 3 . It is easy to find some redundancy in this dependency modeling, that is, the dependency w 1 ? w 3 can be derived from w 1 ? w 2 and w 2 ? w 3 . Specifically, we can model this derivation as a recursive process: We can take w 1 as the starting point, and examine its dependency on other layers, e.g., w 1 ? w 2 . Further, w 2 provides a new starting point to recursively expand the dependency, which further "triggers" w 2 ? w 3 . This recursive process eventually ends with a transitive relation, w 1 ? w 2 ? w 3 . In this case, we only need two dependencies to describe the relations in group g. Similarly, the grouping matrix discussed in Sec. 3.2 also contains massive redundancy and can be compressed into a more compact one, with fewer edges but the same information about layer dependencies.</p><p>In this work, we demonstrate that a graph D that measures local inter-dependency between adjacent layers, termed as Dependency Graph, can be an effective reduction for the grouping matrix G. D differs from G in that it only records the dependencies between adjacent layers with direct connections. Actually, Dependency Graph can be viewed as the transitive reduction <ref type="bibr" target="#b0">[1]</ref> of G, which contains the same vertices but as few edges from G as possible, such that for all G ij = 1, there is a path in D between i to j. As a result, G ij can be derived by checking the existence of the path between vertices i and j in D.</p><p>Network Decomposition. However, we find that building the dependency graph at the layer level is problematic since some basic layers, such as fully-connected layers, have two pruning schemes just as mentioned in Sec. 3.1. In addition to those parameterized layers, neural networks also contain non-parameterized operations such as skip connections, which also affect the dependency between layers <ref type="bibr" target="#b36">[37]</ref>. We show that these issues can be remedied by developing a new notation for describing networks. Specifically, We decompose a network F(x; w) into basic layers first, denoted as F = {f 1 , f 2 , ..., f L }, where each f refers to either a parameterized layer such as convolution or a non-parameterized layers such as ReLU. Instead of modeling layer-level relations, we focus on the fine-grained relationships between layer inputs and outputs. Specifically, we refer to the input and output of f i as f - i and f + i respectively. For any network, we can obtain a fine-grand decomposition for the network, i.e.,</p><formula xml:id="formula_3">F = {f - 1 , f + 1 , ..., f - L , f - L }.</formula><p>Dependency modeling would be made easier with this notation as it allows us to prune the inputs f - i and outputs f + i with different schemes.</p><p>Dependency Modeling. Leveraging this notation, we resketch the neural network as Equation <ref type="formula" target="#formula_4">2</ref>, from which we can find two kinds of general dependencies, i.e., inter-layer dependency and intra-layer dependency, shown as follows:</p><formula xml:id="formula_4">(f - 1 , f + 1 ) ? (f -<label>2</label></formula><formula xml:id="formula_5">Inter-later Dep , f + 2 ) ? ? ? ? (f - L , f + L ) Intra-layer Dep<label>(2)</label></formula><p>where ? indicates the connectivity between two adjacent layers. We show that these dependencies can be detected with the very simple rules:</p><p>? Inter-layer Dependency: dependency f - i ? f + j always emerges in connected layers with f - i ? f + j .</p><p>? Intra-layer Dependency: dependency f - i ? f + i exists if and only if f - i and f + i shares the same pruning schemes, denoted as sch(f</p><formula xml:id="formula_6">- i ) = sch(f + i ).</formula><p>The inter-layer dependency is easy to estimate if the topological structure of the network is known. For those connected layers f - i ? f + j , dependency always exists since f - i and f + j , in this case, correspond to the same intermediate features of the network. The next step is to shed   <ref type="bibr" target="#b31">[32]</ref>, but ignores coupled weights in other layers. Our method as shown in (c) learns group sparsity which forces all coupled parameters to zero, so that they can be easily distinguished by a simple magnitude method. some light on the intra-layer dependency. An intra-layer dependency requires the input and output of a single layer to be pruned simultaneously. There are many layers in networks that meet this condition, such as batch normalization or element-wise operations, whose inputs and outputs are pruned together. This phenomenon is primarily due to the shared pruning scheme between inputs and outputs, i.e., sch(f - i ) = sch(f + i ). As shown in Figure <ref type="figure" target="#fig_2">3</ref>, batch normalization is an element-wise operation and has the same pruning schemes for its output and input. When it comes to layers such as convolutions, their inputs and outputs are pruned in different ways, i.e., w[:, k, :, :] and w[k, :, :, :] as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, leading to sch(f - i ) = sch(f + i ). In this case, there is no dependency between the input and output of layers like convolutions.</p><p>With the above rules, we can formalize the dependency modeling as follows:</p><formula xml:id="formula_7">D(f - i , f + j ) = 1 f - i ? f + j Inter-layer Dep ? 1 i = j ? sch(f - i ) = sch(f + j ) Intra-layer Dep<label>(3)</label></formula><p>where ? and ? refer to a logical "OR" and "AND" operation, and 1 is a indicator function returning "True" is the condition holds. The first term examines the Inter-layer Dependency caused by network connectivity, while the second term examines the intra-layer dependency introduced by a shared pruning scheme between layer input and output. It is worth noting that, DepGraph is a symmetric matrix with</p><formula xml:id="formula_8">D(f - i , f + j ) = D(f + j , f - i ).</formula><p>As such, we can examine all inputs and outputs pairs to estimate the dependency graph. As a conclusion, we visualize an example of DepGraph in Figure <ref type="figure" target="#fig_2">3</ref>. Alg. 1 and 2 summarize the algorithms for graph construction and parameter grouping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pruning with Dependency Graph</head><p>In previous sections, we establish a general method to analyze dependencies in neural networks, producing several groups {g 1 , g 2 , ..., g L } with varied group sizes. Estimating the importance of grouped parameters is a challenging task. Given a pre-defined criterion like L p Norm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Dependency Graph</head><p>Input: A neural network F(x; w)</p><formula xml:id="formula_9">Output: DepGraph D(F, E) f -= {f - 1 , f - 2 , ..., f - L } decomposed from the F f + = {f + 1 , f + 2 , ..., f + L } decomposed from the F Initialize DepGraph D = 0 2L?2L for i = {0, 1, .., L} do for j = {0, 1, .., L} do D(f - i , f + j ) = D(f + j , f - i ) = 1 f - i ? f + j Inter-layer Dep ? 1 i = j ? sch(f - i ) = sch(f + j ) Intra-layer Dep return D Algorithm 2: Grouping Input: DepGraph D(F, E) Output: Groups G G = {} for i = {1, 2, ..., F } do g = {i} repeat UNSEEN = {1, 2, ..., F } -g g = {j ? UNSEEN|?k ? g, D kj = 1} g = g ? g until g = ?; G = G ? {g} return G I(w) = w 2 ,</formula><p>a natural solution is to take the aggregated score I(g) = w?g I(w), ignoring the distribution divergence between different layers. Unfortunately, importance scores independently estimated on a single layer are likely to be unreliable and sometimes conflicting, since a pruned group may contain both important and unimportant weights. To address the problem, we introduce a simple but sufficiently general method, which leverages the grouping ability of DepGraph to comprehensively sparse all parameterized layers within each group, including but not limited to convolutions, batch normalizations, and fully-connected layers. As illustrated in Figure <ref type="figure" target="#fig_4">4</ref> (c), we aim to learn consistent sparsity across all grouped layers, zeroing some dimensions to zeros simultaneously. We flatten and merge grouped parameters as a large parameter matrix w g , where w g [k] retrieves all parameters belonging to k-th prunable dimension like the k-th channel of a CNN block. Now, consistent sparsity can be promoted with a simple weighted shrinkage :</p><formula xml:id="formula_10">R(g, k) = w?wg[k] ? k w 2 2<label>(4)</label></formula><p>where ? k assigns different shrinkage strengths to different dimensions. we use a simple yet controllable exponential strategy to determine the ? as the following:</p><formula xml:id="formula_11">? k = 2 ?(I max g -I g,k )/(I max g -I min g )<label>(5)</label></formula><p>where I g,k is the aggregated score on k-dimension, and I g is the importance score vector of the all groups. The hyper-parameter ? controls the shrinkage strength, ranging from 2 0 , 2 ? . In this work, we use ? = 4 for all experiments. Notably, strong shrinkage will be assigned to those unimportant dimensions to force consistent sparsity. After sparse training, we use a normalized score ?g = k ? I g / {top-k(I g )} to remove parameters, which reveals a relative score to those important dimensions. In this work, we show that such a simple pruning method, when combined with dependency modeling, can achieve comparable performance to modern approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings.</head><p>This paper focuses on basic classification tasks and conducts extensive experiments using a variety of datasets, such as CIFAR <ref type="bibr" target="#b24">[25]</ref> and ImageNet <ref type="bibr" target="#b2">[3]</ref> for image classification, PPI <ref type="bibr" target="#b13">[14]</ref> for graph classification, ModelNet <ref type="bibr" target="#b57">[58]</ref> for classifying 3D point clouds, and AGNews <ref type="bibr" target="#b66">[67]</ref> for text classification. For each dataset, we evaluated our method using the most representative architectures, including ResNe(X)t <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b58">59]</ref>, VGG <ref type="bibr" target="#b47">[48]</ref>, DenseNet <ref type="bibr" target="#b22">[23]</ref>, Mo-bileNet <ref type="bibr" target="#b44">[45]</ref>, GoogleNet <ref type="bibr" target="#b50">[51]</ref>, Vision Transformers <ref type="bibr" target="#b9">[10]</ref>, LSTM <ref type="bibr" target="#b11">[12]</ref>, DGCNNs <ref type="bibr" target="#b55">[56]</ref>, and Graph Attention Networks <ref type="bibr" target="#b51">[52]</ref>. To conduct ImageNet experiments, we use offthe-shelf models from Torchvision <ref type="bibr" target="#b37">[38]</ref>, and pre-train our own models for other datasets. After pruning, the model is then fine-tuned following similar configurations as pretraining, but with a smaller learning rate and fewer iterations. Detailed information regarding hyper-parameters can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on CIFAR</head><p>Performance. CIFAR <ref type="bibr" target="#b24">[25]</ref> is a tiny image dataset, which is widely used to verify the effectiveness of pruning algorithms. We follow existing works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b53">54]</ref>   <ref type="bibr" target="#b68">[69]</ref> 93.80 93.83 +0.03 1.88? CP <ref type="bibr" target="#b27">[28]</ref> 92.80 91.80 -1.00 2.00? AMC <ref type="bibr" target="#b18">[19]</ref> 92.80 91.90 -0.90 2.00? HRank <ref type="bibr" target="#b29">[30]</ref> 93.26 92.17 -0.09 2.00? SFP <ref type="bibr" target="#b17">[18]</ref> 93 ResNet-56 on CIFAR-10, and a VGG network on CIFAR-100, as shown in Table <ref type="table" target="#tab_1">1</ref>. We compare the accuracy of the pruned model as well as the accuracy of the original model before pruning and theoretically estimated speedup ratio MACs(base) MACs(pruned) . Note that baselines like ResRep <ref type="bibr" target="#b6">[7]</ref>, GReg <ref type="bibr" target="#b53">[54]</ref> also deploy sparse training for pruning. A key difference between our algorithm and existing sparsity-based algorithms is that we promote sparse consistently on all grouped layers, including convolutions, batch normalizations and fully-connected layers in our experiments. With this improvement, we are able to take full advantage of the grouped structures to learn better sparse models, and thus improve pruning accuracy.</p><p>Group Sparsity. Consistently sparsified groups are important for pruning, as previously stated. We visualize the sparsity of different groups learned using our method and a baseline method that sparsifies layers independently without taking into account the grouping characteristics of layers. The L 2 norms of grouped parameters are illustrated in Figure <ref type="figure" target="#fig_5">5</ref>. It is easy to find that our method produces strong sparsity at the group level, whereas the baseline method does not produce consistent importance, although each layer has already been sparsified locally. Particularly, group #0 contains shallow layers that are difficult to sparse in both algorithms, and thus will not undergo severe pruning. It is worth noting that, Our algorithm is capable of being combined with more powerful sparse learning techniques like growing regularization <ref type="bibr" target="#b53">[54]</ref> or reparameterization <ref type="bibr" target="#b6">[7]</ref>, but in this paper we only consider the simple norm pruner to keep our method as general as possible to handle different networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Grouping Strategy. To further verify the effectiveness of grouping, we evaluate different grouping strategies on a variety of convolutional networks. Strategies mainly include: 1) No grouping: sparse learning and importance evaluation are performed independently on a single convolutional layer; 2) Convolution-only Grouping: all convolutional layers within the group are sparsified in a consistent manner.</p><p>3) Full Grouping: All trainable layers within a group, such as convolutions, batch normalizations, and fully-connected layers, are sparsified consistently. As shown in Table <ref type="table" target="#tab_2">2</ref>, When we ignore the grouping information in neural networks and sparsify each layer in isolation, the performance of our approach will degrade significantly, and in some cases even collapse as a result of over-pruning. According to the results of Conv-only grouping, the inclusion of more parameters into the group is beneficial to the final performance, but some useful information in the group is still omitted. It is possible to further improve the pruned accuracy by implementing the full grouping strategy. Layer Sparsity. In terms of pruning, layer sparsity is considered an important design space, which determines the structure of pruned neural networks. iments in Figure <ref type="figure" target="#fig_5">5</ref> have shown that different layers are not equally prunable. In most cases, the learned sparsity outperforms the uniform sparsity as shown in table <ref type="table" target="#tab_2">2</ref>. Therefore, we allow the sparsity learning algorithm to determine the sparsity of the layers on its own.   all these groups are easily obtained without any additional human effort. We visualize the dependency graph as well as the derived grouping for DenseNet-121 <ref type="bibr" target="#b22">[23]</ref>, ResNet-18 and Vision Transformers <ref type="bibr" target="#b9">[10]</ref> in Figure <ref type="figure" target="#fig_6">6</ref>. This grouping matrix is derived from the dependency graph as described in the method, with G[i, j] = 1 indicates that the i-th layer is in the same group as the j-th one. DenseNet-121 exhibits a high degree of correlation between layers from the same dense block, which results in large coupled groups during structural pruning. When complex networks are involved, the proposed dependency graph will be very essential, since it is difficult to analyze all dependencies in those networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizability of DepGraph. Results in</head><p>ImageNet. Table <ref type="table" target="#tab_3">3</ref> presents pruning results on ImageNet for several architectures, including ResNet, DenseNet, Mo-bileNet, ResNeXt, and Vision Transformers. The purpose of this work is not to provide state-of-the-art results for various models, thus we are not introducing too many powerful techniques in sparse learning and importance estima-tion. Instead, we show that a simple L1 norm, when combined with dependency modeling, can achieve comparable performance to modern approaches that either use powerful criteria <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b62">63]</ref> or better training techniques <ref type="bibr" target="#b53">[54]</ref>. Besides, our method is general enough for various networks in realworld applications.</p><p>Text, 3D Point Cloud, Graph and More. In addition to CNNs and Transformers, our method is easily applicable to other architectures as well. This part consists of experiments on a variety of data, including texts, graphs, and 3D point clouds, as shown in Table <ref type="table" target="#tab_5">4</ref>. We utilize LSTM for text classification by studying the effectiveness of Dep-Graph on recursive structures in which parameterized layers are coupled due to the element-wise operations. Dep-Graph is also tested on Dynamic Graph CNNs that contain coupled aggregation operations for 3D point clouds. Furthermore, we conduct experiments with graph data, which require entirely different architectures from those used for other tasks. In this experiment, we concentrate on the acceleration of Graph Attention Networks, which have several coupled layers within each GNN layer. Considering the lack of work concerning pruning on these datasets, we combine DepGraph with some classic pruning methods in CNNs to establish all baselines. Our results indicate that our method can be indeed generalized to a wide variety of architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduce Dependency Graph to enable any structural pruning on a wide variety of neural networks. Our work is the first attempt, to the best of our knowledge, to develop a general algorithm that can be applied to a variety of architectures, including CNNs, RNNs, GNNs, and Transformers. While our method can handle most deep learning structures, there are some outlier cases, such as shuffling operations in ShuffleNet, which require further study in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Parameters from different layers are inherently dependent on each other across network architectures, which forces several layers to be pruned simultaneously. For instance, to prune the Conv2 in (a), all other layers {Conv1, BN1, BN2} within the block must be pruned as well. In this work, we introduce a generic scheme, termed as Dependency Graph, to explicitly account for such dependencies and execute the pruning of arbitrary architecture in a fully automatic manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Layer grouping is achieved via a recursive propagation on DepGraph, starting from the f + 4 . In this example, there is no Intra-layer Dependency between convolutional input f - 4 and output f + 4 due to the different pruning scheme illustrated above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>|| ? ? ? ||? ? # || ? ? ? ||? ? || ? ? ? dep ||? ? # || ? ? ? ||? ? || ? ? ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Learning different sparsity schemes to estimate the importance of grouped parameters. Method (a) is used in unstructural pruning which only focuses on the importance of single weight. Method (b) learns structurally sparse layers<ref type="bibr" target="#b31">[32]</ref>, but ignores coupled weights in other layers. Our method as shown in (c) learns group sparsity which forces all coupled parameters to zero, so that they can be easily distinguished by a simple magnitude method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Histogram of group sparsity obtained by sparse learning w/ and w/o grouping, which respectively correspond to the strategy (b) and (c) in Figure.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Dependency graphs (top) and the derived grouping schemes (bottom) for DenseNet-121, ResNet-18 and ViT-Base where complicated groups emerge.</figDesc><graphic url="image-323.png" coords="8,315.09,347.00,71.59,71.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to prune a</figDesc><table><row><cell>Model / Data Method</cell><cell cols="4">Base Pruned ? Acc. Speed Up</cell></row><row><cell>NISP [66]</cell><cell>-</cell><cell>-</cell><cell>-0.03</cell><cell>1.76?</cell></row><row><cell cols="2">Geometric [20] 93.59</cell><cell>93.26</cell><cell>-0.33</cell><cell>1.70?</cell></row><row><cell>Polar</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Pruning results on CIFAR-10 and CIFAR-100.</figDesc><table><row><cell></cell><cell></cell><cell>.59</cell><cell>93.36</cell><cell>-0.23</cell><cell>2.11?</cell></row><row><cell></cell><cell>ResRep [7]</cell><cell>93.71</cell><cell>93.71</cell><cell>+0.00</cell><cell>2.12?</cell></row><row><cell></cell><cell>Ours w/o SL</cell><cell>93.53</cell><cell>93.46</cell><cell>-0.07</cell><cell>2.11?</cell></row><row><cell></cell><cell>Ours</cell><cell>93.53</cell><cell>93.77</cell><cell>+0.24</cell><cell>2.11?</cell></row><row><cell></cell><cell>GBN ( [63])</cell><cell>93.10</cell><cell>92.77</cell><cell>-0.33</cell><cell>2.51?</cell></row><row><cell></cell><cell>AFP ( [6])</cell><cell>93.93</cell><cell>92.94</cell><cell>-0.99</cell><cell>2.56?</cell></row><row><cell></cell><cell>C-SGD ( [4])</cell><cell>93.39</cell><cell>93.44</cell><cell>+0.05</cell><cell>2.55?</cell></row><row><cell></cell><cell cols="2">GReg-1 ( [54]) 93.36</cell><cell>93.18</cell><cell>-0.18</cell><cell>2.55?</cell></row><row><cell></cell><cell cols="2">GReg-2 ( [54]) 93.36</cell><cell>93.36</cell><cell>-0.00</cell><cell>2.55?</cell></row><row><cell></cell><cell>Ours w/o SL</cell><cell>93.53</cell><cell>93.36</cell><cell>-0.17</cell><cell>2.51?</cell></row><row><cell></cell><cell>Ours</cell><cell>93.53</cell><cell>93.64</cell><cell>+0.11</cell><cell>2.57?</cell></row><row><cell></cell><cell>OBD ( [53])</cell><cell>73.34</cell><cell>60.70</cell><cell>-12.64</cell><cell>5.73?</cell></row><row><cell></cell><cell>OBD ( [53])</cell><cell>73.34</cell><cell>60.66</cell><cell>-12.68</cell><cell>6.09?</cell></row><row><cell>VGG19 CIFAR100</cell><cell cols="2">EigenD ( [53]) GReg-1 ( [54]) 74.02 73.34 GReg-2 ( [54]) 74.02</cell><cell>65.18 67.55 67.75</cell><cell>-8.16 -6.67 -6.27</cell><cell>8.80? 8.84? 8.84?</cell></row><row><cell></cell><cell>Ours w/o SL</cell><cell>73.50</cell><cell>67.60</cell><cell>-5.44</cell><cell>8.87?</cell></row><row><cell></cell><cell>Ours</cell><cell>73.50</cell><cell>70.39</cell><cell>-3.11</cell><cell>8.92?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on CIFAR-100 for different grouping strategies and sparsity configurations. The proposed strategy, full grouping, takes all parameterized layers into account during sparse training, while other strategies only leverage partial layers. Accuracy (%) of pruned models with uniform layer sparsity or learned layer sparsity is reported. ?:In some cases, our method over-prunes some dimension to 1, which severely damages the final accuracy.</figDesc><table><row><cell>Architecture</cell><cell>Strategy</cell><cell></cell><cell cols="3">Pruned Accuracy with Uniform / Learned Sparsity</cell></row><row><cell></cell><cell></cell><cell>1.5?</cell><cell>3.0?</cell><cell>6.0?</cell><cell>12?</cell><cell>Avg.</cell></row><row><cell></cell><cell>Random</cell><cell cols="3">71.49 / 72.07 68.52 / 68.16 60.35 / 60.25</cell><cell>53.21 / 48.01</cell><cell>63.39 / 62.15</cell></row><row><cell>ResNet-56</cell><cell>No grouping</cell><cell cols="3">71.96 / 72.07 67.85 / 67.89 62.64 / 63.18</cell><cell>54.52 / 53.65</cell><cell>64.24 / 64.20</cell></row><row><cell>(72.58)</cell><cell>Conv-only</cell><cell cols="3">71.64 / 71.94 68.30 / 69.07 62.44 / 62.63</cell><cell>53.89 / 54.94</cell><cell>64.07 / 64.65</cell></row><row><cell></cell><cell cols="4">Full Grouping 71.68 / 72.57 68.70 / 70.38 63.72 / 65.33</cell><cell>55.23 / 55.92</cell><cell>64.83 / 66.09</cell></row><row><cell></cell><cell>Random</cell><cell cols="3">72.63 / 72.77 71.27 / 70.83 68.97 / 69.16</cell><cell>62.45 / 63.42</cell><cell>63.83 / 69.05</cell></row><row><cell>VGG-19</cell><cell>No Grouping</cell><cell cols="4">73.83 / 55.13 71.40 / 53.21 69.19 / 50.10 65.12 /  ? 3.87 69.14 / 40.58</cell></row><row><cell>(73.50)</cell><cell>Conv-Only</cell><cell cols="3">73.32 / 73.22 71.38 / 71.80 69.66 / 69.85</cell><cell>64.69 / 65.95</cell><cell>69.76 / 70.21</cell></row><row><cell></cell><cell cols="4">Full Grouping 73.11 / 74.00 71.57 / 72.46 69.72 / 70.38</cell><cell>65.74 / 66.20</cell><cell>70.03 / 70.58</cell></row><row><cell></cell><cell>Random</cell><cell cols="3">79.04 / 79.43 77.86 / 78.62 75.47 / 74.52</cell><cell>69.26 / 69.64</cell><cell>75.41 / 75.80</cell></row><row><cell>DenseNet-121</cell><cell>No Grouping</cell><cell cols="3">79.31 / 78.91 78.08 / 78.62 78.62 / 68.57</cell><cell>72.93 / 57.17</cell><cell>77.24 / 70.82</cell></row><row><cell>(78.73)</cell><cell>Conv-Only</cell><cell cols="3">79.18 / 79.74 77.98 / 78.85 76.61 / 77.22</cell><cell>73.30 / 73.95</cell><cell>76.77 / 77.44</cell></row><row><cell></cell><cell cols="4">Full Grouping 79.34 / 79.74 77.97 / 79.19 77.08 / 77.78</cell><cell>74.77 / 75.29</cell><cell>77.29 / 77.77</cell></row><row><cell></cell><cell>Random</cell><cell cols="3">70.90 / 70.69 67.75 / 67.54 61.32 / 62.26</cell><cell>53.41 / 53.97</cell><cell>63.35 / 63.62</cell></row><row><cell>MobileNetv2</cell><cell>No Grouping</cell><cell cols="3">71.16 / 71.28 69.93 / 68.59 66.76 / 37.38</cell><cell>60.28 / 28.24</cell><cell>67.03 / 51.37</cell></row><row><cell>(70.80)</cell><cell>Conv-Only</cell><cell cols="3">71.22 / 71.51 70.33 / 70.15 66.16 / 66.49</cell><cell>61.35 / 63.24</cell><cell>67.27 / 67.85</cell></row><row><cell></cell><cell cols="4">Full Grouping 71.11 / 71.67 70.06 / 70.81 66.48 / 68.02</cell><cell>60.32 / 63.37</cell><cell>66.99 / 68.67</cell></row><row><cell></cell><cell>Random</cell><cell cols="3">77.52 / 77.72 76.47 / 76.15 74.92 / 74.19</cell><cell>69.37 / 69.69</cell><cell>74.57 / 74.44</cell></row><row><cell>GoogleNet</cell><cell>No Grouping</cell><cell cols="3">77.44 / 77.23 76.84 / 74.95 75.60 / 63.78</cell><cell>71.92 / 63.72</cell><cell>75.45 / 69.92</cell></row><row><cell>(77.56)</cell><cell>Conv-Only</cell><cell cols="3">77.33 / 77.62 76.68 / 76.92 75.66 / 74.98</cell><cell>71.90 / 71.87</cell><cell>75.49 / 75.35</cell></row><row><cell></cell><cell cols="4">Full Grouping 77.91 / 77.76 76.90 / 77.00 75.42 / 75.44</cell><cell>71.98 / 72.88</cell><cell>75.53 / 75.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Table 2 also provides some useful results about layer sparsity. This work primarily focuses on two types of sparsity: uniform sparsity and learned sparsity. Using uniform sparsity, neural networks are scaled uniformly, assuming that redundancy is distributed uniformly throughout. However, previous exper-Pruning results on ImageNet.</figDesc><table><row><cell cols="2">Arch. Method</cell><cell cols="4">Base Pruned ? Acc. MACs</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>76.15</cell><cell>-</cell><cell>-</cell><cell>4.13</cell></row><row><cell></cell><cell>ThiNet [36]</cell><cell>72.88</cell><cell>72.04</cell><cell>-0.84</cell><cell>2.44</cell></row><row><cell></cell><cell>SSS [24]</cell><cell>76.12</cell><cell>74.18</cell><cell>-1.94</cell><cell>2.82</cell></row><row><cell></cell><cell>SFP [18]</cell><cell>76.15</cell><cell>74.61</cell><cell>-1.54</cell><cell>2.40</cell></row><row><cell></cell><cell>AutoSlim [65]</cell><cell>76.10</cell><cell>75.60</cell><cell>-0.50</cell><cell>2.00</cell></row><row><cell>ResNet-50</cell><cell>FPGM [20] Taylor [39] Slimable [64] CCP [42]</cell><cell>76.15 76.18 76.10 76.15</cell><cell>75.50 74.50 74.90 75.50</cell><cell>-0.65 -1.68 -1.20 -0.65</cell><cell>2.38 2.25 2.30 2.11</cell></row><row><cell></cell><cell>AOFP-C1 [5]</cell><cell>75.34</cell><cell>75.63</cell><cell>+0.29</cell><cell>2.58</cell></row><row><cell></cell><cell>TAS [9]</cell><cell>77.46</cell><cell>76.20</cell><cell>-1.26</cell><cell>2.31</cell></row><row><cell></cell><cell>GFP [31]</cell><cell>76.79</cell><cell>76.42</cell><cell>-0.37</cell><cell>2.04</cell></row><row><cell></cell><cell>GReg-2 [54]</cell><cell>76.13</cell><cell>75.36</cell><cell>-0.77</cell><cell>2.77</cell></row><row><cell></cell><cell>Ours</cell><cell>76.15</cell><cell>75.83</cell><cell>-0.32</cell><cell>1.99</cell></row><row><cell>DenseNet-121</cell><cell cols="2">DenseNet-121 PSP-1.38G [47] 74.35 74.44 PSP-0.58G [47] 74.35 Ours-1.38G 74.44 Ours-0.58G 74.44</cell><cell>-74.05 70.34 73.98 70.13</cell><cell>--0.30 -4.01 -0.46 -4.31</cell><cell>2.86 1.38 0.58 1.37 0.57</cell></row><row><cell></cell><cell>Mob-v2</cell><cell>71.87</cell><cell>-</cell><cell>-</cell><cell>0.33</cell></row><row><cell>Mob-v2</cell><cell>NetAdapt [60] Meta [33] GFP [31]</cell><cell>-74.70 75.74</cell><cell>70.00 68.20 69.16</cell><cell>--6.50 -6.58</cell><cell>0.24 0.14 0.15</cell></row><row><cell></cell><cell>Ours</cell><cell>71.87</cell><cell>68.46</cell><cell>-3.41</cell><cell>0.15</cell></row><row><cell>NeXt-50</cell><cell>ResNeXt-50 SSS [24] GFP [31] Ours</cell><cell>77.62 77.57 77.97 77.62</cell><cell>-74.98 77.53 76.48</cell><cell>--2.59 -0.44 -1.14</cell><cell>4.27 2.43 2.11 2.09</cell></row><row><cell>ViT-B/16</cell><cell>VIT-B/16 CP-ViT [49] Ours+EMA Ours</cell><cell>81.07 77.91 81.07 81.07</cell><cell>-77.36 79.58 79.17</cell><cell>--0.55 -1.39 -1.90</cell><cell>17.6 11.7 10.4 10.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>also</cell></row></table><note><p>Visualization of DepGraph. Pruning large neural networks can be challenging due to the complexity of grouping parameters. Nevertheless, using the Dependency Graph,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Pruning neural networks for non-image data, including AGNews (text), ModelNet (3D Point Cloud) and PPI (Graph). We report the classification accuracy (%) of pruned model for AG-News and ModelNet and micro-F1 score for PPI.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The transitive reduction of a directed graph</title>
		<author>
			<persName><forename type="first">Alfred</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">D</forename><surname>Michael R Garey</surname></persName>
		</author>
		<author>
			<persName><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="137" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards efficient model compression via learned global ranking</title>
		<author>
			<persName><forename type="first">Ting-Wu</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1518" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Centripetal sgd for pruning very deep convolutional networks with complicated structure</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006">2019. 1, 2, 6</date>
			<biblScope unit="page" from="4943" to="4953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximated oracle filter pruning for destructive cnn width optimization</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Auto-balanced filter pruning for efficient convolutional neural networks</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Resrep: Lossless cnn pruning via decoupling remembering and forgetting</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2021. 1, 2, 3, 6</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to prune deep neural networks via layer-wise optimal brain surgeon</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Network pruning via transformable architecture search</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2008">2020. 1, 2, 6, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Network pruning via performance maximization</title>
		<author>
			<persName><forename type="first">Shangqian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feihu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9270" to="9280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Long short-term memory. Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2012. 2, 6</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note>Dynamic network surgery for efficient dnns</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Soft filter pruning for accelerating deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06866</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Filter pruning via geometric median for deep convolutional neural networks acceleration</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008">2019. 2, 3, 6, 8</date>
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008">2017. 2, 6, 8</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data-driven sparse structure selection for deep neural networks</title>
		<author>
			<persName><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="304" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sejun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07611</idno>
		<title level="m">Layer-adaptive sparsity for the magnitude-based pruning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A signal propagation perspective for pruning neural networks at initialization</title>
		<author>
			<persName><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06307</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<title level="m">Pruning filters for efficient convnets</title>
		<imprint>
			<date type="published" when="2008">2016. 1, 2, 3, 6, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pruning and quantization for deep neural network acceleration: A survey</title>
		<author>
			<persName><forename type="first">Tailin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Glossner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">461</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="370" to="403" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hrank: Filter pruning using high-rank feature map</title>
		<author>
			<persName><forename type="first">Mingbao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Group fisher pruning for practical network compression</title>
		<author>
			<persName><forename type="first">Liyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingmin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2021. 2, 3, 8, 9</date>
			<biblScope unit="page" from="7021" to="7032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008">2017. 2, 3, 5, 8</date>
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Metapruning: Meta learning for automatic neural network channel pruning</title>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3296" to="3305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A gradient flow framework for analyzing network pruning</title>
		<author>
			<persName><forename type="first">Ekdeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubana</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11839</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural network pruning with residual-connections and limited-data</title>
		<author>
			<persName><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Resnet can be pruned 60?: Introducing network purification and unused path removal (p-rm) after weight pruning</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2006">2019. 2019. 2, 4, 6</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Torchvision the machine-vision package of torch</title>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1485" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2019. 2, 8</date>
			<biblScope unit="page" from="11264" to="11272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Logarithmic pruning is all you need</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Orseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Rivasplata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2925" to="2934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Lookahead: a far-sighted alternative of magnitude-based pruning</title>
		<author>
			<persName><forename type="first">Sejun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04809</idno>
		<imprint>
			<date type="published" when="2020">2020. 1, 2, 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Collaborative channel pruning for deep networks</title>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5113" to="5122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Comparing rewinding and fine-tuning in neural network pruning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02389</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Parameterized structured pruning for deep neural networks</title>
		<author>
			<persName><forename type="first">G?nther</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Pernkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Fr?ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, Optimization, and Data Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2006">2014. 1, 2, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Cp-vit: Cascade vision transformer pruning via progressive sparsity prediction</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhezhi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naifeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyao</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04570</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Understanding lstm-a tutorial into long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ralf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Staudemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morris</forename><surname>Rothstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09586</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Eigendamage: Structured pruning in the kroneckerfactored eigenbasis</title>
		<author>
			<persName><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6566" to="6575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09243</idno>
		<title level="m">Neural pruning via growing regularization</title>
		<imprint>
			<date type="published" when="2006">2020. 1, 2, 3, 6</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Accelerate cnns from three dimensions: a comprehensive pruning framework</title>
		<author>
			<persName><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10717" to="10726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4820" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Netadapt: Platform-aware neural network adaptation for mobile applications</title>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Joint-detnas: upgrade your detector with nas, pruning and dynamic distillation</title>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10175" to="10184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00124</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Zhonghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2019. 1, 2, 3, 6, 9</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Slimmable neural networks for edge devices</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Autoslim: Towards oneshot architecture search for channel numbers</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Nisp: Pruning networks using neuron importance score propagation</title>
		<author>
			<persName><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jui-Hsin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Aligned structured sparsity learning for efficient image superresolution</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2695" to="2706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Neuron-level structured pruning using polarization regularizer. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9865" to="9877" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
