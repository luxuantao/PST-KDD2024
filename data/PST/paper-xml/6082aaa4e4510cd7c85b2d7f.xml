<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QEI: Query Acceleration Can be Generic and Efficient in the Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Yuan</surname></persName>
							<email>yifany3@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yipeng</forename><surname>Wang</surname></persName>
							<email>yipeng1.wang@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ren</forename><surname>Wang</surname></persName>
							<email>ren.wang@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rangeen</forename><surname>Basu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roy</forename><surname>Chowhury</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charlie</forename><surname>Tai</surname></persName>
							<email>charlie.tai@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Kim</surname></persName>
							<email>nskim@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">QEI: Query Acceleration Can be Generic and Efficient in the Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA51647.2021.00040</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>data query</term>
					<term>on-chip accelerator</term>
					<term>near-cache processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data query operations of different data structures are ubiquitous and critical in today's data center infrastructures and applications. However, query operations are not always performance-optimal to be executed on general-purpose CPU cores. These operations exhibit insufficient memory-level parallelism and frontend bottlenecks due to unstructured control flow. Furthermore, the data access patterns are not cache-or prefetchfriendly. Based on our performance analysis on a commodity server, query operations can consume a large percentage of the CPU cycles in various modern cloud workloads. Existing accelerator solutions for query operations do not strike a balance between their generality, scalability, latency, and hardware complexity.</p><p>In this paper, we propose QEI, a generic, integrated, and efficient acceleration solution for various data structure queries. We first abstract the query operations to a few regular steps and map them to a simple and hardware-friendly configurable finite automaton model. Based on this model, we develop the QEI architecture that allows multiple query operations to execute in parallel to maximize throughput. We also propose a novel way to integrate the accelerator into the CPU that balances performance, latency, and hardware cost. QEI keeps the main control logic near the L2 cache to leverage existing hardware resources in the core while distributing the data-intensive comparison logic to each last-level cache slice for higher parallelism. Our results with five representative data center workloads show that QEI can achieve 6.5×∼11.2× performance improvement in various scenarios with low overhead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Nowadays, the combination of diverse applications and infrastructure in data centers has created great challenges for both cloud service providers and chip makers in improving data center hardware's performance and efficiency. Researchers have explored the adoption of specialized hardware (or accelerators), such as FPGAs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b65">66]</ref> and GPUs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>, to improve the performance and efficiency of important parts (compute kernels) of such workloads. These accelerators are usually connected to the CPU via an I/O interface such as PCIe. Due to the long communication latency between the core and the PCIe device <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b57">58]</ref>, the compute kernel has to run for a significant amount of time to amortize the communication overhead, making them unsuitable for accelerating fine-grained and latencysensitive operations <ref type="bibr" target="#b6">[7]</ref>. One such operation is data query.</p><p>Data query (or lookup), in general, refers to the process of retrieving data for a given key from one of a handful of popular data structures (see Sec. II for details). Query operations exist in almost all data center workloads. For example, a firewall can use a list of blacklisted keywords to query on a traffic flow to identify malicious requests; a network packet can query on a routing table to determine the output port in a virtual switch; a web server can send query to a database to retrieve a user's profile. Optimizing such operations can benefit a wide range of workloads.</p><p>In addition to numerous software optimizations for query operations, there have been a few proposals to use specialized hardware to accelerate these operations <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b80">81]</ref>. Almost all of them propose to offload the query operations entirely or partially to an accelerator integrated inside a CPU chip. Compared with PCIe-based devices, these on-chip accelerators considerably reduce the communication overhead, making them appealing for the query operations.</p><p>However, there are still substantial limitations with these existing solutions. First, the accelerator should efficiently deal with a wide range of popular applications to justify being integrated into a general-purpose CPU. Most existing solutions either focus on a particular application (e.g., only hash table lookups are accelerated <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b80">81]</ref>) or require multiple instances to support different data structures <ref type="bibr" target="#b44">[45]</ref>, all of which lack generality and efficiency. Second, many existing proposals assume a loose integration of the accelerator with the CPU cores, which still has latency concern, given query operations' strict latency sensitivity in many workloads. Third, the hardware complexity and cost of these solutions make them less practical. For example, the queried data structures seldom reside in a contiguous memory address space (i.e., larger than a 4KB page) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>, which necessitates an address translation of some sort and may require a dedicated memory management unit (MMU) in the accelerator for it to be high-performance. This incurs non-trivial on-chip hardware costs (see Sec. VII for detail). These limitations make many of the existing proposals less attractive for a general-purpose platform. It is highly desirable to have a versatile, efficient, and balanced accelerator design with low latency and high throughput.</p><p>To this end, we propose QEI, which strives to achieve this fine balance. We first investigate the processing steps of various data query operations on popular data structures and observe that they share very similar inputs/outputs and execution patterns. Based on this observation, we abstract data query operations using five steps with three types of operations: memory access, comparison, and arithmetic. With this abstraction, we can map each data query operation to a distinct configurable finite automaton (CFA) -a finite automaton with fixed transition rules but configurable parameters. We design QEI to be capable of executing these CFAs in a way that makes it possible for a single accelerator to process multiple types of data query operations, improving the performance for a wide range of workloads.</p><p>QEI consists of three main components: (1) a Query State Table to store the state information of in-flight query operations, (2) a CFA Execution Engine that is capable of supporting multiple CFAs for different data structures and can be extended via firmware update to support new data structures, and (3) a Data Processing Unit comprising of various processing elements such as ALUs and Comparators. The query is initialized by a new instruction (with two flavors) and is processed by the appropriate CFA model for state transitions and intermediate data processing via micro-operations. Depending on the flavor, the result is returned to either the core or the designated memory space. Such architecture enables QEI to support a wide range of queries efficiently with shared hardware resources.</p><p>Regarding how the accelerator should be integrated into a CPU, prior works generally explore two directions. They either use a distributed design by integrating the accelerator in the core or last-level Cache (LLC) (see Fig. <ref type="figure" target="#fig_4">6a</ref>) or a centralized design by placing the dedicated accelerator hardware away from the core tile (connected to the on-chip fabric through a dedicated port) (see Fig. <ref type="figure" target="#fig_4">6b</ref>). In this paper, we propose a novel integration scheme that builds on the advantages of these schemes to balance throughput, latency, and design complexity. More specifically, in our integration scheme, the accelerator is tightly coupled to the core while still being able to extract a large amount of memory-level parallelism (MLP) by overlapping many query operations (see Fig. <ref type="figure" target="#fig_4">6c</ref>). We place the majority of QEI close to each core's L2 cache and second-level TLB (L2-TLB) for resource sharing and put the Comparators into the Caching and Home Agent (CHA) of each LLC slice to avoid moving large amounts of data into private caches. With this scheme, QEI can conveniently leverage L2-TLB for address translation while performing the data-intensive comparisons in a near-data fashion to exploit parallelism in Non-Uniform Cache Access (NUCA) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44]</ref> design, as well as to avoid private cache pollution.</p><p>We evaluate QEI using the Sniper simulator <ref type="bibr" target="#b10">[11]</ref> with five representative cloud data center workloads. The results show that QEI can achieve ∼ 8× speedup on average and as high as ∼ 10× speedup over the baseline software implementation. We also demonstrate how the integration schemes impact the performance delivered by the accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND CHALLENGES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Structures in Cloud Workloads</head><p>Cloud workloads in data centers are usually dataintensive <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42]</ref>, and the data is typically organized in various data structures with different performance and memory trade-offs. In this section, we discuss the characteristics of some popular data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hash Table.</head><p>A basic hash table is an array where one can store each key-value pair at a dedicated location indexed by hashing the key. Hash tables often appear in network function  virtualization (NFV) environments, where network functions run on general-purpose servers instead of specialized hardware boxes <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56]</ref>. Some examples of such network functions are virtual switch <ref type="bibr" target="#b62">[63]</ref>, firewall <ref type="bibr" target="#b28">[29]</ref>, and load balancer <ref type="bibr" target="#b20">[21]</ref>. Hash tables are also used for in-memory key-value stores <ref type="bibr" target="#b23">[24]</ref> and machine learning workloads <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b72">73]</ref>.</p><p>Tree. <ref type="foot" target="#foot_0">1</ref> Tree is a hierarchical data structure with each tree node storing the key and data. One of the most popular tree data structures used in the cloud is the object tree managed by runtime languages for garbage collection. For example, the garbage collector in Java Virtual Machine (JVM) maintains the live objects in a tree data structure <ref type="bibr" target="#b0">[1]</ref>. A garbage collection event causes a traversal of the object tree to mark and move live objects and recycle dead ones. As most cloud applications are written in managed languages such as Java, garbage collection is a major consumer of CPU cycles <ref type="bibr" target="#b54">[55]</ref>.</p><p>Trie. We distinguish trie from tree because they have distinct implementations and usages. In a basic trie, each child node is indexed by a distinct byte. The node itself does not store a key. Instead, the path to a leaf node implicitly represents a unique key. Trie is commonly used for literal matching or prefix matching. For example, in networking workloads, a routing table applies longest prefix matching (LPM) on IP addresses to decide a route <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b66">67]</ref>. Intrusion prevention systems (IPSs) use literal matching to decide if a networking request is malicious by matching the request's content with a list of keywords <ref type="bibr" target="#b15">[16]</ref>.</p><p>Linked list. Unlike trees and hash tables, linked list is a data structure that can be more easily extended and maintained during runtime. People usually choose linked list when data updates happen frequently. For faster data query speed, a special type of linked list called skip list <ref type="bibr" target="#b64">[65]</ref> is also widely used. Skip list keeps its data sorted and maintains multiple levels of linked list so that the query thread can skip nodes during traversal. One can find skip list usages in database applications such as RocksDB <ref type="bibr" target="#b22">[23]</ref>.</p><p>To better demonstrate the overheads of query operations in cloud workloads, we conduct a profiling study with DPDK, RocksDB, and FLANN on a commodity server with two Intel ® Xeon ® 8160 CPUs <ref type="bibr" target="#b39">[40]</ref> and 64GB DDR4 memory (see Sec. VI for benchmarks details and configuration). Based on our performance profiling with Intel ® VTune ® <ref type="bibr" target="#b38">[39]</ref> and previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b54">55]</ref>, as summarized in Fig. <ref type="figure" target="#fig_1">1</ref>, the data query operations in various workloads take up 23% ∼ 44% of the CPU time. We further use vTune's top-down analysis to investigate the architectural bottleneck of query operations. Workloads can be categorized as being backend bound or frontend bound. We find hash table queries to be backend bound due to the excessive amount of data accesses. For instance, DPDK workload is 7.5% frontend bound and 63.9% backend bound<ref type="foot" target="#foot_2">2</ref> . We observe higher frontend pressure for queries into lined list or tree due to the large number of instructions and data-dependent branches, primarily from pointer chasing. Specifically, the RocksDB workload is 25.9% frontend bound and 9.5% backend bound. Although the out-of-order (OoO) execution of the modern CPU core helps with instruction-level parallelism, we find each query operation can easily generate hundreds of dynamic instructions. Core's ROB and Load-Store Unit can be quickly saturated. These findings motivate us to accelerate query operations by reducing data access overhead (backend bottleneck), dynamic instruction count (frontend/backend bottleneck), and irregular control flow (frontend bottleneck).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Challenges of Designing Query Accelerator</head><p>Challenge 1: generality. Query operations on different data structures can have distinct implementations in software. Previous works only focus on accelerating a specific operation on a particular data structure <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b80">81]</ref> or develop specific hardware for each data structure <ref type="bibr" target="#b47">[48]</ref>, resulting in restricted usage scenarios and poor extensibility. Challenge 2: latency. We find that previous accelerator studies rarely discuss the communication latency between the CPU core and the accelerator. This is partially attributed to the long execution time of many offloaded computing kernels, which amortizes such latency. For example, one can offload a large portion of the computing time in machine learning workloads to a GPU or a dedicated accelerator <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b61">62]</ref>. Communication only happens at the kernel initialization and the data retrieval stages, which is amortized by the kernel's long execution time. However, not all use cases can tolerate such latency. For example, as discussed by Kalia et al. <ref type="bibr" target="#b40">[41]</ref>, the average networking response latency can be tripled when operations are offloaded to a GPU. Our targeted query operations are fine-grained and often used in latency-sensitive workloads, including networking and database applications. The jitters and latency to serve each query are critical to the observed quality of service <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b71">72]</ref>. Challenge 3: design complexity and cost. Adding an accelerator to the CPU die means reduced area and power budget for CPU cores and other components. When the accelerator is not used, it becomes dark silicon, wasting area and leakage power <ref type="bibr" target="#b21">[22]</ref>. Hence, it is desirable to minimize the area and power consumption by the accelerator. One example is the MMU, which can substantially increase the hardware cost of an accelerator. Previous works either assume largely consecutive memory space via huge page <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b78">79]</ref> or dedicated memory management hardware <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48]</ref>. Using huge Fig. <ref type="figure">2</ref> List 1: Routine of the linked list query and its corresponding steps in the abstraction.</p><p>page can easily cause fragmentation, and there is no guarantee that huge pages are available on a system that is not freshly booted. Using dedicated hardware, on the other hand, increases the hardware area and power budget significantly. Later on, we will show that an extra TLB can take a significant amount of area in silicon (see Sec. VII).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ABSTRACTING QUERY OPERATIONS</head><p>Data queries, regardless of the exact type of the data structure, have very similar characteristics. In this section, we summarize these similarities and build an abstract model that can fit different data structures and query algorithms (see Fig. <ref type="figure">2</ref>). Input/output. Each data query operation requires two inputs and one output. The first input is the key to be queried, and the second input is a pointer to the data structure (starting address). Both inputs can be passed to the accelerator through reference, i.e., via pointers to memory locations. The output (result) of the operation is the data being queried, e.g., a node in a linked list. In real applications where the result can be large, a pointer to the actual data is used as the result. Execution pattern. Given the inputs, the query operation starts by accessing an initial location in the data structure. For a tree and a linked list, the query begins at the root node. For a hash table, an offset is generated by hashing the key, and the query operation starts from starting address + offset. For each item in the data structure (either a node in a tree or linked list or an entry in a hash table bucket), the item's key is read out and compared against the queried key. If the keys match, the associated data is returned as the result of this query operation. Otherwise, the query operation will iterate to the next item linked by the current one until a match is found or all potential items have been examined.</p><p>We demonstrate the execution pattern with a linked list query operation shown in List 1. Other data structures share similar flows for query operations with minor modifications.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CFA Model</head><p>Given the common inputs and output, and the formalized execution patterns, we observe that data query operations can be easily represented as CFA. Typically, accelerators use fixed pipelines or functional elements to implement specific algorithms. However, we choose CFA as our model because (1) the steps in data query operations are relatively regular and fixed, which fits the expressiveness of a CFA's state transitions. ( <ref type="formula">2</ref>) CFA enables us to decouple the control logic and the execution units in the hardware design. Different query operations can share the same execution units (e.g., ALUs and Comparators) to amortize the hardware cost and maintain high generality. (3) Compared to the basic finite automaton, CFA is more flexible and enables us to process multiple instances of a single data structure with different parameters. This allows us to implement multiple CFAs in the accelerator to support various data structures and query algorithms efficiently.</p><p>We continue the linked list example to illustrate how CFA can be applied to a query operation in detail (see Fig. <ref type="figure" target="#fig_2">3</ref>). Then we briefly describe how the other data structures differ from the linked list. A query instruction will trigger the idle CFA to issue memory requests for both the queried key and the starting node <ref type="bibr" target="#b0">( 1 )</ref>. Depending on the order in which the results of these two requests are returned, the CFA executes the first comparison via 2 4 or 3 5 . If the comparison result is "mismatch", the CFA goes back to "MEM.N" state and issues the memory request for the next node <ref type="bibr" target="#b5">( 6 )</ref>. The operation continues until a "match" is found, or the next node is NULL. If a "match" is found, the CFA returns query result (the node's value) and becomes idle again via 7 8 .</p><p>Querying a binary search tree or a skip list is similar to linked list, with a slight modification to the comparison state (adding "&gt;" and "&lt;" to know the traversal direction for transition 6 ). For a trie, the next node is indexed by one or more bytes of the queried key. Within a node, we search an index table (e.g., an array) for a match to traverse to the next node. Between "MEM.N" and "COMP", we can insert a state to search the index table. The CFA transits to "DONE" state when we cannot find a match or the node is the leaf node. For a hash table query, one extra state for hash calculation needs to be inserted before 1 . For 1 , the memory node is now a key bucket indexed by the hash value. Also, 6 will load the next entry from the same bucket. With these basic states and transitions, the Fig. <ref type="figure">4</ref>: Format of data structure header (with field size).</p><p>accelerator can even operate on combined data structures such as a hash table of linked lists. To achieve this, we can treat each combined data structure as a unified and unique data structure and assign a unique "subtype" and a dedicated CFA to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Software Usage Model</head><p>As noted above, the accelerator should contain multiple CFA models to support different data structures and query algorithms with the same hardware components. To initiate a specific query operation, the software needs to communicate the specifics of the data structure and the query to the accelerator. This allows the accelerator to use the appropriate CFA and configuration/parameters for the query. For example, the accelerator needs to know the length of the key for comparison. It also needs to know the type of the data structure to invoke the appropriate CFA for the query. These configuration parameters are uniquely specified for each queried data structure. We call this set of configuration parameters the "metadata". We define a single-cacheline (i.e., 64B) header to store the metadata (see Fig. <ref type="figure">4</ref>). This header's fields include the pointer to the data structure, type and subtype (e.g., number of entries in a hash table bucket) of the data structure, the length of the key stored, the size of the entire data structure (for static data structures such as hash table), other flags and reserved bits for future extension. The software is responsible for populating the header properly, and the CFA parses the parameters from it before executing a data query operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. QEI DESIGN A. QUERY Instructions</head><p>To initiate the query operations on QEI and leverage the software abstraction we built in Sec. III, we define an instruction called QUERY. This instruction has two flavors -(1) blocking and (2) non-blocking -which target two distinct use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QUERY_B reg.key/result mem.header_addr</head><p>This instruction sends the header_address and key_address to the accelerator and waits for the result to be returned in the same register as key_address before it can retire. Note that it does not block succeeding instructions from entering core's pipeline (if slots available).</p><p>QUERY_B can be used when there is no independent work available and can be used in small batches, determined by the resource limitations of the accelerator and the core pipeline, to maximize the parallelism. However, once the accelerator resources are filled up, forward progress will be blocked until at least one of the query instruction completes. The OoO core will continue to execute independent instructions until resource limitations are hit due to the incomplete query instructions blocking the head of the OoO window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QUERY_NB impl_reg.header_addr mem.result_addr reg.key</head><p>This flavor of the query instruction has an extra operand indicating the address that the accelerator can write the result to. This instruction retires from the core as soon as the accelerator accepts the request. After the query operation is done, the accelerator writes the result to the designated result_addr provided by the instruction. The non-blocking version does not prevent forward progress, and the program can perform other independent work while the query is being processed by the accelerator, thus maximizing the parallelism without blocking the core resources (e.g., ROB). However, the software is responsible for reading the result from memory and checking the result's completion flags. One way to check for completion is by occasionally polling the result from the output address, but this costs extra cycles. The overhead can be reduced by using wide SIMD SNAPSHOT_READ instruction similar to HALO <ref type="bibr" target="#b78">[79]</ref>. We do not choose hardware interrupt because it requires the OS to handle the interrupt, which is not cheap.</p><p>We show a code snippet that uses the query instructions in buitin format in List 2. As indicated in the snippet, QUERY_NB should be used in algorithms where other independent tasks can be time-multiplexed with the query operations. These instructions can be batched to maximize the parallelism, but care must be taken to prevent overflowing the accelerator resources. An overflow will prevent the accelerator from accepting further query requests and will prevent them from retiring, eventually blocking the machine.</p><p>Update operations (e.g., insert, delete) are still in software. Also, memory concurrency is handled by the software by using appropriate locks and barriers. Since QEI targets read-intensive cases, the "synchronization" happens infrequently. Besides, the overhead of lock and barrier operations can be significantly reduced by using purpose-built hardware mechanisms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b81">82]</ref>, which are orthogonal to the design of QEI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. QEI Microarchitecture</head><p>QEI's goal is to efficiently execute the data query operations of different data structures with a shared pattern (see Sec. II).</p><p>Based on the CFA model we apply in Sec. III, we propose a flexible design that can handle several common data structure queries and can also be extended to support emerging data structures and query algorithms.</p><p>Since we want to leverage memory access parallelism, QEI must support multiple in-flight query operations. This can We depict QEI microarchitecture in Fig. <ref type="figure">5</ref> and describe QEI's three main components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query State Table (QST).</head><p>To hide memory access latency and exploit parallelism, like many other accelerator designs, QEI supports multiple in-flight query operations in parallel. Query State Table stores the current state of all the in-flight queries. Specifically, this information includes the key_address (8B), result_address -valid only for non-blocking queries (8B), type -type of the data structures (1B), state -the current state in the corresponding CFA (1B), data -intermediate data or scratch space (64B), query_mode -blocking or non-blocking (1b), and a ready bit (1b). When inserting a new outstanding query from Query Queue, QST finds the first empty entry and sets the ready bit to 1, and the index of this entry (QST ID) is used for addressing during the state transitions. A completed query releases its QST entry and marks the ready bit to 0. Software is responsible for tracking the availability of the QST slots to make sure that QEI accelerator is not overflown. The intermediate data field is used to stage either a cacheline (i.e., 64B) worth of data from memory or intermediate results from arithmetic/comparison operation. QST acts as a scheduler table Ėvery cycle, it selects a ready entry (in a FIFO manner) to be processed by the CFA Execution Engine and its state to be updated. CFA Execution Engine (CEE). CFA Execution Engine is responsible for processing the entries in the QST and update their state. CEE contains the state transition rules for CFAs of multiple data query flows for various data structures. The rules to be applied to an entry depend on the value of the type field. The CEE processes the intermediate data and updates the state of an entry accordingly, writing back the updated state to the QST. Each state update can be accompanied by one of several different operations on the intermediate data -(1) memory access (in cacheline granularity, i.e., 64B per access)<ref type="foot" target="#foot_3">3</ref> , (2) arithmetic operation, and (3) comparison, which is issued to the appropriate Data Processing Unit. The intermediate data is read from the QST at this time and sent to the processing element along with the QST ID of the query. Once the operation finishes, which can take one or several cycles, the new result is written back to the QST, and the entry is marked ready for further processing. Software can achieve a similar time-multiplexing effect by either using software pipelining techniques or helper threads. However, due to the complex software implementations, they cannot compete with the efficiency of hardware CFAs.</p><p>The state transition rules for several CFAs for querying common data structures are pre-defined in the CEE. However, the CEE is designed as a microcoded control machine (i.e., configurable), and a firmware update <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b82">83]</ref>, with new state transition rules, can be applied to support emerging data structures and query algorithms. In our design, the number of states is limited by the size of the current_state field in the QST and allows for 256 states. This is sufficient for the algorithms we experimented with. However, the field can be made larger if other data structures' queries require more states in their CFAs. Data Processing Unit (DPU). Data Processing Unit consists of multiple processing elements or function units used to perform certain operations on intermediate data. The processing elements include ALUs, comparators, and a hashing unit. For each related state transition in the corresponding CFA, a microoperation can be issued to a data processing element. QEI 's micro-operations include memory access (read), arithmetic and logic operations, and comparison. The micro-operation sequence in a query is defined by its CFA. For example, querying a hash table may require computing a hash function with an input key to produce an output value for further lookup. To do this, CEE first issues a memory micro-operation to fetch the key and a subsequent micro-operation to the hash unit to generate the hashed value. The hashing unit supports common hash functions. Hash functions not supported by the hashing unit can be decomposed into several simple arithmetic operations, such as shift and bit-wise Boolean operations, and calculated using a series of micro-operations. Comparison is another critical step in data query operations for most, if not all, data structures. The comparator elements in the DPU are capable of conducting bit-wise comparisons (&gt;, &lt; or =) of 64-bit values each cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Life of a Query Operation</head><p>A query instruction is fetched like any other instruction by the CPU core and is scheduled by the OoO scheduler to be executed on QEI. Once a query instruction has been issued, depending on whether it is a blocking or non-blocking instruction, the instruction behaves like a load or a store.</p><p>A blocking query operation behaves like a load, occupies space in the Load Queue, and is blocked waiting for data to come back from QEI. Once the query is completed by QEI, the final data is returned to the Load-Store Unit, which wakes up the query instruction to get the data and write it back to the physical register file before it completes and is marked as such in the ROB. From the point of view of the core pipeline, this is very similar to a long-latency load.</p><p>A non-blocking query operation behaves like a store. However, it does not have strict ordering requirements as the software guarantees that ordering violations do not have any functional impact on it. The instruction goes through the Load-Store Unit as a store and is immediately completed upon execution once it is done communicating the required information to QEI. Upon completion of the request by QEI, the result is written back to a memory location.</p><p>Once a query has been issued to QEI, it writes the pointer to the key to the key_address field and the pointer to the header in the data field of an empty QST entry. The pointer to the header is only needed for fetching the metadata and is discarded once processing begins. The state is set to "START", and the ready bit is set. When the CEE begins processing this entry, the first thing it does is issue a read for the metadata and update the state. Once the metadata is ready to be parsed, required fields are extracted from it and written to corresponding QST fields. From here on, the type-specific CFA kicks in. As the CFA goes through various states, it issues micro-operations to the DPU for fetching and processing intermediate data and eventually completes the query. After that, the state of this data query request is changed to "DONE". At this point, the result is returned to the core or designated address via the Result Queue. The corresponding entry is released by setting it to "IDLE" and notifying the core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Exceptions and Interrupts</head><p>When processing a query operation, QEI accesses memory and performs arithmetic operations on intermediate data. This can result in several types of faults and exceptions, e.g., accessing memory that does not belong to the current thread. Once an exception occurs, QEI transitions the query to the "EXCEP-TION" state. For a blocking query, the exception information is sent to the core through the Result Queue. For a non-blocking query, the error code is written to the result memory address so that after polling the designated memory address, the software can find the exception. The entry is then released. Every query that causes an exception eventually gets reported to the core through one of the above two mechanisms, and no special action is required by the application or the core. The software, however, must handle the exception if it needs to recover gracefully.</p><p>Interrupts, including timer interrupts for context switches, are handled by flushing QEI. No special action is required for blocking queries since QEI only holds state for incomplete queries, which will be flushed from the core on an interrupt. If   QST holds any non-blocking queries, an abort code is written to its result memory location so that the application can restart the queries after interrupt handling. This means the flush is not instantaneous and can take a few cycles, depending on the number of non-blocking queries in the QST. To reduce the latency, the writes are done using non-temporal stores, and stores to the same cacheline can coalesce. Technically QEI only needs to wait until all the addresses have been translated for the stores, after which the stores are guaranteed to complete and are handled by the memory element in the DPU. The core cannot start executing interrupt handler code until QEI has been successfully flushed. However, it can start fetching interrupt handler instructions to parallelize some of the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. INTEGRATING QEI INTO A CPU</head><p>As evident from Sec. II-B, the design of QEI needs to strike a delicate balance between generality, latency, design complexity, and cost. Having described how QEI can adapt to various data query operations, we now consider integrating QEI accelerator into the CPU chip, i.e., where to physically place QEI and how to interface it with the other components of the CPU in order to achieve its goal. In this section, we discuss several possibilities (demonstrated in Fig. <ref type="figure" target="#fig_4">6</ref>) and their respective advantages and disadvantages (summarized in Tab. I). More specifically, we try to answer the following question: how should the accelerator be integrated into the CPU?</p><p>We first consider an intuitive scheme <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b79">80]</ref>, where a dedicated functional unit is fully embedded inside the general-purpose core. The accelerator and the core share the MMU and the private caches. Although this design has very low initiation latency, it does not scale well: First, the accelerator resources are private to the core and can only execute queries from this core. Second, the accelerator competes for the data TLB and the private caches and can negatively interfere with the core. Third, the data access latency is not much better than if the query is executed using general-purpose instructions. Due to these limitations, we restrict ourselves to a qualitative evaluation of this design.</p><p>CHA-based Schemes. To solve the scalability issue, the most recent work, HALO <ref type="bibr" target="#b78">[79]</ref>, proposed a CHA-based scheme, depicted in Fig. <ref type="figure" target="#fig_4">6a</ref>. CHA is the LLC controller that is attached to each LLC slice. The CHA-based scheme exploits parallelism by placing accelerators in each CHA and distributing the query requests to these accelerators based on a hash function specific to the NUCA architecture of the particular CPU. This scheme has two major advantages. First, computations and key comparisons are moved closer to the LLC. In many cloud workloads, the data is larger than the core's private caches (i.e., 1MB L2 cache for Intel ® Xeon ® Skylake CPU <ref type="bibr" target="#b77">[78]</ref>). Thus, moving computation near LLC can effectively reduce the memory access latency and private cache pollution <ref type="bibr" target="#b78">[79]</ref>. Second, the accelerators are naturally distributed with LLC slices, which maximizes the parallelism of the query operations. However, CHA itself does not provide address translation capability. HALO's usage scenario assumes that the full data structure can reside within one contiguous page (i.e., huge page). This assumption does not always hold for many cloud workloads <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>, especially when the targeted data structure is a dynamic one, such as linked list. To accommodate querying different kinds of data structures, address translation capability becomes a necessity. With the CHA-based scheme, one may add MMU or TLB into the CHA or use the core's MMU or IOMMU for address translation. Adding MMU to a CHA introduces non-negligible hardware cost and leads to TLB coherence and manageability problems. Using core's MMU or IOMMU adds extra round-trip latency to each memory access and eats into the performance benefits of the accelerator. We show the penalty of these design choices in Sec. VII.</p><p>Device-based Schemes. Another popular way to integrate hardware accelerators into a CPU is as a device attached to a high-speed on-chip or off-chip bus (see Fig. <ref type="figure" target="#fig_4">6b</ref>). Intel ® 's CXL <ref type="bibr" target="#b17">[18]</ref> and IBM's OpenCAPI <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b58">59]</ref> are recent efforts to provide such capability in their proprietary CPUs. This scheme has the least impact on design as it does not change the design of the core or the on-chip network. Different accelerators attached to the same standard device interface can share the memory management hardware (e.g., IOMMU for IO devices) and other interfacing logic. In addition, such standard interfaces can easily support third-party IP blocks. However, a major issue with this scheme is the high request and response latency and limited bandwidth compared to the more integrated designs. The round-trip latency to an OpenCAPI device can be as high as 300 ns <ref type="bibr" target="#b9">[10]</ref>, preventing many latency-sensitive workloads from benefiting from such design. When multiple devices connected to openCAPI are in active use, this latency can be even worse. Since the accelerator is not distributed across the chip, it also creates a single hotspot on the chip, introducing fabric congestion and thermal issues. In our experiments, we observe that each QEI accelerator can saturate as much as 8% of the mesh NoC bandwidth. For a modern CPU with 20 cores or more, if the accelerator is not fully distributed across the chip, it is easy to cause a hotspot. Note that even if the NoC bandwidth is not saturated, a higher bandwidth utilization caused by the hotspot will lead to much longer latency <ref type="bibr" target="#b33">[34]</ref>.</p><p>Alternatively, the accelerator can also be directly connected to the NoC as a heterogeneous core. DASX <ref type="bibr" target="#b47">[48]</ref> is one example. This scheme can keep the access latency of core-accelerator and data-accelerator lower than through the standard device interface. However, this design requires the accelerator to behave like a regular core, which significantly complicates the hardware design. For example, the accelerator has to handle address translation and coherence messages properly on its own, making the hardware design non-trivial <ref type="bibr" target="#b45">[46]</ref>. The accelerator also occupies one NoC stop, which could have been used by a general-purpose core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. QEI Core-Integrated Scheme</head><p>We propose a novel integration scheme for QEI, called "Core-integrated" in the paper depicted in Fig. <ref type="figure" target="#fig_4">6c</ref>, that has the advantages of being close to the core and yet highly scalable.</p><p>The main components of QEI are integrated alongside the core's L2 cache. It shares the memory access hardware units with the L2 cache and uses the L2-TLB, which is typically close to the L2 cache, for address translation. We leverage existing hardware mechanisms in the core without significant changes in their microarchitecture. Since QEI uses the L2 resources, it does not contend for L1 cache and L1 TLB, reducing negative interference. We place the memory-intensive operation -the key comparison -in CHAs to maximize the parallelism and efficiency through near-data computing. We add Comparators in each of the CHA across the chip to maximize the throughput. These CHA-based comparators access the data directly from the LLC, preventing private cache pollution and reducing round-trip latency. Depending on the type of query operation, the key can sometimes be huge. Hence, leaving them in the LLC and doing the comparison in-place can significantly increase throughput and reduce latency.</p><p>The interface between the CEE and Comparators is extended to traverse the on-chip network using remote micro-operations. The CEE calculates the address of the memory location to be compared to the key, translates the address using the L2-TLB, and issues a remote operation to the appropriate Comparator (based on the NUCA hash function) to perform the key comparison and return the result. Note that certain query algorithms might not use the remote comparison feature, and a local comparison in QEI might be sufficient. QEI does fetch cachelines to obtain the next set of pointers in some cases (e.g., linked list query), and a small key comparison can be done in one of the DPU if the key is part of the fetched cacheline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulator</head><p>We implement QEI in Sniper <ref type="bibr" target="#b10">[11]</ref>, a multi-core x86 simulator. We configure the simulator to model a modern Intel ® Skylake-SP server CPU <ref type="bibr" target="#b39">[40]</ref> (see Tab. II). We simulate QEI in five different integration methods, listed below, and compare the results.</p><p>• CHA-TLB. This scheme is similar to HALO described in <ref type="bibr" target="#b78">[79]</ref>. It integrates the accelerator inside each CHA with a dedicated 1024-entry TLB for address translation. • CHA-noTLB. Similar to the first scheme. But this scheme completely leverages the core's MMU for address translation. • Device-direct. This scheme attaches the accelerator directly to the NoC as a special core <ref type="bibr" target="#b47">[48]</ref>. • Device-indirect. It simulates a dedicated accelerator which is connected to the NoC via a standard device interface.</p><p>• Core-integrated. The new QEI scheme proposed in this paper. The QST, CEE, and some of the DPU of the accelerator are placed in the core, close to L2-cache/TLB, while the comparators are distributed in the CHAs. In the Core-integrated, CHA-TLB, and CHA-noTLB schemes, we configure the accelerator to support ten in-flight query operations (i.e., each QST has ten entries), which can keep a decent balance between performance and cost (i.e., 50% ∼ 90% occupancy). For fairness, in Device-Direct and Indirect schemes, we configure the accelerator to support 10×24 (number of cores) in-flight operations.</p><p>We use McPAT <ref type="bibr" target="#b49">[50]</ref> and CACTI <ref type="bibr" target="#b5">[6]</ref> for power and area evaluation in an incremental way. That is, we first configure the CPU in Tab. II and get the baseline power/area. We then add components of the QEI accelerator into the configuration. For components like ALUs and TLBs, we used the default models in the tools. We also change the connection-related configuration, e.g., TLB port counts. We subtract the baseline value from the value with the QEI accelerator, thus get the final value of QEI itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmarks</head><p>DPDK. DPDK <ref type="bibr" target="#b37">[38]</ref> is a popular networking application development library for kernel-bypass network functions. We use the optimized cuckoo hash table library <ref type="bibr" target="#b18">[19]</ref> from DPDK to setup an L3 Forwarding Information Table (FIB) and evaluate its performance. During the evaluation, we configure the hash table to contain various numbers of keys, 16-Bytes in length, to simulate a regular TCP/IP packet header. We also connect multiple hash tables to implement tuple space search algorithm <ref type="bibr" target="#b73">[74]</ref> to show the parallelism of QEI. JVM. Due to the limitations of the simulator, we are not able to directly simulate JVM. Hence we extract OpenJDK <ref type="bibr" target="#b59">[60]</ref>'s serial Mark-and-Sweep garbage collection functionality and set up an independent benchmark for the garbage collection process. We dump a real object tree from running Derby <ref type="bibr" target="#b75">[76]</ref>, a relational database, in SPECjvm2008 <ref type="bibr" target="#b74">[75]</ref> and use it as the input to our benchmark. RocksDB. RocksDB <ref type="bibr" target="#b22">[23]</ref> is a persistent key-value store using Log-Structured Merge-Tree algorithm <ref type="bibr" target="#b60">[61]</ref>. Since QEI targets in-memory and in-cache acceleration, we focus on the in-memory memtables of RocksDB rather than querying the data on the disk. The memtable of RocksDB is a skip list. We first insert 10k items into the database and then do random queries. We use db_bench, the standard performance testing tool for RocksDB, to test with 100B key size and 900B value size for each data item. Snort. Snort <ref type="bibr" target="#b67">[68]</ref> is a popular network IPS. It uses Aho-Corasick (AC) algorithm <ref type="bibr" target="#b1">[2]</ref> for literal matching to detect potential malicious packets. We follow an efficient open-source implementation <ref type="bibr" target="#b32">[33]</ref> to show the speedup of QEI for querying the trie data structure. The dictionary contains around 40K keywords, and we query a 1KB string of characters. FLANN. FLANN <ref type="bibr" target="#b56">[57]</ref> is a library that implements similarity search algorithms widely used in search engines, e.g., searching similar images from a large image database. We run the Locality Sensitive Hashing (LSH) algorithm, which queries a series of hash tables. We use the 100K-item dataset and default parameters for LSH, which are 12 hash tables with 20B key size.</p><p>We identify the query-related snippets in each benchmark as Region-of-Interest (ROI), rewrite these snippets with QEI instructions, run the entire benchmark, and report the performance improvement of such ROIs with QEI. All benchmarks (including baselines) are complied by GCC 5.4 with "O3" optimization and are evaluated in single-thread mode, mostly with default parameters. For each benchmark, we generate queries as quickly and densely as possible and feed them to the benchmarks. This stresses QEI to show peak performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EVALUATION A. Query Operation Speedup</head><p>We show the data query operation speedup of all benchmarks with different schemes in Fig. <ref type="figure">7</ref>. The overall trend is, the Core-integrated scheme and the CHA-based schemes have comparable performance over all benchmarks, while the Device-based schemes sometimes have a significant performance gap compared to the other schemes.</p><p>Not surprisingly, the CHA-TLB scheme achieves the best performance in all benchmarks (i.e., up to 12.7× speedup). The performance gain mainly comes from two aspects. First, data queries are fully distributed to all the accelerators on the CHAs. This is the fully scalable model, as we discussed in previous sections. Second, thanks to the dedicated TLBs, the accelerator can perform address translation locally, without extra round-trips to the core's MMU when TLB-hit. Because of the relatively large TLB size (same as the L2-TLB size), there are few TLB misses in our tests. On the other hand, the CHA-noTLB scheme performs worse than the CHA-TLB because of the extra latency to the core's MMU. However, the performance gap between the two CHA-based schemes is 0.5% ∼ 17.9%, not as much as we initially expected. This is because the parallelism of CHA-based schemes hides such latency to some extent. The Core-integrated scheme can achieve at most 10.4× speedup compared to software baselines. As expected, this scheme enjoys both parallelism and near-data advantages. Keys are compared across multiple CHAs and stay in the LLC. Meanwhile, it leverages the core's L2-TLB for convenient address translation. This reduces the design complexity and cost and eliminates the round-trip latency for address translation in the CHA-noTLB scheme. Since QEI's Query State Context Table is not scaled out of the core, this slightly constrains its parallelism. Thus, the integrated scheme has a 0.9% ∼ 15.0% performance gap compared to the fastest CHA-TLB scheme. However, such a small gap does not defeat the Core-integrated scheme's prominent advantages regarding design complexity and cost (we will show this later), which renders it a more practical solution in real-world CPUs over CHA-based ones.</p><p>Regarding the two Device-based schemes, the performance is worse than the other schemes. This is mainly due to the long access latency involved in these two schemes, which counteracts the benefit of processing multiple in-flight query operations in parallel. For the Device-direct scheme, although it accesses the cache with the latency similar to a regular Fig. <ref type="figure">9</ref>: End-to-end query/packet per second improvement.</p><p>core, the core-accelerator communication overhead is still significant. For the Device-indirect scheme, the cost is even higher since each data access from the accelerator leads to a costly round trip through the device interface. To better understand the impact of the access latency, we also conduct a latency sensitivity study for the Device-indirect scheme and demonstrate it in Fig. <ref type="figure">8</ref>. Here we sweep the accelerator's data access latency, which reflects the overhead of the device interface (including the protocol translation and coherence handling), from 50 cycles to 2000 cycles. We observe a nontrivial performance drop of all workloads when we increase the communication latency. Although the industry keeps improving the throughput and bandwidth of standard interfaces such as OpenCAPI, recent data still shows a much more significant round-trip time of 300ns <ref type="bibr" target="#b9">[10]</ref> comparing to CHA-based or core-integrated model. One way to improve throughput under outstanding latency overhead is to process queries in batch to hide the latency. Although this approach effectively improves throughput <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41]</ref>, it can also lead to much worse average latency and tail latency, as investigated in <ref type="bibr" target="#b4">[5]</ref>. This is not desirable for our targeted latency-sensitive workloads.</p><p>Besides the different integration schemes, the characteristics of the workloads also affect the efficiency of the accelerator. First, the degree of parallelism that QEI can achieve depends on the "density" of the query operations in the workloads. Take RocksDB as an example. The code size of its "seek" loop, where one query operation is conducted, is relatively large. That is, RocksDB executes many other operations (e.g., key's pre-processing, memcpy, and thread management) besides looking up the data structure when process each request. Hence, the core's ROB is filled up pretty quickly since the block version of the query instruction is not retired. In other words, the performance improvement is bounded by the core rather than the accelerator. The core's resource limits the parallelism we can achieve by QEI. Other benchmarks, such as JVM, have a relatively higher query density, enabling the core to issue as many query requests as possible before ROB is filled up. Thus, the effect of parallelism is more prominent.</p><p>Data structures themselves affect the accelerating performance as well. For each query operation of the hash table (e.g., DPDK), the number of memory accesses (namely, header, key, bucket, and key-value pair) is relatively small and fixed compared to other data structures such as skip list. As a result, the processing time of each query is relatively short. In this case, the latency for the core to communicate with the accelerator becomes more prominent. Hence, the Device-based schemes' performance fails to compete with other schemes with much shorter communication latency. For comparison, other data structures such as the tree in JVM and the trie in Snort have much more memory accesses for a single query operation (e.g., 39.9 on average in our JVM benchmark). Moreover, the core to accelerator latency is amortized by the relatively long processing time. Consequently, the performance of the Device-based scheme is closer to that of other schemes. This proves that the Device-based scheme is more suitable for larger kernels that can run for a relatively long time.</p><p>For the full application (i.e., not a library or routine of the program), we also demonstrate the end-to-end query-persecond improvement in Fig. <ref type="figure">9</ref>. It shows that QEI improves end-to-end throughput by 36.2%∼66.7%. Meanwhile, QEI's Core-integrated integration scheme's performance gain is at the same level as the CHA-based schemes'. Note that, since query operations are ubiquitous, just like other accelerators for operations like Malloc <ref type="bibr" target="#b42">[43]</ref>, and garbage collection <ref type="bibr" target="#b54">[55]</ref>, even if the end-to-end performance improvement is not amazingly high, it still helps save a huge number of CPU cycles and thus improve the efficiency and throughput of the data center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Non-blocking Query Evaluation</head><p>We further evaluate the performance benefit of the QUERY_NB instruction. As discussed, some applications limit the parallelism they can benefit from QEI schemes because of their own characteristics. Thus, we evaluate the non-blocking version of query instruction with one representative workload, which demonstrates the ideal use cases of the instruction. Fig. <ref type="figure" target="#fig_1">10</ref> demonstrates the tuple space search results with 5, 10, and 15 tuples based on DPDK's hash library. Tuple space search can be parallelized naturally since the data query to each hash table is independent. For each query, a series of hash tables can be queried concurrently. Usage of the non-blocking query instruction can maximize such parallelism. In this test, the software polls the results every 32 keys. With the non-blocking instruction, it effectively sends 32×(tuple_count) requests in parallel to the accelerator. The results show that as the number of tuples  increases, the speedup also increases due to the increasing parallelism. We also notice that the performance of the Device-based schemes becomes much better than using the blocking instruction. This is because the performance degradation caused by the long access latency of both core to accelerator and accelerator to data is amortized by executing many in-flight operations. This shows that to compete with our proposed Core-integrated scheme and CHA-based scheme, the application has to generate hundreds of requests simultaneously to fully utilize the parallelism of the accelerator. As mentioned in Sec. VI, the QEI of Core-integrated scheme only processes ten in-flight queries concurrently in the integrated CFA. This limited the parallelism even though the comparators can still process key comparisons in parallel. Still, the Core-integrated scheme has a significant latency advantage, as we mentioned previously, which makes this scheme competitive when the tuple count is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Instruction Count Reduction</head><p>We demonstrate the results of the number of dynamic instructions executed by the core in ROIs in Fig. <ref type="figure" target="#fig_1">11</ref>. As expected, with QEI, a significant amount of dynamic instructions in the ROIs can be eliminated. As mentioned in Sec. II-A and <ref type="bibr" target="#b41">[42]</ref>, the performance of cloud workloads is frequently bounded by core's frontend. Thus, reducing the dynamic instruction count can reduce the frontend pressure significantly. This, in turn, improves the efficiency of the whole application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Area and Power Results</head><p>For the area and static power comparison, we compare three configurations. They are (1) QEI-10, which can handle ten in-flight queries simultaneously to represent the CHA and Core-integrated scheme, (2) QEI-10+TLB to handle ten in-flight queries simultaneously plus a dedicated TLB to represent CHA-TLB schemes, and (3) QEI-240 that can handle 240 in-flight queries to represent the two Device-based schemes. It is worth noting that for the CHA-based and Core-integrated schemes, the data is a single accelerator's area and power, while for the Device-based schemes, it is the total area and power of the centralized accelerator.</p><p>We show the area and static power results of these three configurations in Tab. III. In terms of area cost, the QEI-10 configuration only occupies less than 0.2mm 2 without TLB and ∼0.57mm 2 with TLB. The extra TLB incurs significant overhead here, which shows that although CHA-TLB achieves better performance, the hardware area budget limits its practicality. On the other hand, the larger device model (QEI-240), which enjoys a more relaxed design budget, takes up ∼1mm 2 . Considering the size of a typical modern CPU core tile can be around 18mm 2 <ref type="bibr" target="#b77">[78]</ref>, the total area overhead is negligible. Similarly, the static power consumption of QEI is also small compared with the total thermal design power of a CPU chip, which can easily exceed 100W. Besides the raw power and area comparison, there are some other considerations of design trade-offs. For example, the Devicedirect accelerator occupies one NoC stop, which can have been used by a regular core tile. With the Device-direct scheme, we need to remove one core from the CPU chip. This is the hidden cost that can not be shown in the area and power comparison. Other design complexities, such as the logic to make the accelerator behave like a core (i.e., to answer coherence message properly, to manage address mapping), are not easy to be evaluated but are essential factors for accelerator design.</p><p>We show the normalized average dynamic power consumption per query in Fig. <ref type="figure" target="#fig_7">12</ref>. From the results, the accelerators can reduce more than 60% dynamic power overhead compared to the software baseline. This power reduction comes from both the reduced frontend overhead and private cache accesses, which take up a considerable portion of the whole core activity.</p><p>With such power and area efficiency, and considering that QEI is integrated inside the CPU chip, which does not require extra cost for device purchase and maintenance, QEI can largely reduce the server's operational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. RELATED WORK</head><p>On-chip accelerator for fine-grained operations is not a brandnew concept. While prior works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71]</ref> focus on specific operations/applications, QEI is more generic for diverse data structures. We achieve this by abstracting the data query operations and map them to the accelerator's CFA. Minnow <ref type="bibr" target="#b79">[80]</ref> also claims flexibility/programmability, but it does not clearly demonstrate the mapping between its model and software algorithms/routines, and thus its generality.</p><p>The most relevant works to ours are <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>, which also do data query/lookup/analytic accelerations. Each integration scheme has pros and cons, as we discussed in Sec. V. QEI is different from these works regarding both hardware design and integration scheme. No existing accelerator takes such a hybrid scheme and balances every aspect of a design.</p><p>Fully integrated designs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b79">80]</ref> tightly couple the accelerator with the CPU core. Although the latency is minimum, they do not address the scalability and private cache pollution issues. Dedicated accelerator designs such as DASX <ref type="bibr" target="#b47">[48]</ref> have relatively long latency comparing to core integrated design. Thus it is best for applications that are not latency-sensitive and can batch process many requests each time. The design complexity is also another concern since the accelerator needs extra logic to behave like a heterogeneous core. It also occupies an NoC stop, which could have been used by an additional general-purpose core. Other accelerators connected to standard device interfaces will have even longer access latency, which leads to sub-optimal performance. Both Device-based schemes can create hotspots on the CPU chip and congestion in the NoC, which further increases the latency.</p><p>A recently proposed data query accelerator is HALO <ref type="bibr" target="#b78">[79]</ref>. HALO targets on a specific data structure, hash table. It places accelerators inside each LLC slice to get the benefit of near-data computing and parallelism. Some near-memory solutions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53]</ref> have also been proposed for data query/analytic operations. These approaches are limited to particular workloads, and the accelerators have to be able to do address translation either by hardware or software mechanisms. For hardware solutions, they need to have their own MMU or go through the core's MMU or IOMMU. This increases hardware cost and complexity or degrades the performance because of the extra round-trip latency. For software solutions, they require the application to guarantee that the entire data structure can reside in the same page, or a new memory mapping method has to be introduced, which is not easy for many existing workloads and OSs. Compared to these solutions, QEI keeps a decent balance among performance, design complexity, cost, and feasibility. In other words, QEI's generality and practicality distinguish it from other similar works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>We propose QEI, a generic, integrated, and efficient accelerator design for speeding up fine-grained query operations in a diverse set of data structures for cloud infrastructures and applications. The generality of QEI comes from abstracting the various data query operations. The CFA model, which we map the abstraction to, guarantees efficient execution by simple hardware. We then propose a novel scheme for integrating QEI and evaluate it against other schemes. Our results with five representative cloud workloads show that QEI can achieve 6.5×∼11.2× performance improvement in various scenarios at low hardware cost and complexity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Percentage of data query operation among total execution time in different workloads (data structures).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: CFA of linked list query operations. State transition format: {trigger event, condition(s), action(s)}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Different integration schemes for QEI.TABLE I: Comparison of different integration schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Fig. 7: Speedup of lookup operations in different workloads with different schemes.</figDesc><graphic url="image-6.png" coords="9,335.16,194.08,204.17,56.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Fig. 10: Speedup of query operations in tuple space search with different schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: QEI's average dynamic power consumption compared to the software baseline (in percentage).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>:</head><label></label><figDesc>Abstraction of query operations.</figDesc><table><row><cell>struct node{ void* _key; void* _value; struct node *_next; }; ... void* query_linkedlist (void* key, struct node *root){ struct node *current = root; void void* query_linkedlist * query_linkedlist ( (void void* key, * key, struct struct node *root){ node *root){ d struct struct node *current = root; node *current = root; while(current != NULL){ if(!memcmp(current-&gt;_key, \ key, KEY_LENGTH)){ return current-&gt;_value; } current = current-&gt;_next; } return NULL; } if if(!memcmp(current-&gt;_key, \ (!memcmp(current-&gt;_key, \ key, KEY_LENGTH)){ p key, KEY_LENGTH)){ l } } return return NULL; NULL NULL; } } } current = current-&gt;_next; current = current-&gt;_next; } return return current-&gt;_value; current-&gt;_value;</cell><cell>Key &amp; Starting Addr Item (node) Comparison Result Item (node) Result</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>An example of QEI instructions usage.</figDesc><table><row><cell>inline static void* query_b(void* key, void* header){ void* result = key; __qei_query_b(header, result); return result; }</cell></row><row><cell>inline static void query_nb(void** keys, void* header, void** results){ for(int i = 0; i &lt; BATCH_SIZE; i ++){ __qei_query_nb(header, keys, results); keys ++; results ++; } }</cell></row><row><cell>inline static void polling_result(void** results){ __m512 zmm; while(1){ __qei_snapshot_read(results, zmm); if(_mm512_cmpeq_epi64_mask(zmm, 0) != 1){ break; } } }</cell></row><row><cell>List 2:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>QEI accelerator microarchitecture. be done in one of two ways -(1) parallel CFAs by naively replicating all the hardware for the CFA as many times as the number of queries, or (2) pipelined CFAs by only working on one query operation at a time but pipelining multiple queries one after the other. QEI chooses pipelined CFAs but in an OoO fashion. Typically, each query operation requires a series of memory accesses, and the latency for these memory accesses is quite high. During these memory accesses, the CFA for that operation is stuck in the same state until the data comes back and can be processed to determine the next state. Instead of letting a single query monopolize the CFA</figDesc><table><row><cell></cell><cell cols="3">Query State Table</cell><cell></cell><cell></cell><cell></cell><cell>TLB and</cell></row><row><cell cols="2">key_addr</cell><cell cols="2">type state data</cell><cell>…</cell><cell>State Update</cell><cell>Result</cell><cell>Queue</cell><cell>Memory Access</cell></row><row><cell>Query</cell><cell>Queue</cell><cell></cell><cell cols="2">Data Processing ALUs Unit</cell><cell cols="3">CFA Execution Engine Data Structure 1 Data Structure N</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Comparators</cell><cell></cell><cell>…</cell></row><row><cell></cell><cell cols="2">Memory Result</cell><cell>= …</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Fig. 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>hardware, in QEI, the current state of the operation is saved in a table, and the hardware is allowed to work on another query whose data might already be ready. In this way, we time multiplex the hardware to process multiple in-flight queries.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE I :</head><label>I</label><figDesc>Comparison of different integration schemes.</figDesc><table><row><cell>Scheme</cell><cell>Accelerator-Core Latency (cycle)</cell><cell>Accelerator-Data Latency (cycle)</cell><cell>Hardware Cost</cell><cell>Mem HW</cell><cell>Mgmt</cell><cell>NoC Hot Spot</cell><cell>Private $ Pollution</cell><cell>Scalability</cell></row><row><cell>CHA-based</cell><cell>40 ∼ 60</cell><cell>10 ∼ 50</cell><cell>Low</cell><cell cols="2">Dedicated/ Shared</cell><cell>No</cell><cell>No</cell><cell>Good</cell></row><row><cell>Device-based</cell><cell>100 ∼ 500</cell><cell>100 ∼ 500</cell><cell>Medium/High</cell><cell cols="2">Dedicated</cell><cell>Yes</cell><cell>No</cell><cell>Medium</cell></row><row><cell>Core-integrated</cell><cell>10 ∼ 25</cell><cell>20 ∼ 40</cell><cell>Low</cell><cell>Shared</cell><cell></cell><cell>No</cell><cell>No</cell><cell>Good</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II :</head><label>II</label><figDesc>Simulated CPU model configuration.</figDesc><table><row><cell>Item</cell><cell>Configuration</cell></row><row><cell>Cores</cell><cell>24 OoO cores, 2.5GHz</cell></row><row><cell></cell><cell>8-way 32KB L1D/L1I,</cell></row><row><cell>Caches</cell><cell>16-way 1MB L2,</cell></row><row><cell></cell><cell>11-way 33MB shared LLC (split to 24 slices)</cell></row><row><cell>LQ/SQ/ROB Entries</cell><cell>72/56/224</cell></row><row><cell>Memory Controllers</cell><cell>6 DDR4-2666 channels, 19.2GB/s per channel, 4 8-chip DIMMs per channel</cell></row><row><cell></cell><cell>five ALUs per DPU</cell></row><row><cell>QEI Accelerator</cell><cell>two comparators per CHA for CHA-based/Core-integrated</cell></row><row><cell></cell><cell>ten comparators per DPU for Device-based</cell></row><row><cell>NoC</cell><cell>Mesh</cell></row><row><cell>Process</cell><cell>22nm</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE III :</head><label>III</label><figDesc>Area and static power results of QEI.</figDesc><table><row><cell></cell><cell cols="2">Configuration</cell><cell>Area/mm 2</cell><cell>Static Power/mW</cell></row><row><cell></cell><cell cols="2">QEI-10</cell><cell>0.1752</cell><cell>10.8984</cell></row><row><cell></cell><cell cols="2">QEI-10+TLB</cell><cell>0.5730</cell><cell>30.9049</cell></row><row><cell></cell><cell cols="2">QEI-240</cell><cell>1.0901</cell><cell>20.8764</cell></row><row><cell></cell><cell>50%</cell><cell></cell></row><row><cell>Percentage</cell><cell>10% 20% 30% 40%</cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell></row><row><cell></cell><cell>CHA-TLB</cell><cell cols="2">CHA-noTLB</cell><cell>Device-direct Device-indirect Core-integrated</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We do not strictly distinguish tree and graph since they share similar operations from the hardware point of view. For graph-specific issues, such as circle handling, programmers need to properly deal with them in the software.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">Authorized licensed use limited to: University of Illinois. Downloaded on September 29,2021 at 19:31:40 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">These numbers show the percentage of unused pipeline slots caused by either frontend or backend issues.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">QEI is backed by the regular cacheable memory in the coherence domain, following the write-back policy. We assume weak ordering for both the memory accesses issued by QEI and the query requests sent to QEI by the core. If strong ordering is required for software's update operations, lock/mfence instructions should be applied manually in the software.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank David Koufaty, Ryan Carlson, Andrew Herdrich, Ravishankar Iyer, Rajesh Sankaran, as well as the anonymous reviewers for their insightful and helpful feedback. This research is supported by National Science Foundation (funding No. CNS-1705047) and Intel Corporation's Academic research funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Garbage collection and local variable type-precision and liveness in Java virtual machines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Agesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Detlefs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation (PLDI&apos;98)</title>
				<meeting>the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation (PLDI&apos;98)<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">Jun. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient string matching: An aid to bibliographic search</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Corasick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ASPEN: A scalable in-SRAM architecture for pushdown automata</title>
		<author>
			<persName><forename type="first">K</forename><surname>Angstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramaniyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sadredini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;18)</title>
				<meeting>the 51st IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;18)<address><addrLine>Fukuoka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Poptrie: A compressed trie with population count for fast and scalable software IP routing table lookup</title>
		<author>
			<persName><forename type="first">H</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGCOMM Conference (SIGCOMM&apos;15)</title>
				<meeting>the 2015 ACM SIGCOMM Conference (SIGCOMM&apos;15)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">M3x: Autonomous accelerators via context-enabled fast-path communication</title>
		<author>
			<persName><forename type="first">N</forename><surname>Asmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roitzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Härtig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2019 USENIX Annual Technical Conference (ATC&apos;19)</title>
				<meeting>2019 USENIX Annual Technical Conference (ATC&apos;19)<address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CACTI 7: New tools for interconnect exploration in innovative off-chip memories</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attack of the killer microseconds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient virtual memory for big memory servers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;13)</title>
				<meeting>the 40th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;13)<address><addrLine>Tel-Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shredder: GPU-accelerated incremental storage and computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bhatotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th USENIX Conference on File and Storage Technologies (FAST&apos;12)</title>
				<meeting>17th USENIX Conference on File and Storage Technologies (FAST&apos;12)<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Accelerating-Flash-Memory-with-the-High-Performance-Low-Latency-OpenCAPI-Interface-Final-3-21-19</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cantle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Byers</surname></persName>
		</author>
		<ptr target="https://www.opencompute.org/files/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Accelerating flash memory with the high performance, low latency, OpenCAPI interface. pdf</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An evaluation of high-level mechanistic core models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A cloud-scale acceleration architecture</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;16)</title>
				<meeting>the 49th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;16)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SLIDE: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Medini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Farwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gobriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Conference on Machine Learning ans Systems (MLSys&apos;20)</title>
				<meeting>the 3rd Conference on Machine Learning ans Systems (MLSys&apos;20)<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Concurrent data structures with near-data-processing: An architecture-aware implementation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreshet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Bahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM Symposium on Parallelism in Algorithms and Architectures (SPAA&apos;19)</title>
				<meeting>the 31st ACM Symposium on Parallelism in Algorithms and Architectures (SPAA&apos;19)<address><addrLine>Phoenix, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hybrid edge-cloud architecture for reducing on-demand gaming latency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards faster string matching for intrusion detection or exceeding the speed of Snort</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Coit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staniford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcalerney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of DARPA Information Survivability Conference and Exposition II (DISCEX&apos;01)</title>
				<meeting>eeding of DARPA Information Survivability Conference and Exposition II (DISCEX&apos;01)<address><addrLine>Anaheim, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geeps: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Systems (EuroSys&apos;16)</title>
				<meeting>the 11th European Conference on Computer Systems (EuroSys&apos;16)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Compute Express Link (CXL)</title>
		<ptr target="https://www.computeexpresslink.org" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DPDK programmer&apos;s guide: Hash library</title>
		<ptr target="http://doc.dpdk.org/guides/prog_guide/hash_lib.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>DPDK</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Mondrian data engine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pnevmatikatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;17)</title>
				<meeting>the 44th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;17)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maglev: A fast and reliable software network load balancer</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Eisenbud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Contavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kononov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mann-Hielscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cilingiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheyney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hosein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;16)</title>
				<meeting>13th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;16)<address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dark silicon and the end of multicore scaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Amant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;11)</title>
				<meeting>the 38th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;11)<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">RocksDB: A persistent key-value store for fast storage environments</title>
		<author>
			<persName><surname>Facebook</surname></persName>
		</author>
		<ptr target="https://rocksdb.org" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MemC3: Compact and concurrent MemCache with dumber caching and smarter hashing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;10)</title>
				<meeting>the 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;10)<address><addrLine>Lombard, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adopting OpenCAPI for high bandwidth database accelerators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Mulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Hofstee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hidders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Heterogeneous High-performance Reconfigurable Computing (H2RC&apos;17)</title>
				<meeting>the 3rd International Workshop on Heterogeneous High-performance Reconfigurable Computing (H2RC&apos;17)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;12)</title>
				<meeting>the 17th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;12)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Applying fast string matching to intrusion detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A configurable cloud-scale DNN processor for real-time AI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alkalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sapek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;18)</title>
				<meeting>the 45th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;18)<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A framework for understanding vulnerabilities in firewalls using a dataflow model of firewall internals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frantzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kerschbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical near-data processing for in-memory analytics frameworks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Parallel Architecture and Compilation (PACT&apos;15)</title>
				<meeting>the 2015 International Conference on Parallel Architecture and Compilation (PACT&apos;15)<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">APUNet: Revitalizing GPU as packet processing accelerator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;17)</title>
				<meeting>the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;17)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HARE: Hardware accelerator for regular expressions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gogte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>D'antoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;16)</title>
				<meeting>the 49th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;16)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">AC algorithm implementation</title>
		<ptr target="https://github.com/hankcs/AhoCorasickDoubleArrayTrie" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Congestion control for network on chip</title>
		<ptr target="http://engineering.nyu.edu/highspeed/research/past-projects/congestion-control-network-chip" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>High Speed Networking Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accelerating pointer chasing in 3D-stacked memory: Challenges, mechanisms, evaluation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boroumand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE 34th International Conference on Computer Design (ICCD&apos;16)</title>
				<meeting>the 2016 IEEE 34th International Conference on Computer Design (ICCD&apos;16)<address><addrLine>Scottsdale, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards efficient server architecture for virtualized network function deployment: Implications and implementations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;16)</title>
				<meeting>the 49th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;16)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A NUCA substrate for flexible CMP cache sharing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Supercomputing (SC&apos;05)</title>
				<meeting>the 19th International Conference on Supercomputing (SC&apos;05)<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Intel Corporation</title>
		<ptr target="https://www.dpdk.org" />
	</analytic>
	<monogr>
		<title level="m">Data Plane Development Kit (DPDK)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Intel® VTune™ Performance Ayalyzer</title>
		<ptr target="https://software.intel.com/en-us/intel-vtune-amplifier-xe" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Intel Corporation</title>
		<ptr target="https://ark.intel.com/products/120501/" />
	</analytic>
	<monogr>
		<title level="m">Intel-Xeon-Platinum-8160-Processor-33M-Cache-2_10-GHz</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Intel® Xeon® Platinum 8160 Processor</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Raising the bar for using GPUs in software packet processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;15)</title>
				<meeting>the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;15)<address><addrLine>Okaland, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Profiling a warehousescale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;15)</title>
				<meeting>the 42nd IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;15)<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mallacc: Accelerating memory allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)<address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An adaptive, nonuniform cache structure for wire-delay dominated on-chip caches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;02)</title>
				<meeting>the 10th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;02)<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meet the walkers: Accelerating index traversals for in-memory databases</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;13)</title>
				<meeting>the 46th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;13)<address><addrLine>Davis, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting the complexity of hardware cache coherence and some implications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Komuravelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reverse engineering x86 processor microcode</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kollenda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fyrbiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gawlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 26th USENIX Security Symposium (USENIX Security&apos;17)</title>
				<meeting>26th USENIX Security Symposium (USENIX Security&apos;17)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DASX: Hardware accelerator for software data structures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Supercomputing (ISC&apos;15)</title>
				<meeting>the 29th ACM International Conference on Supercomputing (ISC&apos;15)<address><addrLine>Newport Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">KV-direct: High-performance in-memory key-value store with programmable NIC</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP&apos;17)</title>
				<meeting>the 26th Symposium on Operating Systems Principles (SOSP&apos;17)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;09)</title>
				<meeting>the 42nd IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;09)<address><addrLine>New York City, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MiSAR: Minimalistic synchronization accelerator with resource overflow management</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prvulovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;15)</title>
				<meeting>the 42nd IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;15)<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Architectural support for efficient large-scale automata processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kayiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;18)</title>
				<meeting>the 51st IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;18)<address><addrLine>Fukuoka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Near memory key/value lookup acceleration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gokhale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Symposium on Memory Systems (MEMSYS&apos;17)</title>
				<meeting>the 3rd International Symposium on Memory Systems (MEMSYS&apos;17)<address><addrLine>Alexandria, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Livia: Data-centric computing throughout the memory hierarchy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lockerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</title>
				<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</meeting>
		<imprint>
			<publisher>Virtual Event</publisher>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A hardware accelerator for tracing garbage collection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;18)</title>
				<meeting>the 45th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;18)<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ClickOS and the art of network function virtualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raiciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bifulco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)</title>
				<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">FLANN -Fast library for approximate nearest neighbors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="http://www.cs.ubc.ca/research/flann/#publications" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding PCIe performance for end host networking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Audzevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>López-Buedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGCOMM Conference (SIGCOMM&apos;18)</title>
				<meeting>the 2018 ACM SIGCOMM Conference (SIGCOMM&apos;18)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Opencapi</forename><surname>Consortium</surname></persName>
		</author>
		<author>
			<persName><surname>Opencapi</surname></persName>
		</author>
		<ptr target="https://opencapi.org" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Oracle Corporation</title>
		<ptr target="https://openjdk.java.net" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>OpenJDK</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The log-structured merge-tree (LSM-tree)</title>
		<author>
			<persName><forename type="first">P</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>O'neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scale-out acceleration for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Olds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;17)</title>
				<meeting>the 50th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;17)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The design and implementation of Open vSwitch</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pettit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koponen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rajahalme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shelar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Amidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Casado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;15)</title>
				<meeting>the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;15)<address><addrLine>Okaland, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Optimus Prime: Accelerating data transformation in servers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pourhabibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kassir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20), Virtual Event</title>
				<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20), Virtual Event</meeting>
		<imprint>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Skip lists: A probabilistic alternative to balanced trees</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A reconfigurable fabric for accelerating large-scale datacenter services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Constantinides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;14)</title>
				<meeting>the 41st IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;14)<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Modified LC-trie based efficient routing lookup</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of 10th IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunications Systems</title>
				<meeting>eeding of 10th IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunications Systems<address><addrLine>Fort Worth, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Snort: Lightweight intrusion detection for networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Systems Administration Conference (LISA&apos;99)</title>
				<meeting>the 13th Systems Administration Conference (LISA&apos;99)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">It&apos;s time for low latency</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on Hot Topics in Operating Systems (HotOS&apos;13)</title>
				<meeting>the 13th USENIX Conference on Hot Topics in Operating Systems (HotOS&apos;13)<address><addrLine>Napa, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">eAP: A scalable and efficient in-memory accelerator for automata processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sadredini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd International Symposium on Microarchitecture (MICRO&apos;19)</title>
				<meeting>the 52nd International Symposium on Microarchitecture (MICRO&apos;19)<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Accelerating business analytics applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Salapura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nagpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Symposium on High Performance Computer Architecture (HPCA&apos;12)</title>
				<meeting>the 18th International Symposium on High Performance Computer Architecture (HPCA&apos;12)<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Runtime measurements in the cloud: Observing, analyzing, and reducing variance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dittrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Quiané-Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 International Conference on Very Large Data Bases (VLDB&apos;10)</title>
				<meeting>the 2010 International Conference on Very Large Data Bases (VLDB&apos;10)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Scalable and sustainable deep learning via randomized hashing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;17)</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;17)<address><addrLine>Halifax, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Packet classification using tuple space search</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM SIGCOMM Conference (SIGCOMM&apos;99)</title>
				<meeting>the 1999 ACM SIGCOMM Conference (SIGCOMM&apos;99)<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08">Aug. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="https://www.spec.org/jvm2008" />
		<imprint>
			<date type="published" when="2020">SPECjvm2008. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Apache derby</title>
		<author>
			<persName><forename type="first">The</forename><surname>Apache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Project</surname></persName>
		</author>
		<ptr target="https://db.apache.org/derby/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Architectural support for fair reader-writer locking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vallejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beivide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vallejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Unsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 43rd IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;10)</title>
				<meeting>the 2010 43rd IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;10)<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Skylake (server) -Microarchitectures -Intel</title>
		<author>
			<persName><surname>Wikichip</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(server" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">HALO: Accelerating flow classification for scalable packet processing in NFV</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;19)</title>
				<meeting>the 46th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;19)<address><addrLine>Pheoeix, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Minnow: Lightweight offload engines for worklist management and worklist-directed prefetching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)<address><addrLine>Williamsburg, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Leveraging caches to accelerate hash tables and memoization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;52)</title>
				<meeting>the 52nd IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;52)<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Synchronization state buffer: Supporting efficient fine-grain synchronization on many-core architectures</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Sreedhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;07)</title>
				<meeting>the 34th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Methods and systems for microcode patching</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">United States Patent</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2012-10-23">Oct. 23, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
