<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POG: Personalized Outfit Generation for Fashion Recommendation at Alibaba iFashion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Pfadler</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Binqiang</forename><forename type="middle">2019</forename><surname>Zhao</surname></persName>
							<email>binqiang.zhao@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">POG: Personalized Outfit Generation for Fashion Recommendation at Alibaba iFashion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330652</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Recommender systems</term>
					<term>Multimedia content creation</term>
					<term>Fashion Outfit Generation</term>
					<term>Fashion Outfit Recommendation</term>
					<term>Deep Learning</term>
					<term>Transformer</term>
					<term>Self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Increasing demand for fashion recommendation raises a lot of challenges for online shopping platforms and fashion communities. In particular, there exist two requirements for fashion outfit recommendation: the Compatibility of the generated fashion outfits, and the Personalization in the recommendation process. In this paper, we demonstrate these two requirements can be satisfied via building a bridge between outfit generation and recommendation. Through large data analysis, we observe that people have similar tastes in individual items and outfits. Therefore, we propose a Personalized Outfit Generation (POG) model, which connects user preferences regarding individual items and outfits with Transformer architecture. Extensive offline and online experiments provide strong quantitative evidence that our method outperforms alternative methods regarding both compatibility and personalization metrics. Furthermore, we deploy POG on a platform named Dida in Alibaba to generate personalized outfits for the users of the online application iFashion.</p><p>This work represents a first step towards an industrial-scale fashion outfit generation and recommendation solution, which goes beyond generating outfits based on explicit queries, or merely recommending from existing outfit pools. As part of this work, we release a large-scale dataset consisting of 1.01 million outfits with rich context information, and 0.28 billion user click actions from 3.57 million users. To the best of our knowledge, this dataset is the largest, publicly available, fashion related dataset, and the first to provide user behaviors relating to both outfits and fashion items.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Faced with seemingly abundant choices of ever changing styles, fashion outfit recommendation becomes more and more important for modern consumers and has thus attracted interest from the online retail industry. Fashion outfit is a set of fashion items, which appears both visually compatible and functionally irredundant <ref type="bibr" target="#b2">[3]</ref> (see Figure <ref type="figure" target="#fig_0">1</ref> as an example). Compared to traditional item recommendation, fashion outfit recommendation involves a remarkably creative outfit generation process, which requires both innovation and characteristic. Therefore, this task is usually performed by fashion experts, and becomes popular in numerous online fashion communities, such as Lookbook 1 and Chictopia 2 . In Taobao, the largest online consumer-to-consumer platform in China, a new application iFashion is created to support fashion outfit recommendation. Approximately 1.5 million content creators were actively supporting Taobao as of March 31, 2018 3 . Still, the great gap between limited human labor and ever-growing market demands a complementary or even substitution of manual work. To alleviate this problem, we aim to assist the generation and recommendation process in iFashion.</p><p>There exist two requirements in fashion outfit generation and recommendation: 1) the Compatibility of the generated fashion outfits, 2) the Personalization in the recommendation process. Compatibility is a measurement of how harmonious a set of items is. Early studies mainly focus on learning compatibility metric between pairwise items <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b30">30]</ref>, or predicting the popularity of an outfit <ref type="bibr" target="#b11">[11]</ref>. Some recent works attempt to generate outfits by modeling an outfit as an ordered sequence of items <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">18]</ref>. However, it is not reasonable because shuffling items in the outfit should make no difference on its compatibility. Personalization represents how the recommendations meet users' personal fashion tastes. In recent works, personalization is achieved relying on explicit input (e.g., image or text) provided by the user <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b18">18]</ref>. This type of fashion generation works more like search than recommendation since it needs explicit user queries. On the other hand, classic Collaborative  Filtering (CF) methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b22">22]</ref> in recommendation mostly focus on recommending individual items rather than outfits.</p><p>In this work, we collect 1.21 billion user clicks on 4.68 million items and 192 thousand outfits from 5.54 million users in iFashion. As shown in Table <ref type="table" target="#tab_0">1</ref>, 81.3% of the clicked outfits contain the items with the same categories which have been appeared in the latest 50 clicked items from the same user. We observe that the brand, style, and pattern of the items in users' clicked outfits also have high probabilities to appear in users' latest clicked items(we choose the latest 50 items for each user). Figure <ref type="figure" target="#fig_1">2</ref> illustrates three example users with their behaviors on items and outfits. All these results show that users tend to keep similar tastes in individual items and outfits. We find that although fashion outfit generation and recommendation have been studied intensively in recent years, existing works usually study these two requirements separately.</p><p>Therefore, we attempt to build the bridge between fashion outfit generation and recommendation in a real-world application with millions of users. More specifically, we generate personalized outfits by capturing users' interests and tastes from their historical interactions on fashion items. For the Compatibility requirement, we propose a Fashion Outfit Model (FOM) by learning the compatibilities between each item and all the other items within the outfit. Each item should have different weighted interactions to the other items in the outfit. Thus, we set up a masked item prediction task based on the self-attention mechanism <ref type="bibr" target="#b29">[29]</ref>, which masks one item at a time in the outfit, and predicts the masked item based on the context from other items in the outfit. For the Personalization requirement, by integrating user preference into the pre-trained FOM, we propose a Personalized Outfit Generation (POG) model, which can generate compatible and personalized outfits based on users' recent behaviors. Specifically, POG uses a Transformer encoder-decoder architecture <ref type="bibr" target="#b29">[29]</ref> to model both signals from user preference and outfit compatibility. To the best of our knowledge, this is the first study to generate personalized outfits based on users' historical behaviors with an encoder-decoder framework. Last, we develop a platform named Dida, where POG has been deployed, to assist outfit generation and recommendation in large-scale online application iFashion.</p><p>The contributions of this work are summarized as follows:</p><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Fashion is an important application domain of computer vision and multimedia. Much research effort has been made in this domain, focusing on fashion image retrieval <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">32]</ref>, clothing recognition <ref type="bibr" target="#b16">[16]</ref>, clothing parsing <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b33">33]</ref>, attribute learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">15]</ref>, outfit compatibility <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30]</ref>, and fashion recommendation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">15]</ref>. The goal of this work is to compose personalized fashion outfits automatically based on user behaviors, we hence focus on the research areas of outfit generation and recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fashion Outfit Generation</head><p>Methods for fashion outfit generation usually fall within one of two categories. Methods in the first category focus on calculating a pairwise compatibility metric <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b30">30]</ref>. McAuley et al. extract visual features to model human visual preference for a pair of items of the Amazon co-purchase dataset <ref type="bibr" target="#b17">[17]</ref>. Siameses network <ref type="bibr" target="#b30">[30]</ref> estimates pairwise compatibility based on co-occurrence in largescale user behavior data. Methods belonging to the second category, such as presented in <ref type="bibr" target="#b11">[11]</ref> and <ref type="bibr" target="#b2">[3]</ref>, are based on modeling a fashion outfit as a set or an ordered sequence. Li et al. deploy an end-to-end deep learning system which can classify a given outfit as popular or unpopular <ref type="bibr" target="#b11">[11]</ref>. Han et al. train a bidirectional LSTM model to sequentially generate outfits <ref type="bibr" target="#b2">[3]</ref>. These methods generally use simple pooling of item vectors to represent an outfit, or rely heavily on the order of the outfit items. We note that methods belonging to either category hardly consider all the interactions among the items in an outfit. Besides, it is unreasonable to regard an outfit as an ordered sequence, because shuffling items in the outfit should make no difference on its compatibility. We try to explicitly incorporate this into our modeling architecture by requiring that each item should have different interaction weights with respect to other items in one outfit. For example, a "red shirt" should have a higher interaction weight with "blue jeans", but a smaller weight with a pair of "white gloves".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fashion Outfit Recommendation</head><p>Early works on recommendation typically use collaborative filtering to model users' preferences based on their behavior histories <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b22">22]</ref>. However, previous works mostly restrict attention to recommending individual items. There are a few approaches for recommending the whole fashion outfits. Si Liu et al. <ref type="bibr" target="#b15">[15]</ref> propose an occasion-oriented clothing recommendation method based on attributes and categories. The work in <ref type="bibr" target="#b17">[17]</ref> and <ref type="bibr" target="#b2">[3]</ref> requires image or text queries to find complementary clothes. A functional tensor factorization approach is used to suggest sets of items to users in <ref type="bibr" target="#b3">[4]</ref>. As these approaches all require user queries or user uploaded data as input, they are likely to be perceived as less user-friendly by a typical user. Moreover, we note that these approaches are rather impractical to implement in an efficient manner in a large-scale online recommender system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-Attention and Transformer</head><p>Self-attention is an attention mechanism relating different positions of a single sequence <ref type="bibr" target="#b29">[29]</ref>, which has been used successfully in a variety of tasks <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b19">19]</ref>. Transformer <ref type="bibr" target="#b29">[29]</ref>, a transduction model relying on self-attention, has been widely used and greatly improved the performance for language processing tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">21]</ref>. In recent language representation research, bidirectional Transformer encoder has exhibited the best performance (BERT <ref type="bibr" target="#b0">[1]</ref>) when compared to left-to-right Transformer decoder (OpenAI GPT <ref type="bibr" target="#b21">[21]</ref>) and bidirectional LSTM (ELMo <ref type="bibr" target="#b20">[20]</ref>). Recently, multi-head self-attention is also introduced to model users' behavior sequences for sequential recommendation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">25]</ref>. Different from them, in this paper, we adopt the self-attention to model the compatibility in fashion outfit generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET</head><p>Taobao's fashion experts create thousands of outfits everyday. All these manually created fashion outfits are reviewed before they are exhibited online. About 1.43 million outfits have so far been created and reviewed in this manner. We collect the 80 most frequent leaf categories (e.g. sweater, coat, t-shirt, boots, and ring) from the items in all these outfits. All items in the less frequent categories are removed from the outfits. In the rest of the paper, we only consider the items in these 80 leaf categories. After that, we filter the outfits which contain fewer than 4 items. In total, 1.01 million outfits remained, which are composed of 583 thousand individual items. Moreover, we collect clicks on the items and outfits from iFashion's users in recent three months. We select click behaviors from 3.57 million active users who have viewed more than 40 outfits in total. The clicks on outfits are recorded only when more than 10 item clicks happened before from the same user. Then, we build a training sample by pairing an outfit click with the latest 50 item clicks prior to it from the same user. Finally, we obtain 19.2 million training samples which consist of 4.46 million items and 127 thousand outfits. Every item in our dataset is associated with white background image, title, and leaf category.</p><p>To the best of our knowledge, our dataset is the largest publicly available dataset of fashion items with rich information compared to existing datasets, such as WoW <ref type="bibr" target="#b15">[15]</ref>, Fashion-136K <ref type="bibr" target="#b5">[6]</ref>, FashionVC <ref type="bibr" target="#b24">[24]</ref>, Maryland Polyvore <ref type="bibr" target="#b2">[3]</ref>, Polyvore Outfits-D, and Polyvore Outfits <ref type="bibr" target="#b28">[28]</ref>. Moreover, we are the first to provide outfit data and associated user behavior data, which can be exploited for future fashion recommendation research. We provide three datasets describing the outfits, the items, and the user behaviors separately. The statistics of the datasets are shown in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we introduce our methodology in detail. POG is built within a three-step process: we first embed the items. Second, we build FOM, which learns compatibilities of items within an outfit. Last, once training is completed, we use the resulting pre-trained FOM to initialize POG on a Transformer architecture.</p><p>As a first step, we represent all the items using a multi-modal embedding model. Then we introduce FOM and POG in detail. Last, we introduce our Dida platform in this section, which helps to ensure the efficiency and quality requirements in large-scale online application of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-modal Embedding</head><p>For every fashion item f , we compute a nonlinear feature embedding f . The concept of fashion relies mostly on visual and textual information. Most previous works suggest to leverage image and text to learn multi-modal embeddings <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">11]</ref>. In our work, we use a multi-modal embedding model that takes the following input for every item: (1) dense vector encoding the white background picture of the item from a CNN model, (2) dense vector encoding the title of the item obtained from a TextCNN network, which has been pre-trained to predict an item's leaf category based on its title, (3) dense vector encoding a collaborative filtering signal for the item using Alibaba's proprietary Behemoth Graph Embedding platform <ref type="bibr" target="#b31">[31]</ref>, which generates item embeddings based on the co-occurrence statistics of items in recorded user click sessions in the Taobao Mobile Application.</p><p>Our goal is to obtain an embedding space, where similar items are embedded nearby, and different items lie in different regions. We concatenate the embeddings derived from image, text, and CF as input for a final fully connected layer. The output is a d e -dimension vector f . For each item f , we define positive samples f + as those items which belong to the same leaf category as f , and negative samples f − are hence those items which do not fall into the same leaf category. The entire network is then trained using the triplet loss via:</p><formula xml:id="formula_0">L E = f max d(f , f + ) − d(f , f − ) + α, 0<label>(1)</label></formula><p>where the distance metric d represents Euclidean distance, and α as the margin. By minimizing L E , the distance between f and f + in the same category is forced to be smaller than the distance from f − in a different category by some margin α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FOM: Fashion Outfit Model</head><p>Outfit is a set of items, where each item should have different weighted interactions to the other items in the outfit. To capture the item interactions in an outfit, we design a masked item prediction task based on a bidirectional Transformer encoder architecture.</p><p>Masking items one at a time in the outfits, we require the model to fill in the blank with the correct item according to the context. Since every item in the outfit is masked to fuse its left and right context, the compatibility between each item and all the other items within the outfit can be learned from the self-attention mechanism. Hence, the compatibility in the outfit can be learned by combining all theses item compatibilities. Let F denote the set of all outfits. Given an outfit F ∈ F , F = { f 1 , . . . , f t , . . . , f n }, where f t is the t-th item. Let f t be the representation of item f t derived from multi-modal embedding with dimension d e . We use a particular embedding [MASK] for the masked item. Non-masked items are represented by their multi-modal embeddings. We then represent the set of input embeddings as F mask . Given F mask , the task is to predict the masked item rather than reconstructing the entire outfit. More formally, we minimize the following loss function of FOM:</p><formula xml:id="formula_1">L F = − 1 n n mask=1 log Pr (f mask |F mask ; Θ F )<label>(2)</label></formula><p>where Θ F denotes the model parameters, and Pr (•) is the probability of choosing the correct item conditioned on the non-masked items. The model architecture is shown in Figure <ref type="figure" target="#fig_2">3</ref>. We do not use position embedding like Transformer <ref type="bibr" target="#b29">[29]</ref> does, because we take the items in the outfit as a set, not a sequence with position information. Let F mask pass through two fully-connected layers with a Rectified Linear Unit (ReLU) activation in between, to transfer all the input embeddings from the single item space to an outfit space. We call the two fully-connect layers as transition layer, and represent the output as H 0 ∈ R n×d m :</p><formula xml:id="formula_2">H 0 = ReLU( f T 1 ; . . . ; f T n T W F 0 + b F 0 )W F 1 + b F 1 (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where</p><formula xml:id="formula_4">W F 0 ∈ R d e ×d m , W F 1 ∈ R d m ×d m , b F 0 ∈ R n×d m , and b F 1 ∈ R n×d m are learnable parameters.</formula><p>The following Transformer encoder contains multiple layers. Each layer contains a Multi-Head self-attention (MH) sub-layer, and a Position-wise Feed-Forward Network (PFFN) sub-layer, where a residual connection is employed around each of the two sub-layers, followed by Layer Normalization (LN). The definitions of MH and PFFN are identical to the paper <ref type="bibr" target="#b29">[29]</ref>. We define the output of the first sub-layer in layer i as H i 1 . Thus each layer H i can be calculated iteratively:</p><formula xml:id="formula_5">H i = Transformer e (H i−1 ), ∀i = 1, . . . , l<label>(4)</label></formula><formula xml:id="formula_6">Transformer e (H i−1 ) = LN H i−1 1 + PFFN(H i−1 1 )<label>(5)</label></formula><formula xml:id="formula_7">H i−1 1 = LN H i−1 + MH(H i−1 , H i−1 , H i−1 )<label>(6)</label></formula><p>After l layers, we obtain the output G = H l . Let д mask denote the corresponding output of the input [MASK], we then append a softmax layer on top of д mask to calculate the probability of the masked item: where h mask is the ground truth transition embedding of the masked item, and H contains the transition embeddings from all the items in F . One can choose H to be the whole set of item transition embeddings of all the items, however, this is not practical due to the large number of high-dimensional embeddings. Therefore, we obtain h by randomly sampling 3 items from the whole transition set, which are not appeared in the outfit F , together with h mask . This allows the model to learn the compatibility information from the outfit by looking at a diverse set of samples. Note that h mask and h do not correspond to the original item embeddings, but rather the outputs of the transition layer.</p><formula xml:id="formula_8">Pr (f mask |F mask ; Θ F ) = exp(д mask h mask ) h ∈H exp(д mask h)<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">POG: Personalized Outfit Generation Model</head><p>After modeling outfit compatibility, we now consider a generation model which generates personalized and compatible outfit by introducing user preference signals. Take the advantage of encoderdecoder structure, we aim to translate an user's historical behaviors to a personalized outfit. Let U denote the set of all users and F be the set of all outfits. We use a sequence of user behaviors U = {u 1 , . . . , u i , . . . , u m } to characterize an user, where u i are the clicked items by the user. F = { f 1 , . . . , f t , . . . , f n } is the clicked outfit from the same user, where f t are the items in the outfit. At each time step, we predict the next outfit item given previous outfit items and user's click sequence on items U . Thus for pair (U , F ), the objective function of POG can be written as:</p><formula xml:id="formula_9">L (U , F ) = − 1 n n t =1 log Pr f t +1 | f 1 , . . . , f t , U ; Θ (U , F )<label>(8)</label></formula><p>where Θ (U , F ) denotes the model parameters. Pr (•) is the probability of seeing f t +1 conditioned on both previous outfit items and user clicked items. The model architecture is shown in Figure <ref type="figure" target="#fig_3">4</ref>. In POG, the encoder takes user clicked items as the input. Given a special token [START], the decoder then generates an outfit by one item at a time. At each step, the model is auto-regressive consuming the previously generated items as input. The generation stops when a special token [END] appears. In the end, an outfit is generated by composing the output items. We name the encoder as Per network, and the decoder as Gen network. To be precise, the Per network provides a user preference signal, and the Gen network can generate outfits based on both personalization signal and compatibility signal. In the Per network, after a transition layer, a Transformer encoder architecture is followed with a stack of p identical layers. Thus the user preference can be obtained via:</p><formula xml:id="formula_10">B 0 = ReLU u T 1 ; . . . ; u T m T W U 0 + b U 0 W U 1 + b U 1<label>(9)</label></formula><p>B i = Transformer e (B i−1 ), i = 1, . . . , p (10) where B 0 is the output of the transition layer. The Gen network is initialized using the aforementioned pretrained FOM. In the Gen network, after the transition layer, a Transformer decoder architecture is followed. Each Transformer decoder layer contains three sub-layers. The first sub-layer is a Masked Multi-Head self-attention (MMH) mechanism. The masked design ensures that the prediction for a certain item can depend only on the known outputs of previous items. The second sub-layer performs Multi-Head attention (MH) over the output of user behavior C, which aims at introducing user information to the outfit generation process. The third sub-layer is a Position-wise Feed-Forward Network (PFFN). Similarly, we employ residual connections around each of the sub layers, followed by Layer Normalization (LN). Define the output of the first sub-layer in layer i as H i 1 , and the output of the second sub-layer as H i 2 , then each Transformer decoder layer H i can be calculated iteratively: Get the output v t of the t-th step.</p><formula xml:id="formula_11">W U 0 ∈ R d e ×d m , W U 1 ∈ R d m ×d m , b U 0 ∈ R m×d m ,</formula><formula xml:id="formula_12">H 0 = ReLU [f T 1 ; . . . ; f T n ] T W F 0 + b F 0 W F 1 + b F 1 (<label>11</label></formula><formula xml:id="formula_13">)</formula><formula xml:id="formula_14">H i = Transformer d (H i−1 ), i = 1, . . . , q<label>(12)</label></formula><formula xml:id="formula_15">Transformer d (H i−1 ) = LN H i−1 2 + PFFN(H i−1 2 )<label>(13)</label></formula><formula xml:id="formula_16">H i−1 2 = LN H i−1 1 + MH(H i−1 1 , C, C)<label>(14)</label></formula><formula xml:id="formula_17">H i−1 1 = LN H i−1 + MMH(H i−1 , H i−1 , H i−1 )<label>(15)</label></formula><p>6:</p><p>Output the next item f t +1 in H C according to Equation <ref type="formula" target="#formula_20">17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Input f t +1 to the Gen network. 8: until (The [END] mark appears.) where H 0 is the output of the transition layer, and q is the number of layers of the Gen network.</p><formula xml:id="formula_18">W F 0 ∈ R d e ×d m , W F 1 ∈ R d m ×d m , b F 0 ∈ R n×d m , and b F 1 ∈ R n×d m are learnable parameters. The final output of POG is represented as V = H q .</formula><p>In one training sample with one clicked outfit and several clicked items from the same user, the ground truth prediction is the items in the outfit. We define h t +1 as the ground truth transition vector of the t-th output v t . Thus the probability of the next item is calculated as:</p><formula xml:id="formula_19">Pr (f t +1 | f 1 , . . . , f t , U ; Θ (U , F ) ) = exp(v t h t +1 ) h ∈H exp(v t h)<label>(16)</label></formula><p>Again, H is composed of 3 randomly selected embeddings from the transition layer, together with the ground truth vector h t +1 .</p><p>In the inference process, for each output vector v t , we deploy a similarity search to the candidate pool based on the following objective function:</p><formula xml:id="formula_20">h t +1 = arg max h C ∈H C exp(v t h C ) h ∈H C exp(v t h) (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>where C is the candidate item set. Then the item with the transition embedding h t +1 is chosen to be the next item. The generation stops when the [END] mark appears. A detailed generation process is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dida Platform</head><p>In order to ensure an overall smooth user experience, strict quality and efficiency requirements have to be taken into consideration when deploying POG into an online application with millions of users. We thus develop a platform named Dida which is able to generate personalized outfits automatically at very large scales and ensures necessary quality and efficiency standards. Dida platform consists of a number of services including item selection, outfit generation, image composition, and personalized recommendation, as shown in Figure <ref type="figure" target="#fig_6">5</ref>. The outfit generation and recommendation workflow, wherein POG is implemented and deployed, can be described as follows:</p><p>• We support million scale item pools and assist operators to select qualified images with clean background and chosen categories. Images, texts, and CF-data are extracted for selected items and multi-modal embeddings are computed as described in Section 4.1. In the generation process as described in Algorithm 1, we use Faiss <ref type="bibr" target="#b6">[7]</ref> to implement the similarity search. To ensure the quality of the generated outfits, we restrict the similarity search according to certain domain rules provided by fashion experts. For example, if it specifies that one t-shirt, one pair of jeans, and one sport shoes as a category rule, then only jeans and sport shoes can be searched in Faiss after the t-shirt.</p><p>• The generated items from the first step will be composed together with their images in designed templates. After image composition, we recommend outfits to users. The personalized generated outfits from POG are recommended directly to the users. In addition, we also support other recommendation strategies including random recommendation and CF recommendation.</p><p>Dida is widely used by more than one million operators at Alibaba. About 6 million personalized outfits are generated everyday with high quality. So far, the outfits have been recommended to more than 5.4 million users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>We describe our experiments and analyze the results in this section. We conduct both offline and online experiments to compare the compatibility and recommendation performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fashion Outfit Compatibility</head><p>5.1.1 Implementation Details. For multi-modal embedding process, we use 1536 dimensional CNN features derived from Inception Resnet V2 model <ref type="bibr" target="#b26">[26]</ref> as the image features. The text representations are 300 dimensional vectors derived from TextCNN <ref type="bibr" target="#b10">[10]</ref> with a vocabulary size of 420,758. Graph embeddings are 160 dimensional CF vectors from <ref type="bibr" target="#b31">[31]</ref>. We set d e = 128 as the final dimension of the multi-modal embedding. Triplet margin is fixed as α = 0.1. In FOM, the model is composed of a stack of l = 6 layers. We use 8 heads in all multi-heads attentions, and d m = 64 as the dimension of hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Task Settings &amp; Evaluation Metrics.</head><p>To evaluate the performances of multi-modal embeddings and models on predicting the outfit compatibility, we adopted two wide-used tasks following the practices in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b28">28]</ref>.</p><p>Fill In the Blank (FITB). FITB is a task predicting an item from multiple choices that is compatible with other items to fill in the blank. For offline evaluation, we split 10% of the data as a test set. We mask one item at a time for each outfit in the test set. Then for each masked item, we randomly select 3 items from other outfits along with the ground truth item to obtain a multiple choice set. It is reasonable to believe that the ground truth item is more compatible with other items than the randomly selected ones. Finally, the result is evaluated by the accuracy of choosing the correct items.</p><p>Compatibility Prediction (CP). The CP task is to predict whether a candidate outfit is compatible or not. For evaluation, we first build a compatible outfit set which is 10% split from the dataset. Then, we produce the same amount of incompatible outfits as the compatible outfits for a balanced test set by randomly selecting fashion items from the compatible outfit set. In the CP task, a candidate outfit is scored as to whether its constitute items are compatible with each other. The performance is evaluated using the AUC (Area Under Curve) of the ROC (Receiver Operating Characteristic) curve.</p><p>Specifically, in FOM, we solve the FITB task based on Equation <ref type="formula" target="#formula_20">17</ref>, where C is the choice set. For CP task, given an outfit, we simply utilize the negative value of Equation 2 as the compatibility score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Compare Different Modalities.</head><p>We compare each modality and their combinations on compatibility evaluations. The performances are compared in Table <ref type="table" target="#tab_3">3</ref>, which shows that: (1) In both FITB and CP tasks, text works best on its own, but image and CF provide complementary information. (2) CF embedding alone does not work very well, partly because it lacks semantic visual and textual information, which is important in fashion compatibility. <ref type="bibr" target="#b2">(3)</ref> The 1536 dimensional CNN feature derived from Inception Resnet V2 is compressed to 128 after the fully connected layer. The resulting dimension is relatively small to contain the important visual information of the fashion items, which partly explains that the image only modality performs badly. Similar results can also been observed from <ref type="bibr" target="#b11">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Compare Different Models.</head><p>To demonstrate the effectiveness of our model FOM, we compared several alternative models. For fair comparison, all the models share the same embeddings and the dimension of hidden layers.</p><p>• F-LSTM <ref type="bibr" target="#b2">[3]</ref> Given a sequence of fashion items in an outfit, a forward LSTM is trained taking each item as an individual time step.</p><p>• Bi-LSTM <ref type="bibr" target="#b2">[3]</ref> A bidirectional LSTM adds a backward LSTM compared to F-LSTM, thus predicting the next item can be performed in the reverse order also.</p><p>• SetNN <ref type="bibr" target="#b11">[11]</ref> SetNN uses a multi-instance pooling model to aggregate information from individual fashion items to produce a holistic embedding for the outfit. We use mean reduction as the pooling method, which performs the best in experiments as <ref type="bibr" target="#b11">[11]</ref> shows. The original SetNN predicts popularity of an outfit by labeling the outfit preferences. Since popularity does not always indicate compatibility, for fair comparison, we change its training inputs with compatible outfits, and incompatible outfits by randomly selecting items from the training set. • FOM (ours) Our Fashion Outfit Model by learning a masked item prediction task described in Section 4.2.</p><p>In particular, F-LSTM and Bi-LSTM are sequence based models, which take sequences of items as inputs. SetNN and FOM are set based models, which do not enforce a specific order over the fashion items as inputs. Considering the particular requirement of sequence based models which need ordered inputs, we conduct the experiments with both disordered inputs and ordered inputs. The order follows the guide in <ref type="bibr" target="#b2">[3]</ref>, which has a fixed order: tops, bottoms, shoes, and accessories, where we use leaf categories to specify.</p><p>FITB Results. The middle two columns of Table <ref type="table" target="#tab_4">4</ref> shows the results of our model compared with alternative models in the FITB task. From this table, we make the following observations: (1) Sequence based models are sensitive to the input order, while set based models are not. Both F-LSTM and Bi-LSTM have better performances with ordered inputs. SetNN and FOM have similar results with different inputs. This demonstrates that sequence based models can only work well in specific order. (2) Bi-LSTM has better results than F-LSTM both in ordered inputs and unordered inputs. The combination of LSTMs in two directions offers higher accuracy than one directional LSTM. Similar result is also observed in <ref type="bibr" target="#b2">[3]</ref>.</p><p>(3) FOM performs the best for both inputs. It exhibits a 18.04% increase compared to the second best result (Bi-LSTM) in unordered inputs, and a 5.98% increase compared to the second best result (Bi-LSTM) in ordered inputs. We attribute this to the self-attention mechanism in Transformer, which facilitates FOM to calculate the weighted interactions of every item to the others, makes it suitable for modeling compatibility in this task.</p><p>CP Results. The last two columns in Table <ref type="table" target="#tab_4">4</ref> shows the results of our model and others for the CP task. Similar to FITB, sequence based models are still sensitive to the input order. FOM obtains the best performance by significant margins. In unordered inputs, FOM has a 34.90% increase compared to the second best (Bi-LSTM), while in ordered inputs, FOM shows 25.81% increase compared to the second best (Bi-LSTM). Although SetNN is directly trained to predict set compatibility, it still performs the worst. Similar results can also be found in <ref type="bibr" target="#b11">[11]</ref>. We attribute the performance of FOM to the multi-head self-attention mechanism. It learns the compatibilities between each item and all the other items within the outfit, and then an outfit compatibility can be learned by combining all these item compatibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fashion Outfit Generation and Recommendation</head><p>In this section, we conduct extensive online experiments on different fashion models and recommendation methods. The performance of outfit compatibility and recommendation can be evaluated via CTR, which is an explicit metric representing the effectiveness of different models and methods. Further, we show some online example cases based on our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Implementation</head><p>Details &amp; Task Settings. In POG, the Per network is composed of a stack of p = 6 layers, and the Gen network is composed of a stack of q = 6 layers. We use 8 heads in all multi-head attentions, and d m = 64 as the dimension of hidden layers. Our online test data is a manually selected item pool with 1.57 million items. Outfits are generated and recommended via different generation models and recommendation methods on Dida platform. We split the users equally into 8 buckets, each tests a certain generation model with recommendation method. 5.2.2 Compare Different Models and Methods. Since SetNN <ref type="bibr" target="#b11">[11]</ref> cannot generate outfits, we focus on the comparisons among generation available models. It is worth mentioned that none of the existing generation models, including F-LSTM and Bi-LSTM, are available to do personalized recommendation. So we Randomly Recommend (RR) these generated outfits to the users. To compare the recommendation performance better, we also deploy classic CF <ref type="bibr" target="#b22">[22]</ref> methods to recommend. We first estimate a user's most preferred item via measuring its similarities with the items in his or her interaction history by calculating an item-to-item similarity matrix, according to <ref type="bibr" target="#b22">[22]</ref>. Then, the model generated outfit which contains the user's most preferred item is recommended. Online experiments are listed as follows:</p><p>• F-LSTM <ref type="bibr" target="#b2">[3]</ref>+RR Outfits are generated by starting with every item in the test set using the F-LSTM model. The generated outfits are recommended randomly to users.</p><p>• Bi-LSTM <ref type="bibr" target="#b2">[3]</ref>+RR Outfits are generated as paper <ref type="bibr" target="#b2">[3]</ref> presents by taking every item in the test set as the query item. The generated outfits are recommended randomly to users.</p><p>• Gen+RR (ours) Only Gen network is used for generation. We input no user information to POG to evaluate the generation quality only. The generated outfits are recommended randomly.</p><p>• F-LSTM <ref type="bibr" target="#b2">[3]</ref>+CF Outfits are generated by F-LSTM model. The generated outfits are recommended using the CF method.</p><p>• Bi-LSTM <ref type="bibr" target="#b2">[3]</ref>+CF Outfits are generated by Bi-LSTM model. The generated outfits are recommended using the CF method.</p><p>• Gen+CF (ours) Only Gen network is used for generation. The generated outfits are recommended using the CF method.</p><p>• POG (ours) Both the Per network and the Gen network are used in POG to generate and recommend personalized outfits.</p><p>• POG+FOM (ours) The Gen network is initialized using the pre-trained FOM before generating personalized outfits.</p><p>We record the CTR on outfits over a period of seven days, and report the results in Figure <ref type="figure" target="#fig_7">6</ref>. Three conclusions can be observed from the figure: (1) Although the CTR values vary during the seven days, we still observe an obvious phenomenon that the recommendation performance largely relies on the recommendation methods. CF methods have higher CTR than all the RR methods, and POG Day1 Day2 Day3 Day4 Day5 Day6 Day7 methods outperform all the other. POG with pre-trained FOM has the highest CTR in most of the time. It has an improvement of more than 70% in CTR, compared to the state-of-the-art model Bi-LSTM with CF recommendation. We attribute this to the differences between individual item recommendation and outfit recommendation. The most preferred item of one user may not indicate the most preferred outfit. <ref type="bibr" target="#b1">(2)</ref> The compatibility performance of these models matters, but not in an obvious way. This is partly because of the rule based search in the generation process described in Section 4.4, which avoids some extreme bad cases. In most of the time, Gen has the highest CTR both in RR and CF methods. (3) The performance of POG with pre-trained FOM has a little improvement over POG alone. We may conclude that the Gen network in POG is already enough to learn the compatibility of outfits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Case Study.</head><p>We randomly sample some online cases, and present them in Figure <ref type="figure" target="#fig_8">7</ref>. For users with various preferences on the items, (e.g. the first two rows), POG can generate compatible outfits conditioned on the common properties of these items. For users who clicked a lot of similar items, (e.g. the last two rows), POG provides an outfit with the main item, which has similar properties with the clicked items. The outfits may inspire the users with better ideas of how to compose the items more properly, and even influence them to buy the whole outfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a personalized outfit generation model to build the bridge between compatibility and personalization, which are two essential requirements in fashion generation and recommendation. Our model is built in three steps: multi-modal embedding, fashion outfit modeling, and personalized fashion outfit modeling. We use Transformer in our model which helps to find interactions of the items in the outfit, and also helps to connect user behaviors on items and outfits. Our model outperforms other alternative models in outfit compatibility, outfit generation and recommendation by significant margins. Furthermore, we deploy POG on the Dida platform to assist personalized outfit generation in large-scale online application, which is widely used in Alibaba now. We share our data in this paper for future research.</p><p>For the future work, we aim to solve the cold-start problem via building user profiles from similar user groups.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A sample of iFashion application in Taobao. We recommend fashion outfits (sets of fashion items which interact with each other) to users.</figDesc><graphic url="image-1.png" coords="2,53.80,83.68,240.23,158.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of user clicked items and outfits. User (a) is a young girl who likes clothes and outfits in light colors and sweet styles. User (b) is a college boy who clicks several winter outfits after clicking a lot of winter clothes. User (c) is probably an office lady who prefers OL style items as well as outfits.</figDesc><graphic url="image-2.png" coords="2,317.96,83.68,240.25,101.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The architecture of FOM. We mask the items in the outfit one at a time. For example, we mask a pair of jeans in the outfit. The model is learned to choose the correct jeans from a candidate pool, to complement other items in the outfit.</figDesc><graphic url="image-3.png" coords="4,101.79,83.68,408.42,112.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of POG, which is an encoder-decoder architecture with a Per network and a Gen network. The outfit item is generated step by step according to the user preference signal from the Per network and the compatibility signal from the Gen network.</figDesc><graphic url="image-4.png" coords="5,113.80,83.69,384.39,168.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>and b U 1 ∈</head><label>1</label><figDesc>R m×d m are learnable parameters. The output of the Per network is C = B p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Personalized Outfit Generation. Input: Candidate set C; User clicked item sequence U . Output: Personalized fashion outfit F . 1: Transform all items in C into outfit space H C . 2: Input U in the Per network. 3: Input [START] mark in the Gen network. 4: repeat 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Dida platform. We provide services including item selection, outfit generation, image composition, and personalized recommendation.</figDesc><graphic url="image-5.png" coords="6,317.96,83.69,240.25,102.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: CTR of online experiments. Methods based on POG achieve the best performances. CF based methods are better than RR methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The online cases of POG. Four rows correspond to four users. The first five columns are user clicked items sampled from the latest 50 clicks, and the last column is the generated outfit by POG.</figDesc><graphic url="image-6.png" coords="9,65.81,83.69,216.21,181.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The percentage of the same brand, category, style, and pattern of the items in users' clicked outfits appearing in users' latest clicked items.</figDesc><table><row><cell>Property</cell><cell cols="3">brand category style pattern</cell></row><row><cell cols="2">Percentage 56.9%</cell><cell>81.3%</cell><cell>73.0% 53.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Outfits</cell><cell>#Users</cell><cell>#Items</cell></row><row><cell cols="2">Outfit data 1,013,136</cell><cell>-</cell><cell>583,464</cell></row><row><cell>Item data</cell><cell>-</cell><cell cols="2">-4,747,039</cell></row><row><cell>User data</cell><cell cols="3">127,169 3,569,112 4,463,302</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>FITB and CP results of different modalities.</figDesc><table><row><cell>Modality</cell><cell>FITB</cell><cell>CP</cell></row><row><cell>Image</cell><cell>62.85%</cell><cell>80.78%</cell></row><row><cell>Text</cell><cell>68.02%</cell><cell>85.17%</cell></row><row><cell>CF</cell><cell>51.38%</cell><cell>57.92%</cell></row><row><cell>Image+text</cell><cell>68.24%</cell><cell>85.41%</cell></row><row><cell>Image+text+CF</cell><cell>68.71%</cell><cell>86.09%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>FITB and CP results of different models.</figDesc><table><row><cell>Model</cell><cell>FITB</cell><cell></cell><cell>CP</cell><cell></cell></row><row><cell></cell><cell cols="4">Unordered Ordered Unordered Ordered</cell></row><row><cell>F-LSTM[3]</cell><cell>58.07%</cell><cell>62.84%</cell><cell>63.78%</cell><cell>65.04%</cell></row><row><cell>Bi-LSTM[3]</cell><cell>58.21%</cell><cell>64.91%</cell><cell>63.82%</cell><cell>68.61%</cell></row><row><cell>SetNN[11]</cell><cell>49.24%</cell><cell>49.27%</cell><cell>58.31%</cell><cell>58.33%</cell></row><row><cell>FOM (ours)</cell><cell>68.71%</cell><cell>68.79%</cell><cell>86.09%</cell><cell>86.32%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">https://github.com/wenyuer/POG</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank colleagues of our team -Yuchi Xu, Zhiyuan Liu, Wei Li, Jizhe Wang, Mengmeng Wu, Lifeng Wang, and Qiwei Chen for useful discussion and supports. We are grateful to our cooperative team -Wentao Yang, Jun Bao, Yue Hu, Yumeng Liu, Lijie Cao, Yunfeng Zhang, Qichuan Xiao, and Mingkai Wang. We also thank the reviewers for their valuable comments and suggestions that help improve the quality of this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName><forename type="first">Xufeng</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3343" to="3351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning fashion compatibility with bidirectional lstms</title>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
				<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1078" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collaborative fashion recommendation: A functional tensor factorization approach</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
				<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crossdomain image retrieval with a dual attribute-aware ranking network</title>
		<author>
			<persName><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Rogerio S Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale visual recommendations from street fashion images</title>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with GPUs</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visually-aware fashion recommendation and design with generative image models</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-Attentive Sequential Recommendation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
				<meeting>ICDM</meeting>
		<imprint>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining fashion outfit composition using an end-to-end deep learning approach on set data</title>
		<author>
			<persName><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1946" to="1955" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clothes co-parsing via joint image segmentation and labeling with application to clothing retrieval</title>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1175" to="1186" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Amazon. com recommendations: Item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hi, magic closet, tell me what to wear!</title>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Multimedia</title>
				<meeting>the 20th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="619" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryosuke</forename><surname>Goto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03133</idno>
		<title level="m">Outfit Generation and Style Extraction via Bidirectional LSTM and Autoencoder</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Ankur P Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<title level="m">A decomposable attention model for natural language inference</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
				<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00313</idno>
		<title level="m">Neural Compatibility Modeling with Attentive Knowledge Distillation</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neurostylist: Neural compatibility modeling for clothing matching</title>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
				<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="753" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06690</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ivona</forename><surname>Tautkute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Skorupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Brocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Marasek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03002</idno>
		<title level="m">DeepStyle: Multimodal Search Engine for Fashion and Interior Design</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Mariya</forename><forename type="middle">I</forename><surname>Vasileva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Dusad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Rajpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjitha</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09196</idno>
		<title level="m">Learning Type-Aware Embeddings for Fashion Compatibility</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning visual clothing style with heterogeneous dyadic cooccurrences</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balazs</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4642" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Paper doll parsing: Retrieving similar styles to parse clothing items</title>
		<author>
			<persName><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3519" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
