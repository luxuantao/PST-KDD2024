<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ROBOGEN: TOWARDS UNLEASHING INFINITE DATA FOR AUTOMATED ROBOT LEARNING VIA GENERATIVE SIMULATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-11-02">2 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhou</forename><surname>Xian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zackory</forename><surname>Erickson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Held</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Cmu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tsinghua</forename><surname>Iiis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mit</forename><surname>Csail</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Umass</forename><surname>Amherst</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ROBOGEN: TOWARDS UNLEASHING INFINITE DATA FOR AUTOMATED ROBOT LEARNING VIA GENERATIVE SIMULATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-02">2 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2311.01455v1[cs.RO]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments. * Equal contribution. Contribution Statement: Yufei Wang led the implementation of the whole project. Zhou Xian conceptualized the idea, contributed to the locomotion pipeline, and supervised the direction of the project. Feng Chen was in charge of the soft-body manipulation pipeline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>This work is motivated by a long-standing and challenging goal in robotics research: empowering robots with a diverse set of skills, enabling them to operate in various non-factory settings and perform a broad range of tasks for humans. Recent years have witnessed impressive progress in teaching robots various complex skills: from deformable object and fluid manipulation <ref type="bibr">(Lin et al., 2022a;</ref><ref type="bibr" target="#b78">Weng et al., 2022;</ref><ref type="bibr" target="#b84">Xu et al., 2023;</ref><ref type="bibr">Xian et al., 2023c;</ref><ref type="bibr">Wang et al., 2023c;</ref><ref type="bibr" target="#b38">Lin et al., 2020)</ref>, to dynamic and dexterous skills such as object tossing <ref type="bibr">(Zeng et al., 2020)</ref>, in-hand re-orientation <ref type="bibr" target="#b3">(Chen et al., 2022)</ref>, soccer playing <ref type="bibr" target="#b21">(Haarnoja et al., 2023)</ref> and even robot parkour <ref type="bibr">(Zhuang et al., 2023)</ref>. However, these skills still remain compartmentalized, have relatively short horizons, and necessitate human-designed task descriptions and training supervision. Notably, due to the expensive and laborious nature of real-world data collection, many of these skills are trained in simulations with appropriate domain randomization and then deployed to real-world <ref type="bibr" target="#b84">(Xu et al., 2023;</ref><ref type="bibr">Zhuang et al., 2023;</ref><ref type="bibr" target="#b3">Chen et al., 2022)</ref>.</p><p>Indeed, simulation environments have become a crucial driving force behind diverse robotic skill learning <ref type="bibr">(Lin et al., 2022a;</ref><ref type="bibr" target="#b65">Song et al., 2023;</ref><ref type="bibr">Zhuang et al., 2023)</ref>. Compared to exploration and data collection in the real-world, skill learning in simulations offers several advantages: 1) simulated environments provide access to privileged low-level states and unlimited exploration opportunities; Figure <ref type="figure">1</ref>: RoboGen is a robotic agent that generates tasks and environments, and acquire skills automatically. Here we show 25 example tasks generated and corresponding skills learned by RoboGen.</p><p>2) simulation supports massively parallel computation, enabling significantly faster data collection without reliance on considerable investment in robotic hardware and human labor; 3) exploration in simulation allows robots to develop closed-loop policies and error-recovery capabilities, while realworld demonstrations typically offer only expert trajectories. However, robot learning in simulations also presents its own limitations: while exploration and practicing in simulated environments are cost-effective, constructing these environments requires significant labor effort, demanding tedious steps including designing tasks, selecting relevant and semantically meaningful assets, generating plausible scene layouts and configurations, and crafting training supervisions such as reward or loss functions <ref type="bibr" target="#b26">(James et al., 2020;</ref><ref type="bibr" target="#b66">Srivastava et al., 2022;</ref><ref type="bibr" target="#b17">Gu et al., 2023;</ref><ref type="bibr">Li et al., 2023a)</ref>. The onerous task of creating these components and constructing individualized simulation settings of each one of the countless tasks encountered in our daily life is an overwhelming challenge, which significantly constrains the scalability of robotic skill learning even in simulated worlds.</p><p>In light of this, we present a paradigm termed Generative Simulation, marrying the advancements in simulated robotic skill learning and the latest progress in foundation and generative models. Leveraging the generative capabilities of state-of-the-art foundation models, Generative Simulation aims to generate information for all the stages needed for diverse robotic skill learning in simulation: from high-level task and skill proposals, to task-dependent scene descriptions, assets selections and generations, policy learning choices, and training supervisions. Thanks to the comprehensive knowledge encoded in latest foundation models, scene and task data generated this way have potentials to closely resemble the distribution of real-world scenarios. In addition, these models can further provide decomposed low-level sub-tasks, which can be seamlessly handled by domain-specific policy learning approaches, thereby producing closed-loop demonstrations for various skills and scenarios.</p><p>A distinct advantage of our proposed paradigm lies in the strategic design of what modes of knowledge to extract from contemporary foundation models. These models have demonstrated impressive capabilities across various modalities <ref type="bibr" target="#b72">(Touvron et al., 2023;</ref><ref type="bibr" target="#b10">Driess et al., 2023;</ref><ref type="bibr">OpenAI, 2023;</ref><ref type="bibr" target="#b61">Rombach et al., 2022;</ref><ref type="bibr" target="#b15">Girdhar et al., 2023;</ref><ref type="bibr" target="#b28">Kang et al., 2023)</ref>, giving rise to autonomous agents capable of using a range of tools and solving a variety of tasks in the virtual realm <ref type="bibr" target="#b68">(Sur?s et al., 2023;</ref><ref type="bibr" target="#b85">Yang et al., 2023;</ref><ref type="bibr" target="#b64">Shen et al., 2023)</ref>. However, due to the absence of training data pertaining to dynamics, actuations, and physical interactions, these models are yet to fully grasp understandings of what's essential for robots to effectively execute physical actions and interact with the surrounding environments -from discerning the precise joint torque needed for stable locomotion, to high-frequency finger motor commands needed for dexterous manipulation tasks such as rolling a dough. In contrast to recent efforts that employ these foundation models such as Large Language Models (LLMs) for directly yielding policies or low-level actions <ref type="bibr" target="#b36">(Liang et al., 2022;</ref><ref type="bibr">Huang et al., 2023b;</ref><ref type="bibr">Wang et al., 2023b)</ref>, we advocate for a scheme that extracts information that falls neatly within the capabilities and modalities of these models -object semantics, object affordances, common-sense knowledge to identify valuable learning tasks, etc. We use these knowledge to construct environmental playgrounds, and then resort to additional help from physics-grounded simulations, for robots to develop understandings of physical interactions and acquire diverse skills.</p><p>We first described such a paradigm in a recent white paper <ref type="bibr">(Xian et al., 2023a)</ref>, sketched as a promising pathway towards generating diverse data for generalist robot learning. In this paper, we present RoboGen, a comprehensive realization of this paradigm. RoboGen is a generative robotic agent that self-proposes skills to learn, generates scene components and configurations in simulation, labels the tasks with natural language descriptions, and designs proper training supervisions for subsequent skill learning. Our experiments show that RoboGen can deliver a continuous stream of diversified skill demonstrations, spanning tasks including rigid and articulated object manipulation, deformable object manipulation, as well as legged locomotion skills (see Figure <ref type="figure">1</ref>). The diversity of tasks and skills generated by RoboGen surpasses previous human-created robotic skill learning datasets, with minimal human involvement needed beyond several prompt designs and in-context examples. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. When queried endlessly, our system holds the potential for unleashing infinite amount of diversified demonstration data for robot learning, making a step towards fully automated large-scale robotic skill training for generalizable robotic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Robotic skill learning in simulations Various physics-based simulation platforms have been developed in the past to accelerate robotics research <ref type="bibr" target="#b41">(Liu &amp; Negrut, 2021)</ref>. These include rigid-body simulators <ref type="bibr" target="#b5">(Coumans &amp; Bai, 2016;</ref><ref type="bibr" target="#b70">Todorov et al., 2012;</ref><ref type="bibr" target="#b83">Xiang et al., 2020)</ref>, deformable object simulators <ref type="bibr" target="#b48">(Macklin et al., 2014;</ref><ref type="bibr" target="#b38">Lin et al., 2020;</ref><ref type="bibr" target="#b84">Xu et al., 2023;</ref><ref type="bibr" target="#b22">Heiden et al., 2021)</ref>, and environments supporting multi-material and their couplings with robots <ref type="bibr">(Xian et al., 2023c;</ref><ref type="bibr" target="#b13">Gan et al., 2021;</ref><ref type="bibr" target="#b17">Gu et al., 2023)</ref>. Such simulation platforms have been heavily employed in the robotics community for learning diverse skills, including table-top manipulation <ref type="bibr" target="#b14">(Gervet et al., 2023;</ref><ref type="bibr" target="#b73">Tung et al., 2020;</ref><ref type="bibr">Xian et al., 2023b)</ref>, deformable object manipulation <ref type="bibr">(Lin et al., 2022a;</ref><ref type="bibr" target="#b78">Weng et al., 2022;</ref><ref type="bibr">Wang et al., 2023c;</ref><ref type="bibr">Lin et al., 2022b;</ref><ref type="bibr" target="#b76">Wang et al., 2022;</ref><ref type="bibr" target="#b18">Ha &amp; Song, 2022)</ref>, object cutting <ref type="bibr" target="#b22">(Heiden et al., 2021;</ref><ref type="bibr" target="#b84">Xu et al., 2023)</ref>, fluid manipulation <ref type="bibr" target="#b63">(Seita et al., 2023;</ref><ref type="bibr">Xian et al., 2023c)</ref>, as well as highly dynamic and complex skills such as in-hand re-orientation <ref type="bibr" target="#b3">(Chen et al., 2022;</ref><ref type="bibr" target="#b1">Akkaya et al., 2019</ref><ref type="bibr">), object tossing (Zeng et al., 2020)</ref>, acrobatic flight <ref type="bibr" target="#b32">(Kaufmann et al., 2020;</ref><ref type="bibr" target="#b45">Loquercio et al., 2021;</ref><ref type="bibr" target="#b65">Song et al., 2023)</ref>, locomotion for legged robots <ref type="bibr" target="#b4">(Cheng et al., 2023;</ref><ref type="bibr">Zhuang et al., 2023;</ref><ref type="bibr" target="#b59">Radosavovic et al., 2023)</ref> and soft robots <ref type="bibr">(Wang et al., 2023a)</ref>.</p><p>Scaling up simulation environments Apart from building physics engines and simulators, a large body of prior work targeted at building large-scale simulation benchmarks, providing platforms for scalable skill learning and standardized benchmarking <ref type="bibr">(Li et al., 2023a;</ref><ref type="bibr" target="#b38">Lin et al., 2020;</ref><ref type="bibr">Xian et al., 2023c;</ref><ref type="bibr" target="#b86">Yu et al., 2020;</ref><ref type="bibr" target="#b26">James et al., 2020;</ref><ref type="bibr" target="#b54">Mu et al., 2021;</ref><ref type="bibr" target="#b17">Gu et al., 2023;</ref><ref type="bibr" target="#b66">Srivastava et al., 2022)</ref>. Notably, most of these prior simulation benchmarks or skill learning environments are manually built with human labeling. Another line of works attempts to scale up tasks and environments using procedural generation, and generate demonstrations with Task and Motion Planning (TAMP) <ref type="bibr" target="#b6">(Dalal et al., 2023;</ref><ref type="bibr" target="#b49">McDonald &amp; Hadfield-Menell, 2021;</ref><ref type="bibr" target="#b55">Murali et al., 2023;</ref><ref type="bibr">Migimatsu &amp; Bohg, 2019;</ref><ref type="bibr" target="#b71">Toussaint, 2015)</ref>. These methods primarily build on top of manually-defined rules and planning domains, limiting the diversity of the generated environments and skills to relatively simple pick-and-place and object stacking tasks <ref type="bibr" target="#b6">(Dalal et al., 2023;</ref><ref type="bibr" target="#b49">McDonald &amp; Hadfield-Menell, 2021)</ref>. Contrary to these works, we extract the common sense knowledge embedded in foundation models such as LLMs, and use them for generating meaningful tasks, relevant scenes, and skill training supervisions, leading to more diverse and plausible skills.</p><p>Foundation and generative models for robotics Following the rapid advancement in foundation and generative models in domains of imagery <ref type="bibr" target="#b58">(Poole et al., 2022;</ref><ref type="bibr">Liu et al., 2023c</ref>; Melas-Kyriazi  <ref type="bibr" target="#b72">(Touvron et al., 2023;</ref><ref type="bibr" target="#b10">Driess et al., 2023;</ref><ref type="bibr">OpenAI, 2023)</ref>, and other modalities <ref type="bibr">(Liu et al., 2023a;</ref><ref type="bibr" target="#b15">Girdhar et al., 2023;</ref><ref type="bibr">Huang et al., 2023a)</ref>, an active line of works investigate how to use these large-scale models for robotics research, via approaches such as code generation <ref type="bibr" target="#b79">(Wu et al., 2023;</ref><ref type="bibr" target="#b36">Liang et al., 2022)</ref>, data augmentation <ref type="bibr">(Yu et al., 2023a)</ref>, visual imagination for skill execution <ref type="bibr" target="#b11">(Du et al., 2023</ref><ref type="bibr">), sub-task planning (Ahn et al., 2022;</ref><ref type="bibr" target="#b24">Huang et al., 2022;</ref><ref type="bibr" target="#b37">Lin et al., 2023)</ref>, concept generalization of learned skills <ref type="bibr" target="#b2">(Brohan et al., 2023)</ref>, outputting low-level control actions <ref type="bibr">(Wang et al., 2023b)</ref>, and goal specification <ref type="bibr" target="#b29">(Kapelyukh et al., 2023)</ref>. More related to ours are recent methods using LLMs for reward generation <ref type="bibr">(Yu et al., 2023b)</ref>, and sub-task and trajectory generation <ref type="bibr" target="#b19">(Ha et al., 2023)</ref>. In contrast to them, our proposed system aims at a fully automated pipeline that self-proposes new tasks, generate environments and yields diverse skills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Simulation</head><p>We first presented the idea of generative simulation in a recent white paper <ref type="bibr">(Xian et al., 2023a)</ref>. We implemented a proof of concept of generative simulation in <ref type="bibr" target="#b31">(Katara et al., 2023)</ref> by generating assets, task descriptions and decompositions for a Franka arm using image generative models and LLMs. In this paper, we extend this line of research to support a wider range of robot types and a more diverse set of tasks with photo-realistic visuals, powered by a more advanced rendering and physics engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ROBOGEN</head><p>RoboGen is an automated pipeline that utilizes the embedded common sense and generative capabilities of the latest foundation models <ref type="bibr" target="#b56">(OpenAI, 2022;</ref><ref type="bibr" target="#b69">Taori et al., 2023)</ref> for automatic task, scene, and training supervision generation, leading to diverse robotic skill learning at scale. We illustrate the whole pipeline in Figure <ref type="figure" target="#fig_0">2</ref>, composed of several integral stages: Task Proposal, Scene Generation, Training Supervision Generation, and Skill Learning. We detail each of them in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TASK PROPOSAL</head><p>RoboGen starts with generating meaningful, diverse, and high-level tasks for robots to learn. Instead of directly querying an LLM for task proposal, we initialize the system with a specific robot type and an object randomly sampled from a pool. The provided robot and sampled object information are then used as input to an LLM to perform task proposal. Such a sampling procedure ensures the diversity of the generated tasks: for instance, a legged robot such as a quadruped is capable of acquiring a variety of locomotion skills, while a robotic arm manipulator has the potential to execute a diverse range of manipulation tasks when paired with different sampled objects. This initialization step serves as a seeding stage, providing a basis upon which the LLM can condition and subsequently reason and extrapolate to generate a variety of tasks, taking into account both robot and object affordances. Apart from object-based initialization, another choice is to employ examplebased initialization, where we initialize the query with a provided robot and several example tasks sampled from a list 11 pre-defined tasks.</p><p>We use <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> as the LLM to query in our current pipeline, but this backend can be upgraded once better models are available. In the following, we explain details of RoboGen in the context of a robotic arm (e.g., Franka) and tasks generated pertain to object manipulation, using object-based initialization. In this case, the objects used for initialization are sampled from a predefined list including common articulated and non-articulated objects in household scenarios such as oven, microwave, dispenser, laptop, dishwasher, etc., extracted from PartNetMobility <ref type="bibr" target="#b83">(Xiang et al., 2020)</ref> and RLBench <ref type="bibr" target="#b26">(James et al., 2020)</ref>. Having been trained on extensive internet-scale datasets, LLMs like GPT-4 are imbued with a rich understanding of the affordances of these objects, knowledge of how to interact with them, and what kind of meaningful tasks they can be associated with. In order to generate tasks that involve interaction and manipulation of sampled articulated objects by the robot, we construct a query comprising the broad category to which the articulated object belongs to, its articulation tree derived from the URDF file, and a semantics file providing annotations about the object's links, e.g., which link corresponds to the door in a sampled microwave. This information is provided with the PartNetMobility dataset. The category of the articulated object informs what general types of tasks can be performed with this kind of objects, and the articulation tree and the semantics file informs GPT-4 how exactly the object can be articulated, and semantic meanings of each joint and link. The query asks GPT-4 to return a number of tasks that can be performed with the sampled object, where each task consists of the task name, a short description of the task, if there are any other objects that are required for the task besides the provided articulated object, and which joints/links of the articulated object the robot will need to interact with to accomplish the task. Additionally, we include an example input-output pair in the query for GPT-4 to perform in-context learning to improve the quality of its response.</p><p>As a concrete example, given a sampled articulated object being a microwave, where joint 0 is a revolute joint connecting its door, and joint 1 being another revolute joint controlling a knob of the timer, GPT-4 would return a task named as "heat up a bowl of soup", with a task description of "The robot arm places a bowl of soup inside the microwave, closes the door and sets the microwave timer for an appropriate heating duration", additional objects needed for the generated task such as "A bowl of soup", and task-relevant joints and links including joint 0 (for opening the microwave door), joint 1 (for setting the timer), link 0 (the door), and link 1 (the timer knob). For the detailed prompt and example responses, please refer to Appendix B. Note that for cases where we sample non-articulated objects or use example-based initialization, the sampled objects and examples are provided only as a hint for task proposal, and the generated tasks will not be tied to them. For articulated objects, since PartNetMobility is the only articulated object dataset with high quality, and already covers diverse range of articulated assets, we will generate tasks dependent on the sampled asset. For locomotion and soft-body manipulation tasks, we use only example-based initialization, and resort to GPT-4 to populate additional required objects. By repeatedly querying with different sampled objects and examples, we can generate a diverse range of manipulation and locomotion tasks, concerning the relevant object affordances when needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCENE GENERATION</head><p>Given a proposed task, we proceed to generate a corresponding simulation scene for learning skills to accomplish the task. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, the scene components and configurations are generated conditioned on the task description, and the object assets are retrieved or generated to populate the simulation scene subsequently. Specifically, the scene components and configurations are composed of the following elements: queries for relevant assets to be populated into the scene, their physical parameters such as sizes, configurations such as the initial joint angles, and the overall spatial configurations of the assets.</p><p>Obtaining queries for relevant assets Aside from the necessary object assets that are needed for the task generated in the previous step of task proposal, in order to increase the complexity and diversity of the generated scene while resembling object distributions of real-world scenarios, we query GPT-4 to return a number of additional queries for objects that are semantically relevant to the task. See Figure <ref type="figure">1</ref> for examples of the additional objects returned by GPT-4 for a task, e.g., for the task "Open storage, put the toy inside and close it", the generated scene involves additionally a living room mat, a table-top lamp, a book, and an office chair.</p><p>Retrieving or generating assets The generated queries for relevant objects (i.e. their language descriptions) will be used to either search in an existing database <ref type="bibr" target="#b9">(Deitke et al., 2023)</ref>, or to be used as the input to text-to-image <ref type="bibr" target="#b51">(Midjourney, 2022)</ref>, and then image-to-3d mesh generation models <ref type="bibr">(Liu et al., 2023b;</ref><ref type="bibr" target="#b58">Poole et al., 2022)</ref> to generate the 3d textured mesh of the asset. Specifically, we use Objaverse <ref type="bibr" target="#b9">(Deitke et al., 2023)</ref>, a large-scale dataset containing over 800k object assets (3d meshes, texture pictures, and etc.) as the main database for the retrieval. For each object in Objaverse, we obtain a list of language descriptions of it by combining the default annotations and a more cleaned version of annotations from <ref type="bibr" target="#b46">(Luo et al., 2023)</ref>. Given the language description of the asset we want to retrieve, we use Sentence-Bert <ref type="bibr" target="#b60">(Reimers &amp; Gurevych, 2019)</ref> to get the embedding of the description, and retrieve k objects from Objaverse whose language embeddings are the most similar to the language embedding of the target asset. Due to noises in the object annotations, there can be significant discrepancies between the actual asset and the intended target, even when the similarity score in the language embedding space is high. To resolve this, we further leverage a vision-language model (VLM) to verify the retrieved assets and filter out the undesired ones. Specifically, we input an image of the retrieved object to the VLM model, and ask the VLM to caption it. The caption, together with the description of the desired asset and the description of the task, are fed back into GPT-4 to verify if the retrieved asset is appropriate to be used in the proposed task. Since the diverse range of assets from Objaverse is inherently challenging for existing pre-trained models, in order to improve the robustness of our system, we use both Bard <ref type="bibr" target="#b16">(Google, 2022)</ref> and BLIP-2 <ref type="bibr">(Li et al., 2023b)</ref>, to cross verify the validity of the retrieved asset, and adopt the asset only when the captions from both of them are deemed suitable for the task by GPT-4. We use k = 10 in our pipeline, and if all assets are rejected, we resort to text-to-image followed by image-to-mesh generation models to generate the needed assets from the language description. We use Midjourney <ref type="bibr" target="#b51">(Midjourney, 2022)</ref> as our text-to-image generative model, and Zero-1-to-3 <ref type="bibr">(Liu et al., 2023b)</ref> as our image-to-mesh generative model. For soft-body manipulation tasks, in order to obtain a more consistent and controllable target shape for the soft-body under manipulation, we ask GPT-4 to come up with desired target shape, and only use this text-to-image-to-mesh pipeline instead of database retrieval.</p><p>Asset size Assets from Objaverse <ref type="bibr" target="#b8">(Deitke et al., 2022)</ref> or PartNetMobility <ref type="bibr" target="#b83">(Xiang et al., 2020)</ref> are usually not of physically plausible sizes. To account for this, we query GPT-4 to generate the sizes of the assets such that: 1) the sizes should match real-world object sizes; 2) the relative sizes between objects allow a plausible solution for solving the task, e.g., for the task of "putting a book into the drawer", the size of the drawer should be larger than the book. Initial asset configuration For certain tasks, the articulated object should be initialized with valid states for the robot to learn the skill. For example, for the task of "close the window", the window should be initialized in an open state; similarly, for the task of opening the door, the door should be initially closed. Again, we query GPT-4 to set the initial configurations of these articulated objects, specified in joint angles. To allow GPT-4 to reason about the task and the articulated object, the query script contains the task description, the articulation tree and semantic descriptions of the articulated object.</p><p>Scene configuration Scene configuration specifying the location and relevant poses of each asset in the scene is crucial for both producing plausible environments and allowing valid skill learning. E.g., for the task of "retrieving a document from the safe", the document needs to be initialized inside the safe; for the task of "removing the knife from the chopping board", the knife needs to be initially placed on the chopping board. RoboGen queries GPT-4 to generate such special spatial relationships with the task description as the input, and also instructs GPT-4 to place objects in a collision-free manner.</p><p>With the generated scene components and configuration, we populate the scene accordingly. See Figure <ref type="figure">1</ref> for a collection of example scenes and tasks generated by RoboGen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRAINING SUPERVISION GENERATION</head><p>To acquire the skill for solving the proposed task, supervisions for skill learning are needed. To facilitate the learning process, RoboGen first queries GPT-4 to plan and decompose the generated task, which can be long-horizon, into shorter-horizon sub-tasks. Our key hypothesis is, when the task is decomposed into sufficiently short-horizon sub-tasks, each sub-task can be reliably solved by existing algorithms such as reinforcement learning <ref type="bibr" target="#b62">(Schulman et al., 2017;</ref><ref type="bibr" target="#b20">Haarnoja et al., 2018)</ref>, motion planning <ref type="bibr" target="#b30">(Karaman &amp; Frazzoli, 2011)</ref>, or trajectory optimization <ref type="bibr" target="#b27">(Kalakrishnan et al., 2011)</ref>.</p><p>After the decomposition, RoboGen then queries GPT-4 to choose a proper algorithm for solving each sub-task. There are three different types of learning algorithms integrated into RoboGen: reinforcement learning, evolutionary strategy, gradient-based trajectory optimization, and action primitive with motion planning. Each of these is suited for different tasks, e.g., gradient-based trajectory optimization is more suitable for learning fine-grained manipulation tasks involving soft bodies such as shaping a dough into a target shape <ref type="bibr" target="#b84">(Xu et al., 2023;</ref><ref type="bibr">Lin et al., 2022a)</ref>; action primitives coupled with motion planning are more reliable in solving the task such as approaching a target object via a collision-free path; reinforcement learning and evolutionary strategy better suits tasks that are contact rich and involving continuous interaction with other scene components, e.g., legged locomotion, or when the required actions cannot be simply parameterized by discrete end-effector poses, e.g., turning the knob of an oven. We provide examples and let GPT-4 choose online which learning algorithm to use conditioned on the generated sub-task. We consider action primitives including grasping, approaching and releasing a target object. Since parallel jaw gripper can be limited when grasping objects with diverse sizes, we consider a robotic manipulator equipped with a suction cup to simplify object grasping. The grasping and approaching primitives are implemented as follows: we first randomly sample a point on the target object or link, compute a gripper pose that aligns with the normal of the sampled point, and then use motion planning to find a collision-free path to reach the target gripper pose. After the pose is reached, we keep moving along the normal direction until there contact is made with the target object.</p><p>For sub-tasks to be learned using RL or trajectory optimization, we prompt GPT-4 to write the corresponding reward functions with a few in-context examples. For object manipulation and locomotion tasks, the reward functions are based on the low-level simulation states which GPT-4 can query via a provided list of API calls. Further, we ask GPT-4 to suggest the action space for the learning algorithm, e.g., delta-translation of the end-effector, or the target location for the end-effector to move to. Delta-translation is more appropriate for tasks that involve local movements, e.g., opening a door after it has been grasped; for tasks that involve transferring an object to a different location, directly specifying the target as the action space makes the learning easier. For soft body manipulation tasks, the reward has a fixed form specified as the earth-mover distance between the current and target shape of the soft body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SKILL LEARNING</head><p>Once we obtained all the required information for the proposed task, including scene components and configurations, task decompositions, and training supervisions for the decomposed sub-tasks, we are able to construct the scene in simulation for the robot to learn the required skills for completing the task.</p><p>As aforementioned, we use a combination of techniques for skill learning, including reinforcement learning, evolutionary strategy, gradient-based trajectory optimization, and action primitive with motion planning, as each are suitable for different types of tasks. For object manipulation tasks, we use SAC <ref type="bibr" target="#b20">(Haarnoja et al., 2018)</ref> as the RL algorithm for learning the skill. The observation space is the low-level state of the objects and robot in the task. The action space of the RL policy includes the delta translation or target location (determined by GPT-4) of the robot end-effector, along with its delta rotation. We use BIT * <ref type="bibr" target="#b12">(Gammell et al., 2015)</ref> implemented in the Open Motion Planning Library (OMPL) <ref type="bibr" target="#b67">(Sucan et al., 2012)</ref> as the underlying motion planning algorithm for the action primitives. For long-horizon tasks that involve multiple sub-tasks, we adopt a simple scheme of learning each sub-task sequentially: for each sub-task, we run RL N = 8 times and use the end state with the highest reward as the initial state for the next sub-task. For locomotion tasks, the cross entropy method (CEM <ref type="bibr" target="#b7">(De Boer et al., 2005)</ref>) is used for skill learning, which we find to be more stable and efficient than RL. The ground-truth simulator is used as the dynamcis model in CEM, and the actions to be optimized are the joint angle values of the robot. For soft body manipulation tasks, we run gradient-based trajectory optimization with Adam (Kingma &amp; Ba, 2014) for learning the skill, where the gradients are provided by the fully differentiable simulator we use. More details on skill learning can be found in Appendix A.</p><p>? Task Diversity: How diverse are the tasks proposed by RoboGen robotic skill learning?</p><p>? Scene Validity: Does RoboGen generate valid simulation environments that match the proposed task descriptions?</p><p>? Training Supervision Validity: Does RoboGen generate correct task decomposition and training supervisions for the task that will induce intended robot skills?</p><p>? Skill Learning: Does integrating different learning algorithms in RoboGen improve the success rate of learning a skill?</p><p>? System: Combining all the automated stages, can the whole system produce diverse and meaningful robotic skills?</p><p>4.1 EXPERIMENTAL SETUP Our proposed system is generic and agnostic to specific simulation platforms. However, since we consider a wide range of task categories ranging from rigid dynamics to soft body simulation, and also consider skill learning methods such as gradient-based trajectory optimization which necessitates a differentiable simulation platform, we used Genesis for deploying RoboGen, a simulation platform for robot learning with diverse materials and fully differentiable<ref type="foot" target="#foot_1">1</ref> . For skill learning, we use SAC <ref type="bibr" target="#b20">(Haarnoja et al., 2018)</ref> as the RL algorithm. The policy and Q networks are both Multilayer Perceptrons (MLP) of size <ref type="bibr">[256,</ref><ref type="bibr">256,</ref><ref type="bibr">256]</ref>, trained with a learning rate of 3e -4. For each sub-task, we train with 1M environment steps. We use BIT * <ref type="bibr" target="#b12">(Gammell et al., 2015)</ref> as the motion planning algorithm, and Adam Kingma &amp; Ba (2014) for gradient-based trajectory optimization for soft body manipulation tasks. More implementation details of these learning algorithms can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EVALUATION METRICS AND BASELINES</head><p>We use the following metrics and baselines for evaluating our system:</p><p>Task Diversity The diversity of the generated tasks can be measured in many aspects, such as the semantic meanings of the tasks, scene configurations of the generated simulation environments, the appearances and geometries of the retrieved object assets, and the robot actions required to perform the task. For semantic meanings of the tasks, we perform quantitative evaluations by computing the Self-BLEU and the embedding similarity <ref type="bibr">(Zhu et al., 2018)</ref> on the generated task descriptions, where lower scores indicate better diversity. We compare to established benchmarks, including RLBench <ref type="bibr" target="#b26">(James et al., 2020)</ref>, Maniskill2 <ref type="bibr" target="#b17">(Gu et al., 2023)</ref>, Meta-World <ref type="bibr" target="#b86">(Yu et al., 2020)</ref>, and Behavior-100 <ref type="bibr" target="#b66">(Srivastava et al., 2022)</ref>. For object assets and robot actions, we evaluate RoboGen qualitatively using the generated simulation environments and visualizations of learned robot skills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Validity</head><p>To verify that the retrieved objects match the requirements of the task, we compute the BLIP-2 scores <ref type="bibr">(Li et al., 2023b)</ref> between rendered images of the retrieved objects in the simulation scene, and the text descriptions of the objects. We compare with two ablations of our system. A)</p><p>No object verification: we do not use a VLM to verify the retrieved object, and just retrieve objects based on text matching. B) No size verification: we do not use object sizes outputted by GPT-4; instead, we use the default size of the asset as provided in Objaverse or PartNetMobility.</p><p>Training Supervision Validity We perform qualitative evaluations on this by presenting images and videos of the learned skills using the generated decomposition and training supervisions.</p><p>Skill Learning Performance We compare to an ablation where we remove the options of using motion planning-based primitive and gradient-based trajectory optimization, and rely purely on reinforcement learning to learn the skills. Given a task, we run each method with 4 different seeds, and report the mean and std of the task return (we manually verified that the reward functions generated by GPT-4 are correct for the evaluated tasks).</p><p>System We perform qualitative evaluations of the whole system, by providing videos of over 100 learned skills on our websites. In the paper, we show snapshots of representative skills. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Diversity</head><p>The quantitative evaluation results are presented in Table <ref type="table" target="#tab_0">1</ref>. We compare to a version of RoboGen where a total of 106 tasks are generated. As shown, RoboGen achieves the lowest Self-BLEU and embedding similarity compared to all previous benchmarks, indicating that the diversity of the generated tasks is higher than prior manually established benchmarks under both metrics. This demonstrates that RoboGen can generate a set of tasks whose diversity matches or surpasses prior manually crafted skill learning benchmarks and datasets. Scene Validity Figure <ref type="figure" target="#fig_2">4</ref> shows the BLIP-2 score of all compared methods on 7 example generated tasks. As shown, removing the size verification leads to drastic decrease in BLIP-2 score. This is expected as the default asset sizes from Objaverse and PartNetMobility can be drastically different from plausible real-world sizes. The ablation "No object verification" also has lower BLIP-2 score, and with a larger variances, indicating our verification step improves object selection validity. The results demonstrate the importance of using both object and size verification in RoboGen.</p><p>Training Supervision Validity Figure <ref type="figure" target="#fig_1">3</ref> demonstrates the skills learned with the generated training supervisions from RoboGen, i.e., the task decompositions and reward functions, on 4 example longhorizon tasks. As shown, the robot successfully learns skills to complete the corresponding tasks, suggesting that the automatically generated training supervisions are effective in deriving meaningful and useful skills.</p><p>Skill Learning We evaluate on 4 tasks that involve interactions with articulated objects. The results are presented in Table <ref type="table" target="#tab_2">2</ref>. We find that allowing learning algorithm selection is beneficial for achieving higher performance for completing the tasks. When only RL is used, the skill learning completely fails for most tasks.   System Figure <ref type="figure">1</ref> and 3 visualize some of the generated tasks and learned skills from RoboGen. As shown in Figure <ref type="figure">1</ref>, RoboGen can generate diverse tasks for skill learning spanning from rigid/articulated object manipulation, locomotion, and soft body manipulation. Figure <ref type="figure" target="#fig_1">3</ref> further shows that RoboGen is able to deliver long-horizon manipulation skills with reasonable decompositions.</p><p>For extensive qualitative results of proposed tasks and learned skills, please refer to our project site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION &amp; LIMITATIONS</head><p>We introduced RoboGen, a generative agent that automatically proposes and learns diverse robotic skills at scale via generative simulation. RoboGen utilizes the latest advancements in foundation models to automatically generate diverse tasks, scenes, and training supervisions in simulation, making a foundational step towards scalable robotic skill learning in simulation, while requiring minimal human supervision once deployed. Our system is a fully generative pipeline that can be queried endlessly, producing a large number of skill demonstrations associated with diverse tasks and environments. RoboGen is agnostic to the backend foundation models, and can be upgraded continuously using latest models when available.</p><p>Our current system still has several limitations: 1) Large-scale verification of learned skills (i.e., whether the resultant skill really solves the corresponding task by text descriptions) is still a challenge in the current pipeline. This could potentially be addressed by using better multi-modal foundation models in the future. Contemporary works <ref type="bibr">(Ma et al., 2023)</ref> have also explored using environment feedback for iterative refinement on the generated supervisions (reward functions), which we hope to integrate into our paradigm in the future. 2) Our paradigm is intrinsically constrained by a sim-to-real gap when it comes to real-world deployment. However, with the latest and rapid advancement in physically accurate simulation, as well as techniques such as domain randomization and realistic sensory signal rendering, we expect the sim-to-real gap to be narrowed further in the coming future. 3) Our system assumes that with a right reward function, existing policy learning algorithms are sufficient for learning the proposed skills. For the policy learning algorithms we tested in this paper (RL with SAC and an action space of delta end-effector pose, and gradient-based trajectory optimization), we observe they are still not robust enough and multiple runs are often required to produce successful skill demos for certain generated tasks. We leave integration of more powerful policy learning algorithms into RoboGen, e.g., those with better action parameterizations <ref type="bibr">(Zeng et al., 2021;</ref><ref type="bibr" target="#b63">Seita et al., 2023)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS</head><p>For reinforcement learning, we use SAC <ref type="bibr" target="#b20">(Haarnoja et al., 2018)</ref> as the RL algorithm. For both rigid object manipulation and locomotion tasks, the observation space is the low-level state of the objects and robot in the task. The policy and Q networks used in SAC are both Multi-layer Perceptrons (MLP) of size <ref type="bibr">[256,</ref><ref type="bibr">256,</ref><ref type="bibr">256]</ref>. We use a learning rate of 3e -4 for the actor, the critic, and the entropy regularizer. The horizon of all manipulation tasks are 100, with a frameskip of 2, and the horizon for all locomotion tasks are 150, with a frameskip of 4. The action of the RL policy is 6d: where the first 3 elements determines the translation, either as delta translation or target location (suggested by GPT-4), and the second 3 elements determines the delta rotation, expressed as deltaaxis angle in the gripper's local frame. For each sub-task, we train with 1M environment steps.</p><p>For action primitives, we use BIT * <ref type="bibr" target="#b12">(Gammell et al., 2015)</ref> implemented in the Open Motion Planning Library (OMPL) Sucan et al. ( <ref type="formula">2012</ref>) as the motion planning algorithm. For the grasping and the approaching primitive, we first sample a surface point on the target object or link, then compute a gripper pose that aligns the gripper y axis with the normal of the sampled point. The pre-contact gripper pose is set to be 0.03m above the surface point along the normal direction. Motion planning is then used to find a collision-free path to reach the target gripper pose. After the target gripper pose is reached, we keep moving the gripper along the normal until contact is made.</p><p>For soft body manipulation tasks, we use Adam Kingma &amp; Ba (2014) for gradient-based trajectory optimization. We run trajectory optimization for 300 gradient steps. We use a learning rate of 0.05 for the optimizer. The horizons of all manipulation tasks are either 150 or 200. We use Earth Mover's distance between object's current and target shape as the cost function for trajectory optimization.</p><p>For querying GPT-4, we used a temperature between 0.8 -1.0 for task proposal to ensure diversity in the generated tasks. For all other stages of RoboGen, we use temperature values between 0 -0.3 to ensure more robust responses from GPT-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROMPTS</head><p>Pre-defined tasks for example-based initialization of RoboGen.</p><p>""" Task: stack two blocks, with the larger one at the bottom. Object: A small block, and a large block. """, """ Taks: Put the broccoli on the grill pan Objects: a broccoli, a grill pan """, """ Task: Put 1 mug on the cup holder Objects: A mug, a mug tree holder """, """ Task: Pick up the hanger and place it on the clothing rack Objects: a cloth hanger, a clothing rack """, """ Task: Put 1 book into the bookshelf Objects: a book, a bookshelf """, """ Taks: Put the knife on the chopping board Objects: a kitchen knife, a board """, """ Task: Put a old toy in bin Objects: A old toy, a rubbish bin """, """ Task: Place the dishes and cutlery on the table in preparation for a meal Objects: a dish plate, a fork, a spoon, a steak knife """, """ Task: Stack one cup on top of the other Objects: Two same cups """, """ Task: Remove the green pepper from the weighing scales and place it on the floor Objects: A green pepper, a weighing scale """, """ Task: Put the apple on the weighing scale to weigh it Objects: An apple, a weighing scale """,</p><p>In the following, we show all prompts used for generating an articulated object manipulation task using RoboGen.</p><p>Task Proposal prompt. We show an example where the sampled object is a trashcan for object manipulation task generation.</p><p>I will give you an articulated object, with its articulation tree and semantics. Your goal is to imagine some tasks that a robotic arm can perform with this articulated object in household scenarios. You can think of the robotic arm as a Franka Panda robot. The task will be built in a simulator for the robot to learn it.</p><p>Focus on manipulation or interaction with the object itself. Sometimes the object will have functions, e.g., a microwave can be used to heat food, in these cases, feel free to include other objects that are needed for the task. Please do not think of tasks that try to assemble or disassemble the object. Do not think of tasks that aim to clean the object or check its functionality.</p><p>For each task you imagined, please write in the following format: Task name: the name of the task. Description: some basic descriptions of the tasks.</p><p>Additional Objects: Additional objects other than the provided articulated object required for completing the task. Links: Links of the articulated objects that are required to perform the task.</p><p>-Link 1: reasons why this link is needed for the task -Link 2: reasons why this link is needed for the task -... Joints: Joints of the articulated objects that are required to perform the task.</p><p>-Joint 1: reasons why this joint is needed for the task -Joint 2: reasons why this joint is needed for the task -...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Input:</head><p>'''Oven articulation tree links: base link_0 link_1 link_2 link_3 link_4 link_5 link_6 link_7 joints: joint_name: joint_0 joint_type: revolute parent_link: link_7 child_link: link_0 joint_name: joint_1 joint_type: continuous parent_link: link_7 child_link: link_1 joint_name: joint_2 joint_type: continuous parent_link: link_7 child_link: link_2 joint_name: joint_3 joint_type: continuous parent_link: link_7 child_link: link_3 joint_name: joint_4 joint_type: continuous parent_link: link_7 child_link: link_4 joint_name: joint_5 joint_type: continuous parent_link: link_7 child_link: link_5 joint_name: joint_6 joint_type: continuous parent_link: link_7 child_link: link_6 joint_name: joint_7 joint_type: fixed parent_link: base child_link: link_7 ''' '''Oven semantics link_0 hinge door link_1 hinge knob link_2 hinge knob link_3 hinge knob link_4 hinge knob link_5 hinge knob link_6 hinge knob link_7 heavy oven_body '''</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example output:</head><p>Task Name: Open Oven Door Description: The robotic arm will open the oven door. Additional Objects: None Links: -link_0: from the semantics, this is the door of the oven. The robot needs to approach this door in order to open it. Joints:</p><p>The format is as follows: '''yaml -use_table: whether the task requires using a table. This should be decided based on common sense. If a table is used, its location will be fixed at (0, 0, 0). The height of the table will be 0.6m. Usually, if the objects invovled in the task are usually placed on a table (not directly on the ground), then the task requires using a table. # for each object involved in the task, we need to specify the following fields for it.</p><p>-type: mesh name: name of the object, so it can be referred to in the simulator size: describe the scale of the object mesh using 1 number in meters. The scale should match real everyday objects. E.g., an apple is of scale 0.08m. You can think of the scale to be the longest dimension of the object. lang: this should be a language description of the mesh. The language should be a concise description of the obejct, such that the language description can be used to search an existing database of objects to find the object. path: this can be a string showing the path to the mesh of the object. on_table: whether the object needs to be placed on the table (if there is a table needed for the task). This should be based on common sense and the requirement of the task. E.g., a microwave is usually placed on the table. center: the location of the object center. If there isn't a table needed for the task or the object does not need to be on the table, this center should be expressed in the world coordinate system. If there is a table in the task and the object needs to be placed on the table, this center should be expressed in terms of the table coordinate, where (0, 0, 0) is the lower corner of the table, and (1, 1, 1) is the higher corner of the table. In either case, you should try to specify a location such that there is no collision between objects. ''' An example input includes the task names, task descriptions, and objects involved in the task. I will also provide with you the articulation tree and semantics of the articulated object. This can be useful for knowing what parts are already in the articulated object, and thus you do not need to repeat those parts as separate objects in the yaml file.</p><p>Your task includes two parts: 1. Output the yaml configuration of the task. 2. Sometimes, the task description / objects involved will refer to generic/placeholder objects, e.g., to place an "item" into the drawer, and to heat "food" in the microwave. In the generated yaml config, you should change these placeholder objects to be concrete objects in the lang field, e.g., change "item" to be a toy or a pencil, and "food" to be a hamburger, a bowl of soup, etc.</p><p>Example input: Task Name: Insert Bread Slice Description: The robotic arm will insert a bread slice into the toaster. Objects involved: Toaster, bread slice. Only the objects specified here should be included in the yaml file.</p><p>'''Toaster articulation tree links: base link_0 link_1 link_2 link_3 link_4 link_5 joints: joint_name: joint_0 joint_type: continuous parent_link: link_5 child_link: link_0 joint_name: joint_1 joint_type: prismatic parent_link: link_5 child_link: link_1 joint_name: joint_2 joint_type: prismatic parent_link: link_5 child_link: link_2 joint_name: joint_3 joint_type: prismatic parent_link: link_5 child_link: link_3 joint_name: joint_4 joint_type: prismatic parent_link: link_5 child_link: link_4 joint_name: joint_5 joint_type: fixed parent_link: base child_link: link_5 ''' '''Toaster semantics link_0 hinge knob link_1 slider slider link_2 slider button link_3 slider button link_4 slider button link_5 free toaster_body '''</p><p>An example output: '''yaml -use_table: True ### Toaster and bread are usually put on a table.</p><p>-type: mesh name: "Toaster" on_table: True # Toasters are usually put on a table. center: (0.1, 0.1, 0) # Remember that when an object is placed on the table, the center is expressed in the table coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.</p><p>Here we put the toaster near the lower corner of the table. size: 0.35 # the size of a toaster is roughly 0.35m lang: "a common toaster" path: "toaster.urdf" -type: mesh name: "bread slice" on_table: True # Bread is usually placed on the table as well. center: (0.8, 0.7, 0) # Remember that when an object is placed on the table, the center is expressed in the table coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.</p><p>Here we put the bread slice near the higher corner of the table. size: 0.1 # common size of a bread slice lang: "a slice of bread" Path: "bread_slice.obj" ''' Another example input: Task Name: Removing Lid From Pot Description: The robotic arm will remove the lid from the pot. Objects involved: KitchenPot. Only the objects specified here should be included in the yaml file. '''KitchenPot articulation tree links: base link_0 link_1 joints: joint_name: joint_0 joint_type: prismatic parent_link: link_1 child_link: link_0 joint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1 ''' '''KitchenPot semantics link_0 slider lid link_1 free pot_body ''' Output: '''yaml -use_table: True # A kitchen pot is usually placed on the table . 
-type: mesh name: "KitchenPot" on_table: True # kitchen pots are usually placed on a table. center: (0.3, 0.6, 0) # Remember that when an object is placed on the table, the center is expressed in the table coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.</p><p>Here we put the kitchen pot just at a random location on the table. size: 0.28 # the size of a common kitchen pot is roughly 0.28m lang: "a common kitchen pot" path: "kitchen_pot.urdf" ''' Note in this example, the kitchen pot already has a lid from the semantics file. Therefore, you do not need to include a separate lid in the yaml file.</p><p>One more example input: Task Name: Heat a hamburger in the oven. Description: The robotic arm will put a hamburger in the oven and use the oven to heat it. Objects involved: A hamburger, an oven. Only the objects here should be included in the yaml file. Output: '''yaml -use_table: True # Although an oven is usually just placed on the ground, a hamburger is usually placed on the table. -type: mesh name: "Oven" on_table: False # An oven is usually just placed on the floor. center: (1.0, 0, 0) # Remember that when not on a table, the center is expressed in the world coordinate. Since the robot is at (1, 1, 0) and the table is at (0, 0, 0), we place the oven at (1.8, 2, 0) to avoid collision with the table and the robot. size: 0.9 # the size of an oven is roughly 0.9m lang: "a common oven" path: "oven.urdf" -type: mesh name: "hamburger" on_table: True # a hamburger is usually placed on the table. center: (0.2, 0.4, 0) # Remember that when an object is placed on the table, the center is expressed in the table coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.</p><p>Here we put the hamburger just at a random location on the table. size: 0.1 # common hamburgers' diameters are 10 cm. lang: "a common hamburger" path: "hamburger.obj" ''' Rules: -You do not need to include the robot in the yaml file.</p><p>-The yaml file should only include the objects listed in "Objects involved".</p><p>-Sometimes, the task description / objects involved will refer to generic/placeholder objects, e.g., to place an "item" into the drawer, and to heat "food" in the microwave. In the generated yaml config, you should change these placeholder objects to be concrete objects in the lang field, e.g., change "item" to be a toy or a pencil, and "food" to be a hamburger, a bowl of soup, etc.</p><p>Can you do this for the following task: Task Name: Throw Trash Away Description: The robotic arm places an item of trash inside the trash can Objects involved: TrashCan, A pile of trash '''TrashCan articulation tree links: base link_0 link_1 joints: joint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0 joint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1 ''' '''TrashCan semantics link_0 hinge door link_1 free trashcan_body '''</p><p>An example response from GPT-4:</p><p>Output: '''yaml -use_table: False # Trash cans are usually placed on the floor.</p><p>-type: mesh name: "TrashCan" on_table: False # Trash cans are usually placed on the floor. center: (1.5, 1.5, 0) # Remember that when not on a table, the center is expressed in the world coordinate. Since the robot is at (1, 1, 0) and the table is at (0, 0, 0), we place the trash can at (1.5, 1.5, 0) to avoid collision with the table and the robot. size: 0.6 # the size of a common trash can is roughly 0.6m lang: "a common trash can" path: "trashcan.urdf" -type: mesh name: "Trash" on_table: True # Trash is usually picked up from a table or the floor. Here we assume it's on the table. center: (0.5, 0.5, 0) # Remember that when an object is placed on the table, the center is expressed in the table coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.</p><p>Here we put the trash in the middle of the table. size: 0.1 # common size of a pile of trash lang: "a pile of paper waste" # Note here, we changed the generic/placeholder "pile of trash" object to be a more concrete object: a pile of paper waste. path: "paper_waste.obj" ''' More relevant assets prompt.</p><p>Given a task, which is for a mobile Franka panda robotic arm to learn a manipulation skill in the simulator, your goal is to add more objects into the task scene such that the scene looks more realistic. The Franka panda arm is mounted on a floor, at location (1, 1, 0). It can move freely on the floor. The z axis is the gravity axis.</p><p>The input to you includes the following: Task name, task description, the essential objects involved in the task, and a config describing the current task scene, which contains only the essential objects needed for the task. The config is a yaml file in the following format:</p><p>'''yaml -use_table: whether the task requires using a table. This should be decided based on common sense. If a table is used, its location will be fixed at (0, 0, 0). The height of the table will be 0.6m. # for each object involved in the task, we need to specify the following fields for it.</p><p>-type: mesh name: name of the object, so it can be referred to in the simulator size: describe the scale of the object mesh using 1 number in meters. The scale should match real everyday objects. E.g., an apple is of scale 0.08m. You can think of the scale to be the longest dimension of the object.</p><p>lang: this should be a language description of the mesh. The language should be a bit detailed, such that the language description can be used to search an existing database of objects to find the object. path: this can be a string showing the path to the mesh of the object. on_table: whether the object needs to be placed on the table (if there is a table needed for the task). This should be based on common sense and the requirement of the task. center: the location of the object center. If there isn't a table needed for the task or the object does not need to be on the table, this center should be expressed in the world coordinate system. If there is a table in the task and the object needs to be placed on the table, this center should be expressed in terms of the table coordinate, where (0, 0, 0) is the lower corner of the table <ref type="table">,</ref> and<ref type="table" target="#tab_0">(1, 1, 1</ref>) is the higher corner of the table. In either case, you should try to specify a location such that there is no collision between objects. ''' Your is to think about what other distractor objects can be added into the scene to make the scene more complex and realistic for the robot to learn the task. These distractor objects are not necessary for the task itself, but their existence makes the scene look more interesting and complex. You should output the distractor objects using the same format as the input yaml file. You should try to put these distractor objects at locations such that they don't collide with objects already in the scene.</p><p>Here is one example:</p><p>Input:</p><p>Task name: Heat up a bowl of soup in the microwave Task description: The robot will grab the soup and move it into the microwave, and then set the temperature to heat it. Output: '''yaml -name: plate # a plate is a common object placed when there is microwave and bowl of soup, in a kitchen setup lang: a common kitchen plate on_table: True center: (0.8, 0.8, 0) type: mesh path: "plate.obj" size: 0.15 # a plate is usually of scale 0.15m -name: sponge # a sponge is a common object placed when there is microwave and bowl of soup, in a kitchen setup lang: a common sponge on_table: True center: (0.5, 0.2, 0) type: mesh path: "sponge.obj" size: 0.1 # a sponge is usually of scale 0.1m -name: Oven # a oven is a common object placed when there is microwave and bowl of soup, in a kitchen setup lang: a kitchen oven on_table: False # an oven is usually a standalone object on the floor center: (1.8, 0.5, 0) # remember robot is at (1, 1, 0) and table is at (0, 0, 0). So the oven is placed at (1.8, 0.5, 0) in the world coordinate system to avoid collision with other objects. type: mesh path: "oven.obj" size: 0.8 # an oven is usually of scale 0.8m ''' Can you do it for the following task: Task name: Throw Trash Away Task description: The robotic arm places an item of trash inside the trash can Initial config: '''yaml -use_table: false -center: (1.5, 1.5, 0) lang: a common trash can name: TrashCan on_table: false path: trashcan.urdf size: 0.6 type: urdf -center: (0.5, 0.5, 0) lang: a pile of paper waste name: Trash on_table: true path: paper_waste.obj size: 0.05 type: mesh '''</p><p>An example response from GPT-4:</p><p>Sure, here is the output for the given task:</p><p>'''yaml -name: broom # a broom is a common object found near a trash can lang: a common broom on_table: False center: (2, 1.5, 0) type: mesh path: "broom.obj" size: 1.2 # a broom is usually of scale 1.2m -name: dustpan # a dustpan is a common object found near a trash can lang: a common dustpan on_table: False center: (2, 2, 0) type: mesh path: "dustpan.obj" size: 0.3 # a dustpan is usually of scale 0.3m -recycling_bin # a recycling bin is a common object found near a trash can lang: a common recycling bin on_table: False center: (1, 2, 0) type: mesh path: "recycling_bin.obj" size: 0.6 # a recycling bin is usually of scale 0.6m -name: soda_can # a soda can is a common object found near a trash can lang: a common soda can on_table: True center: (0.7, 0.7, 0) type: mesh path: "soda_can.obj" size: 0.1 # a soda can is usually of scale 0.1m '''</p><p>Asset size adjustment prompt. For each object used in the scene, we query GPT-4 to correct its size such that the size matches the real-world size. We continue the above example of throw trash away.</p><p>A robotic arm is trying to manipulate some objects to learn corresponding skills in a simulator. However, the size of the objects might be wrong. Your task is to adjust the size of the objects, such that they match each other when interact with each other; and the size should also match what is commonly seen in everyday life, in household scenarios. Now I will give you the name of the task, the object and their sizes, please correct any unreasonable sizes.</p><p>Objects are represented using a mesh file, you can think of size as the longest dimension of the object. I will write in the following format: ''' Task: task description obj1, mesh, size obj2, mesh, size ''' Please reply in the following format: explanations of why some size is not reasonable. '''yaml obj1, mesh, corrected_size obj2, mesh, corrected_radius '''</p><p>Here is an example: Input: ''' Task: The robotic arm lowers the toilet seat from an up position to a down position Toilet, mesh, 0.2 ''' Output: A toilet is usually 0.6 -0.8m in its back height, so the size is not reasonable --it is a bit too small.</p><p>Below is the corrected size. '''yaml Toilet, mesh, 0.7 ''' Another example: Input: ''' Task: Fill a cup with water under the faucet Faucet, mesh, 0.25 Cup, mesh, 0.3 ''' Output:</p><p>The size of the faucet makes senes. However, the size of the cup is too large for 2 reasons: it does not match the size of tha faucet for getting water under the faucet; and it is not a common size of cup in everyday life. Below is the corrected size. '''yaml Faucet, mesh, 0.25 Cup, mesh, 0.12 ''' One more example to show that even if no change is needed, you should still reply with the same size. Input: ''' Task: Open Table <ref type="table">Drawer</ref> The robotic arm will open a table drawer table, mesh, 0.8 '''</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>The size of the table is reasonable, so no change is needed. '''yaml table, mesh, 0.8 ''' This is also a good example to show that sometimes, the task description might include two objects, e.g., a table and a drawer, yet there is only one object size provided (here the table). This is not an error, but that the other object is part of the provided object, i.e., here the drawer is part of the table. It 's fine, you should then just reply with the corrected size of the object provided, here, the table, in such cases.</p><p>''' Task: Throw Trash Away The robotic arm places an item of trash inside the trash can trashcan, mesh, 0.6 trash, mesh, 0.1 '''</p><p>An example response from GPT-4:</p><p>The size of the trashcan is reasonable as it is a common size for a household trashcan. However, the size of the trash is a bit too large. Trash items are usually smaller to fit inside the trashcan. Below is the corrected size. '''yaml trashcan, mesh, 0.6 trash, mesh, 0.05 '''</p><p>For scene generation, we also need to specify the asset configuration, and any special scene configuration.</p><p>Adjusting articulated object joint prompt. We continue the example of throwing the trash away.</p><p>Your goal is to set the joint angles of some articulated objects to the right value in the initial state, given a task. The task is for a robot arm to learn the corresponding skills to manipulate the articulated object.</p><p>The input to you will include the task name, a short description of the task, the articulation tree of the articulated object, a semantic file of the articulated object, the links and joints of the articulated objects that will be involved in the task, and the substeps for doing the task.</p><p>You should output for each joint involved in the task, what joint value it should be set to. You should output a number in the range [0, 1], where 0 corresponds to the lower limit of that joint angle, and 1 corresponds to the upper limit of the joint angle. You can also output a string of "random", which indicates to sample the joint angle within the range.</p><p>By default, the joints in an object are set to their lower joint limits. You can assume that the lower joint limit corresponds to the natural state of the articulated object. E.g., for a door's hinge joint, 0 means it is closed, and 1 means it is open. For a lever, 0 means it is unpushed, and 1 means it is pushed to the limit.</p><p>Here is an example:</p><p>Input: Task Name: Close the door Description: The robot arm will close the door after it was opened.</p><p>'''door articulation tree links: base link_0 link_1 link_2 joints: joint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0 joint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1 joint_name: joint_2 joint_type: revolute parent_link: link_0 child_link: link_2 ''' '''door semantics link_0 hinge rotation_door link_1 static door_frame link_2 hinge rotation_door ''' Links:</p><p>-link_0: link_0 is the door. This is the part of the door assembly that the robot needs to interact with. Joints: -joint_0: Joint_0 is the revolute joint connecting link_0 (the door) as per the articulation tree. The robot needs to actuate this joint cautiously to ensure the door is closed.</p><p>substeps: approach the door close the door Output:</p><p>The goal is for the robot arm to learn to close the door after it is opened. Therefore, the door needs to be initially opened, thus, we are setting its value to 1, which corresponds to the upper joint limit. '''joint values joint_0: 1 ''' Another example: Task Name: Turn Off Faucet Description: The robotic arm will turn the faucet off by manipulating the switch '''Faucet articulation tree links: base link_0 link_1 joints: joint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0 joint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1 ''' '''Faucet semantics link_0 static faucet_base link_1 hinge switch ''' Links: -link_0: link_0 is the door. This is the part of the door assembly that the robot needs to interact with. Joints: -joint_0: Joint_0 is the revolute joint connecting link_0 (the door) as per the articulation tree. The robot needs to actuate this joint cautiously to ensure the door is closed.</p><p>substeps: grasp the faucet switch turn off the faucet Output:</p><p>For the robot to learn to turn off the faucet, it cannot be already off initially. Therefore, joint_1 should be set to its upper joint limit, or any value that is more than half of the joint range, e.g., 0.8. '''joint value joint_1: 0.8 ''' One more example: Task Name: Store an item inside the Drawer Description: The robot arm picks up an item and places it inside the drawer of the storage furniture '''StorageFurniture articulation tree links: base link_0 link_1 link_2 joints: joint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0 joint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1 joint_name: joint_2 joint_type: prismatic parent_link: link_1 child_link: link_2 ''' '''StorageFurniture semantics link_0 hinge rotation_door link_1 heavy furniture_body link_2 slider drawer ''' Links: -link_2: link_2 is the drawer link from the semantics. The robot needs to open this drawer to place the item inside. Joints: -joint_2: joint_2, from the articulation tree, connects to link_2 (the drawer). Thus, the robot would need to actuate this joint to open the drawer to store the item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>substeps:</head><p>grasp the drawer open the drawer grasp the item put the item into the drawer grasp the drawer again close the drawer release the grasp Output:</p><p>This task involves putting one item into the drawer of the storage furniture. As noted in the substeps, the robot needs to first open the drawer, put the item in, and then close it. Since the articulated object is initialized with the lower joint limit, i.e., the drawer is initially closed, it aligns with the task where the robot needs to first learn to open the drawer. Therefore, no particular joint angle needs to be set, and we just output None. Links: link_0: Link_0, as the trash can lid, needs to be approached and opened by the robot in order to throw away the trash.</p><p>Joints: joint_0: Joint_0, connecting to link_0, needs to be actuated for the robot to open the trash can lid.</p><p>substeps: grasp the trash can lid open the trash can lid grasp the trash put the trash into the trash can release the trash grasp the trash can lid again close the trash can lid</p><p>An example output from GPT-4:</p><p>Output: The task involves the robot learning to throw trash away. The robot needs to learn to open the trash can lid, throw the trash in, and then close the lid. Therefore, we need to set the initial joint angle such that the trash can lid is not already open. We can set joint_0 to its lower limit, which corresponds to the trash can lid being closed.</p><p>'''joint value joint_0: 0 '''</p><p>Special spatial relationship prompt. We keep the "Throw away trash" example.</p><p>Your goal is to output any special spatial relationships certain objects should have in the initial state, given a task. The task is for a robot arm to learn the corresponding skills in household scenarios.</p><p>The input to you will include the task name, a short description of the task, objects involved in the task, substeps for performing the task, If there is an articulated object involved in the task, the articulation tree of the articulated object, the semantic file of the articulated object, and the links and joints of the articulated objects that will be involved in the task.</p><p>We have the following spatial relationships: on, obj_A, obj_B: object A is on top of object B, e.g., a fork on the The goal is for the robot arm to learn to retrieve an item from the refrigerator. Therefore, the item needs to be initially inside the refrigerator. From the refrigerator semantics we know that link_0 is the body of the refrigerator, therefore we should have a spatial relationship as the following: '''spatial relationship In, item, refrigerator, link_0 ''' Another example: Task Name: Turn Off Faucet Description: The robotic arm will turn the faucet off by manipulating the switch Objects involved: faucet '''Faucet articulation tree links: base link_0 link_1 joints: joint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0 joint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1 ''' '''Faucet semantics link_0 static faucet_base link_1 hinge switch ''' Links: link_0: link_0 is the door. This is the part of the door assembly that the robot needs to interact with. Joints: joint_0: Joint_0 is the revolute joint connecting link_0 (the door) as per the articulation tree. The robot needs to actuate this joint cautiously to ensure the door is closed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>substeps: grasp the faucet switch turn off the faucet</head><p>Output:</p><p>There is only 1 object involved in the task, thus no special spatial relationships are required. '''spatial relationship None '''</p><p>Here is a list of primitives the robot can do. The robot is equipped with a suction gripper, which makes it easy for the robot to grasp an object or a link on an object. grasp_object(self, object_name): the robot arm will grasp the object specified by the argument object name. grasp_object_link(self, object_name, link_name): some object like an articulated object is composed of multiple links. The robot will grasp a link with link_name on the object with object_name. release_grasp(self): the robot will release the grasped object. approach_object(self, object_name): this function is similar to grasp_object, except that the robot only approaches the object, without grasping it. approach_object_link(self, object_name, link_name): this function is similar to grasp_object_link, except that the robot only approaches the object's link, without grasping it. Note that all primitives will return a tuple (rgbs, final_state) which represents the rgb images of the execution process and the final state of the execution process. You should always call the primitive in the following format: rgbs, final_state = some_primitive_function <ref type="bibr">(self, arg1, ..., argn)</ref> Here is a list of helper functions that you can use for designing the reward function or the success condition : get_position(self, object_name): get the position of center of mass of object with object_name. get_orientation(self, object_name): get the orientation of an object with object_name. detect(self, object_name, object_part): detect the position of a part in object. E.g., the opening of a toaster, or the handle of a door. get_joint_state(self, object_name, joint_name): get the joint angle value of a joint in an object. get_joint_limit(self, object_name, joint_name): get the lower and upper joint angle limit of a joint in an object, returned as a 2-element tuple. get_link_state(self, object_name, link_name): get the position of the center of mass of the link of an object. get_eef_pos(self): returns the position, orientation of the robot end-effector as a list. get_bounding_box(self, object_name): get the axis-aligned bounding box of an object. You can assume that for objects, the lower joint limit corresponds to their natural state, e.g., a box is closed with the lid joint being 0, and a lever is unpushed when the joint angle is 0.</p><p>For the above task "Fetch item from refrigerator", it can be decomposed into the following substeps, primitives, and reward functions:</p><p>substep 1: grasp the refrigerator door '''primitive rgbs, final_state = grasp_object_link(self, "Refrigerator", "link_1") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "Refrigerator".lower() and grasped_link == "link_1".lower()) '''</p><p>substep 2: open the refrigerator door '''reward def _compute_reward(self):</p><p># this reward encourages the end-effector to stay near door to grasp it. eef_pos = get_eef_pos(self)[0] door_pos = get_link_state(self, "Refrigerator", "link_1") reward_near = -np.linalg.norm(eef_pos -door_pos)</p><p># Get the joint state of the door. We know from the semantics and the articulation tree that joint_1 connects link_1 and is the joint that controls the rotation of the door. joint_angle = get_joint_state(self, "Refrigerator", "joint_1") # The reward is the negative distance between the current joint angle and the joint angle when the door is fully open (upper limit). joint_limit_low, joint_limit_high = get_joint_limit(self, "Refrigerator", "joint_1") target_joint_angle = joint_limit_high diff = np.abs # The reward is to encourage the robot to grasp the item and move the item to be on the table. # The goal is not to just move the soda can to be at a random location out of the refrigerator. Instead, we need to place it somewhere on the table. # This is important for moving an object out of a container style of task. table_bbox_low, table_bbox_high = get_bounding_box(self, "init_table") # the table is referred to as " init_table" in the simulator. table_bbox_range = table_bbox_high -table_bbox_low # target location is to put the item at a random location on the table target_location = np.zeros( <ref type="formula">3</ref> substep 5: grasp the refrigerator door again '''primitive rgbs, final_state = grasp_object_link(self, "Refrigerator", "link_1") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "Refrigerator".lower() and grasped_link == "link_1".lower()) ''' substep 6: close the refrigerator door '''reward def _compute_reward(self):</p><p># this reward encourages the end-effector to stay near door eef_pos = get_eef_pos(self)[0] door_pos = get_link_state(self, "Refrigerator", "link_1") reward_near = -np.linalg.norm(eef_pos -door_pos)</p><p># Get the joint state of the door. The semantics and the articulation tree show that joint_1 connects link_1 and is the joint that controls the rotation of the door. joint_angle = get_joint_state(self, "Refrigerator", "joint_1") # The reward encourages the robot to make joint angle of the door to be the lower limit to clost it. joint_limit_low, joint_limit_high = get_joint_limit(self, "Refrigerator", "joint_1") target_joint_angle = joint_limit_low (1, 0, 0) # when an object is not on the table, the center specifies its location in the world coordinate. lang: a freestanding oven name: oven on_table: false path: oven.urdf size: 0.85 type: urdf ''' '''Oven articulation tree: links: base link_0 link_1 link_2 link_3 link_4 joints: joint_name: joint_0 joint_type: continuous parent_link: link_4 child_link: link_0 joint_name: joint_1 joint_type: continuous parent_link: link_4 child_link: link_1 joint_name: joint_2 joint_type: continuous parent_link: link_4 child_link: link_2 joint_name: joint_3 joint_type: continuous parent_link: link_4 child_link: link_3 joint_name: joint_4 joint_type: fixed parent_link: base child_link: link_4 ''' '''Oven semantics link_0 hinge knob link_1 hinge knob link_2 hinge knob link_3 hinge knob link_4 heavy oven_body ''' Links: link_0: We know from the semantics that link_0 is a hinge knob. It is assumed to be the knob that controls the temperature of the oven. The robot needs to actuate this knob to set the temperature of the oven.</p><p>Joints: joint_0: from the articulation tree, joint_0 connects link_0 and is a continuous joint. Therefore, the robot needs to actuate joint_0 to turn link_0, which is the knob.</p><p>This task can be decomposed as follows:</p><p>substep 1: grasp the temperature knob '''primitive rgbs, final_state = grasp_object_link(self, "oven", "link_0") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "oven".lower() and grasped_link == "link_0".lower()) ''' substep 2: turn the temperature knob to set a desired temperature '''reward def _compute_reward(self):</p><p># This reward encourages the end-effector to stay near the knob to grasp it. eef_pos = get_eef_pos(self)[0] knob_pos = get_link_state(self, "oven", "link_0") reward_near = -np.linalg.norm(eef_pos -knob_pos) joint_angle = get_joint_state(self, "oven", "joint_0") joint_limit_low, joint_limit_high = get_joint_limit(self, "oven", "joint_0") desired_temperature = joint_limit_low + (joint_limit_high -joint_limit_low) / 3 # We assume the target desired temperature is one third of the joint angle. It can also be 1/3, or other values between joint_limit_low and joint_limit_high. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=====================================</head><p>Here is another example: # We know from the semantics that link_0 and link_1 are the lid links. rgbs, final_state = grasp_object_link(self, "box", "link_1") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "box".lower() and grasped_link == "link_1".lower()) '''</p><p>substep 4: open the second lid of the box '''reward def _compute_reward(self):</p><p># This reward encourages the end-effector to stay near the lid to grasp it. eef_pos = get_eef_pos(self)[0] lid_pos = get_link_state(self, "box", "link_1") reward_near = -np.linalg.norm(eef_pos -lid_pos)</p><p># Get the joint state of the second lid. The semantics and the articulation tree show that joint_1 connects link_1 and is the joint that controls the rotation of the second lid link_1. joint_angle = get_joint_state(self, "box", "joint_1") # The reward is the negative distance between the current joint angle and the joint angle when the lid is fully open (upper limit). joint_limit_low, joint_limit_high = get_joint_limit(self, "box", "joint_1") target_joint_angle = joint_limit_high diff = np.abs(joint_angle -target_joint_angle) reward_joint = -diff reward = reward_near + 5 * reward_joint success = diff &lt; 0.1 * (joint_limit_high -joint_limit_low) return reward, success ''' '''action space delta-translation ''' substep 5: grasp the toy car '''primitive rgbs, final_state = grasp_object(self, "toy_car") success = get_grasped_object_name(self) == "toy_car".lower() ''' # another reward is one if the car is inside the box bounding box reward_in = 0 if in_bbox(self, car_position, min_aabb, max_aabb): reward_in += 1 # another reward is to encourage the robot to move the car to be near the box # we need this to give a dense reward signal for the robot to learn to perform this task. reward_reaching = -np.linalg.norm(center -car_position) # The task is considered to be successful if the car is inside the box bounding box success = in_bbox(self, car_position, min_aabb, max_aabb) # We give more weight to reward_in, which is the major goal of the task. reward = 5 * reward_in + reward_reaching + reward_near return reward, success ''' '''action space normalized-direct-translation ''' Since this substep requires moving the item to a target location, we use the normalized-direct-translation.</p><p>Please decompose the following task into substeps. For each substep, write a primitive/a reward function, write the success checking function, and the action space if the reward is used.</p><p>The primitives you can call for the robot to execute: grasp_object(self, object_name): the robot arm will grasp the object specified by the argument object name. grasp_object_link(self, object_name, link_name): some object like an articulated object is composed of multiple links. The robot will grasp a link with link_name on the object with object_name. release_grasp(self): the robot will release the grasped object. approach_object(self, object_name): this function is similar to grasp_object, except that the robot only approaches the object, without grasping it.</p><p>''' substep 7: close the trash can lid '''reward def _compute_reward(self): # This reward encourages the end-effector to stay near the lid eef_pos = get_eef_pos(self)[0] lid_pos = get_link_state(self, "TrashCan", "link_0") reward_near = -np.linalg.norm(eef_pos -lid_pos)</p><p># Get the joint state of the lid. The semantics and the articulation tree show that joint_0 connects link_0 and is the joint that controls the rotation of the lid. joint_angle = get_joint_state(self, "TrashCan", "joint_0") # The reward encourages the robot to make joint angle of the lid to be the lower limit to close it. joint_limit_low, joint_limit_high = get_joint_limit(self, "TrashCan", "joint_0") target_joint_angle = joint_limit_low diff = np.abs(target_joint_angle -joint_angle) reward_joint = -diff reward = reward_near + 5 * reward_joint success = diff &lt; 0.1 * (joint_limit_high -joint_limit_low) return reward, success ''' '''action space delta-translation '''</p><p>Another example of heating up a bowl of soup in the microwave:</p><p>This task can be decomposed as follows:</p><p>substep 1: grasp the microwave door '''primitive rgbs, final_state = grasp_object_link(self, "Microwave", "link_0") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "Microwave".lower() and grasped_link == "link_0".lower()) '''</p><p>substep 2: open the microwave door '''reward def _compute_reward(self):</p><p># Get the joint state of the door. We know from the semantics and the articulation tree that joint_0 connects link_0 and is the joint that controls the rotation of the door. joint_angle = get_joint_state(self, "Microwave", "joint_0") # The reward is the negative distance between the current joint angle and the joint angle when the door is fully open (upper limit). joint_limit_low, joint_limit_high = get_joint_limit(self, "Microwave", "joint_0") target_joint_angle = joint_limit_high </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RoboGen consists of the following stages: A) task proposal, B) scene generation, C)training supervision generation, and D) skill learning with generated information. et al., 2023), language<ref type="bibr" target="#b72">(Touvron et al., 2023;</ref><ref type="bibr" target="#b10">Driess et al., 2023;</ref> OpenAI, 2023), and other modalities(Liu et al., 2023a;<ref type="bibr" target="#b15">Girdhar et al., 2023;</ref> Huang et al., 2023a), an active line of works investigate how to use these large-scale models for robotics research, via approaches such as code generation<ref type="bibr" target="#b79">(Wu et al., 2023;</ref><ref type="bibr" target="#b36">Liang et al., 2022)</ref>, data augmentation(Yu et al., 2023a), visual imagination for skill execution<ref type="bibr" target="#b11">(Du et al., 2023)</ref>, sub-task planning<ref type="bibr" target="#b0">(Ahn et al., 2022;</ref><ref type="bibr" target="#b24">Huang et al., 2022;</ref><ref type="bibr" target="#b37">Lin et al., 2023)</ref>, concept generalization of learned skills<ref type="bibr" target="#b2">(Brohan et al., 2023)</ref>, outputting low-level control actions(Wang et al., 2023b), and goal specification<ref type="bibr" target="#b29">(Kapelyukh et al., 2023)</ref>. More related to ours are recent methods using LLMs for reward generation(Yu et al., 2023b), and sub-task and trajectory generation<ref type="bibr" target="#b19">(Ha et al., 2023)</ref>. In contrast to them, our proposed system aims at a fully automated pipeline that self-proposes new tasks, generate environments and yields diverse skills.</figDesc><graphic url="image-2.png" coords="4,108.00,81.85,396.01,171.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Snapshots of the learned skills on 4 example long-horizon tasks.</figDesc><graphic url="image-40.png" coords="9,333.33,249.57,56.27,56.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: We compare the BLIP-2 score of ablations of RoboGen on 7 tasks. Both ablations achieve lower scores, showing the importance of both object and size verification.</figDesc><graphic url="image-46.png" coords="10,127.80,81.86,356.40,115.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>'</head><label></label><figDesc>it for the following task:Task Name: Throw Trash Away Description: The robotic arm places an item of trash inside the trash can 'joint_type: revolute parent_link: link_1 child_link: link_0 joint_name: joint_1 joint_type: fixed parent_link: base child_link:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>In the last substep the robot already grasps the door, thus only local movements are needed to open it. substep 3: grasp the item '''primitive rgbs, final_state = grasp_object(self, "Item") success = get_grasped_object_name(self) == "Item".lower() ''' substep 4: move the item out of the refrigerator '''reward def _compute_reward(self): # Get the current item position item_position = get_position(self, "Item") # The first reward encourages the end-effector to stay near the item eef_pos = get_eef_pos(self)[0] reward_near = -np.linalg.norm(eef_pos -item_position)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) target_location[0] = table_bbox_low[0] + 0.2 * table_bbox_range[0] # 0.2 is a random chosen number, any number in [0, 1] should work target_location[1] = table_bbox_low[1] + 0.3 * table_bbox_range[1] # 0.3 is a random chosen number, any number in [0, 1] should work target_location[2] = table_bbox_high[2] # the height should be the table height diff = np.linalg.norm(item_position -target_location) reward_distance = -diff reward = reward_near + 5 * reward_distance success = diff &lt; 0.06 return reward, success ''' '''action space normalized-direct-translation ''' Since this substep requires moving the item to a target location, we use the normalized-direct-translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>some more examples of decomposing the task. Reply yes if you understand the goal. ===================================== Yes, I understand the goal. Please proceed with the next example. ===================================== Another example: Task Name: Set oven temperature Description: The robotic arm will turn the knob of an oven to set a desired temperature. Initial config: '''yaml -use_table: false -center:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>#</head><label></label><figDesc>The reward is the negative distance between the current joint angle and the joint angle of the desired temperature. diff = np.abs(joint_angle -desired_temperature) reward_joint provide more examples in the following messages. Please reply yes if you understand the goal.=====================================Yes, I understand the goal. Please proceed with the next example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>substep 6: put the toy car into the box '''reward def _compute_reward(self): # Get the current car position car_position = get_position(self, "toy_car") # This reward encourages the end-effector to stay near the car to grasp it. eef_pos = get_eef_pos(self)[0] reward_near = -np.linalg.norm(eef_pos -car_position) # Get the box body bounding box min_aabb, max_aabb = get_bounding_box_link(self, "box", "link_4") # from the semantics, link_4 is the body of the box. diff = np.array(max_aabb) -np.array(min_aabb) min_aabb = np.array(min_aabb) + 0.05 * diff # shrink the bounding box a bit max_aabb = np.array(max_aabb) -0.05 * diff center = (np.array(max_aabb) + np.array(min_aabb)) / 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>diff = np.abs(joint_angle -target_joint_angle) reward = -diff success = diff &lt; 0.1 * (joint_limit_high -joint_limit_low) last substep the robot already grasps the microwave door, thus only local movements are needed to open the door. substep 3: grasp the bowl of soup '''primitive rgbs, final_state = grasp_object(self, "Bowl of soup") success = get_grasped_object_name(self) == "Bowl of soup".lower() ''' substep 4: put the bowl of soup into the microwave '''reward def _compute_reward(self): # Get the current soup position soup_position = get_position(self, "Bowl of soup") # Get the microwave body bounding box min_aabb, max_aabb = get_bounding_box_link(self, "Microwave", "link_3") # from the semantics, link_3 is the body of the microwave. diff = np.array(max_aabb) -np.array(min_aabb) min_aabb = np.array(min_aabb) + 0.05 * diff # shrink the bounding box a bit max_aabb = np.array(max_aabb) -0.05 * diff center = (np.array(max_aabb) + np.array(min_aabb)) / 2 # First reward is one if the soup is inside the microwave bounding box reward_in = 0 if in_bbox(self, soup_position, min_aabb, max_aabb): reward_in += 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,108.00,81.86,395.97,270.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Task</figDesc><table><row><cell></cell><cell cols="5">RoboGen Behavior-100 RLbench MetaWorld Maniskill2</cell></row><row><cell>Number of Tasks</cell><cell>106</cell><cell>100</cell><cell>106</cell><cell>50</cell><cell>20</cell></row><row><cell>Self-BLEU ?</cell><cell>0.284</cell><cell>0.299</cell><cell>0.317</cell><cell>0.322</cell><cell>0.674</cell></row><row><cell>Embedding Similarity ?</cell><cell>0.165</cell><cell>0.210</cell><cell>0.200</cell><cell>0.263</cell><cell>0.194</cell></row></table><note><p>diversity comparison. Note that RoboGen can generate an unbounded number of tasks when queried repeatedly. Here we compare to a version that has a total number of 106 tasks generated, such that number of tasks is similar to those in prior work.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The return decreases drastically if only RL is used for skill learning.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, as future work.WenhaoYu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et  al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023b. Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. In Conference on Robot Learning, pp. 726-747. PMLR, 2021. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In The 41st international ACM SIGIR conference on research &amp; development in information retrieval, pp. 1097-1100, 2018. Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, and Hang Zhao. Robot parkour learning. arXiv preprint arXiv:2309.05665, 2023.</figDesc><table><row><cell>Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. Tossingbot:</cell></row><row><cell>Learning to throw arbitrary objects with residual physics. IEEE Transactions on Robotics, 36(4):</cell></row><row><cell>1307-1319, 2020.</cell></row><row><cell>Andy Zeng,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>table. in, obj_A, obj_B: object A is inside object B, e.g., a gold ring in the safe. in, obj_A, obj_B, link_name: object A is inside the link with link_name of object B. For example, a table might have two drawers, represented with link_0, and link_1, and in(pen, table, link_0) would be that a pen is inside one of the drawers that corresponds to link_0. Given the input to you, you should output any needed spatial relationships of the involved objects. The robot needs to approach and open this link, which represents one of the refrigerator doors, to reach for the item inside. Joints: joint_1: This joint connects link_1, representing one of the doors. The robot needs to actuate this joint to open the door, reach for the item, and close the door.</figDesc><table><row><cell>Here are some examples:</cell></row><row><cell>Input:</cell></row><row><cell>Task Name:Fetch Item from Refrigerator</cell></row><row><cell>Description: The robotic arm will open a refrigerator door and reach inside to grab an item and then close the</cell></row><row><cell>door.</cell></row><row><cell>Objects involved: refrigerator, item</cell></row><row><cell>'''refrigerator articulation tree</cell></row><row><cell>links:</cell></row><row><cell>base</cell></row><row><cell>link_0</cell></row><row><cell>link_1</cell></row><row><cell>link_2</cell></row><row><cell>joints:</cell></row><row><cell>joint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0</cell></row><row><cell>joint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1</cell></row><row><cell>joint_name: joint_2 joint_type: revolute parent_link: link_0 child_link: link_2</cell></row><row><cell>'''</cell></row><row><cell>'''refrigerator semantics</cell></row><row><cell>link_0 heavy refrigerator_body</cell></row><row><cell>link_1 hinge door</cell></row><row><cell>link_2 hinge door</cell></row><row><cell>'''</cell></row><row><cell>Links:</cell></row><row><cell>link_1: substeps:</cell></row><row><cell>grasp the refrigerator door</cell></row><row><cell>open the refrigerator door</cell></row><row><cell>grasp the item</cell></row><row><cell>move the item out of the refrigerator</cell></row><row><cell>grasp the refrigerator door again</cell></row><row><cell>close the refrigerator door</cell></row><row><cell>Output:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>It returns the min and max xyz coordinate of the bounding box. get_bounding_box_link(self, object_name, link_name): get the axis-aligned bounding box of the link of an object. It returns the min and max xyz coordinate of the bounding box. in_bbox(self, pos, bbox_min, bbox_max): check if pos is within the bounding box with the lowest corner at bbox_min and the highest corner at bbox_max. get_grasped_object_name(self): return the name of the grasped object. If no object is grasped by the robot, return None. The name is automatically converted to the lower case. get_grasped_object_and_link_name(self): return a tuple, the first is the name of the grasped object, and the second is the name of the grasped link. If no object is grasped by the robot, return (None, None). The name is automatically converted to the lower case. gripper_close_to_object(self, object_name): return true if the robot gripper is close enough to the object specified by object_name, otherwise false. gripper_close_to_object_link(self, object_name, link_name): return true if the robot gripper is close enough to the object link, otherwise false.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>EXPERIMENTSRoboGen is an automated pipeline that can be queried endlessly, and generate a continuous stream of skill demonstrations for diverse tasks. In our experiments, we aim to answer the following questions:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Genesis is still under development and not yet available to the public. We build our system on top of an internal version, made accessible to us thanks to the courtesy of the development team.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>-joint_0: from the articulation tree, this is the revolute joint that connects link_0. Therefore, the robot needs to actuate this joint for opening the door.</p><p>Task Name: Adjust Oven Temperature Description: The robotic arm will turn one of the oven's hinge knobs to set a desired temperature. Additional Objects: None Links:</p><p>-link_1: the robot needs to approach link_1, which is assumed to be the temperature knob, to rotate it to set the temperature. Joints: -joint_1: joint_1 connects link_1 from the articulation tree. The robot needs to actuate it to rotate link_1 to the desired temperature.</p><p>Task Name: Heat a hamburger Inside Oven Description: The robot arm places a hamburger inside the oven, and sets the oven temperature to be appropriate for heating the hamburger. Additional Objects: hamburger Links: -link_0: link_0 is the oven door from the semantics. The robot needs to open the door in order to put the hamburger inside the oven. link_1: the robot needs to approach link_1, which is the temperature knob, to rotate it to set the desired temperature. Joints: -joint_0: from the articulation tree, this is the revolute joint that connects link_0 (the door). Therefore, the robot needs to actuate this joint for opening the door. -joint_1: from the articulation tree, joint_1 connects link_1, which is the temperature knob. The robot needs to actuate it to rotate link_1 to the desired temperature.</p><p>Task Name: Set Oven Timer Description: The robot arm turns a timer knob to set cooking time for the food. Additional Objects: None. Links:</p><p>-link_2: link_2 is assumed to be the knob for controlling the cooking time. The robot needs to approach link_2 to set the cooking time. Joints: -joint_2: from the articulation tree, joint_2 connects link_2. The robot needs to actuate joint_2 to rotate link_2 to the desired position, setting the oven timer.</p><p>Can you do the same for the following object:</p><p>'''TrashCan articulation tree links: base link_0 link_1 joints: joint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0 joint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1 ''' '''TrashCan semantics link_0 hinge door link_1 free trashcan_body '''</p><p>An example response from GPT-4:</p><p>Task Name: Open Trash Can Description: The robotic arm will open the trash can lid. Additional Objects: None Links:</p><p>-link_0: This link represents the lid of the trash can, based on the sematics. The robot needs to approach this lid to open it. Joints: -joint_0: The joint_0 in the articulation tree connects link_0 (the lid). Thus, the robot needs to actuate this joint to open the lid.</p><p>Task Name: Throw Trash Away Description: The robotic arm places an item of trash inside the trash can. Additional Objects: A pile of trash Links:</p><p>-link_0: Link_0, as the trash can lid, needs to be approached and opened by the robot in order to throw away the trash. Joints: -joint_0: Joint_0, connecting to link_0, needs to be actuated for the robot to open the trash can lid.</p><p>Scene Generation Prompt. Following the above example, we show the scene generation prompt for the task "Throw Trash Away".</p><p>I need you to describe the initial scene configuration for a given task in the following format, using a yaml file. This yaml file will help build the task in a simulator. The task is for a mobile Franka panda robotic arm to learn a manipulation skill in the simulator. The Franka panda arm is mounted on a floor, at location (1, 1, 0). It can move freely on the floor. The z axis is the gravity axis. Task Name: Throw Trash Away Description: The robotic arm places an item of trash inside the trash can Objects involved: TrashCan, Trash '''TrashCan articulation tree links: base link_0 link_1 joints: joint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0 joint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1 ''' '''TrashCan semantics link_0 hinge door link_1 free trashcan_body ''' Links: link_0: Link_0, as the trash can lid, needs to be approached and opened by the robot in order to throw away the trash.</p><p>Joints: joint_0: Joint_0, connecting to link_0, needs to be actuated for the robot to open the trash can lid.</p><p>substeps: grasp the trash can lid open the trash can lid grasp the trash put the trash into the trash can release the trash grasp the trash can lid again close the trash can lid</p><p>An example output from GPT-4:</p><p>Output:</p><p>The task involves the robot arm placing an item of trash into the trash can. Initially, the trash should not be inside the trash can, so the robot can learn to put it in. Therefore, no special spatial relationships are needed. '''spatial relationship None '''</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training supervision prompt.</head><p>A robotic arm is trying to solve some household object manipulation tasks to learn corresponding skills in a simulator.</p><p>We will provide with you the task description, the initial scene configurations of the task, which contains the objects in the task and certain information about them. Your goal is to decompose the task into executable sub-steps for the robot, and for each substep, you should either call a primitive action that the robot can execute, or design a reward function for the robot to learn, to complete the substep. For each substep, you should also write a function that checks whether the substep has been successfully completed.</p><p>Common substeps include moving towards a location, grasping an object, and interacting with the joint of an articulated object.</p><p>An example task: I will also give you the articulation tree and semantics file of the articulated object in the task. Such information will be useful for writing the reward function/the primitive actions, for example, when the reward requires accessing the joint value of a joint in the articulated object, or the position of a link in the articulated object, or when the primitive needs to access a name of the object. '''Refrigerator articulation tree links: base link_0 link_1 link_2 joints: joint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0 joint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1 joint_name: joint_2 joint_type: revolute parent_link: link_0 child_link: link_2 ''' '''Refrigerator semantics link_0 heavy refrigerator_body link_1 hinge door link_2 hinge door ''' I will also give you the links and joints of the articulated object that will be used for completing the task: Links: link_1: This link is one of the refrigerator doors, which the robot neesd to reach for the item inside. Joints: joint_1: This joint connects link_1, representing one of the doors. The robot needs to actuate this joint to open the door, reach for the item, and close the door.</p><p>For each substep, you should decide whether the substep can be achieved by using the provided list of primitives. If not, you should then write a reward function for the robot to learn to perform this substep. If you choose to write a reward function for the substep, you should also specify the action space of the robot when learning this reward function. There are 2 options for the action space: "delta-translation", where the action is the delta translation of the robot end-effector, suited for local movements; and "normalized-direct-translation", where the action specifies the target location the robot should move to, suited for moving to a target location. For each substep, you should also write a condition that checks whether the substep has been successfully completed. is another lid. Joints: joint_0: from the articulation tree, joint_0 connects link_0 and is a hinge joint. Thus, the robot needs to actuate joint_0 to open link_0, which is the lid of the box. joint_1: from the articulation tree, joint_1 connects link_1 and is a hinge joint. Thus, the robot needs to actuate joint_1 to open link_1, which is the lid of the box.</p><p>This task can be decomposed as follows:</p><p>substep 1: grasp the first lid of the box '''primitive # The semantics shows that link_0 and link_1 are the lid links. rgbs, final_state = grasp_object_link(self, "box", "link_0") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "box".lower() and grasped_link == "link_0".lower()) '''</p><p>substep 2: open the first lid of the box '''reward def _compute_reward(self):</p><p># This reward encourages the end-effector to stay near the lid to grasp it. eef_pos = get_eef_pos(self)[0] lid_pos = get_link_state(self, "box", "link_0") reward_near = -np.linalg.norm(eef_pos -lid_pos)</p><p># Get the joint state of the first lid. The semantics and the articulation tree show that joint_0 connects link_0 and is the joint that controls the rotation of the first lid link_0. joint_angle = get_joint_state(self, "box", "joint_0") # The reward is the negative distance between the current joint angle and the joint angle when the lid is fully open (upper limit). joint_limit_low, joint_limit_high = get_joint_limit(self, "box", "joint_0") target_joint_angle = joint_limit_high diff = np.abs(joint_angle -target_joint_angle) reward_joint = -diff reward = reward_near + 5 * reward_joint success = diff &lt; 0.1 * (joint_limit_high -joint_limit_low) return reward, success ''' '''action space delta-translation ''' substep 3: grasp the second lid of the box '''primitive approach_object_link(self, object_name, link_name): this function is similar to grasp_object_link, except that the robot only approaches the object's link, without grasping it. Note that all primitives will return a tuple (rgbs, final_state) which represents the rgb images of the execution process and the final state of the execution process. You should always call the primitive in the following format: rgbs, final_state = some_primitive_function(self, arg1, ..., argn)</p><p>The APIs you can use for writing the reward function/success checking function: get_position(self, object_name): get the position of center of mass of object with object_name. get_orientation(self, object_name): get the orientation of an object with object_name. get_joint_state(self, object_name, joint_name): get the joint angle value of a joint in an object. get_joint_limit(self, object_name, joint_name): get the lower and upper joint angle limit of a joint in an object, returned as a 2-element tuple. get_link_state(self, object_name, link_name): get the position of the center of mass of the link of an object. get_eef_pos(self): returns the position, orientation of the robot end-effector as a list. get_bounding_box(self, object_name): get the axis-aligned bounding box of an object. The action space you can use for learning with the reward: delta-translation is better suited for small movements, and normalized-direct-translation is better suited for directly specifying the target location of the robot end-effector. You can assume that for objects, the lower joint limit corresponds to their natural state, e.g., a box is closed with the lid joint being 0, and a lever is unpushed when the joint angle is 0. Links: link_0: Link_0, as the trash can lid, needs to be approached and opened by the robot in order to throw away the trash. Joints: joint_0: Joint_0, connecting to link_0, needs to be actuated for the robot to open the trash can lid.</p><p>An example response for the above task "Throw the trash away":</p><p>This task can be decomposed into the following substeps: substep 1: grasp the trash can lid '''primitive rgbs, final_state = grasp_object_link(self, "TrashCan", "link_0") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "TrashCan".lower() and grasped_link == "link_0".lower()) '''</p><p>substep 2: open the trash can lid '''reward def _compute_reward(self):</p><p># This reward encourages the end-effector to stay near the lid to grasp it. eef_pos = get_eef_pos(self)[0] lid_pos = get_link_state(self, "TrashCan", "link_0") reward_near = -np.linalg.norm(eef_pos -lid_pos)</p><p># Get the joint state of the lid. The semantics and the articulation tree show that joint_0 connects link_0 and is the joint that controls the rotation of the lid. joint_angle = get_joint_state(self, "TrashCan", "joint_0") # The reward is the negative distance between the current joint angle and the joint angle when the lid is open (upper limit). joint_limit_low, joint_limit_high = get_joint_limit(self, "TrashCan", "joint_0") target_joint_angle = joint_limit_high # another reward is one if the trash is inside the trash can bounding box reward_in = 0 if in_bbox(self, trash_position, min_aabb, max_aabb): reward_in += 1 # another reward is to encourage the robot to move the trash to be near the trash can # we need this to give a dense reward signal for the robot to learn to perform this task. reward_reaching = -np.linalg.norm(center -trash_position) # The task is considered to be successful if the trash is inside the trash can bounding box success = in_bbox(self, trash_position, min_aabb, max_aabb) # We give more weight to reward_in, which is the major goal of the task. reward = 5 * reward_in + reward_reaching + reward_near return reward, success ''' '''action space normalized-direct-translation ''' Since this substep requires moving the item to a target location, we use the normalized-direct-translation.</p><p>substep 5: release the trash '''primitive rgbs, final_state = release_grasp(self) success = get_grasped_object_name(self) == None ''' substep 6: grasp the trash can lid again '''primitive rgbs, final_state = grasp_object_link(self, "TrashCan", "link_0") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "TrashCan".lower() and grasped_link == "link_0".lower())</p><p># Second reward is to encourage the robot to grasp the soup and move the soup to be near the microwave # we need this to give a dense reward signal for the robot to learn to perform this task. reward_reaching = -np.linalg.norm(center -soup_position)</p><p># The task is considered to be successful if the soup is inside the microwave bounding box success = in_bbox(self, soup_position, min_aabb, max_aabb)</p><p># We give more weight to the first reward which is putting the soup into the microwave. reward = 5 * reward_in + reward_reaching return reward, success ''' '''action space normalized-direct-translation ''' Since this substep requires moving the item to a target location, we use the normalized-direct-translation.</p><p>substep 5: grasp the microwave door again '''primitive rgbs, final_state = grasp_object_link(self, "Microwave", "link_0") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "Microwave".lower() and grasped_link == "link_0".lower()) ''' substep 6: close the microwave door '''reward def _compute_reward(self):</p><p># Get the joint state of the door. We know from the semantics and the articulation tree that joint_0 connects link_0 and is the joint that controls the rotation of the door. joint_angle = get_joint_state(self, "Microwave", "joint_0") # The reward is the negative distance between the current joint angle and the joint angle when the door is fully closed (lower limit). joint_limit_low, joint_limit_high = get_joint_limit(self, "Microwave", "joint_0") target_joint_angle = joint_limit_low diff = np.abs(target_joint_angle -joint_angle) reward = -diff success = diff &lt; 0.1 * (joint_limit_high -joint_limit_low) return reward, success ''' '''action space delta-translation ''' Here from the last substep the robot already grasps the microwave door, thus only local movements are needed to close the door.</p><p>substep 7: grasp the microwave timer knob '''primitive rgbs, final_state = grasp_object_link(self, "Microwave", "link_1") grasped_object, grasped_link = get_grasped_object_and_link_name(self) success = (grasped_object == "Microwave".lower() and grasped_link == "link_1".lower()) ''' substep 8: turn the microwave timer knob to set a desired heating time '''reward def _compute_reward(self):</p><p># Get the joint state of the timer knob. We know from the semantics and the articulation tree that joint_1 connects link_1 and is the joint that controls the timer knob. joint_angle = get_joint_state(self, "Microwave", "joint_1") joint_limit_low, joint_limit_high = get_joint_limit(self, "Microwave", "joint_1") desired_time = joint_limit_low + (joint_limit_high -joint_limit_low) / 2 # We assume the target desired time is half of the joint angle. It can also be one third, or other values between joint_limit_low and joint_limit_high.</p><p># The reward is the negative distance between the current joint angle and the joint angle of the desired time. diff = np.abs(joint_angle -desired_time) reward = -diff # if the difference is small enough, we consider it a success. Here the threshold value is determined as a ratio of the joint angle range. success = diff &lt; 0.1 * (joint_limit_high -joint_limit_low) return reward, success ''' '''action space delta-translation ''' Since the robot already gras Here unfortunately we exceeded the 8k token limit of GPT-4. But the main body of the task decompositions have been finished, and the response is still good to be used.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Michael Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keerthana</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><surname>Herzog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01691</idno>
		<title level="m">Do as i can, not as i say: Grounding language in robotic affordances</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Solving rubik&apos;s cube with a robot hand</title>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciek</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Ribas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07113</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rt-2: Vision-language-action models transfer web knowledge to robotic control</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justice</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15818</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A system for general in-hand object re-orientation</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="297" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Xuxin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11330</idno>
		<title level="m">Legs as manipulator: Pushing quadrupedal agility beyond locomotion</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pybullet, a python module for physics simulation for games, robotics and machine learning</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="http://pybullet.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Imitating task and motion planning with visuomotor transformers</title>
		<author>
			<persName><forename type="first">Murtaza</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caelan</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName><forename type="first">Pieter-Tjerk De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuven</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Deitke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><surname>Objaverse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08051</idno>
		<title level="m">A universe of annotated 3d objects</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Objaverse: A universe of annotated 3d objects</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Deitke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="13142" to="13153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Danny</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayzaan</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03378</idno>
		<title level="m">Palm-e: An embodied multimodal language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning universal policies via text-guided video generation</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00111</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch informed trees (bit*): Sampling-based optimal planning via the heuristically guided search of implicit random geometric graphs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">D</forename><surname>Gammell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Barfoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3067" to="3074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Threedworld: A platform for interactive multi-modal physical simulation</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Mrowca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Traer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhandwaldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megumi</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuno</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lingelbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Feigelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Yamins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Act3d: Infinite resolution action detection transformer for robotic manipulation</title>
		<author>
			<persName><forename type="first">Theophile</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Zhou Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Gkanatsios</surname></persName>
		</author>
		<author>
			<persName><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.17817</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Imagebind</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.05665</idno>
		<title level="m">One embedding space to bind them all</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Bard</surname></persName>
		</author>
		<ptr target="https://bard.google.com/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Maniskill2: A unified benchmark for generalizable manipulation skills</title>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanbo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stone</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04659</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flingbot: The unreasonable effectiveness of dynamic manipulation for cloth unfolding</title>
		<author>
			<persName><forename type="first">Huy</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling up and distilling down: Language-guided robot skill acquisition</title>
		<author>
			<persName><forename type="first">Huy</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.14535</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning agile soccer skills for a bipedal robot with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandy</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruva</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Humplik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saran</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">Y</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Hafner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13653</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Disect: A differentiable simulation engine for autonomous robotic cutting</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Heiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Macklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashraj</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12244</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models</title>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12661</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Inner monologue: Embodied reasoning through planning with language models</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05608</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.05973</idno>
		<title level="m">Composable 3d value maps for robotic manipulation with language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rlbench: The robot learning benchmark &amp; learning environment</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rovick Arrojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3019" to="3026" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stomp: Stochastic trajectory optimization for motion planning</title>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on robotics and automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="4569" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scaling up gans for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Minguk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05511</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dall-e-bot: Introducing web-scale diffusion models to robotics</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Kapelyukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitalis</forename><surname>Vosylius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sampling-based algorithms for optimal motion planning</title>
		<author>
			<persName><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The international journal of robotics research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="846" to="894" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gen2sim: Scaling up robot learning in simulation with generative models</title>
		<author>
			<persName><forename type="first">Pushkal</forename><surname>Katara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Zhou Xian</surname></persName>
		</author>
		<author>
			<persName><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.18308</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Elia</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05768</idno>
		<title level="m">Deep drone acrobatics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation</title>
		<author>
			<persName><forename type="first">Chengshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josiah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjana</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabrael</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lingelbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiankai</forename><surname>Sun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="80" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07753</idno>
		<title level="m">Code as policies: Language model programs for embodied control</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Agia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toki</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bohg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12153</idno>
		<title level="m">Text2motion: From natural language instructions to feasible plans</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Softgym: Benchmarking deep reinforcement learning for deformable object manipulation</title>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Olkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07215</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17275</idno>
		<title level="m">Diffskill: Skill abstraction from differentiable physics for deformable object manipulations with tools</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning visible connectivity dynamics for cloth smoothing</title>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="256" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The role of physics-based simulators in robotics</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Negrut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics, and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="35" to="58" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Annual Review of Control</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Audioldm: Text-to-audio generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">Haohe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xubo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12503</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Ruoshi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rundi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basile</forename><surname>Van Hoorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11328</idno>
		<title level="m">Zero-1-to-3: Zero-shot one image to 3d object</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08133</idno>
		<title level="m">Meshdiffusion: Score-based generative 3d mesh modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning high-speed flight in the wild</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elia</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page">5810</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rockwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.07279</idno>
		<title level="m">Scalable 3d captioning with pretrained models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yecheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osbert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Bastani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12931</idno>
		<title level="m">Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unified particle physics for real-time applications</title>
		<author>
			<persName><forename type="first">Miles</forename><surname>Macklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuttapong</forename><surname>Chentanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae-Yong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Guided imitation of task and motion planning</title>
		<author>
			<persName><forename type="first">James</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Hadfield-Menell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Realfusion: 360 {\deg} reconstruction of any object from a single image</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10663</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Midjourney</surname></persName>
		</author>
		<author>
			<persName><surname>Midjourney</surname></persName>
		</author>
		<ptr target="https://www.midjourney.com/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Object-centric task and motion planning in dynamic environments</title>
		<author>
			<persName><forename type="first">Toki</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bohg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1911.04679" />
		<imprint>
			<date type="published" when="1911">1911.04679. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanbo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stone</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Maniskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14483</idno>
		<title level="m">Generalizable manipulation skill benchmark with large-scale demonstrations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Cabinet: Scaling neural collision detection for object rearrangement with procedural scene generation</title>
		<author>
			<persName><forename type="first">Adithyavairavan</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Chatgpt</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14988</idno>
		<title level="m">Dreamfusion: Text-to-3d using 2d diffusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bike</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushil</forename><surname>Sreenath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03381</idno>
		<title level="m">Learning humanoid locomotion with transformers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Toolflownet: Robotic manipulation with tools via predicting tool flow from point clouds</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Seita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sarthak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Yao</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zackory</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><surname>Held</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1038" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><surname>Hugginggpt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17580</idno>
		<title level="m">Solving ai tasks with chatgpt and its friends in huggingface</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Reaching the limit in autonomous racing: Optimal control versus reinforcement learning</title>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">82</biblScope>
			<biblScope unit="page">1462</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments</title>
		<author>
			<persName><forename type="first">Sanjana</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lingelbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kent</forename><forename type="middle">Elliott</forename><surname>Vainio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="477" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The open motion planning library</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ioan A Sucan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lydia</forename><forename type="middle">E</forename><surname>Moll</surname></persName>
		</author>
		<author>
			<persName><surname>Kavraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics &amp; Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="72" to="82" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Vipergpt: Visual inference via python execution for reasoning</title>
		<author>
			<persName><forename type="first">D?dac</forename><surname>Sur?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08128</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ international conference on intelligent robots and systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Logic-geometric programming: An optimization-based approach to combined task and motion planning</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Toussaint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</title>
		<meeting>the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1930" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hsiao-Yu Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Prabhudesai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamit</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06464</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">3d-oes: Viewpoint-invariant object-factorized environment simulators. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Everett</forename><surname>Spielberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><surname>Softzoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09555</idno>
		<title level="m">A soft robot co-design benchmark for locomotion in diverse environments</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Prompt a robot to walk with large language models</title>
		<author>
			<persName><forename type="first">Yen-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bike</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushil</forename><surname>Sreenath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.09969</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visual haptic reasoning: Estimating contact forces by observing deformable object interactions</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zackory</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="11426" to="11433" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zackory</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.12372</idno>
		<title level="m">One policy to dress them all: Learning to dress people with diverse poses and garments</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fabricflownet: Bimanual cloth manipulation with a flow-based policy</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Bajracharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khush</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><surname>Held</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rika</forename><surname>Antonova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Lepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><surname>Tidybot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.05658</idno>
		<title level="m">Personalized robot assistance with large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation</title>
		<author>
			<persName><forename type="first">Theophile</forename><surname>Zhou Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjia</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ling</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsun-Hsuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10455</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Unifying diffusion models with action detection transformers for multi-task robotic manipulation</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Zhou Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theophile</forename><surname>Gkanatsios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><surname>Fragkiadaki</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>th Annual Conference on Robot Learning, 2023b</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhou Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><surname>Fluidlab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.02346</idno>
		<title level="m">A differentiable environment for benchmarking complex fluid manipulation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Sapien: A simulated part-based interactive environment</title>
		<author>
			<persName><forename type="first">Fanbo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhe</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikuan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11097" to="11107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">Zhenjia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Roboninja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11553</idno>
		<title level="m">Learning an adaptive cutting policy for multi-material objects</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Azarnasab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11381</idno>
		<title level="m">Mm-react: Prompting chatgpt for multimodal reasoning and action</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning</title>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1094" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Scaling robot learning with semantically imagined experience</title>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaspiar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodilyn</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11550</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
