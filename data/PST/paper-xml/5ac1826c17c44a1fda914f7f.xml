<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep-Learning Based Feature Hybrid Framework for Spatiotemporal Saliency Detection inside Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Software</orgName>
								<orgName type="laboratory">Media Technology and System (MTS) Lab</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300350</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinchang</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for excellence in Signal and Image Processing</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300350</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meijun</forename><surname>Sun</surname></persName>
							<email>sunmeijun@tju.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300350</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
							<email>jianmin.jiang@szu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">College of Computer Science &amp; Software Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute for Future Media Computing</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep-Learning Based Feature Hybrid Framework for Spatiotemporal Saliency Detection inside Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA0B54C3CF23D565FECF0154BF426636</idno>
					<idno type="DOI">10.1016/j.neucom.2018.01.076</idno>
					<note type="submission">Received date: 8 September 2017 Revised date: 4 January 2018 Accepted date: 29 January 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>spatiotemporal saliency detection</term>
					<term>Human gaze</term>
					<term>convolutional neural networks</term>
					<term>visual dispersion</term>
					<term>movie highlight extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although research on detection of saliency and visual attention has been active over recent years, most of the existing work focuses on still image rather than video based saliency. In this paper, a deep learning based hybrid spatiotemporal saliency feature extraction framework is proposed for saliency detection from video footages. The deep learning model is used for the extraction of high-level features from raw video data, and they are then integrated with other high-level features. The deep learning network has been found extremely effective for extracting hidden features than that of conventional handcrafted methodology. The effectiveness for using hybrid high-level features for saliency detection in video is demonstrated in this work. Rather than using only one static image, the proposed deep learning model take several consecutive frames as input and both the spatial and temporal characteristics are considered when computing saliency maps. The efficacy of the proposed hybrid feature framework is evaluated by five databases with human gaze complex scenes. Experimental results show that the proposed model outperforms five other state-of-the-art video saliency detection approaches. In addition, the proposed framework is found useful for other video content based applications such as video highlights. As a result, a large movie clip dataset together with labeled video highlights is generated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual saliency has been an important and popular research in image processing for decades with a sole purpose to mimic biological visual perception for machine vision applications. Substantial interests in the field as evidenced by the vast volume of publications, such as application of saliency concept for image/video compression and recognition <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>, automatic image cropping <ref type="bibr" target="#b6">[7]</ref>, non-photorealistic rendering <ref type="bibr" target="#b7">[8]</ref>, adaptive image display on small devices <ref type="bibr" target="#b8">[9]</ref>, movie summarization <ref type="bibr" target="#b9">[10]</ref>, shot detection <ref type="bibr" target="#b10">[11]</ref>, human-robot interaction <ref type="bibr" target="#b11">[12]</ref>, and detection of multi-class geospatial targets <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b13">[14]</ref> have been reported in the last two decades.</p><p>Historically, saliency detection research was first initiated by Treismanand Gelade in 1980 <ref type="bibr" target="#b14">[15]</ref> who proposed the "Feature Integration Theory", which illustrated how visual attention was attracted by features in the imagery. Itti and Koch"s model triggered strong interests in this field of research, including the use of low-level features to map the saliency regions/objects in the image scene <ref type="bibr" target="#b15">[16]</ref>. He et al <ref type="bibr" target="#b16">[17]</ref> proposed a biologically inspired saliency model using high-level object and contextual features for saliency detection based on Judd"s concept <ref type="bibr" target="#b17">[18]</ref>. Further extension of research along this line was reported by Goferman et al <ref type="bibr" target="#b18">[19]</ref> who emphasized that four important factors, including local low-level features, global consideration, visual organization and high-level factors could affect saliency detections strongly. The methodology for feature extractions has also been improved.</p><p>Despite of intensive research in the image based saliency detection, video saliency has not been addressed until recent years. In fact, video saliency is quite different from that of still images, mainly because of the very limited frame-to-frame interval time for the observers" attention to be drawn by features in the scene. Although there are extensions from the image-based saliency models for the video stream such as the temporal intensity and orientation contrasts as dynamic features <ref type="bibr" target="#b19">[20]</ref> <ref type="bibr" target="#b20">[21]</ref>, better frame work is needed for more efficient saliency detection from video footage.</p><p>While most work in the field has been focusing on low level features, human attention prediction is considered to be dominated by some high-level features, such as objects, actions and events. Rudoy et al <ref type="bibr" target="#b21">[22]</ref> employed viewer"s gazing direction and also to use their actions as cue to locate the saliency features, as opposed to the conventional image based pixel feature extraction method. Han et al. <ref type="bibr" target="#b22">[23]</ref> proposed that meaningful objects were important to saliency detection. Based on visual attention and eye movement data, a video saliency detection model was trained and it was found to outperform all other state-of-the-art algorithms.</p><p>On one hand, conventional handcrafted features have proven their success in existing approaches and applications. On the other hand, deep learning networks has shown their great potential in computer vision such as coping with human perception especially for large-scale data and more complicated problems. It is our intension here to combine all of these approaches together to address the challenges for video saliency detection. Some papers <ref type="bibr" target="#b23">[24]</ref> <ref type="bibr" target="#b24">[25]</ref> report that it is effective to combine deep learning based features and handcrafted features for saliency detection. However these methods use only single image and do not consider temporal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T information in video. Wenguang et al <ref type="bibr" target="#b25">[26]</ref> captures spatial and temporal saliency via fully convolutional networks (FCNs) from frame pairs, but only using a frame pairs is not enough to fit the visual staying phenomenon for watching videos. Different with the mentioned works above, we proposed a novel deep learning and handcraft features hybrid framework for spatial dynamic attention of video by using consecutive 7 frames as input.</p><p>This paper focuses on eye fixation prediction task of video streams. A deep learning based combined feature framework is proposed to predict the spatial and temporal saliency regions and visual dispersion amongst video sequences. The features are extracted via an effective deep learning model, and they are then integrated with other handcrafted features. The effectiveness of these combined high-level features for saliency detection from video stream is assessed using five publicly available eye gazing datasets. In addition, a clip-vote dataset with about 596 movie clips and votes from 70 observers has also been employed to validate the applicability of the proposed approach for highlight extraction from movie streams.</p><p>Although research on saliency detection and visual attention have been receiving increasing attention in recent years, most of existing work focuses on still image rather than video based saliency detection. In this paper we proposed a deep learning based hybrid spatiotemporal feature framework for saliency prediction from video streams, and the main contributions of the present work can be summarized as follows:</p><p> A hybrid feature framework is proposed for saliency detection in video. Low-level features extracted from convolutional neural networks are found more effective than other commonly used handcrafted features such as intensity, color, orientation, and texture for saliency detection. The integration of high-level features with the low-level one, as well as the use of a customized classifier rather than that in CNN, are found very useful supplement to our framework. The performance of this hybrid feature framework is validated by five video datasets.  A CNN based feature hybrid method has been proposed for the spatial saliency detection using 7consecutive raw frames.  A 3D CNN with high level object features, scene complexity feature and a cluster weighted model have been employed for temporal dynamic attention detection.  In addition, based on the proposed TDA model, a movie clip dataset is constructed with subjective ranking of highlight levels. To the best of our knowledge, this is the first of such datasets to be made in this field. This data set may be useful for semantic video analysis as it is shown in this work.  According to experimental results, we have shown that this hybrid feature framework outperform five state-of-the-art methods for the saliency detections from five public eye fixation databases. The remaining paper is organized as follows. Section 2 overviewed the related works. Section 3 provides an overview of the proposed approach, including definition of spatiotemporal attention and ground truth determination. In Section 4 and 5 the spatial dynamic attention model and temporal dynamic attention model are discussed, respectively, along with details in terms of feature extraction. Section 6 presents the experimental results and discussions on five publicly available datasets. In Section 7, experiments on our constructed movie clip database are reported. Finally, some concluding remarks are drawn in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Saliency detection models in general can be categorized into visual attention prediction and saliency object detection. In this paper, we propose a deep learning framework for predicting eye fixation locations where a human observer may fixate <ref type="bibr" target="#b26">[27]</ref>[28] <ref type="bibr" target="#b28">[29]</ref>. Itti and Koch use low-level features to map the saliency regions/objects in the image scene <ref type="bibr" target="#b15">[16]</ref>. Koch and Ullman <ref type="bibr" target="#b27">[28]</ref> introduced a feed-forward bottom up model to combine features in the form of a saliency map to represent the most visually attractive region across the whole scene. Then a number of studies extended the concept and applied it for still image applications <ref type="bibr" target="#b29">[30]</ref>[31] <ref type="bibr" target="#b31">[32]</ref>. The idea is then further expanded by Itti et al <ref type="bibr" target="#b19">[20]</ref> who proposed a rather influential saliency based visual attention model for rapid scene analysis in 1998. Recent work by Zhang et al <ref type="bibr" target="#b32">[33]</ref> showed that simple, but effective, saliency detection could be achieved through the entropy of images in both RGB and LAB color spaces. Other work by Han et al <ref type="bibr" target="#b33">[34]</ref> also demonstrated the effectiveness of sparse coding representations for saliency detection from still images. Details of other variety of approaches can be found in a recent survey by Itti"s group <ref type="bibr" target="#b34">[35]</ref>.</p><p>For video Visual attention prediction, Quite different from that of the still image extended saliency model had been the bottom-up spatiotemporal methodology, which integrated the static and dynamic saliencies as proposed by Marat et al <ref type="bibr" target="#b10">[11]</ref> for video application. Kim et al <ref type="bibr" target="#b35">[36]</ref> extended the image based center-surround method for video stream by adding another temporal dimension. Hou and Zhang <ref type="bibr" target="#b36">[37]</ref>proposed a spectral residual saliency model based on the similarity implied redundancies amongst image frames. In Guo et al <ref type="bibr" target="#b37">[38]</ref>,a saliency map was obtained when the amplitude spectrum of the quaternion transformed video frame remained at nonzero constant value. In Cui et al <ref type="bibr" target="#b38">[39]</ref>, a temporal spectral analysis method was proposed for fast motion saliency detection in video. Hou and Zhang <ref type="bibr" target="#b39">[40]</ref> proposed to assess the entropy of features using incremental coding length (ICL). The dynamic and static saliency was computed by selecting features with longer code length increment value. Zhang D. et al <ref type="bibr" target="#b40">[41]</ref> proposes an unsupervised event saliency revealing framework by extracts features from multiple modalities to represent each shot in the given video collection. Wang W. et al <ref type="bibr" target="#b41">[42]</ref> use a novel spatiotemporal saliency detection method to estimate salient regions in videos based on the gradient flow field and energy optimization. What"s more, co-saliency detection has received tremendous research interest in recent years, <ref type="bibr" target="#b42">[43]</ref>[44] <ref type="bibr" target="#b44">[45]</ref> was all proposed a new framework for Co-saliency Detection and achieve satisfy results.</p><p>Deep learning models such as convolutional neural networks (CNN) and fully convolutional networks (FCN) has successfully used in computer vision and now it received more and more attentions in the visual attention prediction. But some methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T <ref type="bibr" target="#b45">[46]</ref>[47] <ref type="bibr" target="#b47">[48]</ref> give up the temporal features which is very important in video, only use the single video frame and the deep learning models and other works <ref type="bibr" target="#b48">[49]</ref>[50]use the combining deep learning based features and handcrafted features in saliency detection. So the previous work did not apply to visual attention prediction. In <ref type="bibr" target="#b50">[51]</ref>, two stream convolutional networks for learning complementary information was used for dynamic attention prediction and video object segmentation. However, these methods train their models on multi-frame which causes heavy computational burden. W Wang et al <ref type="bibr" target="#b51">[52]</ref> use the FCN to capture the spatial and temporal saliency information in Video salient object detection. A CNNs Based RGB-D Saliency Detection via Cross-View Transfer and Multi-View Fusion was proposed by J. Han et al <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of the proposed approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Introduction of spatiotemporal dynamic attention</head><p>In recent years, saliency detection from static images has been intensively investigated. Deduced from several image cues, Kochand Ullman <ref type="bibr" target="#b27">[28]</ref> introduced the concept of saliency map to represent the conspicuousness regions of images. By using eye ball tracking devices to capture the viewer"s most attended region of interest when people looking at specified images, several eye fixation datasets have been constructed, which have facilitated a number of methods for supervised-learning based image saliency detection <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b62">[63]</ref>. However, the work on the behaviors of viewers" attention when they are watching video remains rare.</p><p>Unlike image saliency which only distributes in the spatial domain, video saliency can be distributed in both spatial and temporal dimensions. In other words, there are two kinds of visual attentions that are needed to address for video saliency detection, i.e. spatial and temporal distributions of audience"s dynamic attention. They are respectively defined as spatial dynamic attention (SDA) and temporal dynamic attention (TDA).</p><p>SDA shares the same meaning with the saliency map used in the (still) image saliency <ref type="bibr" target="#b19">[20]</ref>. For each video, a pixel-wise SDA value ranged within [0-1] is determined. A pixel with a larger SDA value meaning it is more likely to be gazed at. Therefore, SDA can be used to indicate the spatial intrinsic consistency when different observers are watching the same video.</p><p>TDA also has a value ranged within [0-1] yet it is frame based, which indicates the variation of human"s attention on a continuous frame sequence as it is determined by using a temporal window of consecutive frames. This means that TDA is a measure of inter-observer agreement in the temporal domain. The assumption here is that a frame with larger inter-observer agreement, which corresponds to a higher TDA value, is considered more salient than those with low TDA values. Promising results using TDA for the detection of video highlights in movies are also reported in Section 7.</p><p>In this paper SDA is designed to be calculated pixel by pixel. TDA is estimated by using frame sequence and each frame is assigned a TDA value, its ground truth can also be deduced from the eye fixation database. Similar definitions can be found in Jiang et al <ref type="bibr" target="#b63">[64]</ref>, which use dynamic attention value for each frame to calculate Visual Attention Index (VAI). In Han et al <ref type="bibr" target="#b22">[23]</ref>, spatial and temporal visual attention (SVA/TVA) values are determined by using object based visual features. As visual cues are not the only factor that influence human"s attention <ref type="bibr" target="#b64">[65]</ref>, a more thorough framework is preferred to meet the biological plausibility of visual computation in determining SDA and TDA from videos. This framework should be flexible and include different kinds of heterogeneous features. With the employment of deep neural network, the proposed framework enables such flexibility to allow new features to be easily included rather than using only visual cues as in existing work within the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T Fig. <ref type="figure">2</ref>. One sample frame extracted from the movie "Gladiator" when the evil King was verballyinfuriating the General (left) and results of predicting attention from the proposed method (right) to indicate that observers were in fact attracted by his facial expression.</p><p>To some extent, the proposed video saliency framework is similar to video highlight extraction as there is potential to use detected saliency attention to promote the extraction of video highlights. However, there are some differences between them as explained in an example below. As shown in Fig. <ref type="figure">2</ref>, a frame from the movie "Gladiator" is extracted when the evil King was verbally infuriating the General. According to the highlight extraction algorithm in <ref type="bibr" target="#b65">[66]</ref>, the highlight degree of this part is low due to the accumulated stimulus of slow motion and low sound energy. On the contrary the eye fixation was concentrating on their faces, which actually demonstrated that observers were attracted by this part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Computation of ground truth</head><p>To train the model and assess the performance, ground truth maps for human spatiotemporal dynamic attention are needed. Although there are no such databases of human attention, motivated by the findings that observer"s attention and eye movement are highly correlated <ref type="bibr" target="#b66">[67]</ref>, some indirect ground truth can be derived from a large collection of human-eye gazing data. The computation of ground truth has been introduced in <ref type="bibr" target="#b22">[23]</ref>, where the spatiotemporal gaze density is generated using three human-eye gazing database, the VAGBA dataset <ref type="bibr" target="#b53">[54]</ref>, The Lübeck INB Dataset <ref type="bibr" target="#b54">[55]</ref> and the IVB database <ref type="bibr" target="#b55">[56]</ref>. For simplicity and consistent experimental comparison, the same ground truth computation method is adopted in our paper. In addition to the three datasets above, another two datasets, CRCNS <ref type="bibr" target="#b56">[57]</ref> and DIEM (Dynamic Image and Eye Movement) <ref type="bibr" target="#b57">[58]</ref>), are also used. This is because the lengths of these video streams are longer and their scenes are more complicated and the detailed description of these datasets is given in Section 6 (see in Table <ref type="table">1</ref>).</p><p>For each video in a given dataset, a 3D Gaussian model is used to estimate the probability of a pixel to be fixated in the spatiotemporal fixation map S <ref type="bibr" target="#b26">[27]</ref>. As defined below, the ground truth value of SDA can be calculated by:</p><formula xml:id="formula_0">         p M p t y x pj k O p K g g g S 1 2 2 2 2 1 } ) ( 2 ) ( exp{ ) (    (1)</formula><p>Here g is a pixel in the spatial fixation map S of current frame k , the th j gaze position of the th p observer in k is denoted as</p><formula xml:id="formula_1">) , , ( , t y x g j p k  where ] , 1 [ O p  and ] , 1 [ p M j </formula><p>, where O and P M are the total number of observers and recorded gaze positions of th p observer, respectively. The spatial standard deviation of the 3D Gaussian kernel x  and y  is about 0.01 in image width. The temporal standard deviation t  is equal to 130ms.</p><p>To calculate the TDA ground truth value which can evaluate the similarity of eye fixed of these observers in each frame, a"leave one out" method is adopted. Each time the spatial fixation map of all observers except one is computed similar as the SDA. We extract the top 25% points in this fixation map which have the highest probability of being fixed. And then compute the percentage of the visual fixations of the remaining observer that fall within those areas. If the remaining observer also looks at those areas, the TDA value will be 1. This process is iterated for all observers and can be defined by</p><formula xml:id="formula_2">O g p s k T O p p k K    1 } ), | ( { ) (   (2)</formula><p>Here p k g is all the gazing points of the th p remaining observer in frame k ,  is a threshold function to keep the top 25% fixated probability points in the spatial fixation map p s K | , which is calculated from all observers except the th p remaining observer, the  is a function for calculating the percentage of the visual fixations of the remaining observer that fall within saliency parts of the threshold spatial fixation density map.  means this process is iterated for all observers. In addition, a new clip-based video dataset is built to explorefurther applicationsof the proposed TDA model, such as video summarization <ref type="bibr" target="#b67">[68]</ref> and highlight extraction <ref type="bibr" target="#b65">[66]</ref>. For each of the video clip in the dataset, there are 70 subjects" selections, representing as 0/1 to indicate its attractiveness to the viewer. The TDA values are then combined with subjects" score and taken as features to train a SVM. Relevant technical detail and experimental results are discussed in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SDA deep learning</head><p>Based on the previously introduced spatiotemporal dynamic attention framework, a SDA predicting model is proposed in this section and detailed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">2D CNN based feature extraction</head><p>For most existing saliency detection approaches as found in <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b19">[20]</ref>[69]- <ref type="bibr" target="#b72">[73]</ref>, they all suffer from one big drawback where their classifiers are reliedon complex handcrafted features computed from the raw inputs. They may work well for biologically motivated low-level features such as intensity, color, orientation and texture. In real-world scenarios, however, it is not known which features are really important for achieving the task at hand asthe choice of features is highly goal-dependent. This has inevitably limited their generalization ability for dealing with complicated cases, such as video characterized by having wide variety of themes.</p><p>In recent years, deep learning models <ref type="bibr" target="#b73">[74]</ref>- <ref type="bibr" target="#b77">[78]</ref> have been proposed to learn hierarchy of features by building maps from low-level features to high-levelones. Applications of deep neural networks based systems have shown very competitive performance with respected to many other classifiers in many fields; such as human action recognition <ref type="bibr" target="#b78">[79]</ref>[80] <ref type="bibr" target="#b80">[81]</ref>, tracking <ref type="bibr" target="#b81">[82]</ref>, nature language processing <ref type="bibr" target="#b82">[83]</ref>, audio processing <ref type="bibr" target="#b83">[84]</ref>, segmentation <ref type="bibr" target="#b84">[85]</ref>, and denoising <ref type="bibr" target="#b85">[86]</ref>. The ImageNet database has been established to provide a comprehensive basis for image classification and object recognition research <ref type="bibr" target="#b86">[87]</ref>[88] <ref type="bibr" target="#b88">[89]</ref>. However, those image based methods cannot be directly applied for video saliency detection due to the lack of temporal information and dynamic attention model. For predicting of dynamic human attentions in videos, eye gazing video datasets have beenused in this paper for training the spatiotemporal video saliency model.</p><p>A 2D CNN architecture can be constructed by stacking multiple layers of convolution and subsampling in an alternating pattern. Each convolutional layer is used to extract features from a local neighborhood of the feature maps in the previous layer. As defined in Eq. ( <ref type="formula">1</ref>), the value of a unit at position ) , ( y x in the th j feature map in the th i layer can be calculated by</p><formula xml:id="formula_3">) tanh( ) )( ( ) 1 ( 1 0 1 0 ij q y p x m i m P p Q q pq ijm xy ij b v w v i I             (3)</formula><p>where ) tanh( represents the hyperbolic tangent function; i P and i Q are respectively the width and height of the convolutional kernel; m is the index of feature maps in the optimizedthrough back-propagation (BP) algorithm. As for the max-pooling layers, their purpose is to achieve spatial invariance by reducing the resolution of the feature maps <ref type="bibr" target="#b89">[90]</ref>. Meanwhile, max-pooling reduces the computational complexity of upper layers by selecting superior invariant features. The architecture of Convolution Network resembles the VGG-M model <ref type="bibr" target="#b90">[91]</ref> and static saliency model <ref type="bibr" target="#b91">[92]</ref>. Static spatial saliency mapis obtained from single frame which does not contain temporal information <ref type="bibr" target="#b92">[93]</ref>. The convolution network for single frame (Figure <ref type="figure">2(a)</ref>) can be summarized as input layer (640×480×3)-&gt;convolution layer 1 (7×7×96)-&gt;local response normalization-&gt;max pooling layer 1 (3×3 stride 2)-&gt;convolution layer 2 (5×5×256) -&gt;max pooling 2 (3×3 stride 2)-&gt;convolution layer 3 (3×3×512)-&gt; convolution layer 4 (5×5×512)-&gt; convolution layer 5 (5×5×512)-&gt; convolution layer 6 (7×7×256)-&gt; convolution layer 7 (11×11×128)-&gt; convolution layer 8 (13×13×1)-&gt;deconvolution layer 1 (8×8×1 stride 4). That is, the input is a single frame, and last layer is deconvolution aims to produce a saliency map which are the same size as the input. In all convolution layers, the stride was set to 1. And all convolution layers except the last one are followed by a rectified linear unit non-linearity (ReLU) layer. The convolution network composed of 10 weight layer, and transfer learning was used in the first 3 convolution layers to initialize the weights with the pre-trained weights from the VGG-CNN-M network in <ref type="bibr" target="#b90">[91]</ref>. The remaining weights were initialized randomly using the strategy in <ref type="bibr" target="#b93">[94]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">High-level features: object features</head><p>Human attentions, is mainly driven by objects, as validated in a number of previous works <ref type="bibr" target="#b94">[95]</ref>[96][97] <ref type="bibr" target="#b97">[98]</ref>. Some examples are given in Fig. <ref type="figure" target="#fig_3">4</ref>, which shows that the eye gazing positions are always focused on the objects in a scene, such as pedestrian and human faces. It is further found that human attention can be biased toward objects and move with objects <ref type="bibr" target="#b21">[22]</ref>. As a result, it is natural to assume that the region around objects may be more attractive to be gazed at.</p><p>Inspired by the object-biased attractiveness of human attention, Han et al <ref type="bibr" target="#b22">[23]</ref> extended the object bank method <ref type="bibr" target="#b98">[99]</ref> to compute a response map for each video frame. In total there are 177 kinds of objects, and each has a specific trained detector. In this paper, the same object detection method in <ref type="bibr" target="#b22">[23]</ref> is used. Based on the fact that pedestrian attracts more attention than other objects, a specific pedestrian detection algorithm <ref type="bibr" target="#b99">[100]</ref> has been adopted in this work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental setup for determining SDA</head><p>In our experiments for SDA, five video datasets are employed. All the frames are resizedto 640×480×3 pixels. In order to train an optimizing convolution network for single frame image model, Caffe framework <ref type="bibr" target="#b100">[101]</ref> is used in the model, and Stochastic Gradient Descent (SGD) with the Euclidean loss distance is used between the predicted saliency maps of the single frame with the ground truth maps. All obtained saliency values of single frame are normalized to [0, 1]. For the network, we trained over 200K iterations and set the initial learning rate to 10 -7 , the momentum to 0.9 and the weight decay to 0.0005. Validation training is used in this model. For the training dataset, 80% videos are used for training and the rest for validation.</p><p>During the training process, the proposed CNN model for single frameis firstly pre-trained with labeled saliency maps. The pre-trained CNN is then used for feature extraction in SDA framework. In the framework, consecutive 7 frames are using as input (Figure <ref type="figure">2</ref>(b)), noted as frame i to frame i-6. Each frame"s spatial saliency map is obtained by using the pre-trainedCNN. At each pixel location of the output saliency map, 3×3 pixels are concatenated as feature vector. So we obtained 9×7=63 dimensional feature vector for each pixel location from all the 7 saliency maps. Then the output vector is integrated with 177 dimensional object feature vector, and does a regression by SVM. The finally feature vector for each pixel is an integrated feature with 240 dimensions. For regression purpose, a linear SVM is adopted for its simplicity and effectiveness <ref type="bibr" target="#b22">[23]</ref> <ref type="bibr" target="#b101">[102]</ref>. A five-fold cross-validation is employed to optimize theparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">TDA deep learning</head><p>In this section, we will explain how to predict temporal dynamic attention by using the proposed framework. It is well-known that the visual gazinglocations of viewers are not always constant when they are watching video. The distribution of visual gazing location can be very dispersive particularly when the video/scene is not very interesting. However, the viewer"s gazingtends to be focused atthe same place whenan interesting event was happening. This phenomenon is known as visual dispersion for still images <ref type="bibr" target="#b102">[103]</ref> or temporal visual attention for videos <ref type="bibr" target="#b22">[23]</ref>. Examples of visual dispersionare given in Fig. <ref type="figure">5</ref> to illustrate the concept. Based on our proposed framework, the visual dispersion is calculated foreach frame of the eye gazing dataset (see in Section 4.2). Using the value of visual dispersion as the ground truth of the TDA,the proposed features are mapped to visual dispersion as detailed below. Fig. <ref type="figure">5</ref>. Examples of visual dispersion when there is no region of interest (left) or there is a region of interest, the ball, where the eye fixation is focused upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">3D CNN filter</head><p>Extraction of spatial and temporal features from multiple contiguous frames is commonly performed for the analysis and understanding of video <ref type="bibr" target="#b78">[79]</ref>[104][105] <ref type="bibr" target="#b105">[106]</ref>. In our framework, a 3D CNN architectureis proposed for spatiotemporal feature extraction from the raw video data. In the proposed 3D CNN architecture, a 3D kernel is applied to a cuboid formed by stacking multiple contiguous frames together, similar to that as suggested in <ref type="bibr" target="#b78">[79]</ref> <ref type="bibr" target="#b80">[81]</ref>. As an extension to Eq. ( <ref type="formula">3</ref>), the value of a unit at position ) , , ( z y x on the th j feature map in the th i layer is defined by</p><formula xml:id="formula_4">) tanh( 1 0 1 0 1 0 ) )( )( ( ) 1 ( ij m P p Q q R r r z q y p x m i pqr ijm xyz ij b v w v i i i                 (5)</formula><p>where the variables were as same as Equation ( <ref type="formula">3</ref>).</p><p>An overview of our proposed 3D CNN architecture is illustrated in Fig. <ref type="figure" target="#fig_4">6</ref> and the workflow issummarized as follow:</p><p>i. Use the original frame size and take 7 contiguous frames as a group as input for the 3D CNN; ii. Filtered data maps are obtained from the 1 st convolutional layer, which contains 16 convolution kernels with filters size 7 × 7× 3 giving 16 feature maps as the first layer output ; iii. Features maps are obtained by max pooling of the 16 filtered data maps using 8 × 8 max pooling; iv. The 2 nd convolutional layer has a set of 32 3D convolutional kernels with filters size 7×6×3 and outputs 32 feature maps; v. Features maps are then processed by another max pooling of 6 × 6 subsampling step. vi. Eventually a full connection is applied with a 128D feature vector extracted as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Algorithm 1: 3D CNN architecture Input: 7 contiguous frames Output: a 128D feature vector 1: Function : 3D CNN 2: 16 convolution kernels with filters size 7 × 7× 3 to calculated with the input to get the filtered data maps; 3: 8 × 8 max pooling was used to calculated the features maps; 4: 32 convolutional kernels with filters size 7×6×3 to calculated with the features maps; 5: 6 × 6 max pooling was used to calculated the features maps; 6: a full connection was apply in the features maps; 7: End function Note that at the training stage, all the parameters are optimized through back-propagation (BP) network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">High-level features: object features</head><p>The high-level image representation used in the proposed model is object based features <ref type="bibr" target="#b98">[99]</ref> (see in Section 4.2), which is employed to determine the TDA feature below. By using the 177 object feature maps extracted from each frame in the SDA stage, every object feature map is translated to a likelihood value within [0,1], thusa 177 dimensional feature vector is derived from each frame. More details about the mapping process can be referred to <ref type="bibr" target="#b98">[99]</ref>. Each element in the vector representsthe possibility/likelihood whether the corresponding object is contained in this frame.</p><p>Although in our opinion this object feature works on the computation of TDA, there is still room for further improvement especially in the consideration of contextual information. One example here is the object of a football in the context of soccer game, which can be very attractive when it is rolling towards the goal post. However, it becomes much less attractive if the ball rolls out of the field. As a result, it is essential to address the contrast or relationship between the objects in the high-level semanticspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">High-level features: scene complexity response</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Another high-level feature we proposed in this paper is the scene complexity response, which is motivated by the fact that human"s attention can vary dramatically as according to the complexity of the scenes being observed. The amount of visual information as well as the visualclutter in a picture may contribute to the variability of observers" attention <ref type="bibr" target="#b106">[107]</ref>.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">7</ref> the distribution of gazing position in the first scene is more disperse, simply because of the more clutter in the first scenethan the second. From the neurobiological point of view, this phenomenonstems from the co-functioning of two mechanisms in our human visual system (HVS), i.e. bottom-up and top-down mechanism <ref type="bibr" target="#b42">[43]</ref> <ref type="bibr" target="#b107">[108]</ref>. Although the bottom-up mechanism occurs first, the top-down mechanism plays a more dominant role when they work together.</p><p>However, the above is true only if when the scene is static. For videos, it is believed that the two mechanisms dominate the attention alternately, depending on the complexity of the scene <ref type="bibr" target="#b106">[107]</ref>.When the scene complexity is low to medium, high-level features such as object attribution, content and event etc. affect observer"s attention the most. Once the video scene turns to be too complicated, e.g. with too many objects, too much cluttering and/or too fast motion etc., the effect of the top-down mechanism is found reduced whilst the bottom-up mechanism becomes dominant. One possible explanation is that conventional image based features cannot be accurately extracted from complicated scenes. As a result, the top-down mechanism fails to work. As shown in Fig. <ref type="figure" target="#fig_5">7</ref>, the first row is a frame of scenecontains many objects and the corresponding heat map indicates that the eye fixation is dispersive. The second row is a scene of football game, where the eye fixation is focused on the main object of the ball.</p><p>To assess the visual complexity in frames, we firstly scale the frame to a size of 256 256. Then a color mean shift segmentation and region fusion method <ref type="bibr" target="#b102">[103]</ref> is employed to segment the image into sub-regions. If the image is too clutter the number of sub-regions will be large and vice versa. Finally, the number of sub-regions is normalized by dividing a maximal value. Fig. <ref type="figure" target="#fig_5">7</ref> shows two scenes and their eye fixation map and sub-region segmentation map. Thenumber of sub-regions of the segmented picture is also given. The calculation of this feature for the TDA is defined as following:</p><formula xml:id="formula_5">Max k k F HI ) ( ) (  <label>(6)</label></formula><p>Here the function  refers to the color mean-shift segmentation and region fusion method in <ref type="bibr" target="#b102">[103]</ref>. AUCs of various models for the prediction of eye movements on five eye tracking datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Ours+Center OBJ <ref type="bibr" target="#b22">[23]</ref> GBVS <ref type="bibr" target="#b15">[16]</ref> MLSN <ref type="bibr" target="#b110">[111]</ref> MCDL <ref type="bibr" target="#b111">[112]</ref> STAD <ref type="bibr" target="#b112">[113]</ref> SAG <ref type="bibr" target="#b113">[114]</ref> TNSR <ref type="bibr" target="#b44">[45]</ref> AUC   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experimental setup for determining TDA</head><p>Given a combined feature set (including 3D CNN features, object based features and scene complexity features) extracted from each frame in a video set as well as the frame-based ground truth TDA value which is normalized within [0, 1], the task in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>determining TDA value for each frame can be regarded as a regression problem. Specifically, the ratio of training and testing videos is set as 2:1. To suppress the onset effect where the central bias is the strongest, the frames within the first 1.5s are discarded. A 7-frame temporal window is then used to form a 3-D feature volume and feed into the 3D CNN for feature extraction, also the object feature and scene complexity response feature are calculated per frame.</p><p>The TDA value per frame is the measure of inter-observer agreement for one fixed frame. A higher TDA value of a frame means more attention consistency of viewers on the current time of video, vice-versa.</p><p>For the classifier selection, the cluster weighted model (CWM) <ref type="bibr" target="#b108">[109]</ref> is adopted to predict the TDA value for each frame. CWM is a generalization of Gaussian mixture models, which can achieve more accurate linear regression results than simple linear models. The source code of CWM can be downloaded from the web of <ref type="bibr" target="#b109">[110]</ref>.</p><p>Let a CWM contain N clusters in a feature space and each cluster is associated with a linear regression function. To do a mapping from the feature space to an output value (TDA in this paper), all the regression functions are mixed. The mixture proportions are determined by the CWM"s mixture coefficients and the conditional probability of sample"s features which satisfies the Gaussian distribution when representing each cluster i c , where i=1,…, N. As a result, the joint probability density of a frame TDA rating j r and the frame feature vector j f is decided by:</p><formula xml:id="formula_6">   N i i j j i i i j j c f r p c f p c p f r p 1 ) , | ( ) | ( ) ( ) , (<label>(7)</label></formula><p>where ) ( i c p is the weight for the cluster i c .</p><p>)</p><formula xml:id="formula_7">| ( i i c f p</formula><p>is the probability reflecting the influence of the cluster i c in the feature space, and ) , | ( i i j c f r p models the output distribution of the cluster. They can be formulated as the multivariate Gaussian with mean i u</p><formula xml:id="formula_8">and covariance matrix  i .        2 1 2 1 | | ) 2 ( ) 2 ) ( ) ( exp( ) | ( i u f u f c f p L i i j T i j i i  (8) 2 2 * 2 ) 2 ) ( exp( ) , | ( i i j T i j i i j f w r c f r p     <label>(9)</label></formula><p>Where * j f indicates the original feature vector j f with a 1 concatenated to its end. We have obtained the feature j f of the jth frame, then the estimated rating which should minimizes expected squared error under the model is</p><formula xml:id="formula_9">     N i i i i N i i i i j T i j c f p c p c f p c p f w r 1 1 * ) | ( ) ( ) | ( ) ( ˆ (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>For the model"s parameter set } , , , ), ( {</p><formula xml:id="formula_11"> i i i i w i u c p</formula><p> , it can be determined using the expectation-maximization (EM) method as detailed in <ref type="bibr" target="#b108">[109]</ref> <ref type="bibr" target="#b109">[110]</ref>. In the initialization process, ) ( i c p is set to a uniform distribution, 2 i  and  i are initialized to be the variances of the TDA values and features of the training frames, respectively. i u is estimated by randomly selected feature vectors from the training dataset, and i w is set to zero. For a given number of clusters, to avoid the local minima, the EM procedure is repeated for 30 times and the result with the maximum log likelihood isused. In addition, the optimal number of clusters is determined by minimizing the total mean-squared error in a five-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Learning and experimental results</head><p>To verify the effectiveness of the proposed models for SDA/TDA computation, intensive experiments were performed on five datasets, including VAGBA <ref type="bibr" target="#b53">[54]</ref>, Lübeck INB <ref type="bibr" target="#b54">[55]</ref>, IVB <ref type="bibr" target="#b55">[56]</ref>, CRCNS <ref type="bibr" target="#b56">[57]</ref> and DIEM <ref type="bibr" target="#b57">[58]</ref>. All of these five eye fixation datasets can be downloaded from the Internet. The main contents of these datasets are summarized and compared in Table <ref type="table">1</ref>, where three of them (IVB, VAGBA and Lübeck INB) had been used in <ref type="bibr" target="#b22">[23]</ref>. There as on to consider two more eye tracking datasets in our experiments is that they have extra length, dynamic camera, additional scene categories, and contained shot cut within the sequences, especially for CRCNS (MTV data) and DIEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T Fig. <ref type="figure" target="#fig_0">10</ref>. Results of saliency maps extracted from GBVS <ref type="bibr" target="#b15">[16]</ref>, OBJ <ref type="bibr" target="#b22">[23]</ref>, MLSN <ref type="bibr" target="#b110">[111]</ref>, MCDL <ref type="bibr" target="#b111">[112]</ref>, STAD <ref type="bibr" target="#b112">[113]</ref>, SAG <ref type="bibr" target="#b113">[114]</ref>, TMSR <ref type="bibr" target="#b44">[45]</ref> and the proposed method in comparison of the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluation criteria and benchmarking approaches</head><p>For the evaluation of the proposed SDA (spatial dynamic attention) and TDA (temporal dynamic attention) models, two widely used metrics are used for quantitative performance assessment, including Receiver Operation Characteristics (ROC) with area-under-curve (AUC) and Precision versus Recall (P-R) Curve. For benchmarking purpose seven state of the art approaches <ref type="bibr" target="#b15">[16]</ref>[23] <ref type="bibr" target="#b110">[111]</ref>[112] <ref type="bibr" target="#b112">[113]</ref>[114] <ref type="bibr" target="#b44">[45]</ref> which have been shown almost the best performance on public datasets are selected. For these five benchmarking methods, the default parameters and the same datasets are used for training, testing and comparison with our proposed approach.</p><p>Among these seven benchmarking approaches, the first is the bottom-up visual saliency model from Harel et al. <ref type="bibr" target="#b15">[16]</ref>, in which feature maps are constructed based on Itti's method <ref type="bibr" target="#b19">[20]</ref> and followed by normalization using a fully connected graph built over all grid locations of each feature (abbr. GBVS in the following part). The second is object-based spatial visual model from Han et al. <ref type="bibr" target="#b22">[23]</ref> (abbr. OBJ in the following part), where object features are mapped to human eye fixation density via learning. Experimental results show that OBJ method is more robust than traditional methods that use contrast-based features, such as those in <ref type="bibr" target="#b114">[115]</ref>[116] <ref type="bibr" target="#b116">[117]</ref>. The third method (abbr. MLSN) proposed a multi-layer sparse network to learn low, mid and high level features from natural images. The fourth method (abbr. MCDL) use a multi-context deep learning framework for saliency object detection. Both global context and local context are taken in account. The fifth method (abbr. STAD) proposed a dynamic consistent spatiotemporal attention model by considering objects and motions in video. The sixth method (abbr. SAG) proposed a saliency-aware geodesic video object segmentation. The last method (addr. TMSR) proposed a novel approach based on two-stage multi-view spectral rotation co-clustering for co-saliency detection.</p><p>Since there is a strong bias to the center of the image for human fixations <ref type="bibr" target="#b17">[18]</ref>[118], we also model this center bias in our SDA model by calculating the distance from current pixel to the center of the frame in the process of training and predicting. This forms a one dimensional vector and is denoted as center feature in this paper, which is also concatenated to the SDA feature set for evaluation.</p><p>In addition, Pearson correlation coefficients <ref type="bibr" target="#b118">[119]</ref> between the predicted TDA and the ground truth are also used in assessing the performance of the proposed TDA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">SDA results from ROC analysis</head><p>For each SDA calculation method, the results are evaluated using the average receiver operating characteristic (ROC) and AUC analysis, which are shown in Fig. <ref type="figure" target="#fig_6">8</ref> and Table <ref type="table" target="#tab_0">2</ref> for comparison. The ROC curve is plotted as the False Positive Rate vs. Hit Rate,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>which is used to compare the performance of the proposed SDA regression model with other approaches. The moving threshold range is [1, 0] with a step 0.1. The AUC is calculated as the area under the ROC curve to demonstrate the overall performance of a given saliency model. As can be seen, the proposed SDA model with Center feature produces the best results. Even without using the Centre feature, SDA is still found to outperform most others in all the five datasets. The MCDL method shows almost the same performance with the proposed method but it needs more computation cost to train multi deep learning frameworks.</p><p>To compare the quality of different saliency maps obtained by SDA methods, the P-R curves are illustratedin Fig. <ref type="figure" target="#fig_7">9</ref>. In general, the proposed SDA model generates the best results over the five datasets, though OBJ method yields slightly better results in the IVB dataset when the recall value is between 0.14 and 0.44.</p><p>In addition, saliency maps extracted from different methods are also visually compared in Fig. <ref type="figure" target="#fig_0">10</ref>, where the distribution of predicted eye fixation positions is used to generate the heat map. In most situations our predicted saliency maps match with the ground truth the best. For example, in the third row of "gamecube02" from the CRCNS dataset, our result is focusing on the main saliency object around the screen center as defined in the ground truth. Moreover, our model shows less attention to the text and sign at two corners on the top than other methods, which validates the efficacy of the proposed features. There are some weaknesses of our model, as in the second row, we have detected too many saliency regions, though the main object has been accurately identified. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">The Earth Mover's distance analysis for TDA</head><p>In addition to the high level features assessed above, the Earth Mover"s Distance <ref type="bibr" target="#b119">[120]</ref> between the randomly selected TDA and the ground truth are computed. To compute the TDA, the ratio of training and testing data is set as 2:1 in each dataset and relevant results are reported in Fig. <ref type="figure" target="#fig_8">11</ref> for comparison.Three datasets, IVB, CRCNS and VAGBA are used for comparisonas they respectively have low, medium and high spatial resolutions for testing. As can be seen, in all the three datasets the proposed TDA model consistently outperforms theothers, though the average EMD from the several approaches are not satisfied and can be further improved upon. Due to the large number of objects contained in the video scene, object detection becomes a hard task in this context. As a result, how to address this difficulty for improving feature extraction and CNN architecture design is worth for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">The Earth Mover's distance evaluation for SDA</head><p>To evaluate the proposed method more sufficiently, the Earth Mover"s Distance (EMD) <ref type="bibr" target="#b119">[120]</ref> is employed here. The EMD is a measure of the distance between two multi-dimensional distributions. Relevant SDA results are reported in Table <ref type="table" target="#tab_2">3</ref> for comparison.</p><p>Furthermore, the histogram intersection similarity is computed between the normalized saliency map and the ground truth. Relevant SDA results are reported in Table <ref type="table" target="#tab_3">4</ref>.</p><p>According to the EMD results, the proposed method performs better than others in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Further discussions of the proposed framework</head><p>In this paper, we propose a deep learning based feature hybrid framework for video saliency detection. The feature extraction by the deep learning network plays the most important role in this work for saliency detection, which is further improved by concatenating some high-level global features. Rather than putting all the hand-tuned features as an input layer to a deep learning network, they are combined with the output of the pre-trained deep learning net work for easy implementation and highly flexibility to adapt heterogeneous architecture features. For example, a video clip with high sound energy may indicate some exciting elements that draw more viewers" attention. Thus the sound feature can be taken into account to dynamically optimize the saliency area and consistency of attention in a frame. It is feasible in the proposed framework to concatenate the sound energy feature to other extracted features. As a result, our design is more flexible and better compatible with heterogeneous architecture features.</p><p>Another reason that the hand-tuned high-level features are not taken as input to CNN is the extremely high computation cost on the five video databases for training. Although the deep learning networks have been significantly improved comparing with the original artificial neural networks model, it still needs considerable computational cost for training and tuning, especially for videos. It takes about 5 days to train the deep learning network even with a high performance personal computer. More input data means more training time is needed. Though possibly the high-level feature maps may improve the results from the deep learning network, Histogram intersection results of SDA on five eye tracking datasets (higher is better). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>we still need the balance between efficiency and efficacy. Actually, the proposed method is still competitive with state of the art methods, such as <ref type="bibr" target="#b111">[112]</ref>, where a similar deep CNN is used with joint global features and local features. As compared in Table <ref type="table" target="#tab_0">2</ref>, the proposed approach has not sacrificed the accuracy yet provides great flexibility to be further improved by considering more discriminative features, no matter they are homogeneous or heterogeneous. The features used in this paper have shown their efficacy for video saliency detection, however motion based features have been excluded. This is due to the lack of stability of the extracted motion vectors, especially when there is clutter in the background, camera motion and multiple objects moving at the same time. Inaccurate motion features lead to unsatisfied saliency detection. Although there has been work on motion based video object extraction <ref type="bibr" target="#b120">[121]</ref>, it fails to work for quite a few videos in our datasets. This will be further investigated as we believe that accurate extraction of video objects and their motion vectors is of great importance for content understanding and saliency detection in videos.</p><p>For the choice of deep learning networks, CNN is employed as it has been successfully applied in a wide range of applications including, but not limited to, handwriting digital recognition, image classification, object recognition and speech recognition <ref type="bibr" target="#b78">[79]</ref>- <ref type="bibr" target="#b88">[89]</ref>. Considering the temporal window (frame buffer) requested for properly determining the temporal attention model, the long-short term memory networks (LSTMs) <ref type="bibr" target="#b121">[122]</ref>may be potentially used for its strong ability in modeling sequential data, such as sound and text. However, in our problem the input is 2D frame sequence which is hardly to be processed in parallel for LSTMs. In <ref type="bibr" target="#b122">[123]</ref> four LSTMs are employed to process an image along four different directions, where the results are found comparable to those from CNN. However, how to apply LSTMs for video saliency detection still needs a lot of work to do and it is worth for further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Film clips ranking based database</head><p>In this group of experiments, the proposed spatial-temporal video saliency model is applied for predicting movie highlights. Relevant results on a large clip-based database collected by us are reported below to further validate the efficacy of the proposed methodologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.1.Construction of the movie clip dataset</head><p>The movie clip database collected by us contains 596 clips, which are manually chosen from a total of 20 movies. As listed in Table <ref type="table" target="#tab_5">5</ref>, these movies cover four major genres including action, horror, war and disaster. For each movie, 15-18 clips are extracted, with a length varying from 40 seconds to 2 minutes. The criterion of choosing clips here is to select three kinds of clips whose highlight levels are low, medium and high whilst keeping the number of clips in each category nearly identical. For the convenience of quantitative assessment, a user experiment was conducted to establish the ground truth where each movie clip was labeled according to the votes by a group of users as the measurement of corresponding highlight level. The user group contains 70 undergraduate students, 30 females and 40 males, who have received detailed instructions of how the experiment is conducted. During the two-day experiment each user was asked to watch these 596 movie clips on a computer screen with a noise canceling headphone. Afterwards, the viewer was asked to vote whether the clip was exciting, i.e. a highlight clip. Eventually, the highlight level for each clip is determined as a ratio between the number of votes and it is then normalized into [0-1] by using the  The advantage of the clip datasetwe built is that it is easy for understanding and implementation. The viewers need not to remember the complex ranking criterion. Instead, the only thing is to record their feelings about whether a clip is exciting enough, which makes the resultcloser to the intrinsic excitement of the video content. Of course there are disadvantages of the dataset. Firstly, as the isolated clips are presented without the context information, this may mislead viewers. Secondly, the ground true is not captured directly when the clip is being watched. Nevertheless, the use of viewer"s majority vote may potentiallycancel outeffects caused by their subconsciously subjective experience and personal habit.</p><p>However, showing isolated clips is almost certainly suboptimal. For future work we believe that eye gazing tracking and electroencephalographic signal are better ways to generate ground truth for video highlight extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Results and analysis</head><p>A simple test model has been adopted to map several low-level features to highlight-level semantics of movie highlight/excitement. To show the effectiveness of the TDA, TDA is regarded as a high-level feature in the movie clip which is further compared with the frequently-used low-level features, such as motion, shot cut density and sound energy <ref type="bibr" target="#b65">[66]</ref>. By concatenating the TDA feature with the conventional features, a linear kernel SVM is employed for training and predicting in this movie clip highlight predicting problem. The ratio of training and testing data is 5:2 where again the best value for the parameter C is selected by a five-fold cross-validation.</p><p>For each movie clip, the absolute mean error between the predicted highlight score and the ground truth is computed for quantitative performance assessment. As the range of mean error is within [0-1], it is evenly divided into 10 sub-ranges. A histogram is used to show how many clips have mean prediction error lies within each sub-range, as illustrated in Fig. <ref type="figure" target="#fig_11">13</ref> for comparison. With the TDA feature, the number of clips with their mean prediction errors less than 0.1 has increased from nearly 80 to over 110. It is seen that more than 110 clips have prediction errors reduced to less than 0.1. For all other sub-ranges, the number of clips from the new approach is consistently reduced, showing the better efficacy of the proposed model for the detection of movie highlights/excitementwhen TDA feature is included in the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, a deep-learning based hybrid feature extraction framework is proposed to address the problem of video saliency description and characterization. With the combination of proposed novel deep learning networks and conventional methods as feature extractors, the final hybrid features are used to predict the spatial and temporal saliency. Two detail applications are implemented for detection of spatiotemporal dynamic attention. Novel deep learning networks architecture for spatial saliency is proposed by considering both the spatial and temporal characteristics. A movie clip dataset with manually labeled video highlights. An application of highlight clip extraction is carried out by using temporal saliency results. A careful and thoughtful experimental research is shown that the proposed framework achieves the advantage that the outputs outperform five other methods Future work will focus on finding novel architecture of deep learning network for video dynamic attention model. Especially for the dataset of videos in the wild <ref type="bibr" target="#b123">[124]</ref>, a human factor independence novel model is needed. To achieve this, the challenge is how to extract reliable and informative feature from the unconstrained videos <ref type="bibr" target="#b124">[125]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Feature Hybrid Deep Learning Based Framework for spatiotemporal saliency extraction.</figDesc><graphic coords="4,141.44,460.27,298.03,128.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.The proposed feature hybrid spatiotemporal CNN model for the computation of SDA. (a) The architecture of the CNN model for single frame. (b) The feature hybrid framework for SDA with consecutive 7 frames as input.</figDesc><graphic coords="6,57.21,78.86,455.97,208.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>which is connecting to current layer; pq ijm w is the value at position ) , ( q p of the convolutional kernel and ij b represents the bias for the current feature map. At the training stage, all the parameters are A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.Examples show how objects affect human dynamic attention in videos.</figDesc><graphic coords="7,134.56,358.49,303.38,188.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The 3D CNN architecture used in the proposed framework forthe extraction of TDAfeature.</figDesc><graphic coords="8,68.00,405.93,444.89,148.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visual dispersion is affected by scene complexity which is measuring by using image sub-region segmentation.</figDesc><graphic coords="9,93.68,257.58,383.18,176.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. ROC curves of various SDA models for eye movement prediction in five eye tracking datasets: CRCNS, DIEM, Lubeck INB, IVB and VAGBA.</figDesc><graphic coords="11,367.20,394.06,162.64,142.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9.Precision-Recall curves of the various SDA models for the prediction of eye movements on five eye tracking benchmarks.</figDesc><graphic coords="11,112.65,536.31,172.16,150.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11.Comparison of the Earth Mover"s Distance (lower is better) between the predicted TDA which are random selected and ground truth in IVB, CRCNS and VAGBA datasets by using proposed method and others including 2D CNN as a baseline of the comparision, OBJ, 3D CNN, 3D CNN+ Scene Complexity,3D CNN+OBJ and TDA.</figDesc><graphic coords="14,122.86,546.12,326.82,159.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>S C R I P T total number of users, i.e. 70. The experimental highlight level for each movie clip is plotted in Fig.12, which shows that our dataset contains clips with different highlight levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Histogram of sorted user votes for clips</figDesc><graphic coords="17,187.06,112.42,196.96,129.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Results for movie highlight prediction: By using the TDA feature, more clips fall in smaller error range than the result from<ref type="bibr" target="#b65">[66]</ref>.</figDesc><graphic coords="17,142.41,545.72,286.26,169.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,34.88,78.86,504.18,289.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>EMD results of SDA on five eye tracking datasets (smaller is better).</figDesc><table><row><cell></cell><cell>Ours</cell><cell>Ours+Center</cell><cell>OBJ</cell><cell>GBVS</cell><cell>MLSN</cell><cell>MCDL</cell><cell>STAD</cell><cell>SAG</cell><cell>TMSR</cell></row><row><cell>EMD in IVB</cell><cell>0.4988</cell><cell>0.4735</cell><cell>0.5734</cell><cell>0.6870</cell><cell>0.5100</cell><cell>0.5611</cell><cell>0.6608</cell><cell>0.5069</cell><cell>0.4905</cell></row><row><cell>EMD in CRCNS</cell><cell>0.6508</cell><cell>0.6231</cell><cell>0.6908</cell><cell>0.7125</cell><cell>0.6695</cell><cell>0.6453</cell><cell>0.7270</cell><cell>0.6515</cell><cell>0.6415</cell></row><row><cell>EMD in DIEM</cell><cell>0.6203</cell><cell>0.6117</cell><cell>0.6861</cell><cell>0.6883</cell><cell>0.6352</cell><cell>0.6662</cell><cell>0.7093</cell><cell>0.6313</cell><cell>0.6455</cell></row><row><cell>EMD in INB</cell><cell>0.3751</cell><cell>0.3666</cell><cell>0.3930</cell><cell>0.4714</cell><cell>0.4394</cell><cell>0.3833</cell><cell>0.4503</cell><cell>0.4016</cell><cell>0.4123</cell></row><row><cell>EMD in VAGBA</cell><cell>0.4013</cell><cell>0.3843</cell><cell>0.4213</cell><cell>0.4505</cell><cell>0.4337</cell><cell>0.3966</cell><cell>0.4782</cell><cell>0.4033</cell><cell>0.4081</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 LIST</head><label>5</label><figDesc>OF MOVIES USED IN CONSTRUCTING OF OUR DATASET.</figDesc><table><row><cell cols="2">No. Movie Names</cell><cell>Genres</cell></row><row><cell>1</cell><cell>Red (2010)</cell><cell>Action/Comedy</cell></row><row><cell>2</cell><cell>Mission: Impossible III(2006)</cell><cell>Action/Thriller</cell></row><row><cell>3</cell><cell>Live Free or Die Hard (2007)</cell><cell>Action/Thriller</cell></row><row><cell>4</cell><cell>Gladiator (2000)</cell><cell>Action/Drama</cell></row><row><cell>5</cell><cell>Terminator 2: Judgment Day (1991)</cell><cell>Action/Sci-Fi</cell></row><row><cell>6</cell><cell>The Day After Tomorrow (2004)</cell><cell>Disaster/Sci-Fi</cell></row><row><cell>7</cell><cell>Twister (1996)</cell><cell>Disaster/Drama</cell></row><row><cell>8</cell><cell>Titanic (1997)</cell><cell>Disaster/Romance</cell></row><row><cell>9</cell><cell>The Perfect Storm (2000)</cell><cell>Disaster/Drama</cell></row><row><cell>10</cell><cell>2012 (2009)</cell><cell>Disaster/Family</cell></row><row><cell>11</cell><cell>The Silence of the Lambs (1991)</cell><cell>Horror/Crime</cell></row><row><cell>12</cell><cell>Silent Hill (2006)</cell><cell>Horror/Thriller</cell></row><row><cell>13</cell><cell>The Shining(1997)</cell><cell>Horror</cell></row><row><cell>14</cell><cell>Final Destination (2000)</cell><cell>Horror/Thriller</cell></row><row><cell>15</cell><cell>Alien: Resurrection (1997)</cell><cell>Horror/Sci-Fi</cell></row><row><cell>16</cell><cell>Black Hawk Down (2001)</cell><cell>War/History</cell></row><row><cell>17</cell><cell>The Lord of the Rings: The Return of</cell><cell>War/Action</cell></row><row><cell></cell><cell>the King (2003)</cell><cell></cell></row><row><cell>18</cell><cell>Troy (2004)</cell><cell>War/Romance</cell></row><row><cell>19</cell><cell>Brave heart (1995)</cell><cell>War/History</cell></row><row><cell>20</cell><cell>Saving Private Ryan (1998)</cell><cell>War/History</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would greatly thank the Editors and anonymous reviewers for their constructive comments to further improve the clarity and quality of this paper.The authors wish to acknowledge the support they received from the National Natural Science Foundation, China, under grants 61572351, 61772360, and the joint project funded by the Royal Society of Edinburgh and NSFC 61211130125.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is bottom-up attention useful for object recognition?</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Foveation scalable video coding with automatic fixation selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="254" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time foveated multi-resolution system for low-bandwidth video communication</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Perry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Vision and Electronic Imaging</title>
		<meeting>Human Vision and Electronic Imaging</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="294" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised feature selection via hierarchical regression for web image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video super-resolution using an adaptive superpixel-guided auto-regressive model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognitions</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="59" to="71" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video super-resolution based on automatic key-frame selection and feature-guided variational optical flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing -Image Communication</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="875" to="886" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaze-based interaction for semi-automatic photo cropping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &quot;06: Proc. the SIGCHI conference on Human Factors in computing systems</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="771" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stylization and abstraction of photographs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="769" to="776" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved seam carving for video retargeting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video summarization using a visual attention model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guironnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pellerin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th European Signal Proc. Conf</title>
		<meeting>15th European Signal . Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1784" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling spatio-temporal saliency to predict gaze direction for short videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ho-Phuoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Granjon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guyader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gue´rin-Dugue´</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="243" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On constructing a communicative space in HRI</title>
		<author>
			<persName><forename type="first">C</forename><surname>Muhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sagerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th German Conf. Artificial Intelligence</title>
		<meeting>30th German Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="264" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J, Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensor rank selection for multimedia analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="376" to="392" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A biologically inspired computational model for image saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Multimedia Conf</title>
		<meeting>ACM Int. Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1465" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rapid biologically-inspired scene classification using features shared with visual attention</title>
		<author>
			<persName><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="312" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning video saliency from human gaze using candidate selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rudoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1147" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial and temporal visual attention prediction in videos using eye movement data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="140" to="153" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual Saliency Detection Based on Multi scale Deep CNN Features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transon Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chic, CNN for saliency detection with low-level feature integration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chenb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="212" to="220" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning For Video Saliency Detection</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variability of eye movements when viewing dynamic natural scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">219</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cultural variation in eye movements during scene perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nisbett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="12629" to="12633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling visual attention via selective tuning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y K</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nuflo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="507" to="545" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Detecting salient regions in an image: From biological evidence to computer implementation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milanese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Geneva</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using a saliency map for active spatial selective attention: Implementation &amp;initial results</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Saliency detection by combining spatial and spectral information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Lett</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1987" to="1989" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An object-oriented visual saliency detection framework based on sparse coding representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2009" to="2021" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="207" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatiotemporal saliency detection and its applications in static and dynamic scenes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="446" to="456" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recog</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal spectral residual: fast motion saliency detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Multimedia Conf</title>
		<meeting>ACM Int. Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="617" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revealing Event Saliency in Unconstrained Video Collection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1746" to="1758" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Consistent Video Saliency Using Local Gradient Flow Optimization and Global Refinement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4185</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Unified Metric Learning-Based Framework for Co-saliency Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on Circuits and Systems on Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Co-Saliency Detection via a Self-Paced Multiple-Instance Learning Framework</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revisiting Co-Saliency Detection: A Novel Approach Based on Two-Stage Multi-View Spectral Rotation Co-clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3196</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3074" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic co-segmentation in videos</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="760" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Yu Visual Saliency Detection Based on Multiscale Deep CNN Features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Chic CNN for saliency detection with low-level feature integration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chenb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="212" to="220" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for dynamic saliency prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04730</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video Salient Object Detection via Fully Convolutional Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">CNNs Based RGB-D Saliency Detection via Cross-View Transfer and Multi view Fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Free viewing of dynamic stimuli by humans and monkeys</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Boehnke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Intrinsic dimensionality predicts the saliency of natural dynamic scenes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1080" to="1091" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Eye-tracking database for a set of standard video sequences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hadizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Enriquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Baji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="898" to="903" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automatic foveation for video compression using a neurobiological model of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1304" to="1318" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Clustering of gaze during dynamic scene viewing is predicted by motion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semi-supervised change detection method for multi-temporal hyperspectral images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="363" to="375" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sparse coding for image denoising using spike and slab prior</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="12" to="20" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Latent Semantic Minimal Hashing for Image Retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="355" to="368" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Image reconstruction by an alternating minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="661" to="670" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scene Parsing From an MAP Perspective</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON CYBERNETICS</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1876" to="1886" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Key frame-based video summary using visual attention clues</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Decorrelation and distinctiveness provide with human-like saliency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dosil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advanced Concepts for Intelligent Vision Systems</title>
		<meeting>Advanced Concepts for Intelligent Vision Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Affective video content representation and modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The integration of parallel and serial processing mechanisms in visual search: evidence from eye movement recordings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Maioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Benaglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="364" to="372" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hierarchical modeling and adaptive clustering for real-time summarization of rush videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="906" to="917" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Rev. Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="194" to="204" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Contrast-based image attention analysis by using fuzzy growing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ACM Int. Multimedia Conf</title>
		<meeting>11th ACM Int. Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Modeling attention to salient proto objects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1395" to="1407" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A coherent computational approach to model bottom-up visual attention</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="802" to="817" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Scaling learning algorithms towards AI, Large-Scale Kernel Machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="321" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Detecting human actions in surveillance videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. TREC Video Retrieval Evaluation Workshop</title>
		<meeting>TREC Video Retrieval Evaluation Workshop</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th European Conf. Computer Vision</title>
		<meeting>11th European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Human tracking using convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1610" to="1623" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Machine Learning Conf</title>
		<meeting>25th Int. Machine Learning Conf</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inform. Proc. Systems</title>
		<meeting>Advances in Neural Inform. . Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Convolutional networks can learn to generate affinity graphs for image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Helmstaedter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Briggman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="538" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inform. Proc. Systems</title>
		<meeting>Neural Inform. . Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Proceedings</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A vision-based robotic grasping system using deep learning for 3D object recognition and pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Robotics and Biomimetics</title>
		<meeting>Robotics and Biomimetics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1175" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Shallow and deep convovlutional networks for saliency prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mc</forename><surname>Guinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">O</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for dynamic saliency prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cagdasbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Eye movements in iconic visual search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1447" to="1463" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A theory of eye movements during target acquisition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Rev</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="787" to="835" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Objects predict fixations better than early saliency</title>
		<author>
			<persName><forename type="first">W</forename><surname>Einhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Saccadic selectivity in complex visual search displays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pomplun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1886" to="1900" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Object bank: a high-level image representation for scene classification and semantic feature sparsification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1378" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Caffe: convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelhamer</forename><surname>Evan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donahue</forename><surname>Jeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karayev</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girshick</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guadarrama</forename><surname>Sergio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename><surname>Trevor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Combining MLC and SVM Classifiers for Learning Based Decision Making: Analysis and Evaluations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="page">423581</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Prediction of the inter-observer visual congruency (IOVC) and application to image ranking</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baccino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Multimedia Conf</title>
		<meeting>ACM Int. Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision</title>
		<meeting>IEEE Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Efficient detection of temporally impulsive dirt impairments in archived films</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Human action segmentation and recognition via motion and shape analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="438" to="445" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Measuringvisual clutter</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nakano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Visual correlates of fixation selection: effects of scale and time</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="643" to="659" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Gershenfeld</surname></persName>
		</author>
		<title level="m">The Nature of Mathematical Modeling</title>
		<meeting><address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Estimating perception of scene layout properties from global image features</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning to Predict Eye Fixations for Semantic Contents Using Multi-layer Sparse Network</title>
		<author>
			<persName><forename type="first">Chengyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">22</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Saliency Detection by Multi-Context Deep Learning</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Wanliouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1265" to="1274" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Video Saliency Detection via Dynamic Consistent Spatio-Temporal Attention Modelling</title>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="3395" to="3402" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Bayesian surprise attracts human attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1295" to="1306" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Segmenting salient objects from images and videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Learning a saliency map using fixated locations in natural scenes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">The relation between pearson&apos;s correlation coefficient r and salton&apos;s cosine measure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Egghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leydesdorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1027" to="1036" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<title level="m">Analysis of Scores, Datasets, and Models in Visual Saliency Prediction, Proceedings of the 2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="921" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Auto-directed video stabilization with robust L1 optical camera paths</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2983" to="2996" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
		<author>
			<persName><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos &quot;in the wild</title>
		<author>
			<persName><forename type="first">Jingen</forename><forename type="middle">;</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Jiebo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">He was also awarded a Ph.D. in Electronic Imaging and Media Communication from Bradford University,U.K. He is currently with Dept. of Electronic and Electrical Engineering, University of Strathclyde. His research interests focus mainly on visual computing and multi media signal processing, especially osemantic content extraction for video analysis and understanding and hyperspectral imaging. Dong Zhang (zhang_dong@tju.edu.cn) is currently working towards his M.S. degree in school of computer science and technology at Tianjin University. His current research intersts mainly focus on Computer Image Processing</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">2009. 2008. 2009. 1994. 1997</date>
			<pubPlace>Tianjin, China; Tianjin, China; UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science from Tianjin University (TJU) ; Computer Science from Tianjin University (TJU)</orgName>
		</respStmt>
	</monogr>
	<note>She is now an associate professor in School of Computer Science and Technology, TJU. She once was a visiting scholar of INRIA institute, France, from 2007 to 2008. Her current research interests include computer graphics, hyperspectral imaging, and image processing. Jianmin Jiang(jianmin.jiang@szu.edu.cn, corresponding author) received a PhD from the University of Nottingham</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">He has been a chartered engineer, fellow of IEE/IET, fellow of RSA</title>
		<author>
			<persName><surname>Glamorgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Wales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">he joined the University of Bradford, UK, as a Chair Professor of Digital Media, and Director of the Digital Media &amp; Systems Research Institute. He worked at the University of Surrey, UK, as a full professor during 2010-2015 and as a</title>
		<meeting><address><addrLine>China; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2002. during 2010-2013</date>
		</imprint>
		<respStmt>
			<orgName>at Tianjin University ; Research Institute for Future Media Computing at the College of Computer Science &amp; Software Engineering, Shenzhen University</orgName>
		</respStmt>
	</monogr>
	<note>distinguished professor (1000-plan. member of EPSRC College in the UK, and EU FP-6/7 evaluator</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
