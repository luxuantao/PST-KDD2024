<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interstellar: Using Halide&apos;s Scheduling Language to Analyze DNN Accelerators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-26">26 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
							<email>xuany@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Qiaoyi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Setter</surname></persName>
							<email>setter@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
							<email>kozyraki@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
							<email>horowitz@ee.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
							<email>jingpu@alumni.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ankita</forename><surname>Nayak</surname></persName>
							<email>ankitan@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Bell</surname></persName>
							<email>sebell@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
							<email>kaidicao@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Heonjae</forename><surname>Ha</surname></persName>
							<email>hunjaeha@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Priyanka</forename><surname>Raina</surname></persName>
							<email>praina@stanford.edu</email>
						</author>
						<author>
							<persName><surname>Interstellar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Mingyu Gao</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University Steven Bell</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University Heonjae Ha</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Stanford University Priyanka Raina</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interstellar: Using Halide&apos;s Scheduling Language to Analyze DNN Accelerators</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-26">26 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3373376.3378514</idno>
					<idno type="arXiv">arXiv:1809.04070v2[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>dataflow</term>
					<term>domain specific language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that DNN accelerator micro-architectures and their program mappings represent specific choices of loop order and hardware parallelism for computing the seven nested loops of DNNs, which enables us to create a formal taxonomy of all existing dense DNN accelerators. Surprisingly, the loop transformations needed to create these hardware variants can be precisely and concisely represented by Halide's scheduling language. By modifying the Halide compiler to generate hardware, we create a system that can fairly compare these prior accelerators. As long as proper loop blocking schemes are used, and the hardware can support mapping replicated loops, many different hardware dataflows yield similar energy efficiency with good performance. This is because the loop blocking can ensure that most data references stay on-chip with good locality and the processing units have high resource utilization. How resources are allocated, especially in the memory system, has a large impact on energy and performance. By optimizing hardware resource allocation while keeping throughput constant, we achieve up to 4.2Ã— energy improvement for Convolutional</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have recently displaced classical image processing and machine learning methods due to their state-of-the-art performance on many tasks, particularly in recognition, localization, and detection <ref type="bibr" target="#b17">[18]</ref>. As the number of applications for DNNs has grown, so have proposals for DNN accelerators. NeuFlow created a 2D systolic array for convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref>. The DianNao family was built around customized inner-product units <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. Eyeriss highlighted the importance of on-chip dataflow on energy efficiency and proposed a row-stationary heuristic <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. And Google's TPU used a simple yet efficient systolic dataflow on a large 2D processing array <ref type="bibr" target="#b19">[20]</ref>. These are just a few of the recent publications on DNN acceleration. All these proposals stated the advantages of their approaches over conventional general-purpose baseline platforms, yet the architectures and dataflows differ significantly across these approaches.</p><p>To help us understand and further improve these DNN accelerators, we realized that, since all the hardware designs perform the same computation, i.e., a seven-level loop nest for convolution as in Algorithm 1, the space of all accelerators can be formally specified by how they transform (block, reorder, and parallelize) the loop nest. We use this insight to create a formal taxonomy of DNN accelerators that expresses design choices as different loop transformations. For example, to improve data reuse and energy efficiency, the loops can be blocked and reordered to better schedule the computation, such that most data references are captured by the smallest and the most efficient memory buffer. The on-chip dataflow choices can also be represented as parallelizing different loops on multiple hardware processing units, known as spatial loop unrolling.</p><p>Prior work, like Timeloop <ref type="bibr" target="#b29">[30]</ref>, also adopted similar loopbased approaches to represent and analyze the design space for DNN accelerators systematically. We extend this work by showing that the loop transformations we use to specify the micro-architecture and dataflow choices of DNN accelerators are almost a subset of the loop transformation and memory allocation primitives provided by Halide's scheduling language <ref type="bibr" target="#b33">[34]</ref>. Halide is a domain-specific language for image processing applications. It provides all the required facilities to perform these loop transformations to convert a single application into efficient implementations on CPU, GPU, and more recently specialized hardware <ref type="bibr" target="#b31">[32]</ref>.</p><p>To create any possible DNN dataflow and storage hierarchy, we extend Halide, enabling it to create hardware accelerators for dense linear algebra in addition to image processing. The decoupling between algorithm and schedule within the Halide system makes it easy to explore different DNN mappings and hardware by simply changing the schedules in the Halide code. Using this system makes it easy to recreate the previously proposed designs and fairly compare their resulting performance and energy efficiency.</p><p>The extended Halide system allows us to create a systematic optimization framework, which can efficiently study the impact of dataflow and underlying hardware architecture design choices. Our results using Halide and the optimization framework show, with proper loop blocking, many dataflow choices can achieve similar and close-to-optimal energy efficiency. This large number of "optimal" solutions results from the fact that operands in most convolutional layers in DNNs have high reuse rates so, as long as properly blocked, most data references occur locally. Tailoring the loop blocking to each architecture is key; the blocking approach is usually more critical than the dataflow choice. For the layers that do not have enough data reuse to exploit, e.g., fully-connected layers that are not batched together, overall energy is dominated by off-chip main memory accesses, thus the on-chip dataflow still does not have a large impact. From a performance perspective, it is important that the hardware supports unrolling multiple loops onto one spatial dimension of the underlying hardware, named as replication, to achieve good resource utilization.</p><p>In fact, energy efficiency is more tightly tied to the design of the hierarchical memory system and how each level in this hierarchy is sized. Since every multiply-add (MAC) involves fetching many operands from a memory (usually a register file, RF) and the cost of each RF fetch is proportional to the RF size, it is most efficient to adopt a relatively small RF. This small first-level memory creates the need for a memory hierarchy, since the size ratio between the adjacent memory levels needs to be in a certain range to balance the total energy cost of accessing data at each level in the memory hierarchy. Using these insights, we created an efficient optimizer for these types of Halide programs to jointly optimize memory system with schedules, which achieves up to 4.2Ã—, 1.6Ã—, and 1.8Ã— energy improvement over the original Eyeriss accelerator for various CNNs, LSTMs, and MLPs respectively. This paper makes the following contributions:</p><p>â€¢ Introduces a systematic approach to precisely and concisely describe the design space of DNN accelerators as schedules of loop transformations. â€¢ Shows that both the micro-architectures and dataflow mappings for existing DNN accelerators can be expressed as schedules of a Halide program, and extends the Halide schedule language and the Halide compiler to produce different hardware designs in the space of dense DNN accelerators. â€¢ Creates a tool to optimize the memory hierarchy, which is more important than the choice of dataflow, achieving a 1.8Ã— to 4.2Ã— energy improvement for CNNs, LSTMs, and MLPs. The next section briefly reviews DNN accelerators. Then Section 3 describes how these accelerators can be characterized by their loop nest structures. Section 4 introduces Halide, explains how its scheduling language expresses the transformations we need, and shows how we use it to generate different accelerator implementations. To help us rapidly evaluate these designs, Section 5 discusses an analytical model, and how we validated this model. We then use this model to evaluate the energy and performance of different designs in Section 6. Finally Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Diversity of DNN Accelerators</head><p>While independent research efforts often converge to a few common approaches, this does not seem to be the case for DNN acceleration. The NeuFlow architecture was a 2D systolic array for CNNs, where each processing element (PE) communicated only with its neighbors, and data was streamed to and from DRAM <ref type="bibr" target="#b12">[13]</ref>. Its successor TeraDeep used a fixed loop-blocking strategy for CONV layers <ref type="bibr" target="#b15">[16]</ref>. The DianNao family was built around customized inner-product units. The first generation used a single level of small buffers <ref type="bibr" target="#b5">[6]</ref>, while in a later iteration the original unit was surrounded by a large eDRAM that stored the complete data sets <ref type="bibr" target="#b9">[10]</ref>. Another version specially built for embedded systems further extended to a 2D PE mesh that supported optimized inter-PE data propagation <ref type="bibr" target="#b11">[12]</ref>. More recently, Eyeriss highlighted the importance of such on-chip dataflow for energy efficiency, and proposed using row-stationary dataflow as a heuristic solution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Neurocube and Tetris combined the spatial PE arrays with 3D-stacked DRAM to reduce the main memory access cost <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>. FlexFlow leveraged the complementary effects of different dataflow styles and mixed them on the same PE array to improve resource utilization <ref type="bibr" target="#b26">[27]</ref>. To improve its efficiency, Eyeriss V2 <ref type="bibr" target="#b6">[7]</ref> designed a flexible interconnect to support different replication schemes. In addition to CNNs, Google's TPU used a simple systolic dataflow on a large 2D array of PEs, which could also be used for MLPs and LSTMs <ref type="bibr" target="#b19">[20]</ref>. Tangram investigates the dataflow optimizations for coarse-grain parallelism to eliminate excessive data duplication in the on-chip buffers for tiled NN accelerators <ref type="bibr" target="#b14">[15]</ref>. Song in <ref type="bibr" target="#b38">[39]</ref> proposed to reorganize dataflows to improve the data reuse for non-standard convolutional layers in Generative Adversarial Networks (GANs). Other prior work has also implemented architectures that are flexible to support multiple different dataflow types <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">43]</ref>. Beyond dense matrix computations, other designs have explored DNN sparsity and proposed specialized dataflow schedules <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>. Another group of designs have transformed DNN processing into the frequency domain to reduce computations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>On FPGA platforms, Zhang et al. <ref type="bibr" target="#b46">[47]</ref> adopted the Roofline model to explore loop blocking, but considered only two levels of memory and only minimized off-chip bandwidth rather than total memory energy. Alwani et al. <ref type="bibr" target="#b4">[5]</ref> fused the computation of different NN layers, and Li et al. <ref type="bibr" target="#b25">[26]</ref> mapped the entire CNN onto an FPGA in a pipelined manner, both to reduce intermediate data writeback. Shen et al. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> optimized FPGA resource utilization by using a heterogeneous design. Sharma et al. <ref type="bibr" target="#b34">[35]</ref> provided hand-optimized templates for generating dataflow architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DNN Accelerator Design Space</head><p>All the DNN accelerators discussed in Section 2 have demonstrated improvements over general-purpose baselines, and explored how the parameters they experimented with would affect their performance and efficiency. Unfortunately, without an understanding of the global design space, each paper explores a different part of the space -perhaps coupling together independent parameters -so this approach leads to conflicting reports on the "optimal" parameters. To avoid this problem we first wanted to understand the space Algorithm 1 CONV layer: simple seven nested loops.</p><formula xml:id="formula_0">for b = 0 until B do for k = 0 until K do for c = 0 until C do for y = 0 until Y do for x = 0 until X do for f y = 0 until F Y do for f x = 0 until F X do O[b][k][x][y] += I[b][c][x + f x ][y + f y ] Ã—W[k][c][f x ][f y ]</formula><p>of all possible dense DNN accelerators. We thought this was possible since each accelerator computes the same result, so the differences must be in the resources available to compute the result, and the way the computation is scheduled to use these resources. To understand how this works, let's review the computation which needs to be performed. A DNN is a directed acyclic graph (DAG) of various types of layers. The CONV layer computation is summarized as:</p><formula xml:id="formula_1">O[b][k][x][y] = Câˆ’1 c=0 F Y âˆ’1 f y =0 F X âˆ’1 f x =0 I[b][c][x + f x ][y + f y ] Ã— W[k][c][f x ][f y ]</formula><p>and also shown in Algorithm 1 as seven levels of nested loops. The nested loops generate output feature maps (fmaps) O, which have K channels of X Ã—Y images, by processing the input fmaps I of C channels. The fmap data are processed in batches (B) to increase parallelism and data reuse. W contains the weights as K 3D stencil filters with size C Ã—F X Ã—F Y . By summarizing this computation by this loop nest, we can express computation beyond the batched CONV layers, including non-batched operations, fully-connected (FC) layers, etc., by setting some loop bounds to 1. For example, the FC layer computes a matrix vector multiplication and can be described using the same nested loops with only C, K, and B loops, while all the other loops bounds are set to 1.</p><p>DNNs also include other layer types such as pooling, normalization, and element-wise. However, CONV and FC layers dominate the computation and memory communication, so we focus on them in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Space Overview</head><p>The design parameters that have been widely studied to optimize data reuse and performance of DNNs are loop blocking <ref type="bibr" target="#b44">[45]</ref> and dataflow <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>. However, when exploring such software scheduling decisions, prior designs often used different hardware resource allocations. We therefore consider a corresponding three-dimensional design space for DNN accelerators, as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Dataflow: DNN accelerators often exploit parallelism to improve performance by using multiple processing elements (PEs) simultaneously. Essentially it executes one or more loops in Algorithm 1 in parallel through spatial loop unrolling. The data access and communication patterns across the multiple PEs are determined by the dataflow scheme <ref type="bibr" target="#b7">[8]</ref>. Typically, the dataflow is carefully orchestrated so that data accesses to more expensive memories, including the storage in other PEs and the large shared buffers, can be minimized. We will provide a comprehensive dataflow taxonomy in Section 3.2. For now we use stationary characteristics from <ref type="bibr" target="#b7">[8]</ref> as the dataflow labels, such as weight stationary (WS), row stationary (RS), etc., to represent the choices in the dataflow dimension in Figure <ref type="figure" target="#fig_0">1</ref>. Here we assume dataflow choice is purely a subset of mapping space, which is orthogonal to the underlying architecture. Later we will provide more explanations about this assumption.</p><p>Resource Allocation: Hardware resource allocations, such as the dimensions of the PE array and the size of each level in the memory hierarchy, are also essential to the performance and efficiency of the accelerator. They determine the computation throughput, the location of the data, and the energy cost and latency for each memory access. For example, since the energy cost and latency of each data access grow with memory size, an efficient design needs to carefully size each memory level to optimally balance buffering sufficient data for high locality while being as small as possible to minimize fetch energy. In summary, the resource allocation axis needs to cover various hardware choices. Here in Figure <ref type="figure" target="#fig_0">1</ref> we mainly consider the number of MAC units N , and the memory size S i at each level i, represented as a vector (N , S 1 , S 2 , . . .).</p><p>Loop Blocking: Assuming a multi-level memory hierarchy (e.g., register files, on-chip SRAM, and off-chip DRAM), we want to schedule the computation to maximize the data reuse in the near, smaller memories to lower overall energy cost. Since all the data fetched -inputs, outputs, and weights -can be potentially reused, an optimal schedule must choose the best data reuse opportunities. The techniques of loop blocking and reordering, which together we refer to as loop blocking <ref type="bibr" target="#b44">[45]</ref>, transforms the seven nested loops in Algorithm 1 into a much larger number of loops, and generates different data access/reuse patterns for each memory level.</p><p>Using these three factors enables us to create a design space that is comprehensive and systematic, enabling us to project existing DNN accelerators as shown in Figure <ref type="figure" target="#fig_0">1</ref>. We do not explicitly show the loop blocking schemes for each architecture in this figure, since most prior work did not report their loop blocking strategy, or simply exhaustively searched for an ad-hoc optimized scheme. This figure clearly shows the wide design space current accelerators occupy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Formal Dataflow Taxonomy</head><p>Since blocking is by definition operations on loop nests, we next show how the dataflow of an accelerator can also be represented by loop operations. Noticing the connection between dataflow and spatial loop unrolling, we represent the dataflow of an accelerator through the mapping of particular loops to the parallel computation structures, similar to the terminology for the spatial partitioning in Timeloop <ref type="bibr" target="#b29">[30]</ref>. In other words, the data communication pattern is determined by which loops are spatially unrolled in hardware, and which are not. For example, if the X and Y loops are unrolled onto the 2D array, then each PE produces a single output pixel. This output stationary pattern implies that input pixels will be reused across neighbor PEs as they contribute to multiple output pixels in a convolution, and the filter weights shared by all output pixels must be transferred to all PEs. If we instead unroll the F X and F Y loops, we obtain a weight stationary pattern, where the weights stay and are reused within the same PEs, but the inputs and outputs are spatially broadcast or accumulated.</p><p>To concisely represent dataflows using spatial loop unrolling schemes on 2D PE arrays, we use the syntax U | V , where U and V denote the loops unrolled across the vertical and horizontal dimensions, respectively. Given L-level (excluding those with loop bounds as 1) nested loops in the algorithm and d spatial dimensions in the accelerator, there are L d possible dataflow choices. For a 2D array and the unblocked algorithm, the number of dataflow types is 7  2 = 21 for a CONV layer, and 3  2 = 3 for a fully-connected layer. However, the above considered dataflows, which unroll a single loop at each spatial dimension, can potentially result Table <ref type="table">1</ref>. Common dataflows from <ref type="bibr" target="#b7">[8]</ref> expressed using spatially unrolled loops.</p><formula xml:id="formula_2">C1 C2 C3 under-utilized (a) No replication. C1 C2 C3 C1 C2 C3 C1 C2 C3 C1 C2 C3 C1 C2 C3 X1 X2 X3 X4 X5 (b) With replication.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataflow Representation</head><p>Output stationary</p><formula xml:id="formula_3">X | Y Weight stationary F X | F Y Row stationary F Y | Y Weight stationary C | K</formula><p>in under-utilizating computation resources. For instance, as illustrated in Figure <ref type="figure" target="#fig_1">2a</ref>, when unrolling a loop C with size of 3 on the vertical dimension of a 16Ã—16 PE array, only 3 of the 16 rows of PEs are utilized, leaving the remaining PEs idle. To overcome this issue, in addition to unrolling loop C, another loop such as X is also unrolled by a factor of 5, as shown in Figure <ref type="figure" target="#fig_1">2b</ref>, improving the utilization ratio from 3/16 to 15/16. Therefore, it is of great importance to support unrolling multiple loops onto one spatial dimension. This improvement is called replication <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, which processes multiple small loops in parallel to increase resource utilization. Our loopbased taxonomy can also nicely and consistently express it using U | VW or UW | V , depending on the replicated dimension. When replication is supported, the number of dataflow choices for a layer further increases. Note that with replication (i.e. mapping multiple loops onto the same physical dimension), the data communication pattern is no longer uniform: intra-loop data can be communicated among nearest-neighbor PEs, while inter-loop data have to be sent multiple hops away with higher communication cost. Syntactically, we represent this by ordering the loops mapped to the same dimension, where the PEs generated by unrolled loops to the left have shorter communication distances than the loops on the right. Figure <ref type="figure" target="#fig_2">3</ref> shows an example of unrolling both C and K loops onto a 1D array. The eight PEs have been divided into two groups, each working on a output channel K i . Within each group, different PEs process different input channels C i . The outputs are only communicated among the nearest PEs, while the inputs have to transfer from one group to the other with a cost four times that of nearest neighbor communication. As was mentioned, Eyeriss V2 <ref type="bibr" target="#b6">[7]</ref>, enables flexible replication by providing flexible interconnections between the processing elements. Fortunately, the interconnects provided by some hardware targets, including FPGAs and CGRAs, naturally provide this flexibility.</p><p>Advantages: The loop-based approach builds upon the ideas of stationary characteristics <ref type="bibr" target="#b7">[8]</ref>, and provides a more precise definition of each dataflow while expanding the range of flows that can be described. Table <ref type="table">1</ref> shows several common dataflows in prior work expressed using our taxonomy. As an example, C | K is a widely adopted dataflow (Figure <ref type="figure" target="#fig_0">1</ref>, and <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>) due to its flexibility to also map matrix multiplications in MLPs and LSTMs. Even though C | K also keeps the weight stationary in PEs, its data reuse pattern is quite different from the weight-stationary dataflow F X | F Y introduced by <ref type="bibr" target="#b7">[8]</ref>. Furthermore, more complicated dataflows, such as a hybrid weight and output stationary pattern, are easy to represent in our taxonomy, e.g., C | KX , demonstrating its completeness.</p><p>Using the loop-based dataflow taxonomy, dataflows and loop blocking schemes can now both be expressed as transformations of the seven nested loops in Algorithm 1. There are many existing approaches to find loop transformations that optimize some cost functions, in order to either realize optimal software implementations on CPUs or GPUs, or generate optimal hardware designs using FPGAs or ASICs. These include general approaches like Polyhedral analysis <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b50">51]</ref> and studies specific to DNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>In the next section, we show that these loop transformations can be expressed in Halide's scheduling language, and we use this language to specify the schedules and hardware resources (both memory and compute units) of any specific DNN accelerator design. Although Halide and polyhedral models both support affine loop transformations, we choose Halide over creating a new system with a polyhedral model, in order to leverage HalideÃ¢Ä‚Å¹s compact scheduling primitives and mature compilation framework. This allows us to separate the hard optimization problem -finding the right schedule, from the mechanical transformation and manual effort to implement it.</p><p>Halide <ref type="bibr" target="#b33">[34]</ref> is a domain-specific language (DSL), originally designed for image processing but generally applicable to dense loop-structured computation including linear algebra and DNNs. The key idea in Halide is to split the computation to be performed (the algorithm) from the order in which it is done (the schedule). To allow the user to express the implementation order, Halide provides a compact and elegant language to represent loop transformations. These transformations along with commands that create intermediate storage are sufficient to completely specify a DNN accelerator, and the blocking of the algorithm which most efficiently utilizes it.</p><p>Halide algorithm: Halide represents the computation algorithm in pure functional form. The following example shows the Halide algorithm for a CONV layer with 3Ã—5Ã—5 filter size.</p><p>1 // To perform a 5 x 5 convolution with 3 channels 2 // RDom(xMin, xExt, yMin, yExt, kMin, kExt) 3 RDom r(-2, 5, -2, 5, 0, 3); 4 output(x, y, k) += input(x + r.x, y + r.y, r.z)</p><formula xml:id="formula_4">5 * w(r.x + 2, r.y + 2, r.z, k);</formula><p>The RDom keyword defines a multi-dimensional reduction domain, over which an iterative computation such as a summation is performed. A RDom is defined by the minimum position and extent in each of its dimensions. In the CONV layer example, the RDom covers the width and height of the filters and the number of input fmaps, over which the accumulation will iterate.</p><p>While the algorithm defines the functionality of the computation, it does not specify the ordering of parallel operations or data accesses. These are further controlled using Halide schedules, which consist of scheduling primitives applied to various stages of the algorithm. These primitives can express the required loop blocking, resource allocation, and, with minor extensions, dataflow choices as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Halide Schedules</head><p>With only small extensions to the current Halide scheduling language, we can explore all hardware architectures and software scheduling choices in the design space introduced in Section 3. Table <ref type="table" target="#tab_0">2</ref> summarizes how the scheduling primitives control each of the three design space dimensions. Listing 1 shows an example Halide schedule for the above Halide algorithm of a CONV layer.</p><p>The algorithm and schedule provide a user-facing language to construct hardware. Figure <ref type="figure">4</ref> pictorially shows this example schedule in details. From left to right, we iteratively apply three sets of scheduling primitives to achieve the final accelerator structure. Listing 2 presents the intermediate representation (IR) generated by Halide, corresponding to  Listing 2. Intermediate representation generated by Halide after using in and compute_at combined with split and reorder, corresponding to the third phase after step (2) in Figure <ref type="figure">4</ref>. the third phase in Figure <ref type="figure">4</ref>. The rest of this section describes the three transformations in that figure <ref type="figure">.</ref> Loop blocking: The existing Halide scheduling primitives are primarily designed for loop transformations on general-purpose processors, but the syntax and semantics also support loop blocking on accelerators thanks to the same underlying principles. Lines 2-4 of Listing 1 use split to break the x and y loops into two levels, where the inner loops Step ( <ref type="formula">1</ref>)</p><p>Step ( <ref type="formula">2</ref>)</p><p>Step (3)</p><p>Figure <ref type="figure">4</ref>. The initial design fetched the data as one large block from memory. After the split and reorder, the data is broken into 4 smaller tiles. Next a local buffer for one tile is allocated, and finally a 4 PE systolic array is implemented to process the data.</p><p>have 8 iterations. split can also be applied repeatedly to create more levels. reorder interchanges the loops, setting the order of computation and data access. These two primitives can realize different loop blocking schemes, splitting the data into multiple smaller subtiles (4 tiles of 8Ã—8 in this example) that are processed in a certain order (x then y).</p><p>Step (1) in Figure <ref type="figure">4</ref> visually shows the four tiles that are created due to the new loops (lines 3-4 and 13-14 in Listing 2).</p><p>Resource allocation of memory hierarchy: After splitting the data, we need to allocate SRAM buffer resources so that each data subtile can be cached on-chip while being processed to reduce their access cost. The existing primitives in and compute_at (lines 5-6 of Listing 1) together introduce additional memory levels, and specify at which loop iteration to fetch which subtiles into the buffers (Figure <ref type="figure">4</ref>). The compiler combines this information with loop sizes, and instantiates the correct number of memory levels with appropriate size and data layout for each buffer. As shown by lines 5-12 in Listing 2, by calling in and compute_at together for both input and w, two local buffers with required sizes are allocated within loop xo for input and weight data respectively. This allows us to explore different resource allocation choices. For each memory level, we use a double buffer design (Figure <ref type="figure" target="#fig_5">5</ref>), which enables overlapping computation and data fetch: during the computation of the current tile, the next tile is loaded into the alternate buffer.</p><p>Dataflow and PE array micro-architecture: While the previous two dimensions can be covered using existing Halide primitives, expressing dataflow requires extensions to support the complex on-chip data propagation patterns uniquely appearing in accelerators. First, like Pu et al. <ref type="bibr" target="#b31">[32]</ref>, we overload the existing unroll primitive to specify spatial loop unrolling onto the PE array. As discussed in Section 3.2,  given a 1D dataflow U or 2D dataflow U | V , we spatially unroll the loops U and V on each physical dimension, respectively. For example, in Figure <ref type="figure">4</ref> step (3), the loop xi is unrolled to execute in parallel on 4 systolic PEs.</p><p>Second, we support various types of PE array micro-architectures. We introduce a new primitive, systolic, which realizes a systolic PE array as shown in Figure <ref type="figure" target="#fig_5">5a</ref>, and allows direct inter-PE data communication without always fetching data from higher-level data buffers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>. Combined with different unroll primitives, we can realize different dataflows on the PE array. Figure <ref type="figure" target="#fig_7">6a</ref> maps the F Y | Y dataflow used in Eyeriss <ref type="bibr" target="#b7">[8]</ref>, by unrolling the F Y and Y loops. It transfers multiple rows of filter weights horizontally, and accumulates multiple columns of output fmaps vertically. Alternatively, Figure <ref type="figure" target="#fig_7">6b</ref> performs matrix multiplications using dataflow C | K, which is used by a large group of designs including Google's TPU <ref type="bibr" target="#b19">[20]</ref>.</p><p>Without applying systolic, the PEs are by default organized into reduction tree structures <ref type="bibr" target="#b5">[6]</ref>, as shown in Figure <ref type="figure" target="#fig_5">5b</ref>. Figure <ref type="figure" target="#fig_7">6c</ref> provides one example dataflow on a 1D reduction tree, which unrolls the loop C to multiply input pixels from different input fmaps with their corresponding weights, and accumulates the products into a single output pixel in an output fmap.</p><p>Using the two micro-architectures in Figure <ref type="figure" target="#fig_5">5</ref> as building blocks, we can generate a variety of accelerator designs by composition. They can be described by applying unroll at different loop levels, and calling systolic at the corresponding unrolled loops. For instance, we can have a reduction tree of PEs acting as a node in a systolic array, or multi-level reduction trees, effectively supporting a wide range of designs including ARM ML processor <ref type="bibr" target="#b0">[1]</ref> and NVDLA <ref type="bibr" target="#b1">[2]</ref>. We could additionally introduce new primitives similar to systolic, to support other PE array micro-architectures if desired.  Accelerator scope: Finally, we introduce an additional primitive, accelerate, which defines the scope of the hardware accelerator and the interface to the rest of the system, in a similar manner to Pu et al. <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>Given this concise and concrete expression for DNN accelerators using Halide schedules, we extended the Halide compiler to generate hardware from these descriptions. As a result, different DNN accelerator designs and mapping schemes can be realized by simply changing the Halide schedule associated with the same Halide algorithm. We can also use it to easily recreate previously proposed designs for a fair comparison (see <ref type="bibr">Section 6)</ref>.</p><p>Our implementation of the Halide toolchain is built on top of Pu's work <ref type="bibr" target="#b31">[32]</ref>, which was designed for generating hardware for image processing pipelines. To create our system, we needed to extend this work in three important ways. The first was to add support for systolic arrays, which was challenging due to the diversity of the PE connectivity and the complexity of creating a state machine for the control logic inside each PE. Next, hardware virtualization was added to map different layers in a DNN (represented as different Halide functions) onto the same hardware module. Otherwise, naÃ¯vely instantiating spatially separated modules for each layer in large DNNs would consume unrealistically large silicon area. Finally, we extended Pu's work to generate ASIC as well as FPGA implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methodology</head><p>Our Halide-based accelerator design flow in Section 4 supports both FPGA and ASIC backends. The FPGA results enabled us to validate that the system was functional and produced efficient designs: it achieved similar GOPs and DSP utilization when compared against manually optimized designs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>ASIC Hardware synthesis toolchain: Our extended Halide compiler generates C++ code specialized for Catapult High-Level Synthesis, which is then compiled to RTL designs in Verilog. We synthesize the RTL designs in a 28 nm technology using Synopsys Design Compiler. Standard cells and memory models from commercial vendors are used for power, performance, and area analysis. We use 16-bit arithmetic for inference tasks throughout this paper. All of our ASIC designs achieve 400 MHz frequency with no timing violations. For power analysis, the appropriate switching activities are set on all the primary ports and propagated through the design using the design tools.</p><p>Analysis framework: To allow for rapid design exploration, we also developed an analytical model to estimate the performance and energy efficiency of the ASIC DNN accelerators. We use CACTI 6.5 <ref type="bibr" target="#b28">[29]</ref> to model SRAM arrays and tune its parameters to match our 28 nm commercial memory library. For small arrays and register files (RFs), we use the Cadence XtensaProcessor Generator <ref type="bibr" target="#b2">[3]</ref> to extract energy numbers based on our standard cell library. Table <ref type="table" target="#tab_2">3</ref> shows the energy cost of accessing memories with different sizes. Note that our energy ratios between memories and MAC are larger than those reported in Eyeriss <ref type="bibr" target="#b7">[8]</ref>. There are several reasons: we use a 28 nm technology instead of 65 nm; our memory is highly banked with higher energy cost; and our MAC units consume lower energy as their activity factors are relatively low with data stationary patterns. Nevertheless, our analytical framework works with different technology processes, and it is easy to supply new cost models to study   To compute the overall memory energy in an L-level hierarchy, we adopt a model similar to <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_5">E = L i=1 #acc i Ã— e i where #acc i = L j=i RT j</formula><p>Here e i is the energy of accessing the ith level once. The total numbers of accesses are affected by data reuses RT i at different memory levels. It is defined as the number of times the data are accessed by its immediate lower-cost (child) level during its lifetime in this level. To support direct inter-PE communication in systolic arrays, we treat neighbor PEs as an additional level in the hierarchy. We distinguish the cost for different communication distances (Figure <ref type="figure" target="#fig_2">3</ref>) which is an improvement over <ref type="bibr" target="#b7">[8]</ref>.</p><p>Since all designs today are power constrained, finding the optimal accelerator design now becomes an optimization problem of minimizing E over the 3D design space, similar to Yang's work <ref type="bibr" target="#b43">[44]</ref>. e i is determined by the resource  <ref type="table" target="#tab_2">3</ref>), and RT i can be directly calculated from the dataflow and loop blocking schemes. In this work, we simply perform a conservatively pruned search over the full design space guided by domain-specific knowledge. This analytical model is available at https://github.com/xuanyoya/ Interstellar-CNN-scheduler.</p><p>Framework validation: We have thoroughly validated the accuracy of our model by comparing its results to complete designs generated by our synthesis toolchain. Table <ref type="table" target="#tab_3">4</ref> shows three example designs we have generated in ASIC platforms, and Figure <ref type="figure" target="#fig_9">7</ref> shows the energy comparison between our analytic model and post-synthesis results. The resulting errors are less than 2%. Furthermore, our framework is also able to reproduce the results from <ref type="bibr" target="#b7">[8]</ref> with small differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Using our dataflow taxonomy and the ability to rapidly generate and evaluate large numbers of accelerator designs with Halide, we first explore different dataflow and loop blocking choices, and then consider hardware resource optimizations. At the end we leverage the characteristics of these results to introduce an efficient optimizer for DNN accelerators.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Impact of Dataflow and Loop Blocking</head><p>Figure <ref type="figure" target="#fig_11">8</ref> compares the energy efficiency of different dataflow choices. We use the CONV3 layer in AlexNet and 1Ã—1 reduction layer 4C3R in GoogLeNet Inception (4c) module as examples. The other layers have also been investigated, and share a similar trend. More DNNs will be studied in Section 6.3. We use the optimal loop blocking scheme for each dataflow. We can see that when optimized loop blocking schemes are applied, many different dataflows achieve similar and close-to-optimal energy efficiency on the same hardware configuration. We have evaluated three different hardware configurations: the blue one is the same as Eyeriss <ref type="bibr" target="#b7">[8]</ref>, with 512 B register file (RF), 128 KB SRAM buffer, and 16Ã—16 PE array per PE; the red one uses a different array bus design, which disables inter-PE communication and broadcasts all data from the global buffer; the green one uses a smaller 64 B RF to lower its access energy (see Section 6.2). Figure <ref type="figure" target="#fig_11">8b</ref> and 8d also show the cases with batch size 1, which is most commonly used in mobile systems; the conclusion is consistent across different batch sizes. The small influence of dataflow remains true over a wide set of experiments including different layer types, different PE array structures and sizes, different memory configurations, and different energy cost models.</p><p>On the other hand, in Figure <ref type="figure" target="#fig_12">9</ref> we observe that the PE array utilization (active PE ratio for each run), and therefore the computation throughput, is more sensitive to different dataflow choices than the energy efficiency for some convolution layers. Without replication (Figure <ref type="figure" target="#fig_12">9a</ref>), the overall utilization can vary significantly and stay low for many dataflow choices. However, using proper replication substantially improves the utilization and eliminates most of the differences among all dataflows (Figure <ref type="figure" target="#fig_12">9b and 9c</ref>). These results also imply that accelerators that support a diversity of replication schemes, such as CGRAs and Eyeriss V2 <ref type="bibr" target="#b6">[7]</ref>, will generally achieve higher overall utilization compared to the ones with fixed interconnects. Here, the impact of the interconnect bandwidth to evaluate the overall performance is not included in the analysis, as prior works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref> have already studied such impact. Figure <ref type="figure" target="#fig_12">9b</ref> also demonstrates, for the CONV3 layer in AlexNet, that the C | K dataflow achieves 20% higher utilization than the others such as F Y | Y . This is because the channel dimensions C and K are typically the largest in most CONV and FC layers, so it is easier to unroll them onto a fixed-sized PE array with small fragmentation. For this reason, we will use the C | K dataflow in the rest of this paper. Not all the computational layers have as much data sharing to exploit. Weight sharing in FC layers only comes from batching, and some applications limit the batch size to be small, even one. Interestingly, even in these computations the dataflow does not have a large influence on performance or energy. For computations with limited reuse, the data must come from the off-chip DRAM, or the last level on-die storage, if it is large enough. The storage properties at this level will limit the device's energy and performance, so for this class of application, the design of the computation units is less important.</p><p>Instead of dataflow choices, Figure <ref type="figure" target="#fig_13">10</ref> shows the design space of loop blocking for AlexNet CONV3, using a 512 B RF, corresponding to the blue configuration in Figure <ref type="figure" target="#fig_11">8a</ref>. The energy variance of different blocking schemes is much more significant than that of dataflow, and only 30% of the schemes fall within 1.25Ã— of the minimum energy. This indicates that loop blocking has a large impact on energy efficiency.</p><p>Observation 1: With the same hardware resources, many different dataflows are able to achieve similar and close-tooptimal energy efficiency, as long as proper loop blocking and replication are used.</p><p>In hindsight, this result is not surprising. When the DNNs exhibit enormous data reuse opportunities, as long as high data reuse is achieved through proper loop blocking schemes, the resulting energy efficiency should be good. When the reuse is limited, the performance is limited by the bandwidth of the last level in the memory hierarchy instead of the PE array. These two situations are further illustrated in Figure <ref type="figure" target="#fig_14">11</ref>, where the left bars with 512 B RF show the energy breakdown of the optimal dataflow for the blue configuration in Figure <ref type="figure" target="#fig_11">8a</ref>. For CONV layers with high reuse, most energy is consumed in the RF level rather than the array buses or intermediate buffers. By optimally blocking the computation, nearly all accesses (98%) occur at the RF level, making it the dominant energy component. For FC layers with limited reuse, most of the DRAM energy is inevitable, since the data have to be fetched at least once from off-chip (compulsory misses). On the other hand, the on-chip communication is  generally only a small portion of the total energy, and therefore blocking choice has a more substantial impact on the overall energy efficiency than dataflow choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Impact of Hardware Resource Allocation</head><p>Another interesting result from Figure <ref type="figure" target="#fig_14">11</ref> is that the total energy is always dominated by the RF level with a 512 B RF. This result indicates that this resource allocation may be suboptimal. Figure <ref type="figure" target="#fig_15">12</ref> shows the impact of memory resource allocation on energy efficiency. The energy is accumulated across all layers (including FC layers) in AlexNet, and contains both computation and memory access portions. It indicates that using a smaller RF size such as 32 or 64 B can improve the total energy efficiency by up to 2.6Ã—. If we also increase the global SRAM buffer size, the energy efficiency can further improve. However, when SRAM buffer size grows beyond 256 KB, the benefit becomes negligible. Given the significant area cost, it is not always necessary to use large global buffers. The right bars in Figure <ref type="figure" target="#fig_14">11</ref>, which give the energy breakdown of using a 64 B RF, illustrate that the energy decreases dramatically for all the CONV layers due to the much lower energy cost per access of the smaller RF. At the same time, more accesses go to the inter-PE array level and the global buffer, since the smaller RF captures less data reuse inside each PE. But reducing the RF size has almost no impact on the DRAM energy, as the data are still efficiently reused in the global buffer. Overall, a smaller RF achieves significantly better energy efficiency, with a more balanced energy breakdown among different memory hierarchy levels.</p><p>Observation 2: The total energy of an efficient system should not be dominated by any individual level in the memory hierarchy.</p><p>Observation 2 also explains why some output-stationary and weight-stationary designs do not perform well, as discussed by <ref type="bibr" target="#b7">[8]</ref>. Those designs cannot capture sufficient reuse at the RF level, and result in high energy consumption at the DRAM level, which dominates the overall energy.</p><p>However, there is an exception for Observation 2. When DRAM dominates the total energy but the number of DRAM accesses is already minimized (fetching the input once and writing back output once), the DNN is memory bound, and based on Amdahl's law, little further optimization can be achieved for the memory hierarchy. This is the case particularly for a batch size of 1, and MLPs and LSTMs that contain many FC layers.</p><p>We also investigate whether changing the hierarchy itself can further improve the energy efficiency. Due to the dominant role of the RF level, we add another level of private register file. The largest energy efficiency improvement is obtained when sizing each memory level is based on the rule that the ratio of the on-chip storage sizes between the adjacent levels should be around 4 to 16. Specifically, the overall energy efficiency of AlexNet is improved by 25%, by choosing 16 B and 256 B to be the two level register file sizes, and 256 KB to be the global buffer size.</p><p>Figure <ref type="figure" target="#fig_16">13</ref> depicts the optimal memory resource allocation and the corresponding total energy for AlexNet when varying PE array size. We use only one level of RF here. These correspond to the optimal points on the optimizing plane shown in Figure <ref type="figure" target="#fig_0">1</ref>. With increasing numbers of PEs, the optimal memory size at each level grows sub-linearly. Ideally we would like to keep the same amount of data reuse with constant storage capacity for each PE, which would lead to linearly increased memory size. However, the access cost of each memory level grows with its size (Table <ref type="table" target="#tab_2">3</ref>), which slows down the optimal capacity scaling to sub-linear. Between the RF and the SRAM buffer, the data reuse in RF is more critical. So the RF level has a stronger trend to keep constant capacity per PE. But it is eventually bounded by the size of the next-level, i.e., the SRAM buffer.</p><p>Also we notice that the total energy reduces slightly with the increasing number of PEs. This indicates that larger arrays will have a significant effect on throughput and a small change in energy efficiency. The energy improvement is achieved by buffering more data on chip for reuse, and since most communication is nearest neighbor, the larger die does not increase communication costs significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">An Efficient Optimizer</head><p>With the large number of hardware and software choices for DNN accelerators, exhaustive search for the optimal designs is usually infeasible. Instead, using the observations above, we can speed up the optimization process by pruning the search space and evaluating only a small number of candidates using the framework from Section 5.</p><p>We developed an auto-optimizer that efficiently finds energy efficient accelerator designs for given DNNs. The optimizer takes as input the DNN topology, the energy cost model, and various constraints such as the total chip area. First, according to Observation 1, we fix the dataflow to be C | K, and only search the design points on the optimizing plane in Figure <ref type="figure" target="#fig_0">1</ref>. Next, we only evaluate a subset of hardware configurations with the optimal size of each memory level satisfying Observation 2, leveraging the rule that the ratio of the on-chip storage sizes and the adjacent levels should be around 4 to 16. It outputs an optimized design with corresponding Halide schedule primitives, which can then be fed into our hardware synthesis toolchain.</p><p>We use four CNNs, three LSTMs, and two MLPs as benchmarks to demonstrate the effectiveness of our efficient optimizer. All DNNs evaluated use 16-bit precision. The CNNs are AlexNet, VGG-16 <ref type="bibr" target="#b37">[38]</ref>, MobileNet <ref type="bibr" target="#b18">[19]</ref>, and GoogleNet <ref type="bibr" target="#b41">[42]</ref> with batch size 16. The LSTM-M and LSTM-L are proposed by Google for sequence-to-sequence learning <ref type="bibr" target="#b40">[41]</ref> with embedding sizes 500 and 1000. We also study the Recurrent Highway Network (RHN) <ref type="bibr" target="#b49">[50]</ref>. The MLPs are from <ref type="bibr" target="#b10">[11]</ref> with batch size 128. We use two baselines, both using dataflow C | K, which are the two left columns in Figure <ref type="figure" target="#fig_17">14</ref>. The smaller chip uses a memory hierarchy similar to Eyeriss <ref type="bibr" target="#b7">[8]</ref>, and 16Ã—16 PE array, whose area and power budgets are suitable for mobile platforms. The larger chip uses 128Ã—128 PE array with a 8 B register per PE, 64 KB for the first-level global buffer, and a 28 MB second-level global buffer, similar to cloud-based accelerators such as TPU <ref type="bibr" target="#b19">[20]</ref>. Figure <ref type="figure" target="#fig_17">14</ref> demonstrates the energy efficiency gain achieved by the efficient optimizer. We can improve the energy efficiency by up to 3.5Ã—, 2.7Ã—, and 4.2Ã— for VGG-16, GoogleNet and MobileNet, up to 1.6Ã— for LSTMs, and up to 1.8Ã— for MLPs. The optimal memory hierarchy uses 16 B and 128 B for the first-level and second-level register files, with a 256 KB global SRAM double buffer. This hardware configuration is shared by all the layers in the DNNs. The overall system energy consumption is not dominated by the RF level. The energy efficiency for the nine benchmarks are 1.85, 1.42, 0.87, 0.35, 0.49, 0.47, 0.5, 0.46, and 0.48 TOPs/W, respectively. Notice that even though the larger system has a smaller RF size, its energy is better than the smaller system. This is because with a much larger global SRAM buffer, it can store all the input and output data and the layer weights, and the accesses to DRAM are eliminated when switching to the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>To help elucidate factors that matter for DNN accelerators, we realized that Halide's scheduling language was almost rich enough to describe the design space of all possible accelerators. By extending the scheduling language to include local communication, and creating a hardware backend, we were able to generate and hence fairly compare all proposed dense DNN accelerators. The results show that as long as the data reuse and the resource utilization are maximized by proper loop blocking and mapping replication, many hardware dataflow choices will be near optimal. Not surprisingly, optimizing the memory hierarchy had a large influence on energy efficiency, with smaller local registerfiles and deeper memory hierarchies providing the best performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. 3D design space for DNN accelerators. The positions of labels or vectors on each axis only represent different choices without specific information about ordering or distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Computation resource utilization can be improved by replication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Loops C and K are unrolled onto a 1D array with dataflow CK. Outputs are communicated between adjacent PEs, while inputs are communicated across groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7 / 9 /</head><label>79</label><figDesc>/ Copy input to buffer. 8 ibuf[...] = input[...] / Allocate local buffer for w. 10 alloc wbuf[5, 5, 3, 1] 11 // Copy w to buffer. 12 wbuf[...] = w[...] 13 for (yi, 0, 8) 14 for (xi, 0, 8) 15 for (r.z, 0, 3) 16 for (r.y, -2, 5) 17 for (r.x, -2, 5) 18 output(xi, yi, 0) += 19 ibuf(xi + r.x, yi + r.y, r.z) 20 * wbuf(r.x + 2, r.y + 2, r.z, 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. DNN accelerator architecture consisting of a PE array and a memory hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>A systolic dataflow F Y | Y , generated by unrolling the output fmap height Y and filter weight height F Y . A systolic dataflow C | K , generated by unrolling the input and output fmap dimensions C and K . A reduction tree of dataflow C, generated by unrolling the input fmap dimension C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Different PE array micro-architectures generated from Halide scheduling primitives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Energy breakdown comparison between actual synthesized designs and the analytical model. E y e r is s M o d e l CONV1 E y e r is s M o d e l CONV2 E y e r is s M o d e l CONV3 E y e r is s M o d e l CONV4 E y e r is s M o d Energy breakdown comparison between reported Eyeriss model and our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Validation of the analytical model against post-synthesis results and previous published model [8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Batch 1 (GoogleNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Design space of dataflow for AlexNet CONV3 and GoogLeNet 4C3R layers. The Y-axis is the energy consumed to execute the entire batch. Different dataflows are shown horizontally, with only the most common choices labeled for clarity. All dataflows use replication and the optimal loop blocking schemes. Different colors represent different hardware resource allocations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. PE array utilization for the energy-optimal dataflow choices on AlexNet CONV3 layer with and without replication, and GoogleNet 4C3R layer with replication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Design space of loop blocking for AlexNet CONV3 using dataflow C | K with 512 B RF per PE.</figDesc><graphic url="image-1.png" coords="11,60.62,72.00,226.80,127.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Energy breakdown comparison between 512 B and 64 B RF sizes with the same dataflow. Using a 64 B RF reduces the overall energy significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Memory hierarchy exploration with dataflow C | K. Different RF sizes per PE are shown horizontally. Lines with different colors correspond to different SRAM buffer sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 .</head><label>13</label><figDesc>Figure<ref type="bibr" target="#b12">13</ref>. The optimal memory resource allocation and the corresponding total energy when varying PE array size.</figDesc><graphic url="image-2.png" coords="12,60.62,72.00,226.80,128.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Overall energy efficiency improvement by using the auto-optimizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Halide scheduling primitives that control each dimension of the 3D design space. Listing 1. An example Halide schedule for the CONV layer algorithm.</figDesc><table><row><cell></cell><cell>Dimensions</cell><cell>Scheduling primitives</cell></row><row><cell></cell><cell>Loop blocking</cell><cell>split, reorder</cell></row><row><cell></cell><cell>Resource allocation</cell><cell>in, compute_at</cell></row><row><cell></cell><cell>Dataflow</cell><cell>unroll, systolic</cell></row><row><cell></cell><cell>Overall scope</cell><cell>accelerate</cell></row><row><cell cols="2">1 Var xo, yo, xi, yi;</cell></row><row><cell cols="3">2 output.update().split(x, xo, xi, 8)</cell></row><row><cell>3</cell><cell cols="2">.split(y, yo, yi, 8)</cell></row><row><cell>4</cell><cell cols="2">.reorder(xi, yi, xo, yo);</cell></row><row><cell cols="3">5 input.in().compute_at(output, xo);</cell></row><row><cell cols="3">6 w.in().compute_at(output, xo);</cell></row><row><cell cols="3">7 output.accelerate({input, w});</cell></row><row><cell cols="3">8 output.update().unroll(xi, 4);</cell></row><row><cell cols="3">9 output.update().systolic({xi});</cell></row></table><note>1 // To generate output of size 16 x 16 x 64 2 for (k, 0, 64) 3 for (yo, 0, 2) 4 for (xo, 0, 2) 5 // Allocate local buffer for input.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Energy per 16-bit access with various register file (RF) and SRAM sizes, and for a MAC operation, one hop communication cost and a DRAM access.</figDesc><table><row><cell cols="2">RF Size Energy (pJ)</cell><cell></cell><cell></cell></row><row><cell>16 B</cell><cell>0.03</cell><cell cols="2">SRAM Size Energy (pJ)</cell></row><row><cell>32 B</cell><cell>0.06</cell><cell>32 KB</cell><cell>6</cell></row><row><cell>64 B</cell><cell>0.12</cell><cell>64 KB</cell><cell>9</cell></row><row><cell>128 B</cell><cell>0.24</cell><cell>128 KB</cell><cell>13.5</cell></row><row><cell>256 B</cell><cell>0.48</cell><cell>256 KB</cell><cell>20.25</cell></row><row><cell>512 B</cell><cell>0.96</cell><cell>512 KB</cell><cell>30.375</cell></row><row><cell>MAC</cell><cell>0.075</cell><cell>DRAM</cell><cell>200</cell></row><row><cell>Hop</cell><cell>0.035</cell><cell></cell><cell></cell></row><row><cell cols="4">more advanced technologies. Also, many of our observations</cell></row><row><cell cols="3">in Section 6 are technology-independent.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>ASIC designs for model validation.</figDesc><table><row><cell cols="4">Name Dataflow PE Array RF SRAM</cell></row><row><cell>OS4</cell><cell>X</cell><cell>1D, 4</cell><cell>32 B 32 KB</cell></row><row><cell>OS8</cell><cell>X</cell><cell>1D, 8</cell><cell>64 B 64 KB</cell></row><row><cell>WS16</cell><cell>C | K</cell><cell cols="2">2D, 4Ã—4 64 B 32 KB</cell></row><row><cell>allocation (Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>The authors want to thank Kayvon Fatahalian, Jonathan Ragan-Kelley, Andrew Adams and Stephen Richardson for the insightful discussion, Keyi Zhang for his help on LaTex formatting, and the anonymous reviewers for their valuable comments. This work was supported by the DSSoC DARPA grant, the Stanford AHA Agile Hardware Center and Affiliates Program, the Stanford SystemX Alliance, the Stanford Platform Lab, and SRC Center for Research on Intelligent Storage and Processing-in-memory (CRISP).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://developer.arm.com/products/processors/machine-learning/arm-ml-processor/" />
		<title level="m">ARM ML Processor</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://nvdla.org/" />
		<title level="m">NVDLA</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://ip.cadence.com/ipportfolio/tensilica-ip" />
		<title level="m">Tensilica customizable processor IP</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cnvlutin: Ineffectualneuron-free deep neural network computing</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayler</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><forename type="middle">Enright</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fusedlayer CNN accelerators</title>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DianNao: A small-footprint highthroughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="308" />
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference (ISSCC)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="262" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DaDianNao: A machine-learning supercomputer</title>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Annual ACM/IEEE International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PRIME: A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongpan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ShiDianNao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">42nd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neuflow: A runtime reconfigurable dataflow processor for vision</title>
		<author>
			<persName><forename type="first">ClÃ©ment</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berin</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Corda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Akselrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
	<note>Eugenio Culurciello, and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TETRIS: Scalable and efficient neural network acceleration with 3D memory</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tangram: Optimized coarse-grained dataflow for scalable nn accelerators</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;19</title>
				<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="807" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A 240 G-ops/s mobile coprocessor for deep neural networks</title>
		<author>
			<persName><forename type="first">Jonghoon</forename><surname>Vinayak Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aysegul</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berin</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="696" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">EIE: Efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>CoRR, abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Luc Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)</title>
				<editor>
			<persName><forename type="first">Amir</forename><surname>Salek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emad</forename><surname>Samadiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chris</forename><surname>Severn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gregory</forename><surname>Sizikov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Snelham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jed</forename><surname>Souter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Steinberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andy</forename><surname>Swing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mercedes</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gregory</forename><surname>Thorson</surname></persName>
		</editor>
		<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA)<address><addrLine>Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross; Bo Tian, Horia Toma, Erick Tuttle,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">Jun 2017</date>
		</imprint>
	</monogr>
	<note>datacenter performance analysis of a tensor processing unit</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neurocube: A programmable digital neuromorphic architecture with high-density 3D memory</title>
		<author>
			<persName><forename type="first">Duckhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeha</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sek</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudhakar</forename><surname>Yalamanchili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="380" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Taesik Na, and Saibal Mukhopadhyay. Design of an energy-efficient accelerator for training of convolutional neural networks using frequency-domain computation</title>
		<author>
			<persName><forename type="first">Burhan</forename><surname>Jong Hwan Ko</surname></persName>
		</author>
		<author>
			<persName><surname>Mudassar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Design Automation Conference 2017, DAC &apos;17</title>
				<meeting>the 54th Annual Design Automation Conference 2017, DAC &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of dnn dataflow: A data-centric approach</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MAESTRO: an open-source infrastructure for modeling dataflows within deep learning accelerators</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno>CoRR, abs/1805.02566</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MAERI: Enabling flexible dataflow mapping over DNN accelerators via reconfigurable interconnects</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;18</title>
				<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="461" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A high performance FPGA-based accelerator for large-scale convolutional neural networks</title>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xitian</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuegong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 26th International Conference on Field Programmable Logic and Applications (FPL)</title>
				<imprint>
			<date type="published" when="2016-08">Aug 2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FlexFlow: A flexible dataflow accelerator architecture for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Wenyan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="553" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Polymage: Automatic optimization for image processing pipelines</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Teja Mullapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Vasista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;15</title>
				<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="429" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norm</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 40th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to dnn accelerator evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2019-03">March 2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SCNN: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Programming heterogeneous systems from an image processing DSL</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with embedded FPGA platform for convolutional neural network</title>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">FrÃ©do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIG-PLAN Conference on Programming Language Design and Implementation, PLDI &apos;13</title>
				<meeting>the 34th ACM SIG-PLAN Conference on Programming Language Design and Implementation, PLDI &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="519" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From high-level deep neural models to FPGAs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2016-10">Oct 2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overcoming resource underutilization in spatial CNN accelerators</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mechael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 26th International Conference on Field Programmable Logic and Applications (FPL)</title>
				<imprint>
			<date type="published" when="2016-08">Aug 2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Maximizing CNN accelerator efficiency through resource partitioning</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mechael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017-06">Jun 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards efficient microarchitectural design for accelerating unsupervised GANbased deep learning</title>
		<author>
			<persName><forename type="first">Mingcong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huixiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="66" to="77" />
		</imprint>
	</monogr>
	<note>IEEE International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abinash</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarma</forename><surname>Vrudhula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Sun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs</title>
		<author>
			<persName><forename type="first">Xuechao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Design Automation Conference 2017, DAC &apos;17</title>
				<meeting>the 54th Annual Design Automation Conference 2017, DAC &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A Systematic Framework to Analyze the Design Space of DNN Accelerators</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Ardavan Pedram, and Mark Horowitz. A systematic approach to blocking convolutional neural networks</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Burton Rister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Bhagdikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Kvatinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04209</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalpel: Customizing DNN pruning to the underlying hardware parallelism</title>
		<author>
			<persName><forename type="first">Jiecao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lukefahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Palframan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reetuparna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="548" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Optimizing FPGA-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;17</title>
				<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cambricon-X: An accelerator for sparse neural networks</title>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">G</forename><surname>Zilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>KoutnÃ­k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>CoRR, abs/1607.03474</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Recurrent highway networks</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving high level synthesis optimization opportunity through polyhedral transformations</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Rupnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/SIGDA International Symposium on Field Programmable Gate Arrays, FPGA &apos;13</title>
				<meeting>the ACM/SIGDA International Symposium on Field Programmable Gate Arrays, FPGA &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="9" to="18" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
