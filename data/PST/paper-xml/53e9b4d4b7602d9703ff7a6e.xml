<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0375614FBEFD7C134C6DD0B8EA1ACE7E</idno>
					<idno type="DOI">10.1109/TNNLS.2012.2237183</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Margin Multiple Kernel Learning</head><p>Xinxing Xu, Ivor W. Tsang, and Dong Xu, Member, IEEE Abstract-Multiple kernel learning (MKL) has been proposed for kernel methods by learning the optimal kernel from a set of predefined base kernels. However, the traditional L 1 MKL method often achieves worse results than the simplest method using the average of base kernels (i.e., average kernel) in some practical applications. In order to improve the effectiveness of MKL, this paper presents a novel soft margin perspective for MKL. Specifically, we introduce an additional slack variable called kernel slack variable to each quadratic constraint of MKL, which corresponds to one support vector machine model using a single base kernel. We first show that L 1 MKL can be deemed as hard margin MKL, and then we propose a novel soft margin framework for MKL. Three commonly used loss functions, including the hinge loss, the square hinge loss, and the square loss, can be readily incorporated into this framework, leading to the new soft margin MKL objective functions. Many existing MKL methods can be shown as special cases under our soft margin framework. For example, the hinge loss soft margin MKL leads to a new box constraint for kernel combination coefficients. Using different hyper-parameter values for this formulation, we can inherently bridge the method using average kernel, L 1 MKL, and the hinge loss soft margin MKL. The square hinge loss soft margin MKL unifies the family of elastic net constraint/regularizer based approaches; and the square loss soft margin MKL incorporates L 2 MKL naturally. Moreover, we also develop efficient algorithms for solving both the hinge loss and square hinge loss soft margin MKL. Comprehensive experimental studies for various MKL algorithms on several benchmark data sets and two real world applications, including video action recognition and event recognition demonstrate that our proposed algorithms can efficiently achieve an effective yet sparse solution for MKL. Index Terms-Multiple kernel learning, support vector machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>K ERNEL methods, such as support vector machine (SVM) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and kernel principal component analysis have shown to be powerful tools for numerous applications. However, their generalization performances are often decided by the choice of the kernel <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, which represents the similarity between two data points. For kernel methods, a poor kernel can lead to impaired prediction performance, thus many works <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b9">[10]</ref> have been proposed for learning the optimal kernel for kernel methods.</p><p>One of the pioneering works for kernel learning was proposed to simultaneously train the SVM classifier and learn the kernel matrix <ref type="bibr" target="#b8">[9]</ref>. However, learning the general kernel matrix is a non-trivial task. The learning problem is generally formulated as a semi-definite programming problem, which suffers from the high computational cost even for one hundred training samples. Thus, this approach can be applicable for small scale data sets only. To reduce the computational cost, Lanckriet et al . <ref type="bibr" target="#b8">[9]</ref> further assumed that the kernel is in the form of a linear combination of a set of predefined base kernels. Then the SVM classifier and the kernel combination coefficients are learned simultaneously, which is known as Multiple Kernel Learning (MKL). Since the proposed objective function has a simplex constraint for the kernel combination coefficients, it is also known as L 1 MKL.</p><p>There are two major research directions for MKL methods, in which the first one focuses on the development of efficient learning algorithms, while the second one focuses on the improvement of the generalization performance. For the first direction, Bach et al . <ref type="bibr" target="#b10">[11]</ref> employed a sequential minimization optimization (SMO) method for solving medium-scale MKL problems. Sonnenburg et al . <ref type="bibr" target="#b11">[12]</ref> applied a semi-infinite linear programming (SILP) strategy by reusing the state-ofthe-art SVM implementations for solving the subproblems inside the MKL optimization more efficiently, which makes MKL applicable to large scale data sets. The similar SILP strategy was also used by <ref type="bibr" target="#b12">[13]</ref> for multiclass MKL problem. Following <ref type="bibr" target="#b11">[12]</ref>, the sub-gradient based method <ref type="bibr" target="#b13">[14]</ref> and the level-method <ref type="bibr" target="#b14">[15]</ref> have been proposed to further improve the convergence for solving MKL problems.</p><p>Although the optimization efficiency for L 1 MKL has been improved in recent years, Cortes et al . <ref type="bibr" target="#b15">[16]</ref> and Kloft et al . <ref type="bibr" target="#b16">[17]</ref> showed that the L 1 MKL formulation from <ref type="bibr" target="#b8">[9]</ref> cannot achieve better prediction performance when compared with the simplest method using the average of base kernels (i.e., average kernel) for some real world applications. To improve the effectiveness, lots of new MKL formulations <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b31">[32]</ref> have recently been proposed.</p><p>The simplex constraint for the traditional L 1 MKL formulation usually yields a sparse solution. The recent works in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref> conjectured that such a sparsity constraint may omit some useful base kernels for the prediction. Thereafter, they introduced a L 2 -norm constraint to replace the L 1 -norm constraint in L 1 MKL, leading to a non-sparse solution for the kernel combination coefficients. The L 2 -norm constraint was further extended to the L p -norm ( p &gt;1) constraint in <ref type="bibr" target="#b16">[17]</ref>. Other MKL variants (e.g., <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b32">[33]</ref>, and <ref type="bibr" target="#b33">[34]</ref>) were proposed by removing the L 1 -norm constraint, while directly adding one regularization term based on the L 1 -norm, L 2 -norm, or L p -norm of the kernel combination coefficients to the objective function, which are indeed equivalent to the formulation as in <ref type="bibr" target="#b16">[17]</ref>. To further improve the efficiency of L p MKL, Xu et al . <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr">Kloft et al . [17]</ref> proposed an analytical way to update the kernel combination coefficients. The SMO strategy was also employed in <ref type="bibr" target="#b33">[34]</ref> to accelerate the optimization for the L p MKL problem. In <ref type="bibr" target="#b21">[22]</ref>, a L 2 -norm regularizer of the kernel combination coefficients is directly added to the objective function while keeping the simplex constraint fixed. Alternatively, Yang et al . <ref type="bibr" target="#b22">[23]</ref> used the elastic net regularizer on the kernel combination coefficients as a constraint for MKL. Note that the elastic net regularizer in the block-norm form first appeared in <ref type="bibr" target="#b10">[11]</ref> as a numerical tool for optimizing the L 1 MKL and was further discussed in <ref type="bibr" target="#b24">[25]</ref> with a variant form. Moreover, the extensions of elastic net regularizer for MKL in primal form with more general block-norm regularization were also discussed in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b26">[27]</ref>. However, it is still unclear why these regularizers can enhance the prediction performances for MKL.</p><p>To answer this question, in this paper, we first show that the traditional L 1 MKL can be deemed as hard margin MKL, which only selects the base kernels with the minimum objective and throws away other useful base kernels. Then, we propose a novel soft margin perspective for MKL problems by starting from the dual of the traditional MKL method. The proposed soft margin framework for MKL is in analogy to the well-known soft margin SVM <ref type="bibr" target="#b35">[36]</ref>, which makes SVM more robust in real applications by introducing a slack variable for each of the training data. Similarly, with the introduction of a slack variable for each of the base kernels, we propose three novel soft margin MKL formulations, namely, the hinge loss soft margin MKL, the square hinge loss soft margin MKL, and the square loss soft margin MKL by using different loss functions.</p><p>The square loss soft margin MKL formulation incorporates L 2 MKL naturally. The square hinge loss soft margin MKL connects a few MKL methods using the elastic net like regularizers or constraints. The hinge loss soft margin MKL leads to a totally new formulation, which bridges L 1 MKL and the simplest approach based on the average kernel by using the different hyper-parameter values. These three cases reveal the connections between many independently proposed algorithms in the literature under our framework of soft margin MKL for the kernel learning, thus explain why the regularization, such as the L 2 -norm or the elastic net like regularizer/constraint helps to improve the performance over L 1 MKL in a new perspective.</p><p>In summary, the core contributions of this paper are listed in the following.</p><p>1) A novel soft margin framework for MKL is proposed. Particularly, a kernel slack variable is first introduced for each of the base kernels when learning the kernel. Three new MKL formulations, namely the hinge loss soft margin MKL, the square hinge loss soft margin MKL, and the square loss soft margin MKL are also developed under this framework. 2) A new block-wise coordinate descent algorithm based on the analytical updating rule for learning the kernel combination coefficients is developed to efficiently solve the new hinge loss soft margin MKL problem. With our proposed framework, a simplex projection method is also introduced to solve the square hinge loss soft margin MKL, leading to a more efficient optimization procedure when compared with the existing optimization algorithms for elastic net MKL. 3) Comprehensive experimental results on the benchmark data sets and two video applications, including real video event recognition and action recognition demonstrate the effectiveness and efficiency of our proposed soft margin MKL learning framework. Compared with L 2 MKL (L p MKL), the new hinge loss soft margin MKL and the square hinge loss soft margin MKL have much sparser solution for kernel combination coefficients; nevertheless, these two MKL models can achieve better generalization performance. This defends the effectiveness using sparse kernel combination coefficients in MKL. This paper is organized as follows. In Section II, we first review ν-SVM and MKL. In Section III, a unified framework for soft margin MKL is proposed, and three novel soft margin MKL formulations are developed based on different loss functions for the kernel slack variables. New formulations for MKL are developed under our proposed soft margin MKL framework, and some existing formulations for MKL are revisited as the special cases under this framework. Then, a new block-wise coordinate descent algorithm for solving the hinge loss soft margin MKL and a simplex projection-based algorithm for solving the square hinge loss soft margin MKL are introduced in Section IV. Experimental results on the standard benchmark data sets and the YouTube and Event6 data sets from computer vision applications are shown in Section V. Finally, the conclusive remarks and the future work are presented in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES AND RELATED WORKS</head><p>Throughout the rest of this paper, we use the superscript to denote the transpose of a vector, and 0, 1 ∈ R l denote the zero vector and the vector of all ones, respectively. We also define α y as the element-wise product between two vectors α and y. Moreover, ||μ|| p represents the L p -norm of a vector μ, and the inequality μ = [μ 1 , . . . , μ l ] ≥ 0 means that μ i ≥ 0 for i = 1, . . . l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ν-SVM</head><p>Given a set of labeled training data S = {(x i , y i )|i = 1, . . . , l} sampled independently from X × Y with X ⊂ R n and Y = {-1, +1}, a kernel matrix K ∈ R l×l is usually constructed by using a mapping function φ(x) to map the data x from X to a reproducing kernel Hilbert space H such that k(x i , x j ) = φ(x i ) φ(x j ). Then, ν-SVM <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref> learns the decision function</p><formula xml:id="formula_0">f (x) = l i=1 α i y i k(x, x i ) + b (1)</formula><p>where </p><formula xml:id="formula_1">ξ i -ρ s. t. y i f (x i ) ≥ ρ -ξ i , ξ i ≥ 0 ∀i = 1, . . . , l<label>(2)</label></formula><p>where ρ/ f H is the margin separation between two opposite classes and C &gt; 0 is the regularization parameter. Note one can show that C = 1/lν, where ν is the lower bound of fraction of outliers <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>. By using the duality property, it is easy to show that the dual of the objective in</p><formula xml:id="formula_2">(2) is max α∈A SVM{K, α}<label>(3)</label></formula><p>where SVM{K, α} = -1/2(α y) K(α y) is the dual of the objective in SVM, and</p><formula xml:id="formula_3">A = {α|α 1 = 1, α y = 0, 0 ≤ α ≤ C1} (4)</formula><p>is the domain for α. From the Karush-Kuhn-Tucker (KKT) conditions of (2), one can show that the optimal solution α in the dual (3) is the same as that in the primal (2). Hence, for the given training set S and K, the maximization of SVM{K, α} with respect to α ∈ A indeed gives the solution of the SVM classifier in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. L 1 MKL</head><p>Now, we review MKL <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>. With a set of given M base kernels K = {K 1 , . . . , K M }, L 1 MKL tries to learn the optimal kernel combination coefficients and the decision function f simultaneously. When the ν-SVM model is used, the primal problem of L 1 MKL with block-norm regularization is written as</p><formula xml:id="formula_4">min f m ,b,ρ,ξ i 1 2 M m=1 f m H m 2 + C l i=1 ξ i -ρ s.t. y i M m=1 f m (x i ) + b ≥ ρ -ξ i , ξ i ≥ 0. (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>The first term in <ref type="bibr" target="#b4">(5)</ref> is the group lasso regularizer <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> to choose groups of nonlinear features with small model complexity f m H m , in which each group of nonlinear features is induced by using one base kernel. By using the Lagrangian method, the dual of</p><formula xml:id="formula_6">L 1 MKL is max α∈A,τ τ : SVM{K m , α} ≥ τ ∀m = 1, . . . , M<label>(6)</label></formula><p>where SVM{K m , α} = -1/2(α y) K m (α y). Alternatively, the dual (6) of L 1 MKL can also be written as</p><formula xml:id="formula_7">max α∈A min μ∈M M m=1 μ m SVM{K m , α}<label>(7)</label></formula><p>where μ = [μ 1 , . . . , μ M ] , μ m is the coefficient to measure the importance of the mth base kernel, and 1 Although this formulation looks different from the original one proposed in <ref type="bibr" target="#b36">[37]</ref>, they are essentially equivalent according to <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b39">[40]</ref>.</p><formula xml:id="formula_8">M = {μ|0 ≤ μ, M m=1 μ m = 1}</formula><p>is the domain for μ. Then the final classifier is given by</p><formula xml:id="formula_9">f (x) = l i=1 α i y i M m=1 μ m k m (x, x i ) + b.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hard Margin Perspective for L 1 MKL</head><p>From the constraints in <ref type="bibr" target="#b5">(6)</ref>, each SVM dual objective is no less than τ . The "error" is not allowed for each SVM dual objective that is below τ , and only the base kernels with the objective equal to τ are retained. In other words, the objective of L 1 MKL is essentially the same as max α∈A min m=1,...,M SVM{K m , α}, which learns the SVM classifier by first choosing the model with the minimal objective. Ideally, only one base kernel will be chosen. Hence, L 1 MKL usually gets a very sparse solution for the kernel combination coefficients, and some useful base kernels may not be used. Since the constraints in ( <ref type="formula" target="#formula_6">6</ref>) "push" the SVM dual objectives as large as possible, the variable τ can be considered as the hard margin in L 1 MKL, which reveals the hard margin property of L 1 MKL in the margin point of view, and paves the way for soft margin MKL formulations in the sequel.</p><p>Remark that the non-sparse L 2 MKL <ref type="bibr" target="#b15">[16]</ref> was proposed by substituting the simplex constraint with the <ref type="bibr" target="#b16">[17]</ref> directly extends the MKL formulation in ( <ref type="formula" target="#formula_7">7</ref>) by simply substituting the simplex constraint with the</p><formula xml:id="formula_10">L 2 -norm ball constraint μ ∈ {μ|0 ≤ μ, M m=1 μ 2 m ≤ 1}. L p MKL</formula><formula xml:id="formula_11">L p -norm constraint μ ∈ {μ|0 ≤ μ, M m=1 μ p m ≤ 1} for p &gt; 1.</formula><p>However, the kernel combination coefficients of these two models are always nonzeros, resulting in impaired prediction performance especially when many noisy or irrelevant base kernels are included. Therefore, how to remove noisy or irrelevant base kernels, and how to keep and emphasize the useful base kernels are the key issues for MKL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SOFT MARGIN FRAMEWORK FOR MKL</head><p>MKL learns the classifier and the optimal kernel simultaneously with a set of predefined base kernels. These base kernels can be obtained by using any well-known kernel functions (e.g., Gaussian kernel function, polynomial kernel function, spline kernel function, etc.,) with different kernel parameters or specially designed by domain experts for the learning task. Moreover, in computer vision tasks, such as image classification or video event recognition, different types of features are extracted from lots of feature extraction methods (e.g., SITF <ref type="bibr" target="#b42">[43]</ref>, STIP <ref type="bibr" target="#b43">[44]</ref>, and HOG <ref type="bibr" target="#b44">[45]</ref>, etc.,). Even with the same feature extraction method, there are many parameters. Usually, each type of features can be used to form a base kernel <ref type="bibr" target="#b45">[46]</ref> for representing images/videos. However, only some base kernels are informative for classification, and others may be irrelevant or even harmful. Recent studies show that the combination of several features can achieve better prediction performance for computer vision applications. However, L 1 MKL usually chooses only one or few base kernels due to its hard margin property. On the other hand, we always obtain the dense solution for kernel combination coefficients by using L 2 -MKL, L p -MKL, and the simplest method based on average kernel. Some noisy or irrelevant base kernels are inevitably included for prediction.</p><p>As pointed out in Section II-C, L 1 MKL is indeed a hard margin MKL, which only selects the base kernels with the minimum objective. This could easily suffer from the overfitting problem especially when some base kernels are formed by using noisy features. Recall that hard margin SVM assumes that the data of two opposite classes can always be separated with the hard margin, and the error is not allowed for training the model. However, to make SVM applicable for real applications, the slack variables were introduced to hard margin SVM in <ref type="bibr" target="#b35">[36]</ref>. The introduction of the slack variables allows some training errors for the training data, thus alleviating over-fitting encountered in hard margin SVM.</p><p>Inspired by the success of slack variables for SVM <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b46">[47]</ref>, in this section, we introduce the concept of the kernel slack variable for each of the base kernels, and develop a soft margin MKL framework, which is the counterpart to soft margin SVM. Herein, we have the following definition.</p><p>Definition 1: Given M base kernels K = {K 1 , . . . , K M } for the training data S = {(x i , y i )|i = 1, . . . , l} sampled independently from X × Y, we define kernel slack variable to be the difference of the target margin τ and the SVM dual objective SVM{K m , α} for the given kernel K m ∈ K as</p><formula xml:id="formula_12">ζ m = τ -SVM{K m , α} ∀m = 1, . . . , M.<label>(8)</label></formula><p>Then, the loss introduced from the kernel slack variable is defined as</p><formula xml:id="formula_13">z m = (ζ m ) ∀m = 1, . . . , M<label>(9)</label></formula><p>where (•) is any general loss function.</p><p>In the following, we mainly consider three loss functions, namely, the hinge loss (i.e., (ζ m ) = max(0, ζ m )), the square hinge loss (i.e., (ζ m ) = (max(0, ζ m )) 2 ), and the square loss</p><formula xml:id="formula_14">(ζ m ) = ζ 2 m .</formula><p>Based on these loss functions on the kernel slack variables, we will present our proposed soft margin MKL formulations, respectively. Note that our soft margin MKL framework can cater for not only the abovementioned loss functions but also many other loss functions. These three loss functions are studied due to their simplicity and successful utilization in the standard soft margin SVM formulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hinge Loss Soft Margin MKL</head><p>Based on the definition of the kernel slack variable for each base kernel, we are now ready to propose our soft margin MKL formulations. When the hinge loss is used for the kernel slack variables, we have the following objective function for the hinge loss soft margin MKL: <ref type="bibr" target="#b9">(10)</ref> The objective of the above hinge loss soft margin MKL is to maximize the margin τ while considering the "errors" from the given M base kernels. The parameter θ balances the contribution of the loss term represented by slack variables ζ m 's and the margin τ . To further discover the properties of the newly proposed hinge loss soft margin MKL formulation, we have the following proposition.</p><formula xml:id="formula_15">min τ,α∈A,ζ m -τ + θ M m=1 ζ m s.t. SVM{K m , α} ≥ τ -ζ m , ζ m ≥ 0, m = 1, . . . , M.</formula><p>Proposition 2: The solution of the following optimization problem is also the solution of hinge loss soft margin MKL: min μ∈M 1 max α∈A J(μ, α) <ref type="bibr" target="#b10">(11)</ref> where the objective function is</p><formula xml:id="formula_16">J(μ, α) = -1/2 M m=1 μ m (α y) K m (α y) and M 1 = {μ| M m=1 μ m = 1, 0 ≤ μ ≤ θ 1}.</formula><p>The proof of this proposition is shown in the Appendix. Note that the objective function J(μ, α) is the same as the one in the hard margin MKL formulation, and the difference is in the constraint for the coefficients μ. In hard margin MKL, the constraint for μ is the simplex constraint</p><formula xml:id="formula_17">μ ∈ M = {μ| M m=1 μ m = 1, 0 ≤ μ}.</formula><p>In contrast, we have μ ∈ M 1 in our new hinge loss soft margin MKL. This new constraint enforces the values of the μ no more than the regularization parameter θ , which can prevent extreme large values of kernel combination coefficients frequently encountered in hard margin MKL. We similarly observe the counterpart property of this formulation from the relationship between the hard margin SVM <ref type="bibr" target="#b47">[48]</ref> and the hinge loss soft margin SVM <ref type="bibr" target="#b35">[36]</ref>. For the hard margin SVM, the boundary constraint for α is given by 0 ≤ α, while the constraint is 0 ≤ α ≤ C1 for the hinge loss soft margin SVM. Note C in the soft margin SVM and θ in our soft margin MKL are the regularization parameters that balance the training error and the complexity of the model.</p><p>We also have the following interesting observations for this new objective function:</p><p>1) θ should be in the range {θ |θ ≥ 1/M}, otherwise there is no solution to the proposed problem. This can be easily verified from the constraints in (11); 2) when θ = 1/M, according to constraint M 1 in <ref type="bibr" target="#b10">(11)</ref>, we can obtain the uniform solution for μ (i.e., μ = 1/M1); 3) when θ ≥ 1, the constraint M 1 in <ref type="bibr" target="#b10">(11)</ref> becomes the same as M in the hard margin MKL (i.e., L 1 MKL <ref type="bibr" target="#b13">[14]</ref>). We clearly observe that the structural risk function is well controlled by introducing the penalty parameter θ , and the solution of the MKL problem can be changed by varying this parameter, which gives a novel perspective to the MKL problems. This objective function also bridges L 1 MKL and the simple approach using average kernel by choosing different regularization parameter θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Square Hinge Loss Soft Margin MKL</head><p>When we define the loss function for the kernel slack variables as the square hinge loss, then we can arrive at the following objective function for the square hinge loss soft margin MKL:</p><formula xml:id="formula_18">min τ,α∈A,ζ m -τ + θ 2 M m=1 ζ 2 m s.t. SVM{K m , α} ≥ τ -ζ m , m = 1, . . . , M. (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>Similar to the hinge loss soft margin MKL, τ is the margin of the final classifier, and each SVM dual objective for the base kernels is lower bounded by the difference between the margin τ and the kernel slack variable ζ m . We also have the following proposition.</p><p>Proposition 3: The solution of the following optimization problem gives the solution of square hinge loss soft margin MKL:</p><formula xml:id="formula_20">min μ∈M 2 max α∈A J(μ, α) + 1 2θ M m=1 μ 2 m (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>where the function</p><formula xml:id="formula_22">J(μ, α) is J(μ, α) = -1/2 M m=1 μ m (α y) K m (α y) and M 2 = {μ| M m=1 μ m = 1, 0 ≤ μ}.</formula><p>The proof of this proposition is similar to that of Proposition 2, thus it is omitted. Compared with L 1 MKL, this formulation shares the same simplex constraint for μ, but it has one more L 2 -norm regularization term 1/2θ M m=1 μ 2 m for the coefficients in the objective function. The regularization parameter θ balances the regularization for μ and the margin of the classifier J(μ, α).</p><p>The relationship between hard margin MKL and the square hinge loss soft margin MKL is also similar to that between hard margin SVM and the square hinge loss soft margin SVM <ref type="bibr" target="#b35">[36]</ref>, where the constraint for α remains the same while one more regularization term l i=1 α 2 i /2C is added in the objective function of the hard margin SVM formulation.</p><p>Note the simplex constraint is removed from L 2 MKL to L 1 MKL. In contrast, this formulation still has the simplex constraint. The previous work <ref type="bibr" target="#b21">[22]</ref> has used such type of regularization by directly adding the L 2 -norm regularization term for the kernel combination coefficients in the objective function of L 1 MKL. To further discover the connections of our square hinge loss soft margin MKL with previous works, we have the following proposition.</p><p>Proposition 4: The primal form of the square hinge loss soft margin MKL is given as</p><formula xml:id="formula_23">min μ, f m ,b,ρ,ξ i 1 2 M m=1 f m 2 H m μ m + C l i=1 ξ i -ρ + 1 2θ M m=1 μ 2 m s.t. y i M m=1 f m (x i ) + b ≥ ρ -ξ i , ξ i ≥ 0 M m=1 μ m = 1, 0 ≤ μ. (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>Proof: With fixed μ, we can write the Lagrangian as</p><formula xml:id="formula_25">L = 1/2 M m=1 f m 2 Hm /μ m + C l i=1 ξ i -ρ + 1/2θ M m=1 μ 2 m -l i=1 α i y i ( M m=1 f m (x i ) + b) -ρ + ξ i -l i=1 ξ i β i ,</formula><p>where α i ≥ 0, β i ≥ 0 are the Lagrange multipliers of the corresponding constraints. By setting the derivatives of the primal variables f m , b, ρ, ξ i to be zeros, we can get the corresponding KKT conditions. By replacing the primal variables in the Lagrangian with the KKT conditions, we can arrive at the min max optimization problem as shown in <ref type="bibr" target="#b12">(13)</ref>. Together with Proposition 3, we prove the proposition.</p><p>In the primal form, the objective function can be denoted as f = arg min f ( f )+ R emp ( f ), where ( f ) is the regularization term for the functional f , R emp ( f ) is the empirical risk term from the given training samples. Specifically, for ( <ref type="formula" target="#formula_23">14</ref>), we</p><formula xml:id="formula_26">have ( f ) = 1/2 M m=1 f m 2 H m /μ m + 1/2θ M m=1 μ 2</formula><p>m with μ ∈ M 2 and R emp ( f ) is the standard hinge loss from the training samples. In this formulation, we can see that the L 1 -norm of the kernel combination coefficients is enforced in the constraint, and the L 2 -norm of the kernel combination coefficients is penalized in ( f ). Therefore, it is essentially the elastic net regularization <ref type="bibr" target="#b48">[49]</ref> for MKL. In <ref type="bibr" target="#b22">[23]</ref>, the regularization is given as</p><formula xml:id="formula_27">( f ) = 1/2 M m=1 f m 2 H m /μ m under the elastic net constraint v M m=1 μ m + (1 -v) M m=1 μ 2 m ≤ 1, μ ≥ 0.</formula><p>This can be regarded as a variant of ( <ref type="formula" target="#formula_23">14</ref>) after considering the general conversion between Tikhonov regularization and Ivanov regularization as shown in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">Th. 1]</ref>.</p><p>Several existing works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref> have also been proposed for MKL with the (generalized) elastic net regularization in the primal form with the blocknorm regularization, without explicitly containing the kernel combination coefficients μ. For instance, the work in <ref type="bibr" target="#b10">[11]</ref> utilized the regularization term 2 to facilitate the optimization of the L 1 MKL problem. Interestingly, it can be shown that the primal form of square hinge loss soft margin MKL in ( <ref type="formula" target="#formula_23">14</ref>) is equivalent to the formulations in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b24">[25]</ref> by seeking the entire regularization path <ref type="bibr" target="#b49">[50]</ref>.</p><formula xml:id="formula_28">( f ) = λ/2 M m=1 f m 2 H m + 1/2( M m=1 f m H m )</formula><p>Note that, the square norm M m=1 ζ 2 m for the kernel slack variables ζ m in ( <ref type="formula" target="#formula_18">12</ref>) can be readily extended to a more general norm . . , M. The extensions of elastic net MKL in the primal form with more general block-norm regularization are also discussed in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b26">[27]</ref>, which can also be deemed as the soft margin MKL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Square Loss Soft Margin MKL</head><p>By setting the margin τ = 0 in <ref type="bibr" target="#b11">(12)</ref>, the loss function for the kernel slack variables becomes the square loss, and we can get the following square loss soft margin MKL:</p><formula xml:id="formula_29">min α∈A,ζ m θ 2 M m=1 ζ 2 m s.t. -SVM{K m , α} ≤ ζ m , m = 1, . . . , M. (<label>15</label></formula><formula xml:id="formula_30">)</formula><p>The L 2 MKL comes out naturally from (15) under our soft margin MKL framework according to the proposition.</p><p>Proposition 5: The solution of the following problem gives the solution of square loss soft margin MKL: min μ∈M 3 max α∈A J(μ, α) <ref type="bibr" target="#b15">(16)</ref> where the function J(μ, α) is J(μ, α) = -1/2 M m=1 μ m (α y) K m (α y) and M 3 = {μ| M m=1 μ 2 m ≤ 1, 0 ≤ μ}. Proof: It can also be proven by introducing the Lagrangian multipliers, i.e., the dual variables μ m for each of the inequality constraint, we can arrive at the following dual form:</p><formula xml:id="formula_31">max μ≥0 min α∈A -1/2θ M m=1 μ 2 m + 1/2 M m=1</formula><p>μ m (α y) K m (α y). By using Theorem 1 from <ref type="bibr" target="#b16">[17]</ref>, it is easy to show that for one specific parameter θ the above optimization problem is equivalent to: min μ≥0,μ μ≤1 max α∈A -1/2 M m=1 μ m (α y) K m (α y), which is essentially L 2 MKL <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Thus, we conclude that our proposed soft margin MKL framework also incorporates L 2 MKL as one special case.</p><p>By considering a more generalized norm on ζ m beyond the L 2 -norm, the formulation can be further extended to</p><formula xml:id="formula_32">min α∈A,ζ m θ/2 M m=1 ζ p/( p-1) m s.t. -SVM{K m , α} ≤ ζ m , m = 1, . . . , M,</formula><p>which can be similarly reformulated as L p MKL ( p &gt; 1) <ref type="bibr" target="#b16">[17]</ref>. In general, L p MKL <ref type="bibr" target="#b16">[17]</ref> can be regarded as a special case of our soft margin MKL as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OPTIMIZATION FOR SOFT MARGIN MKL</head><p>In this section, we propose new optimization algorithms for our proposed soft margin MKLs.</p><p>All the optimization problems can be changed to the min max optimization problem, so we adopt the alternating optimization approach, which was widely used in previous works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, to alternatively learn the kernel combination coefficients and the model parameter by leveraging the standard SVM implementations. Note that the recent works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref> proposed a new analytical updating rule for L p MKL by considering the special structure in the primal form of L p MKL. This type of solution can avoid the time consuming procedure for searching the new updating point for the kernel combination coefficients. Although the convergence when using p = 1 is not proven, the stable convergence to the optimal solution was experimentally observed in <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b34">[35]</ref>. Besides, the similar analytical updating rule was also adopted in <ref type="bibr" target="#b50">[51]</ref>. In this paper, we also propose a new analytical solution for updating the kernel combination coefficients based on the structure of our new objective function for the hinge loss soft margin MKL. For the square hinge loss soft margin MKL, a simplex projection method is proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Block-Wise Coordinate Descent Algorithm for Solving the Primal Hinge Loss Soft Margin MKL</head><p>We have the following proposition. Proposition 6: The following problem is the primal form for hinge loss soft margin MKL:</p><formula xml:id="formula_33">min μ∈M 1 , f m ,b,ρ,ξ i 1 2 M m=1 f m 2 H m μ m + C l i=1 ξ i -ρ s.t. y i M m=1 f m (x i ) + b ≥ ρ -ξ i , ξ i ≥ 0. (<label>17</label></formula><formula xml:id="formula_34">)</formula><p>Proof: The Lagrangian can be written as</p><formula xml:id="formula_35">L = 1 2 M m=1 f m 2 H m μ m + C l i=1 ξ i -ρ - l i=1 α i y i ( M m=1 f m (x i ) + b) -ρ + ξ i - l i=1 ξ i β i - M m=1 μ m η m - M m=1 ζ m (θ -μ m ) + τ ( M m=1 μ m -1) (18)</formula><p>where α i ≥ 0, β i ≥ 0, η m ≥ 0, ζ m ≥ 0, and τ are the Lagrange multipliers for the corresponding constraints. By setting the derivatives of the Lagrangian in <ref type="bibr" target="#b17">(18)</ref> with respect to the primal variables f m , b, ρ, ξ i , and μ m to be zeros, and substituting the primal variables back into the Lagrangian according to the corresponding KKT conditions, we have:</p><formula xml:id="formula_36">max τ,α∈A,ζ m -τ -θ M m=1 ζ m s.t. - 1 2 (α y) K m (α y) ≥ -τ -ζ m ζ m ≥ 0, m = 1, . . . , M. (<label>19</label></formula><formula xml:id="formula_37">)</formula><p>By multiplying -1 to the objective function in <ref type="bibr" target="#b18">(19)</ref>, it is converted into a minimization problem. Substituting τ with -τ , we arrive at the same formulation as the hinge loss soft margin MKL in <ref type="bibr" target="#b9">(10)</ref>. Thus, we complete the proof.</p><p>In the primal form as in ( <ref type="formula" target="#formula_6">6</ref>), we have the box constraint for μ, i.e., μ ∈ M 1 = {μ| M m=1 μ m = 1, 0 ≤ μ ≤ θ 1}. The work in <ref type="bibr" target="#b51">[52]</ref> proposed a family of structured sparsity to improve the lasso for linear regression problem. Specifically, a box constraint is directly enforced on the unknown regression variables to enforce the structured sparsity. By simplifying our model to the linear case without the group structure <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, we could include <ref type="bibr" target="#b51">[52]</ref> as a special case.</p><p>The primal problem in ( <ref type="formula" target="#formula_33">17</ref>) is convex in the objective function <ref type="bibr" target="#b13">[14]</ref> and linear in the constraints, thus it is convex. It can be solved by using the block-wise coordinate descent algorithm <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p><p>1) Fix μ, Update f m , b, ρ, ξ i : With a fixed μ, the optimization problem in <ref type="bibr" target="#b16">(17)</ref> becomes a standard maximum margin SVM problem, which can be equivalently reformulated as a standard Quadratic Programming (QP) problem with respect to α as shown in <ref type="bibr" target="#b19">(20)</ref>, and many efficient QP solvers can be readily used to obtain the optimal α</p><formula xml:id="formula_38">max α∈A - 1 2 M m=1 μ m (α y) K m (α y).<label>(20)</label></formula><p>After obtaining the optimal α, the primal variables f m , b, ρ, ξ i can be recovered accordingly.</p><p>2) Fix f m , b, ρ, ξ i , Update μ: With fixed f m , b, ρ, ξ i , the optimization problem in <ref type="bibr" target="#b16">(17)</ref> reduces to the following convex programming problem:</p><formula xml:id="formula_39">min μ∈M 1 M m=1 a m μ m (<label>21</label></formula><formula xml:id="formula_40">)</formula><p>with</p><formula xml:id="formula_41">a m = 1/2 f m 2 H m = 1 2 μ 2 m (α y) K m (α y).</formula><p>The remaining problem is how to efficiently solve the subproblem <ref type="bibr" target="#b20">(21)</ref>. Let us suppose all the base kernels are positive definite, and then we have a m &gt; 0 for m = 1, . . . , M. Without the loss of generality, we also assume that a m has been sorted such that a 1 ≥ a 2 ≥ • • • ≥ a M . Inspired by the Lagrangian multipliers method <ref type="bibr" target="#b52">[53]</ref> used for simplex projection, we introduce the Lagrangian multipliers λ, η m s, and ζ m s for the constraints in <ref type="bibr" target="#b20">(21)</ref>. Then we can get the following Lagrangian:</p><formula xml:id="formula_42">L = M m=1 a m μ m - M m=1 μ m η m - M m=1 ζ m (θ -μ m ) +λ( M m=1 μ m -1). (<label>22</label></formula><formula xml:id="formula_43">)</formula><p>Setting the derivative of L with respect to μ m to be zeros, we have the following KKT condition:</p><formula xml:id="formula_44">-a m μ 2 m -η m + ζ m + λ = 0 (<label>23</label></formula><formula xml:id="formula_45">)</formula><p>with the complementary KKT conditions μ m η m = 0, ζ m (θ -μ m ) = 0, and λ( M m=1 μ m -1) = 0. Thus, for 0 &lt; μ m &lt; θ, we have</p><formula xml:id="formula_46">-a m μ 2 m + λ = 0, or μ m = a m λ . (<label>24</label></formula><formula xml:id="formula_47">)</formula><p>If a m &gt; 0, we have μ m &gt; 0. Thus, for the case that all the a m 's are larger than 0, the constraint 0 ≤ μ m can be replaced by 0 &lt; μ m . If we know ω, the number of elements in μ whose value strictly equals to θ , the solution of the above problem can be directly obtained as:</p><formula xml:id="formula_48">μ m = ⎧ ⎨ ⎩ θ, m ≤ ω (1-ωθ) √ a m M p=ω+1 √ a p , m &gt; ω. (<label>25</label></formula><formula xml:id="formula_49">)</formula><p>We have the following two lemmas to obtain the solution for the problem in <ref type="bibr" target="#b20">(21)</ref>.</p><p>Lemma 7: Let μ * be the optimal solution to problem <ref type="bibr" target="#b20">(21)</ref>, and suppose a p &gt; a q for any two given indices p, q ∈ {1, . . . , M}. If μ * q = θ , then we have μ * p = θ . Proof: Suppose that μ * is the optimal solution to the problem in <ref type="bibr" target="#b20">(21)</ref>, and μ * q = θ . If using proof by contradiction, we have μ * p &lt; θ. Let μ be another vector whose elements have the same value with μ * except that μp = μ * q and μq = μ * p . Then, we observe that μ satisfies all the constraints in <ref type="bibr" target="#b20">(21)</ref>. Thus,</p><formula xml:id="formula_50">M m=1 a m /μ * m -M m=1 a m / μm = a p /μ * p + a q /μ * q -a p / μp -a q / μq = (a p -a q )(1/μ * p -1/θ ) &gt; 0. So we have M m=1 a m /μ * m &gt;</formula><p>M m=1 a m / μm , which contradicts with the assumption that μ * is the optimal solution to <ref type="bibr" target="#b20">(21)</ref>. So the original assumption is incorrect and thus we complete the proof.</p><p>Lemma 8: Let μ * be the optimal solution to the problem in <ref type="bibr" target="#b20">(21)</ref>, and suppose that a 1 ≥ a 2 ≥ • • • ≥ a M . Then ω, the number of elements whose value strictly equals to θ in μ * , is</p><formula xml:id="formula_51">min p ∈ {0, 1, . . . , M -1} √ a p+1 (1 -pθ) M m= p+1 √ a m &lt; θ .</formula><p>The proof is similar with that of Lemma 7 by using the proof by contradiction and thus it is omitted here.</p><p>3) Whole Optimization Procedure: Based on the above derivations, we can easily develop the whole optimization procedure for the hinge loss soft margin MKL, and the detailed block-wise coordinate descent algorithm is shown in Algorithm 1.</p><p>Algorithm 1 Procedure of the Block-Wise Coordinate Descent Algorithm for Hinge Loss Soft Margin MKL 1: Initialize μ 1 . 2: t = 1 3: while stop criteria is not reached do 4: Obtain α t by solving the subproblem in (20) using the standard QP solver with μ t 5: Calculate a m and update μ t +1 by solving the subproblem in (21) 6: t = t + 1 7: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Simplex Projection Method for Solving the Square Hinge Loss Soft Margin MKL</head><p>For solving the square hinge loss soft margin MKL, we directly solve the problem in <ref type="bibr" target="#b12">(13)</ref>. With a fixed μ, the optimization problem with respect to α is a standard QP problem, which can be optimized by using the QP solver.</p><p>With a fixed α, the projected gradient descent-based algorithm is used to update the kernel combination coefficients. Following <ref type="bibr" target="#b5">[6]</ref>, the gradient p t of the optimization problem in <ref type="bibr" target="#b12">(13)</ref> with respect to μ can be calculated as:</p><formula xml:id="formula_52">p m = -h m + 1 θ μ m , m = 1, . . . , M<label>(26)</label></formula><p>where</p><formula xml:id="formula_53">h m = 1/2(α y) K m (α y).</formula><p>Then, the coefficient μ is updated by using the coefficients μ t at the current iteration, namely</p><formula xml:id="formula_54">μ sub = M 2 (μ t -η t p t ) (<label>27</label></formula><formula xml:id="formula_55">)</formula><p>where M 2 (•) is the simplex projection operation and η t is the updating step size determined by the standard line search strategy. The simplex projection operation is a standard QP problem, which can also be solved by using the general QP solver. However, due to the special simplex constraint for μ, the efficient simplex projection method in <ref type="bibr" target="#b53">[54]</ref> is used in this paper. The detailed optimization procedure is shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we first evaluate different MKL algorithms on the benchmark data sets. Then we show the experimental studies on two real computer vision applications (i.e., video action recognition and video event recognition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison Algorithms</head><p>We evaluate the following algorithms. 1) AveKernel: We use average combination of the base kernels. Specifically, the kernel combination coefficients is given by μ = 1/M1, then the maximum margin classifier is learnt by SVM. 2) SimpleMKL <ref type="bibr" target="#b13">[14]</ref>: The classifier and the kernel combination coefficients are optimized by solving the L 1 MKL problem as in <ref type="bibr" target="#b4">(5)</ref>. 3) L 2 MKL <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>: The classifier and the kernel combination coefficients are optimized under the constraint ||μ|| 2 ≤ 1.  to L p MKL, which demonstrates that whether the solution is sparse or non-sparse should not be the main factor for the effectiveness of MKL methods.</p><p>Table <ref type="table" target="#tab_2">II</ref> shows the mean CPU time costs for training each of the model on the training set. The average rank for each algorithm is also listed in the last row of the table. We can observe that generally AveKernel using the single average kernel is the fastest since SVM model is trained only once for prediction. For MKL algorithms, SGMKL and SimpleMKL are comparable to each other but they are less efficient when compared with other methods. The L p MKL is more efficient due to the analytical solution for the kernel combination coefficients <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>. For SM1MKL and SM2MKL, the training is very efficient thanks to the analytical updating rule for SM1MKL and the efficient simplex projection procedure for SM2MKL. Moreover, SM2MKL is much faster than SGMKL due to the utilization of the simplex projection method in our optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Measuring the Impact of Noisy Base Kernels for Different MKL Algorithms</head><p>From the soft margin point of view, we also analyze the characteristics for the MKL methods by using the regularization on the kernel slack variables. Specifically, some MKL formulations are more sensitive to noisy base kernels. To verify it, we compare AveKernel with other MKL methods using different loss functions on the kernel slack variables, including L 1 MKL (hard margin), L 2 MKL (square loss), SM1MKL (hinge loss), and SM2MKL (square hinge loss). We use the first round of experiments for "Diabetes" from the benchmark data set to show the results of different algorithms when using noisy base kernels. The feature vector is augmented with r * d dimensions of randomly generated features, where d is the dimension of the original feature vector and r is the percentage of the augmented noisy features in the range of {0, 0.2, 0.4, . . . , 1.2}. Fig. <ref type="figure">2</ref> shows the accuracy of different MKL methods when using different levels of the noisy features for "Diabetes." We can clearly observe that AveKernel can achieve good results when the base kernels are clean. But when there are more noisy base kernels, the performance of AveKernel becomes much worse than the other algorithms. Moreover, in this experiment, the hinge loss for the kernel slack variables is the most robust loss function when there are strong noisy base kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments on YouTube for Action Recognition</head><p>In computer vision applications, many features can be extracted for the image or video data sets, and the best results are usually obtained by fusing multiple types of features. However, some features may only be suitable for some specific applications and may even be harmful for other applications. Thus, how to fuse or combine different features is an important problem for computer vision applications. In the following, we will show the effectiveness of MKL algorithms for action recognition <ref type="bibr" target="#b55">[56]</ref>.</p><p>1) Experimental Setting: We evaluate different MKL algorithms on the YouTube data set, which contains 11 action categories: basketball shooting, biking/cycling, diving, golf swinging, horseback riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog. The data set contains a total number of 1168 video sequences. We follow the pre-defined partitions as in <ref type="bibr" target="#b55">[56]</ref>, where the whole data set is partitioned to 25 folds. In order to compare the generalization ability of the different MKL formulations, we further choose 20 folds for training and use the remaining five folds for testing. The 20 training folds are also used to determine the parameters for all the algorithms. Four types of features, namely, Trajectory, HOG, HOF, and MBH <ref type="bibr" target="#b56">[57]</ref>, are extracted from each of the video sequences. Then the base kernels are constructed from each of the four types of features by using the χ 2 -kernel. The kernel mapping function is given as k(x i , x j ) = exp -γ D(x i , x j ) , where D(x i , x j ) is the χ 2 distance between any two videos for each type of features, and γ = 1/A4 n-1 with A being the mean value of the χ 2 distances between all the training samples. The kernel parameter n is from {-1, -0.5, . . . , 1}, thus a total number of 20 base kernels are used in the experiment.</p><p>For the performance evaluation, we use the non-interpolated average precision (AP), which has been widely used as the performance metric for image and video retrieval applications. It corresponds to the multipoint average precision values of a precision-recall curve and incorporates the effect of recall. Mean AP (MAP) means the mean of APs over all the 11 semantic action concepts.</p><p>2) Experimental Results: We report the MAP, the mean number of selected kernels (MNK), and the mean training CPU time (MTT) in Table III on this data set. The results are based on the mean of the 11 evaluated concepts. We can observe that the MAP of SimpleMKL is 87.47% and it is worse than AveKernel (88.39%), which indicates that L 1 MKL (SimpleMKL) may throw away some useful base kernels due to the hard margin property. We also observe that all the soft margin formulations L 2 MKL, L p MKL, SGMKL, SM1MKL, and SM2MKL achieve better results when compared with AveKernel and L 1 MKL (SimpleMKL) and SM1MKL is the best in terms of MAP.</p><p>As shown in Table <ref type="table" target="#tab_3">III</ref>, we also observe that AveKernel and L 2 MKL select all the 20 base kernels, and L 1 MKL selects the smallest number of base kernels, (i.e., 3.09 base kernels on average). SM1MKL, SM2MKL, and SGMKL select fewer base kernels than AveKernel and L p MKL. Again, we conclude that whether the solution is sparse or non-sparse is not the key factor for the effectiveness of the MKL methods even though our new formulations can obtain sparser solutions compared with L p MKL.</p><p>We also find that the training time of AveKernel is much faster than other MKL algorithms, and SGMKL and SimpleMKL are slower when compared with other MKL algorithms, such as L 2 MKL, SM1MKL, and SM2MKL, which have similar training time. L p MKL becomes slower in this experiment due to the smaller p value obtained from cross validation. SM2MKL is much faster than SGMKL due to the efficient simplex projection method proposed under our soft margin framework. In general, our new SM1MKL outperforms other MKL learning algorithms in terms of both efficiency and effectiveness on this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments on Event6 for Video Event Recognition 1) Experimental Setting:</head><p>We evaluate different algorithms on another real world Event6 data set <ref type="bibr" target="#b57">[58]</ref>. This data set contains 1101 videos, in which 924 videos are used as the training set and the remaining 177 are used as the test set. Six events (i.e., "wedding," "birthday," "picnic," "parade," "show," and "sports") are used for performance evaluation. Two types of local features (i.e., "STIP," "SIFT") are extracted from each of the video sequences, and then K-means is used to build the visual vocabularies for each of the local features. The spatial pyramid is also used to construct the final feature vector, in which two levels are used. Thus, four types of distances from two types of features and two pyramid levels are calculated as suggested in <ref type="bibr" target="#b57">[58]</ref>.</p><p>For any given distance D, four types of kernels are used as the base kernels: Gaussian kernel (i.e., k(  (i.e., k(x i , x j ) = 1/(γ D 2 (x i , x j ) + 1)), and inverse distance (ID) kernel (i.e., k(x i , x j ) = 1/( √ γ D(x i , x j ) + 1)), where D(x i , x j ) denotes the distance between two samples x i and x j . We set γ = 4 n-1 γ 0 , where n ∈ {-2, -1, . . . , 2} and γ 0 = 1/A with A being the mean value of square distances between all the training samples, thus a total number of 80 base kernels are constructed from the four types of distances. Please refer to <ref type="bibr" target="#b57">[58]</ref> for more details on the features and the kernels.</p><formula xml:id="formula_56">x i , x j ) = exp -γ D 2 (x i , x j ) ), Laplacian kernel (i.e., k(x i , x j ) = exp - √ γ D(x i , x j ) ), inverse square distance (ISD) kernel</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Experimental Results:</head><p>We report the MAP, the mean number of the selected base kernels (MNK), and the mean training CPU time (MTT) over all the six events in Table <ref type="table" target="#tab_4">IV</ref>. The MAP for AveKernel is only 44.33%, which is the worst on this data set. A possible explanation is the poor performance of the STIP features as shown in <ref type="bibr" target="#b57">[58]</ref>. L 1 MKL (SimpleMKL) can improve the MAP to 47.14%, and L p MKL can further improve the performance to 53.49%. SM2MKL and SGMKL achieve comparable performances. However, our newly proposed SM1MKL achieves the best MAP 54.84%. We observe that AveKernel can be much worse when the base kernels are noisy. While SimpleMKL can discard the noisy base kernels, it may also discard some useful base kernels due to the hard margin property. Although L p MKL improves the performance, the generalized square loss is usually more sensitive to the outliers than the (square) hinge loss, thus it cannot achieve the best result. The hinge loss for the kernel slack variables should be the most robust one on this data set, thus SM1MKL achieves the best results when compared with other algorithms.</p><p>In terms of the MNK, AveKernel selects all the base kernels, and L 2 MKL still selects as more as possible base kernels, and L p MKL selects fewer base kernels due to a smaller value p determined from cross-validation. SGMKL, SM1MKL, and SM2MKL can also select fewer base kernels when compared with AveKernel and L 2 MKL. As for the training time, our new algorithms are still very efficient. SM1MKL is faster than SGMKL and SimpleMKL and it is comparable to L p MKL. Again, we observe that the utilization of simplex projection method for SM2MKL significantly improves the efficiency, so SM2MKL is much faster than SGMKL, which again demon-strates it is beneficial to use our soft margin MKL framework to develop new efficient optimization method for improving the efficiency of square hinge loss soft margin MKL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have proposed a novel soft margin framework for MKL by introducing the kernel slack variables for kernel learning. Based on the formulation, we then propose the hinge loss soft margin MKL, the square hinge loss soft margin MKL, and the square loss soft margin MKL. We additionally discover their connections with previous MKL methods and compare different MKL formulations in terms of the robustness of loss functions defined on the kernel slack variables. Comprehensive experiments have been conducted on the benchmark data sets and the YouTube and Event6 data sets from computer vision applications. The experimental results demonstrate the effectiveness of our proposed framework.</p><p>In the future, we plan to analyze the theoretical bounds for the proposed soft margin MKLs and study their extensions to multi-class settings as well as investigate how to extend our MKL techniques for solving the more general ambiguity problem in <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX PROOF OF PROPOSITION 2</head><p>We can rewrite <ref type="bibr" target="#b9">(10)</ref> in the following form: </p><p>where the domain for α is A = {α|α 1 = 1, α y = 0, 0 ≤ α ≤ C1}.</p><p>The Lagrangian of ( <ref type="formula" target="#formula_57">28</ref>) is where μ m ≥ 0 and z m ≥ 0 are the non-negative Lagrangian multipliers for the inequalities in <ref type="bibr" target="#b27">(28)</ref>. Setting the derivatives of the Lagrangian with respect to the primal variables τ and ζ m as zeros, we arrive at</p><formula xml:id="formula_58">M m=1 μ m = 1 (30) θ -μ m -z m = 0, m = 1, . . . , M. (<label>31</label></formula><formula xml:id="formula_59">)</formula><p>Substituting the ( <ref type="formula">30</ref>) and ( <ref type="formula" target="#formula_58">31</ref>) back into the Lagrangian, we finish the proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>with 1</head><label>1</label><figDesc>&lt; p &lt; ∞ in a similar fashion as from L 2 MKL to L p MKL (see Section III-C for more discussions). We can then obtain a more general p/( p -1)-hinge loss soft margin MKL as min τ,α∈A,ζ m -τ + θ/2 M m=1 ζ p/( p-1) m s.t. SVM{K m , α} ≥ τ -ζ m , ζ m ≥ 0, m = 1, .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The average number of selected base kernels for each of the methods on the benchmark data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) K m (α y) ≥ τ -ζ m ζ m ≥ 0 m = 1, . . . , M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>) K m (α y) + τ -ζ m<ref type="bibr" target="#b28">(29)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>EVALUATION [MEAN CLASSIFICATION ACCURACY (%) ± STANDARD DEVIATION] FOR DIFFERENT ALGORITHMS ON THE BENCHMARK DATA SETS. THE NUMBER IN THE PARENTHESIS SHOWS THE RANK OF EACH ALGORITHM IN TERMS OF THE MEAN CLASSIFICATION ACCURACY ± 2.76 (7) 81.98 ± 3.20 (5) 82.47 ± 3.23 (1) 81.85 ± 3.08 (6) 82.10 ± 3.15 (3) 81.60 ± 4.21 (4) 82.47 ± 3.28 (1) Diabetes 75.22 ± 4.02 (7) 75.30 ± 3.35 (6) 75.91 ± 2.83 (4) 75.61 ± 2.71 (5) 76.00 ± 2.92 (3) 76.35 ± 2.79 (1) 76.26 ± 2.94 (2)</figDesc><table><row><cell></cell><cell>AveKernel</cell><cell>SimpleMKL</cell><cell>L 2 MKL</cell><cell>L p MKL</cell><cell>SGMKL</cell><cell>SM1MKL</cell><cell>SM2MKL</cell></row><row><cell cols="8">Heart 81.60 Australian 85.94 ± 2.24 (2) 85.12 ± 1.82 (4) 85.07 ± 1.84 (6) 85.27 ± 1.77 (3) 84.78 ± 1.80 (7) 86.23 ± 1.94 (1) 85.12 ± 1.82 (4)</cell></row><row><cell cols="8">Ionosphere 90.29 ± 4.01 (7) 91.81 ± 1.92 (3) 91.71 ± 2.70 (4) 91.33 ± 2.64 (5) 91.90 ± 2.39 (2) 91.33 ± 2.82 (5) 92.10 ± 2.38 (1)</cell></row><row><cell cols="8">Ringnorm 95.42 ± 2.01 (7) 98.25 ± 1.07 (1) 96.67 ± 1.47 (6) 97.67 ± 1.10 (4) 97.67 ± 1.35 (4) 98.00 ± 1.12 (2) 97.75 ± 1.36 (3)</cell></row><row><cell>Banana</cell><cell cols="7">73.00 ± 5.79 (7) 90.08 ± 2.95 (1) 88.58 ± 2.72 (6) 89.50 ± 2.43 (4) 89.50 ± 2.43 (4) 89.75 ± 2.39 (3) 90.08 ± 2.68 (1)</cell></row><row><cell cols="8">FlareSolar 67.04 ± 3.45 (7) 67.59 ± 3.99 (6) 68.64 ± 2.96 (1) 68.59 ± 2.93 (2) 67.94 ± 3.32 (5) 68.59 ± 2.93 (2) 68.59 ± 2.93 (2)</cell></row><row><cell>Average Rank</cell><cell>6.28</cell><cell>3.71</cell><cell>4.00</cell><cell>4.14</cell><cell>4.00</cell><cell>2.57</cell><cell>2.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II TRAINING</head><label>II</label><figDesc>TIME EVALUATION [MEAN CPU TIME (SECOND) ± STANDARD DEVIATION] FOR DIFFERENT ALGORITHMS ON THE BENCHMARK DATA SETS. THE NUMBER IN THE PARENTHESIS SHOWS THE RANK OF EACH ALGORITHM IN TERMS OF THE MEAN CPU TIME</figDesc><table><row><cell></cell><cell>AveKernel</cell><cell>SimpleMKL</cell><cell>L 2 MKL</cell><cell>L p MKL</cell><cell>SGMKL</cell><cell>SM1MKL</cell><cell>SM2MKL</cell></row><row><cell>Heart</cell><cell cols="7">0.1938 ± 0.039 (1) 19.32 ± 11.71 (6) 9.023 ± 1.957 (5) 4.273 ± 2.533 (3) 129.9 ± 92.55 (7) 1.928 ± 3.517 (2) 8.245 ± 4.498 (4)</cell></row><row><cell cols="8">Diabetes 1.075 ± 0.5016 (1) 410.8 ± 89.47 (6) 154.5 ± 17.50 (4) 83.62 ± 63.18 (3) 1206 ± 694.8 (7) 39.41 ± 22.79 (2) 240.6 ± 88.93 (5)</cell></row><row><cell cols="8">Australian 1.134 ± 0.144 (1) 194.6 ± 25.60 (6) 159.6 ± 76.61 (5) 82.94 ± 63.39 (4) 897.3 ± 535.5 (7) 23.58 ± 18.77 (3) 22.96 ± 3.542 (2)</cell></row><row><cell cols="8">Ionosphere 0.5656 ± 0.2193 (1) 146.2 ± 48.85 (6) 40.92 ± 12.64 (5) 26.16 ± 24.25 (3) 618.8 ± 529.0 (7) 14.58 ± 11.60 (2) 31.90 ± 20.41 (4)</cell></row><row><cell cols="8">Ringnorm 0.606 ± 0.292 (1) 297.3 ± 10.4.7 (6) 69.18 ± 40.48 (2) 277.5 ± 288.4 (5) 1035 ± 547.8 (7) 165.6 ± 142.8 (3) 239.7 ± 36.16 (4)</cell></row><row><cell cols="8">Banana 0.1734 ± 0.1051 (1) 15.07 ± 5.127 (4) 15.71 ± 1.656 (5) 49.74 ± 37.16 (7) 16.85 ± 6.792 (6) 11.56 ± 5.480 (2) 15.23 ± 17.55 (3)</cell></row><row><cell cols="8">FlareSolar 0.7609 ± 0.2173 (1) 2243 ± 6244 (7) 152.5 ± 24.54 (4) 382.7 ± 414.4 (5) 649.5 ± 253.7 (6) 81.03 ± 129.8 (3) 58.69 ± 38.95 (2)</cell></row><row><cell>Average Rank</cell><cell>1.00</cell><cell>5.86</cell><cell>4.28</cell><cell>4.28</cell><cell>6.71</cell><cell>2.43</cell><cell>3.43</cell></row><row><cell cols="4">Fig. 2. Performance of MKL when using different loss functions on kernel</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">slack variables with respect to the level of noisy features for "Diabetes."</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>EVALUATION FOR DIFFERENT ALGORITHMS ON THE YOUTUBE DATA SET IN TERMS OF THE MEAN AVERAGE PRECISION (MAP %), THE MNK, AND THE MTT OVER 11 CONCEPTS ON THE TEST SET</figDesc><table><row><cell></cell><cell>AveKernel</cell><cell>SimpleMKL</cell><cell>L 2 MKL</cell><cell>L p MKL</cell><cell>SGMKL</cell><cell>SM1MKL</cell><cell>SM2MKL</cell></row><row><cell>MAP (%)</cell><cell>88.39</cell><cell>87.47</cell><cell>88.66</cell><cell>89.21</cell><cell>89.20</cell><cell>89.26</cell><cell>89.09</cell></row><row><cell>MNK</cell><cell>20</cell><cell>3.09</cell><cell>20</cell><cell>12.91</cell><cell>8.64</cell><cell>9.09</cell><cell>8.54</cell></row><row><cell>MTT (Second)</cell><cell>1.04</cell><cell>57.77</cell><cell>36.78</cell><cell>123.8</cell><cell>191.8</cell><cell>36.48</cell><cell>45.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>EVALUATION FOR DIFFERENT ALGORITHMS ON THE VIDEO EVENT DATA SET IN TERMS OF THE MEAN AVERAGE PRECISION (MAP %), THE MNK, AND THE MTT OVER SIX EVENTS ON THE TEST SET</figDesc><table><row><cell></cell><cell>AveKernel</cell><cell>SimpleMKL</cell><cell>L 2 MKL</cell><cell>L p MKL</cell><cell>SGMKL</cell><cell>SM1MKL</cell><cell>SM2MKL</cell></row><row><cell>MAP (%)</cell><cell>44.33</cell><cell>47.14</cell><cell>53.34</cell><cell>53.49</cell><cell>53.81</cell><cell>54.84</cell><cell>53.98</cell></row><row><cell>MNK</cell><cell>80.00</cell><cell>3.63</cell><cell>68.00</cell><cell>60.50</cell><cell>60.53</cell><cell>53.83</cell><cell>61.77</cell></row><row><cell>MTT (Second)</cell><cell>2.297</cell><cell>542.9</cell><cell>261.1</cell><cell>396.4</cell><cell>1639</cell><cell>410.1</cell><cell>367.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0"><p>Available at: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets and http://www.fml.tuebingen.mpg.de/Members/raetsch/benchmark. The data sets include Heart, Diabetes, Australian, Ionosphere, Ringnorm, Banana, and FlareSolar.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by Multi-plAtform Game Innovation Centre (MAGIC) in Nanyang Technological University. MAGIC is funded by the Interactive Digital Media Programme Office (IDMPO) hosted by the Media Development Authority of Singapore. IDMPO was established in 2006 under the mandate of the National Research Foundation to deepen Singapore's research capabilities in interactive digital media (IDM), fuel innovation and shape the future of media.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 2 Procedure of the Iterative Approach for Square Hinge Loss Soft Margin MKL 1: Initialize μ 1 . 2: t = 1. 3: while stop criteria is not reached do 4: Obtain α t by solving the subproblem in <ref type="bibr" target="#b19">(20)</ref> using the standard QP solver with μ t 5: Calculate μ sub that can reduce the objective function value for the problem in (13) 6: Update μ t +1 = μ sub 7: t = t + 1 8: end while 4) L p MKL <ref type="bibr" target="#b16">[17]</ref>: The classifier and the kernel combination coefficients are optimized under the constraint ||μ|| p ≤ 1 with p ≥ 1. 5) SGMKL <ref type="bibr" target="#b22">[23]</ref>: The sparse generalized multiple kernel learning as in <ref type="bibr" target="#b22">[23]</ref>, where the constraint for the kernel combination coefficients is the elastic net constraint, i.e., <ref type="bibr" target="#b5">6</ref>) SM1MKL: Our proposed hinge loss soft margin MKL, in which the classifier and the kernel combination coefficients are optimized by solving the hinge loss soft margin MKL problem. 7) SM2MKL: The square hinge loss soft margin MKL, in which the classifier and the kernel combination coefficients are optimized by solving the square hinge loss soft margin MKL problem. To be consistent with previous works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, the experiments for different MKL algorithms are all based on the C-SVC formulation as used in <ref type="bibr" target="#b13">[14]</ref>, and the SVM QP problem is solved by using the LibSVM C-SVC QP solver. 2  For the SimpleMKL codes downloaded from the web, 3 we additionally change the SVM solver in their implementation with the LibSVM QP solver. For L p MKL, the implementation is available in Shogun toolbox <ref type="bibr" target="#b54">[55]</ref>; however, we implement the algorithm by using the analytical updating rule for the kernel combination coefficients exactly as in <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b34">[35]</ref> for better utilization of the LibSVM QP solver for fair comparison. For SGMKL <ref type="bibr" target="#b22">[23]</ref>, we download their MATLAB implementation, 4 and replace the SVM QP solver with the LibSVM QP solver, and also use Mosek 5 to solve the subproblem for updating the kernel combination coefficients in their implementation.</p><p>The SVM regularization parameter C is set in the range of {0.01, 0.1, 1, 10, 100} for all the algorithms on all the data sets in the following experiments. One more model parameter p is introduced for L p MKL, v is introduced for SGMKL, and θ is introduced for SM1MKL and SM2MKL. These parameters are set as follows:</p><p>1) for L p MKL, p ∈ {1, 32/31, 16/15, 8/7, 4/3, 2, 3, ∞};</p><p>2) for SGMKL, v is in the range of {0, 0.1, 0.2, . . . , 1};</p><p>2 Available at: http://www.csie.ntu.edu.tw/~cjlin/libsvm/.</p><p>3 http://asi.insa-rouen.fr/enseignants/∼arakotom/code/mklindex.html. 4 Available at: http://appsrv.cse.cuhk.edu.hk/∼hqyang/doku.php?id=gmkl. 5 Available at: http://www.mosek.com/.</p><p>3) for SM1MKL, θ is set to be 1/ν M, where ν is a ratio parameter from {1/M, 0.1, 0.2, . . . , 1}; 4) for SM2MKL, θ is in the range of {10 -5 , . . . , 10 4 , 10 5 }. Then all the algorithms have multiple sets of parameters, and the optimal parameters are determined by using five-fold cross validation on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Benchmark Data Sets</head><p>We first evaluate our proposed algorithms on some benchmark data sets. The experiments are conducted on seven publicly available data sets. 6  1) Experimental Settings: For the construction of base kernels on these benchmark data sets, we follow the method in <ref type="bibr" target="#b13">[14]</ref> by designing the base kernels in the following manner:</p><p>1) Gaussian kernels using ten different bandwidth parameters from {2 -3 , 2 -2 , . . . , 2 6 } by using all the variables and each single variable; 2) polynomial kernels with the degree from {1, 2, 3} by using all the variables and each single variable. We randomly partition the data set into two parts, namely 70% for training and the rest 30% for testing. For each partition, all the dimensions of samples in the training set are normalized to have zero mean and unit variance, while the samples in the test set are normalized accordingly. The experiments are then repeated 10 times, and the mean accuracy and the standard deviation on each test set are reported for comparison.</p><p>2) Experimental Results: Table <ref type="table">I</ref> shows the performance comparison of different algorithms, which demonstrates the effectiveness of our proposed MKL formulations when compared with the other MKL formulations. The average rank for each algorithm is calculated in the last row of Table <ref type="table">I</ref>. The average rank of SM2MKL is 2.00, and the average rank of SM1MKL is 2.57. So, SM1MKL and SM2MKL achieve similar performances. SimpleMKL follows SM1MKL and achieves the third position. In terms of the rank, SGMKL and L 2 MKL are a bit worse than SimpleMKL, and AveKernel is the worst. The results show that AveKernel and L p MKL cannot outperform L 1 MKL, probably because of redundant base kernels constructed in this setting. In terms of the loss functions defined on the kernel slack variables, the square loss is usually more sensitive to outliers than (square) hinge loss, thus the generalization ability of L 2 MKL (L p MKL) may be limited when compared with the hinge loss soft margin MKL (SM1MKL) and square hinge loss soft margin MKL (SM2MKL).</p><p>The average numbers of selected base kernels for different MKL formulations are shown in Fig. <ref type="figure">1</ref>. We observe that SimpleMKL (L 1 MKL) selects the smallest number of base kernels on most of the data sets, and L 2 MKL selects almost all the base kernels, leading to dense solutions. The AveKernel selects all the base kernels. SGMKL, SM1MKL, and SM2MKL generally obtain sparser solutions when compared </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reducing the number of support vectors of SVM classifiers using the smoothed separable case approximation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geebelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="682" to="688" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalized SMO algorithm for SVM-based multitask learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cherkassky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="997" to="1003" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An introduction to kernel-based learning algorithms</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning With Kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On kernel-target alignment</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="367" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Choosing multiple parameters for support vector machines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="131" to="159" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning the kernel with hyperkernels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1043" to="1071" />
			<date type="published" when="2005-07">Jul. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient hyperkernel learning using second-order cone programming</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="58" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building sparse multiple-kernel SVM classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="827" to="839" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple kernel learning, conic duality, and the SMO algorithm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale multiple kernel learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1531" to="1565" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiclass multiple kernel learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1191" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sim-pleMKL</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2491" to="2512" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An extended level method for efficient multiple kernel learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1825" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">L2 regularization for learning kernels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncertainty Artif. Intell</title>
		<meeting>Conf. Uncertainty Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">L p -norm multiple kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="953" to="997" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel learning for novelty detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Kernel Learn</title>
		<meeting>Workshop Kernel Learn</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient and accurate Lp-norm multiple kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online-batch strongly convex multikernel learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">More generality in efficient multiple kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining derivative and parametric kernels for speaker verification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Longworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech Language Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="748" to="757" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient sparse generalized multiple kernel learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="446" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A unifying view of multiple kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECML/PKDD</title>
		<meeting>ECML/PKDD</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="66" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spicymkl: A fast algorithm for multiple kernel learning with thousands of kernels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="108" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stage learning kernel algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ultrafast optimization algorithm for sparse multikernel learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Conf. Mach. Learn</title>
		<meeting>28th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multikernel learning with onlinebatch optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="227" to="253" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1667" to="1680" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition using context and appearance distribution features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: A new feature extraction framework with dense spatial sampling for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="473" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning the discriminative power-invariance trade-off</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiple kernel learning and the SMO algorithm</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Theera-Ampornpunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2361" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple and efficient multiple kernel learning by group lasso</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1175" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">New support vector algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1207" to="1245" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A formal analysis of stopping criteria of decomposition methods for support vector machines</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1045" to="1052" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training ν-support vector classifiers: Theory and algorithms</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2119" to="2147" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stat. Soc., B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Consistency of the group lasso and multiple kernel learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1179" to="1225" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLT</title>
		<meeting>COLT</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stat. Soc., B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Computing regularization paths for learning multiple kernels</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Conf</title>
		<meeting>NIPS Conf</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Composite kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szafranski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="103" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A family of penalty functions for structured sparsity</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Conf</title>
		<meeting>NIPS Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1612" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient learning of label ranking by soft projections onto polyhedra</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1567" to="1599" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient projections onto the 1 -ball for learning in high dimensions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The SHOGUN machine learning toolbox</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Widmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Behr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Bona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1799" to="1802" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2010-07">Jul. 2010</date>
			<biblScope unit="page" from="1959" to="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Handling ambiguity via inputoutput kernel learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Mining</title>
		<meeting>IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Co-labeling: A new multiview learning approach for ambiguous problems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Mining</title>
		<meeting>IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
