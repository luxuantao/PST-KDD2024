<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Center Network Virtualization: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Md</forename><forename type="middle">Faizul</forename><surname>Bari</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Raouf</forename><surname>Boutaba</surname></persName>
							<email>rboutaba@cs.uwaterloo.ca</email>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Esteves</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lisandro</forename><surname>Zambenedetti</surname></persName>
						</author>
						<author>
							<persName><roleName>Md. Golam</roleName><forename type="first">Maxim</forename><surname>Podlesny</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Golam</forename><surname>Rabbani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Faten Zhani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Q</forename><surname>Rabbani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Zhani are with the D.R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<addrLine>200 University Avenue West</addrLine>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">David R. Cheriton School of Computer Sci-ence</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Division of IT Con-vergence Engineering</orgName>
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<postCode>790-784</postCode>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Federal University of Rio Grande do Sul</orgName>
								<address>
									<addrLine>Av. Bento Gonc ¸alves</addrLine>
									<postCode>9500</postCode>
									<settlement>Porto Alegre</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Center Network Virtualization: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">90B7A1BA7FEB7322165D83505C6C1A06</idno>
					<idno type="DOI">10.1109/SURV.2012.090512.00043</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data Center Architecture</term>
					<term>Virtualization</term>
					<term>Virtualized Data Center</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the growth of data volumes and variety of Internet applications, data centers (DCs) have become an efficient and promising infrastructure for supporting data storage, and providing the platform for the deployment of diversified network services and applications (e.g., video streaming, cloud computing). These applications and services often impose multifarious resource demands (storage, compute power, bandwidth, latency) on the underlying infrastructure. Existing data center architectures lack the flexibility to effectively support these applications, which results in poor support of QoS, deployability, manageability, and defence against security attacks. Data center network virtualization is a promising solution to address these problems. Virtualized data centers are envisioned to provide better management flexibility, lower cost, scalability, better resources utilization, and energy efficiency. In this paper, we present a survey of the current state-of-the-art in data center networks virtualization, and provide a detailed comparison of the surveyed proposals. We discuss the key research challenges for future research and point out some potential directions for tackling the problems related to data center design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>server utilization and high operational cost. The situation improved with the emergence of server virtualization technologies (e.g., VMware <ref type="bibr" target="#b3">[4]</ref>, Xen <ref type="bibr" target="#b4">[5]</ref>), which allow multiple virtual machines (VMs) to be co-located on a single physical machine. These technologies can provide performance isolation between collocated VMs to improve application performance and prevent interference attacks. However, server virtualization alone is insufficient to address all limitations of todays data center architectures. In particular, data center networks are still largely relying on traditional TCP/IP protocol stack, resulting in a number of limitations:</p><p>• No performance isolation: Many of todays cloud applications, like search engines and web services have strict requirements on network performance in terms of latency and throughput. However, traditional networking technologies only provide best-effort delivery service with no performance isolation. Thus, it is difficult to provide predictable quality of service (QoS) for these applications.</p><p>• Increased security risks: Traditional data center networks do not restrict the communication pattern and bandwidth usage of each application. As a result, the network is vulnerable to insider attacks such as performance interference and Denial of Service (DoS) attacks <ref type="bibr" target="#b5">[6]</ref>. • Poor application deployability: Today many enterprise applications use application-specific protocols and address spaces <ref type="bibr" target="#b6">[7]</ref>. Migrating these applications to data center environments is a major hurdle because it often requires cumbersome modifications to these protocols and the application source code. • Limited management flexibility: In a data center environment where both servers and networks are shared among multiple applications, application owners often wish to control and manage the network fabric for a variety of purposes such as load balancing, fault diagnosis, and security protection. However, traditional data center network architectures do not provide the flexibility for tenants to manage their communication fabric in a data center.</p><p>• No support for network innovation: Inflexibility of the traditional data center architecture prohibits network innovation. As a result, it is difficult to introduce changes in traditional data center networks such as upgrading network protocols or introducing new network services.</p><p>In the long run, it will reduce the effectiveness of the initial capital investment in data center networks. ization aims at creating multiple virtual networks (VNs) on top of a shared physical network substrate <ref type="bibr" target="#b7">[8]</ref> allowing each VN to be implemented and managed independently. By separating logical networks from the underlying physical network, it is possible to introduce customized network protocols and management policies. Also, since VNs are logically separated from one another, implementing performance isolation and application QoS is facilitated. Management procedures of VNs will be more flexible because every VN has its own control and management system. Furthermore, isolation offered in network virtualization environments can also minimize the impact of security threats. Finally, the deployment of new applications and services in virtualized data center environments is facilitated by customized protocols and address spaces, which expedites network innovation. So far, most of the existing work on network virtualization has been focused on virtualizing traditional Internet Service Provider (ISP) networks. Thus, virtualizing data center networks is a relatively new research direction, and a key step towards fully virtualized data center architectures.</p><p>While virtualizing data center networks addresses all of the aforementioned issues, it also opens a variety of new research challenges including virtualization techniques, addressing schemes, performance isolation, scalability, failure tolerance, monitoring, interfacing, security, pricing, and resource management. In this paper, we present a survey of recent research on virtualizing data center networks. Our contributions are three-fold: first, we provide a summary of the recent work on data center network virtualization. Second, we compare these architectures and highlight their design tradeoffs. Finally, we point out the key future research directions for data center network virtualization. To the best of our knowledge, this work is the first to survey the literature on virtualizing data center networks.</p><p>The remainder of the survey is organized as follows. After introducing the terminology and definitions pertaining to data center virtualization (Section II), we summarize the proposals (Section III) related to data center network virtualization and compare them from different perspectives (Section IV). We then discuss the key research challenges for future explorations (Section V) and, finally, conclude our paper (Section VI). In this section, we present the terminology relevant to data center network virtualization that we will be using in this paper. Table <ref type="table">I</ref> provides a list of abbreviations used throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Center</head><p>A data center (DC) is a facility consisting of servers (physical machines), storage and network devices (e.g., switches, routers, and cables), power distribution systems, cooling systems.</p><p>A data center network is the communication infrastructure used in a data center, and is described by the network topology, routing/switching equipment, and the used protocols (e.g., <ref type="bibr">Ethernet and IP)</ref>. In what follows, we present the conventional topology used in data centers and some other topologies that have been recently proposed.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a conventional data center network topology <ref type="bibr" target="#b8">[9]</ref>. In this topology, the Top-of-Rack (ToR) switch in the access layer provides connectivity to the servers mounted on every rack. Each aggregation switch (AS) in the aggregation layer (sometimes referred to as distribution layer) forwards traffic from multiple access layer (ToR) switches to the core layer. Every ToR switch is connected to multiple aggregation switches for redundancy. The core layer provides secure connectivity between aggregation switches and core routers (CR) connected to the Internet. A particular case of the conventional topology is the flat layer 2 topology, which uses only layer 2 switches.</p><p>Clos topology is a topology built up from multiple stages of switches <ref type="bibr" target="#b9">[10]</ref>. Each switch in a stage is connected to all switches in the next stage, which provides extensive path diversity. Figure <ref type="figure" target="#fig_1">2</ref> shows an example of a three-stage Clos topology.</p><p>Fat-tree topology <ref type="bibr" target="#b10">[11]</ref> is a special type of Clos topology that is organized in a tree-like structure, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. The topology built of k-port switches contains k pods; each of them has two layers (aggregation and edge) of k/2 switches. Each of (k/2) 2 core switches has one port connected to each of k pods. The i-th port of any core switch is connected to pod i so that consecutive ports in the aggregation layer of each pod switch are connected to core switches on k/2 strides. Each edge switch is directly connected to k/2 end-hosts; each of the remaining k/2 ports of an edge switch is connected to k/2 ports of an aggregation switch <ref type="bibr" target="#b11">[12]</ref>.</p><p>The above topologies have the properties that make them suitable for data center networks. However, data center topologies are not limited to the topologies presented in this Section. For example, BCube <ref type="bibr" target="#b12">[13]</ref> is a data center network architecture based on hyper-cube topology. The interested reader can find a comparison of recent data center network topologies in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Center Virtualization</head><p>A Virtualized Data Center is a data center where some or all of the hardware (e.g., servers, routers, switches, and links) are virtualized. Typically, a physical hardware is virtualized using software or firmware called hypervisor that divides the equipment into multiple isolated and independent virtual instances. For example, a physical machine (server) is virtualized via a hypervisor that creates virtual machines (VMs) having different capacities (CPU, memory, disk space) and running different operating systems and applications.</p><p>A Virtual Data Center (VDC) is a collection of virtual resources (VMs, virtual switches, and virtual routers) connected via virtual links. While a Virtualized Data Center is a physical data center with deployed resource virtualization techniques, a Virtual Data Center is a logical instance of a Virtualized Data Center consisting of a subset of the physical data center resources. A Virtual Network (VN) is a set of virtual networking resources: virtual nodes (end-hosts, switches, routers) and virtual links; thus, a VN is a part of a VDC. A network virtualization level is one of the layers of the network stack (application to physical) in which the virtualization is introduced. In Figure <ref type="figure" target="#fig_3">4</ref>, we show how several VDCs can be deployed over a virtualized data center.</p><p>Both network virtualization and data center virtualization rely on virtualization techniques to partition available resources and share them among different users, however they differ in various aspects. While virtualized ISP (VNs) networks mostly consist of packet forwarding elements (e.g., routers), virtualized data center networks involve different types of nodes including servers, routers, switches, and storage nodes. Hence, unlike a VN, a VDC is composed of different types of virtual nodes (e.g., VMs, virtual switches and virtual routers) with diverse resources (e.g., CPU, memory and disk). In addition, in the context of network virtualization,  Infrastructure-as-a-Service virtual links are characterized by their bandwidth. Propagation delay is an important metric when nodes are geographically distributed. However, since a data center network covers a small geographic area, the propagation delay between nodes is negligible; hence, it is always ignored when defining VDC virtual links <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Another key difference between data center networks and ISP networks is the number of nodes. While the number of nodes in ISP backbones is in order of hundreds (e.g., 471, 487, and 862 nodes in Sprintlink, AT&amp;T, and Verio ISPs, respectively <ref type="bibr" target="#b16">[17]</ref>), it can go up to thousands in today's data centers (e.g., around 12000 servers in one Google Compute cluster <ref type="bibr" target="#b17">[18]</ref>). This can potentially raise scalability issues, and increase management complexity.</p><p>Furthermore, different from ISP networks, data center networks are built using topologies like the conventional tree, fattree, or Clos topologies with well defined properties, allowing to develop embedding algorithms optimized for such particular topologies (e.g., Ballani et al. <ref type="bibr" target="#b15">[16]</ref> proposed an embedding scheme applicable only to tree topology).</p><p>In summary, data center network virtualization is different from ISP network virtualization, because one has to consider different constraints and resources, specific topologies, and degrees of scalability. For a survey of ISP network virtualization the interested reader is referred to <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Business Model</head><p>In this section, we define main stakeholders in DC virtualization environment.</p><p>Specifically, one of the differences between the traditional networking model and network virtualization model is participating players. In particular, whereas the former assumes that there are two players: ISPs and end-users, the latter proposes to separate the role of the traditional ISP into two: an Infrastructure Provider (InP) and a Service Provider (SP). Decoupling SPs from InPs adds opportunities for network innovation since it separates the role of deploying networking mechanisms, i.e., protocols, services (i.e., SP) from the role of owning and maintaining the physical infrastructure (i.e., InP).</p><p>In the context of data center virtualization, an InP is a company that owns and manages the physical infrastructure of a data center. An InP leases virtualized resources to multiple service providers/tenants. Each tenant creates a VDC over the physical infrastructure owned by the InP for further deployment of services and applications offered to end-users. Thus, several SPs can deploy their coexisting heterogeneous network architectures required for delivering services and applications over the same physical data center infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LITERATURE SURVEY</head><p>The virtualization of data center networks is still in its infancy, and recent research has mainly focused on how to provide basic functionalities and features including the partitioning of data center network resources, packet forwarding schemes and network performance isolation. Accordingly, we focus our attention in this survey on:</p><p>• Packet forwarding schemes which specify the rules used to forward packets between virtual nodes. • Bandwidth guarantees and relative bandwidth sharing mechanisms that provide network performance isolation and more efficient network resource sharing, respectively.</p><p>• Multipathing techniques used to spread the traffic among different paths in order to improve load-balancing and fault-tolerance.</p><p>Nevertheless, there are other features worth considering when virtualizing data centers such as security, programmability, manageability, energy conservation, and fault-tolerance. So far, however, little attention has been paid to these features in the context of data center virtualization. We provide more details about the challenges related to these features in the future research directions section (Section V). In the following, we briefly describe the features we focus on in the paper, and then survey the proposals.</p><p>A forwarding scheme specifies rules for sending packets by switching elements from an incoming port to an outgoing port. A FIB allows to map MAC address to a switch port when making a decision about packet forwarding.</p><p>To support relative bandwidth sharing, congestioncontrolled tunnels <ref type="bibr" target="#b5">[6]</ref> may be used, typically implemented within a shim layer that intercepts all packets entering and leaving the server. Each tunnel is associated with an allowed sending rate on that tunnel implemented as a rate-limiter. The other alternatives <ref type="bibr" target="#b26">[27]</ref> are group allocation for handling TCP traffic, rate throttling for controlling UDP traffic, and centralized allocation for supporting more general policies, e.g., handling specific flows. The first alternative uses fair queueing; the second one relies on a shim layer below UDP.</p><p>One of the techniques to achieve bandwidth guarantee is the use of rate-limiters <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b25">[26]</ref>. In particular, a rate-limiter module is incorporated into a hypervisor of each physical machine; its role is to ensure that every VM does not exceed the allocated bandwidth. GateKeeper runs as a user level process in the Linux hypervisor (dom 0). It relies on the Open vSwitch (also running in the hypervisor) to track rates of each flow. Bandwidth guarantee in GateKeeper is achieved through rate limiter implemented using Linux hierarchical token bucket (HTB) scheduler running in the Xen hypervisor (dom 0) in the end hosts. CloudNaaS relies on Open vSwitch, which, although not explicitly stated, can be used for rate limiting. The deployment of rate limiters located at end-hosts makes it possible to avoid explicit bandwidth reservation at switches as long as the VDC management framework ensures that traffic crossing each switch does not exceed corresponding link capacity.</p><p>The main multipathing mechanisms used in data center networks are ECMP (Equal Cost Multipathing) <ref type="bibr" target="#b28">[29]</ref> and VLB (Valiant Load Balancing) <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. To achieve load balancing, ECMP spreads traffic among multiple paths that have the same cost calculated by the routing protocol. VLB selects a random intermediate switch that will be responsible for forwarding an incoming flow to its corresponding destination. ECMP and VLB are implemented in L3 switches. On the other hand, path diversity available in data center networks can be exploited not only to provide fault tolerance but also to improve load balancing. In particular one effective technique to achieve load balancing is to create multiple VLANs that are mapped to different paths between each source and destination pair. This allows to distribute the traffic across different paths <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>. T raditional DC <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref> √ √ SP AIN <ref type="bibr" target="#b19">[20]</ref> √</p><formula xml:id="formula_0">Diverter [21] √ NetLord [22] √ √ V ICT OR [23] √ V L2 [24] √ √ P ortLand [25] √ √ Oktopus [16] √ SecondNet [15] √ √ Seawall [6] √ Gatekeeper [26] √ NetShare [27] √ √ SEC2 [28] √ CloudN aaS [7] √ √</formula><p>In Table <ref type="table">II</ref>, we provide the classification of the surveyed projects according to the features they cover, and emphasize that a project may address more than one feature. A checkmark shows the features that are inherent to the surveyed proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Traditional data center (DC)</head><p>Virtualization in current data center architectures is commonly achieved by server virtualization. Each tenant owns a group of virtual servers (machines), and isolation among tenants is achieved through VLANs. Data centers relying on this simple design can be implemented using commodity switches and popular hypervisor technologies (e.g., VMware <ref type="bibr" target="#b3">[4]</ref>, Xen <ref type="bibr" target="#b4">[5]</ref>). Besides, tenants can define their own layer 2 and layer 3 address spaces.</p><p>The main limitation of current data center architectures is scalability since commodity switches were not designed to handle a large number of VMs and the resulting amount of traffic. In particular, switches have to maintain an entry in their FIBs (Forwarding Information Base) for every VM, which can dramatically increase the size of forwarding tables. In addition, since VLANs are used to achieve isolation among tenants, the number of tenants is limited to 4096 (the number of VLANs allowed by the 802.1q standard <ref type="bibr" target="#b18">[19]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Diverter</head><p>Supporting logical partitioning of IP networks is essential for better accommodation of applications and services needs in large-scale multi-tenant environments like data centers. Diverter <ref type="bibr" target="#b20">[21]</ref> is a software-only approach to network virtualization for a data center network that assumes no need for configuring switches or routers.</p><p>Diverter is implemented in a software module (called VNET) installed on every physical machine. When a VM sends an Ethernet frame, VNET replaces the source and the destination MAC addresses by the ones of the physical machines that host the source and the destination VMs, respectively. Then switches perform packet forwarding using the MAC addresses of the physical machines. VNET uses a modified version of the ARP protocol to discover any physical machine hosting a particular VM. Diverter requires that every VM have an IP address format encoding the tenant identity, the subnet, and the virtual machine address (currently use 10.tenant.subnet.vm); thus, no address clashes occur between tenants. VNET performs routing between subnets using MAC address rewriting, which gives the illusion of traversing a gateway. Summarizing, Diverter provides layer 3 network virtualization that allows every tenant to control his own IP subnets and VMs addresses.</p><p>The main limitation of the proposal is that it does not provide any QoS guarantee, the support of which the authors consider as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. NetLord</head><p>To maximize revenue, providers of Infrastructure-as-a-Service (IaaS) <ref type="bibr" target="#b31">[32]</ref> are interested in a full utilization of their resources. One of the most effective ways to achieve that is by maximizing the number of tenants using the shared infrastructure. NetLord <ref type="bibr" target="#b21">[22]</ref> is a network architecture that strives at scalability of tenant population in data centers. The architecture virtualizes L2 and L3 tenant address spaces, which allows tenants to design and deploy their own address spaces according to their needs and deployed applications. The key idea of NetLord presented in Figure <ref type="figure" target="#fig_4">5</ref> is to encapsulate tenant L2 packets and transmit them over L2 fabric employing L2+L3 encapsulation. A L2 packet header specifies MAC addresses of a source VM and a destination VM. A source NetLord Agent (NLA) deployed on each physical server controls all VMs on the server. Specifically, it encapsulates an L2 packet by adding an extra L2 header and L3 header as follows. The extra source and destination L2 addresses determine the MAC addresses of the ingress and egress switches of a server hosting a source VM respectively. The extra source IP address reveals the ID of a tenant MAC address space, which enables the tenant to use multiple L2 spaces. The extra destination IP address specifies the port of an egress switch for forwarding packets to a destination server, and the ID of a tenant hosting a source and a destination VMs.</p><p>A packet is transferred over a data center network to an egress switch through the underlying L2 fabric over the path chosen by VLAN selection algorithm of SPAIN <ref type="bibr" target="#b19">[20]</ref> (scheme relying on the VLAN support in existing commodity Ethernet switches to provide multipathing) 1 . Packet forwarding from an egress switch to a destination server is based on an L3 lookup of an egress port. An NLA forwards packets on a destination server to a destination VM using the IDs of a tenant and its MAC address space, and the destination L2 address of a VM in the encapsulated tenant packet. To support virtual routing, NetLord uses the same routing mechanism as Diverter <ref type="bibr" target="#b20">[21]</ref>. To support SPAIN multipathing and keep per-tenant configuration information, NetLord uses several databases (referred to as Configuration Repository in Figure <ref type="figure" target="#fig_4">5</ref>).</p><p>NetLord assumes that the edge switches support basic IP forwarding, however, not every Commercial Off-the-Shelf (COTS) switch <ref type="bibr" target="#b32">[33]</ref> does that. The proposed encapsulation implies a higher packet size, which increases drops and fragmentation. Besides, NetLord uses SPAIN for multipath forwarding operating on a per-flow basis, which is not scalable. Finally, although the architecture provides isolation among tenants, it does not support any bandwidth guarantee. 1 The description of SPAIN is provided later in the paper </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. VICTOR</head><p>Cloud tenants have a need to migrate services across data centers, to balance load within and across data centers, or to optimize performance of their services. On the other hand, cloud users want to have fast and efficient delivery of services and data. One approach that allows to achieve the above objectives of tenants and users is migration of VMs. To avoid service interruption, a VM should keep the same IP address during migration. Although that is not a challenge for migration within the same IP network, providing migration over different networks is not straightforward. VICTOR (Virtually Clustered Open Router) <ref type="bibr" target="#b22">[23]</ref> is a network architecture for supporting migration of VMs across multiple networks that enables migrating VMs to keep their original IP addresses.</p><p>The main idea of VICTOR shown in Figure <ref type="figure" target="#fig_5">6</ref> is to create a cluster of Forwarding Elements (FE) (L3 devices) that serve as virtual line cards with multiple virtual ports of a single virtualized router. Thus, the aggregation of FEs performs data forwarding for traffic in a network. FEs are distributed over several networks, which helps to support migration of VMs across multiple networks. The control plane is supported by one or several Centralized Controllers (CC). A VM is deployed on a server connected to only one edge FE. A CC maintains a topology table that specifies the connectivity This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p><p>between FEs, and an address table that determines the connectivity between each VM and a FE, to which a server hosting the VM is connected. CC calculates the routing path from each FE to VMs and spreads that information among FEs, which rely on these routing tables for forwarding packets.</p><p>The main limitation of VICTOR is that it requires supporting FIBs of large sizes leading to scalability issues concerning FEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. VL2</head><p>VL2 <ref type="bibr" target="#b23">[24]</ref> is a data center network architecture that aims at achieving flexibility in resource allocation. In VL2, all servers belonging to a tenant (termed "service" in the paper) share a single addressing space regardless of their physical location meaning that any server can be assigned to any tenant.</p><p>VL2 is based on a non-oversubscribed Clos topology (see Figure <ref type="figure" target="#fig_1">2</ref>) that provides easiness of routing and resilience. Packets are forwarded using two types of IP addresses: location-specific addresses (LAs) and application-specific addresses (AAs) used by switches and servers, respectively. VL2 relies on a directory system for AA-to-LA mappings. Before sending a packet, a VL2 server encapsulates the packet with the LA address of the destination ToR switch. Switches are not aware of AA addressing since they forward packets using LAs only. At the destination ToR switch, the packet is decapsulated and delivered to the destination AA server. To exploit path diversity, VL2 design relies on VLB and ECMP to spread traffic among multiple paths.</p><p>The separation between the addressing spaces of switches and servers improves the scalability of VL2 since ToR switches do not have to store forwarding information for a large number of servers. Furthermore, the VL2 directory system eliminates the need for ARP and DHCP requests, which are common sources of broadcast traffic in data centers. In addition, VLB and ECMP allow for a graceful degradation of the network after failures.</p><p>One limitation of VL2 is the lack of absolute bandwidth guarantees between servers, which is required by many applications (e.g., multimedia services). The proposal is also highly coupled to the underlying (Clos) topology, and requires that switches implement OSPF, ECMP, and IP-in-IP encapsulation, which can limit its deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. PortLand</head><p>VM population scalability, efficient VM migration, and easy management are important characteristics of current and nextgeneration data centers. PortLand <ref type="bibr" target="#b24">[25]</ref> addresses all these issues for a multi-rooted fat-tree topology (see Figure <ref type="figure" target="#fig_2">3</ref>). In particular, the architecture proposes an L2 routing mechanism employing the properties of that topology. It supports plugand-pay functionality for L2, which significantly simplifies administration of a data center network.</p><p>The main idea of PortLand is to use a hierarchical Pseudo MAC (PMAC) addressing of VMs for L2 routing. In particular, a PMAC has a format of pod.position.port.vmid, where pod is the pod number of an edge switch, position is its position in the pod, port is the port number of the switch the end-host is connected to, and vmid is the ID of a VM deployed on the end host. The fabric manager (a process running on a dedicated machine) is responsible for helping with ARP resolution, multicast, and fault tolerance. Using kport switches, forwarding table at each switch is limited to O(k) records due to the properties of a multi-rooted fat-tree topology. An edge switch, to which a server hosting a VM is connected, maps an actual MAC (AMAC) of the VM to PMAC. The position of a switch in the topology may be set manually by an administrator, or, automatically through Location Discovery Protocol (LDP) proposed by the authors that relies on the properties of the underlying topology.</p><p>Despite PortLand benefits, there are several issues limiting the architecture. First, it requires multi-rooted fat-tree topology making PortLand inapplicable to other used data center network topologies. Second, resolving ARP requests by a single server (i.e., the fabric manager) makes the architecture vulnerable to malicious attacks on the fabric manager, which lead to service unavailability if the fabric manager fails to perform address resolution. Third, each edge switch should have at least half of its ports connected to servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. SEC2</head><p>To ensure wide adoption of cloud computing over data centers, it is important to provide all tenants with security guarantees. In particular, one of the important security issues is isolation of virtual networks dedicated to different tenants. Although using VLANs may be a potential solution for supporting isolated networks in a data center, there are several limitations of VLANs. First, the maximum number of VLANs is 4K because of the size of the VLAN ID space. Second, per-user control of security policy is a challenge. Third, having a large number of VLANs in the same data center network may induce complexity in network management and increased control overhead. Secure Elastic Cloud Computing (SEC2) <ref type="bibr" target="#b27">[28]</ref> aims to resolve these drawbacks by separating packet forwarding and access control.</p><p>SEC2 is a data center network architecture that uses network virtualization techniques to provide secured elastic cloud computing service as shown in Figure <ref type="figure" target="#fig_6">7</ref>. Network virtualization is supported through Forwarding Elements (FEs) and a Central Controller (CC). FEs are essentially Ethernet switches with the ability to be controlled from a remote CC that stores address mapping and policy databases. FEs perform address mapping, policy checking and enforcement, and packet forwarding. The network architecture has two levels: one core domain and multiple edge domains incorporating physical hosts. An edge domain is assigned a unique eid, and is connected to the core domain by one or more FEs. Each customer subnet has a unique cnet id so that a VM can be identified by (cnet id, IP). To isolate different customers within each edge domain, SEC2 uses VLAN with the scope limited within the same edge domain, thus, eliminating the limitation of the number of customers that can be supported due to VLAN ID size. If a customer offers public access to a VM, an FE forces all external packets to traverse through firewall and NAT middleboxes before reaching the private network. The advantage of SEC2 is that it does not require specialized routers or switches across the entire data center network. In addition, SEC2 supports VM migration <ref type="bibr" target="#b22">[23]</ref> and VPC (Virtual Private Cloud) service, in which each user private network in a cloud is connected to its on-site network via Virtual Private Network (VPN) <ref type="bibr" target="#b33">[34]</ref>.</p><p>One of the limitations of SEC2 is that one edge domain cannot support VLANs of more than 4K different tenants. In addition, since FEs add outer MAC header when destination VM is not within the edge domain, SEC2 requires switches that support jumbo frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. SPAIN</head><p>The current spanning tree protocol (STP) used for large Ethernet LANs is inefficient for supporting modern data center networks, since it does not exploit path diversity offered by data center networks, resulting in limited bi-section bandwidth and poor reliability <ref type="bibr" target="#b34">[35]</ref>. Smart Path Assignment In Networks (SPAIN) <ref type="bibr" target="#b19">[20]</ref> uses the VLAN support in existing commodity Ethernet switches to provide multipathing over arbitrary topologies.</p><p>SPAIN computes disjoint paths between pairs of edge switches, and pre-configures VLANs to identify these paths. An end-host agent installed on every host spreads flows across different paths/VLANs. To improve load balancing and avoid failures, the agent can change paths for some flows. For instance, if the traffic is not evenly spread across the paths, the agent can change the VLANs used by some flows. The agent also detects failed paths and re-routes packets around the failures by using a different path.</p><p>Whereas SPAIN provides multipathing, and improves load balancing and fault-tolerance, the proposal has some scalability issues. In particular, although path computation algorithm proposed by SPAIN is executed only when the network topology is designed or significantly changed, the scheme is computationally expensive for complicated topologies. In addition, SPAIN requires that switches store multiple entries for every destination and VLAN; it creates more pressure on switch forwarding tables than the standard Ethernet does. Furthermore, the number of paths is limited to the number of VLANs allowed by the 802.1q standard (4096) <ref type="bibr" target="#b18">[19]</ref>. Finally, maintaining a mapping table between flows and VLANs leads to an additional overhead in each end-host. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Oktopus</head><p>Although infrastructure providers offer to tenants ondemand computing resources through allocating VMs in data centers, they do not support performance guarantees on network resources to tenants. The mismatch between the desired and achieved performance by tenants leads to the following problems. First, variability of network performance induces unpredictable application performance in data centers making application performance management a challenge. Second, unpredictable network performance can decrease application productivity and customer satisfaction, leading to revenue losses. Oktopus <ref type="bibr" target="#b15">[16]</ref> is the implementation of two virtual network abstractions (virtual cluster and virtual oversubscribed cluster) for controlling the tradeoff between the performance guarantees offered to tenants, their costs, and the provider revenue. Oktopus not only increases application performance, but offers better flexibility to infrastructure providers, and allows tenants to find a balance between higher application performance and lower cost.</p><p>A virtual cluster shown in Figure <ref type="figure" target="#fig_7">8a</ref> provides the illusion of having all VMs connected to a single non-oversubscribed virtual switch. This is geared towards data-intensive applications like MapReduce that are characterized by all-to-all traffic patterns. A virtual oversubscribed cluster illustrated in Figure <ref type="figure" target="#fig_7">8b</ref> emulates an oversubscribed two-tier cluster that is a set of virtual clusters interconnected via a virtual root switch-that suits applications featuring local communication patterns. A tenant can choose the abstraction and the degree of the oversubscription of the virtual network based on the communication pattern of the application the tenant plans to deploy in the VDC (e.g., user-facing web-applications, data intensive applications). Oktopus uses a greedy algorithm for the resource allocation to the VDC.</p><p>The main limitation of Oktopus is that it works only for tree-like physical network topologies. Thus, an open question is how to implement the abstractions of Oktopus for other topologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. SecondNet</head><p>SecondNet <ref type="bibr" target="#b14">[15]</ref> focuses on providing bandwidth guarantees among multiple VMs in a multi-tenant virtualized data center. In addition to computation and storage, the architecture also accounts for bandwidth requirements when deploying a VDC.</p><p>The main component of the SecondNet architecture shown in Figure <ref type="figure" target="#fig_8">9</ref> is the VDC manager that creates VDCs based on a requirement matrix that defines the requested bandwidth between VM pairs. SecondNet defines three basic service types: a high priority end-to-end guaranteed service (type 0), a better than best-effort service (type 1) that offers bandwidth guarantees for the first/last hops of a path, and a besteffort service (type 2). SecondNet uses a modified forwarding scheme called port-switching source routing (PSSR) that forwards packets using predefined port numbers instead of MAC addresses. PSSR improves the scalability of the data plane as paths are calculated at the source node. In this way, intermediate switches do not have to make any forwarding decision.</p><p>SecondNet achieves high scalability by moving information about bandwidth reservations from switches to server hypervisors. Besides, SecondNet allows resources (VM and bandwidth) to be dynamically added to or removed from VDCs (i.e., elasticity). Using migration, SecondNet is also able to handle failures and reduce resource fragmentation. In addition, PSSR can be implemented with Multiprotocol Label Switching (MPLS) <ref type="bibr" target="#b35">[36]</ref>, which makes it easily deployable.</p><p>The main limitation of SecondNet is that its performance may depend on the physical topology of a network. For example, while the BCube <ref type="bibr" target="#b12">[13]</ref> network achieves high network utilization, VL2 and fat-tree networks cannot. Further, SecondNet does not consider other performance characteristics that can be important to tenants such as latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Gatekeeper</head><p>Rodrigues et al. <ref type="bibr" target="#b25">[26]</ref> look at the problem of network performance isolation formulating the associated requirements and devising a new scheme meeting those requirements named Gatekeeper. In particular, the authors argue that a solution for network performance isolation should be scalable in terms of the number of VMs, predictable in terms of the network performance, robust against malicious behaviour of tenants, and flexible concerning the minimum and maximum performance guarantees.</p><p>Gatekeeper focuses on providing guaranteed bandwidth among VMs in a multiple-tenant data center, and achieving a high bandwidth utilization. In general, achieving a strict bandwidth guarantee often implies non effective utilization of a link bandwidth when free capacity becomes available. Gatekeeper addresses this issue by defining both the minimum guaranteed rate and maximum allowed rate for each VM pair. These parameters can be configured to achieve minimum bandwidth guarantee, while ensuring that link capacities are effectively utilized by tenants. Gatekeeper creates one or more logical switches that interconnect VMs belonging to the same tenant. The virtual NIC (vNIC) of each receiving VM monitors the incoming traffic rate using a set of counters, and reports congestion to the vNIC of the sender that is exceeding its minimum guarantee by the largest amount. The rate limiter at the sender uses this information to control its traffic rate to reduce the level of congestion. Although fault-tolerance is not discussed in the paper, we believe that fault-tolerance is easily implementable by Gatekeeper since each vNIC can simply recalculate the fair share of each flow upon detecting a failure.</p><p>Like many existing schemes, Gatekeeper does not consider other performance metrics such as latency. Besides, Gatekeeper is still under development: the key features like dynamic creation and deletion of rate limiters are yet to be implemented. Furthermore, the scale of the experimental evaluation is small (with only two tenants and six physical machines). We believe that a complete implementation and more realistic experimental evaluation are necessary to truly evaluate the effectiveness of Gatekeeper in real cloud environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L. CloudNaaS</head><p>CloudNaaS <ref type="bibr" target="#b6">[7]</ref> is a virtual network architecture that offers efficient support for deploying and managing enterprise applications in clouds. In particular, the architecture provides a set of primitives that suit the requirements of typical enterprise applications including application-specific address spaces, middlebox traversal, network broadcasting, VM grouping, and bandwidth reservation. Although many other datacenter network virtualization proposals have already addressed some of these issues (application specific address spaces and bandwidth reservation), they do not fully address all of the above issues. Motivated by this observation, CloudNaaS aims at providing a unified, comprehensive framework for running enterprise applications in clouds.</p><p>CloudNaaS relies on OpenFlow forwarding to achieve the objectives (e.g., middlebox traversal) mentioned above. An application deployment in CloudNaaS includes several steps. First, an end-user specifies the network requirements to the cloud controller using the primitives defined by a network policy language. After the network requirements are translated into a communication matrix, the cloud controller determines the placement of VMs and generates network-level rules that can be installed on switches. Currently, CloudNaaS use a modified greedy bin-packing heuristic for placing VMs that takes into consideration communication locality. In addition, CloudNaaS provides several techniques to reduce the number of entries required in each switch, which include: (1) using a single path for best-effort traffic; (2) using limited paths for QoS traffic based on the number of traffic classes specified by type-of-service (ToS) bits; and, (3) assigning contiguous addresses to VMs placed behind the same edge switch, and use wildcard bits for aggregating IP forwarding entries. Besides, CloudNaaS also supports online mechanisms for handling failures and changes in the network policy specification by reprovisioning the VDCs. Currently, CloudNaaS is implemented using OpenFlow-enabled switch for forwarding; the end hosts use Open vSwitch based network stack for forwarding and compliance with OpenFlow.</p><p>One limitation of CloudNaaS is that limiting the traffic to a few paths may lead to congestion and/or poor network utilization. Finding a better trade-off between scalability and network utilization is still a challenging problem for CloudNaaS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M. Seawall</head><p>Seawall <ref type="bibr" target="#b5">[6]</ref> is a bandwidth allocation scheme that allows infrastructure providers to define how the bandwidth will be shared in a data center network with multiple tenants. The idea of Seawall is to assign weights to network entities generating traffic (e.g., VMs, process), and to allocate bandwidth according to these weights in a proportional way. Seawall uses congestion-controlled tunnels between pairs of network entities to enforce bandwidth sharing policies. A shim layer implemented as a NDIS (Network Driver Interface Specification) packet filter is responsible for intercepting packets and adapting the rate the sender transmits packets at.</p><p>Seawall enforces bandwidth isolation among different tenants, and prevents malicious tenants from consuming all network resources. Besides, Seawall requires that a physical machine maintains state information only for its own entities, which improves scalability. Further, Seawall is agnostic to the transport protocol used by tenants, the number of flows used by an entity, and the number of destinations an entity sends traffic to. In all cases, Seawall shares the bandwidth proportionally and enforces isolation. Moreover, Seawall allows weights to be dynamically modified to accommodate changes in tenants requirements. Although Seawall does not address failures explicitly, it is adaptive to dynamic network conditions, making it fault-tolerant.</p><p>The first Seawall prototype was implemented only on Windows 7 and Hyper-V. Moreover, without admission control, it is unlikely that Seawall will be able to achieve absolute bandwidth guarantees for an increasing number of entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N. NetShare</head><p>NetShare <ref type="bibr" target="#b26">[27]</ref> tackles the problem of bandwidth allocation in virtualized data center networks proposing a statistical multiplexing mechanism that does not require any changes in switches or routers. NetShare allocates bandwidth for tenants in a proportional way and achieves high link utilization for infrastructure providers. In NetShare, data center network links are shared among services, applications, or corporate groups, rather than among individual connections. In this way, one service/application/group cannot hog the available bandwidth by opening more connections.</p><p>NetShare can be implemented in three possible ways: group allocation, rate throttling, and centralized allocation. NetShare uses group allocation to handle TCP flows. Group allocation uses fair queueing to provide fair bandwidth allocation among different services and is implemented through Deficit Round Robin (DDR) <ref type="bibr" target="#b36">[37]</ref>. Rate throttling is used to control the traffic generated by UDP sources and avoid excessive bandwidth consumption, and is implemented through a shim layer placed below UDP at each host. The shim layer controls the sending rate by analyzing the traffic measured at the receiver side and adjusting the sending rate accordingly. To implement more general policies, e.g., to allocate unused bandwidth to specific flows, the scheme uses the centralized bandwidth allocation. NetShare relies on the routing protocol to handle failures, and multipath is possible with the use of ECMP.</p><p>Scalability of NetShare can be an issue because queues have to be configured at each switch port for each service/application. Moreover, NetShare relies on the specific features of Fulcrum switches to implement its mechanisms, which reduces its deployability. In addition, NetShare aims to achieve fairness in bandwidth allocation, and, thus, does not provide any absolute bandwidth guarantees to services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMPARISON</head><p>Whereas the previous section surveys prominent research proposals and their salient features, this section compares these proposals using a set of qualitative metrics. In particular, we evaluate each proposal using the following five criteria: scalability, fault-tolerance, deployability, QoS support, and load-balancing. Scalability and fault-tolerance are important design concerns for data centers comprising large numbers of servers and network resources, and expected to support a large number of tenant applications. As data centers typically use commodity servers and network hardware today, deployability is a key issue that concerns how much change to the infrastructure is necessary for implementing a particular architecture. QoS is an increasing concern of tenants and is important to the success of virtualized data center architectures. Finally, loadbalancing is an important objective of network operators for traffic engineering and minimizing congestion in data center networks. We summarize the results of our comparison in Tables III-VI. Each table compares the proposals using specific criteria of a particular feature. In the following subsections, we will provide detailed discussion of our evaluation of each performance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scalability</head><p>Achieving high scalability in virtualized data centers requires address spaces that support large number of tenants and their VMs. Furthermore, since todays commodity switches often have limited memory size, it is necessary to keep the This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. Table <ref type="table">VII</ref> shows the maximum number of tenants, VMs per tenant, and the size of the forwarding table. The maximum numbers of tenants and VMs depend mainly on the number of bits used to identify tenants and VMs. The number of VMs per tenant depends on the address space supported by IPv4, which can be extended when using IPv6. Depending on the forwarding scheme, the size of the forwarding table depends on the number of VMs, physical machines, switches or pods. In practise, the number of VMs is higher than the number of physical machines, which is in turn higher than the number of switches. We also notice that VICTOR and Portland do not support multi-tenancy.</p><p>Among the architectures surveyed in the paper, Second-Net, Seawall, and Gatekeeper achieve high scalability by keeping states at end-hosts (e.g., hypervisors) rather than in switches. NetLord and VL2 achieve high scalability through packet encapsulation maintaining the forwarding state only for switches in the network. Diverter is also scalable, because its switch forwarding table contains only MAC addresses of the physical nodes (not those of VMs). On the other hand, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CloudN aaS</head><p>Unlimited the tenant id is handled 2 32  Number of edge switches by the management server SPAIN, VICTOR, and CloudNaaS are less scalable because they require maintaining a per-VM state in each switching/forwarding element. Although CloudNaaS provides some optimization for improving scalability, such an optimization limits path-diversity provided in the network and deteriorates overall effectiveness of the approach. Further, CloudNaaS is currently implemented using OpenFlow, and OpenFlow is known to have scalability issues in large data center due to use of centralized controllers. SEC2 is not scalable because the addressing scheme limits the numbers of tenants and subnets supported in the network. NetShare relies on a centralized bandwidth allocator, which makes it difficult to scale to large data centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fault-tolerance</head><p>In the context of virtualized data centers, fault-tolerance covers failure handling of components in the data plane (e.g., switches and links) and control plane (e.g., lookup systems). We found that most of the architectures were robust against failures in data plane components. For instance, SecondNet uses a spanning tree signalling channel to detect failures, and its allocation algorithm to handle them. A SPAIN agent can switch between VLANs in the occurrence of failures, NetLord relies on SPAIN for fault-tolerance, and VL2 and NetShare rely on the routing protocols (OSPF). Diverter, VICTOR, and SEC2 employ the underlying forwarding infrastructure for failure recovery. Schemes such as Oktopus and CloudNaaS handle failure by re-computing the bandwidth allocation for the affected network. Schemes including Seawall and Gatekeeper can adapt to failures by re-computing the allocated rates for each flow.</p><p>Control plane components in data center network architectures include centralized lookup systems for resolving address queries (NetLord, VICTOR, VL2, Portland, SEC2), centralized flow management technologies (CloudNaaS uses OpenFlow), spanning tree signalling (SecondNet), and routing protocols (NetShare and VL2). Failures of these control plane components can lead to malfunctioning of part or the whole data center and result in inability to detect failures in the data plane.</p><p>The impact of failures in architectures with control plane based on spanning tree protocols depends on the time that the protocol takes to converge after topology changes. Adaptations in the basic spanning tree protocol such as Rapid Spanning Tree Protocol (RSTP) <ref type="bibr" target="#b37">[38]</ref> can reduce the convergence time. Similar to STP, failures in instances of routing protocols such as OSPF require routes recalculation, which may take a variable time depending on the size of the network and on the current protocol configuration. However, as shown in <ref type="bibr" target="#b23">[24]</ref>, the convergence time of OSPF (less than one second) is not a prohibitive factor in real data center networks.</p><p>OpenFlow, used by CloudNaas, is based on a centralized controller that defines the behaviour of OpenFlow-based switches through a set of rules and associated actions. The centralized design of the OpenFlow controller makes it prone to failures and performance bottlenecks. HyperFlow <ref type="bibr" target="#b38">[39]</ref> is a proposal aiming at providing logically centralized but physically distributed OpenFlow controllers. In HyperFlow, when a failure occurs in one controller, the switches associated with the failed controller are reconfigured to communicate with another available controller.</p><p>Distributed lookup systems can be used to minimize the negative impact of failures in address lookup systems. For example, VL2 architecture proposes the use of replicated state machine (RSM) servers to implement a replicated directory system, which enables reliability without affecting performance.</p><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deployability</head><p>As mentioned previously, deployability is a key aspect of any data center network virtualization architecture. In our comparison summarized in Tables III-VI, we evaluate the deployability of an architecture as high if the architecture can be deployed over commodity switches with software modifications. On the other hand, low deployability refers to architectures requiring devices with the features that are not available in every switch (e.g., support L3 forwarding, specific protocols).</p><p>We summarize our detailed comparison with respect to deployability in Table <ref type="table">VIII</ref>, which describes the required features to be implemented in hypervisors (on physical machines), edge switches, and core switches. Commodity switches support mainly L2 forwarding and VLAN technology whereas Commodity hypervisors create only isolated VA forwarding scheme specifies rules for sending packets by switching elements from an incoming port to an outgoing port. A FIB allows to map MAC address to a switch port when making a decision about packet forwarding.Ms. The table also shows, which scheme requires a centralized management server. Depending on the scheme, this server can have different functionalities such as address management (Portland, VL2), tenants management (NetLord and VL2), routing computation (VICTOR and SEC2), and resource allocation (SecondNet).</p><p>We can observe that while some surveyed architectures (SPAIN, Diverter, and Seawall) require change only in the hypervisor, most of the surveyed architectures require extra hardware features. In particular, these features include MACin-MAC (SEC2) encapsulation, L3 forwarding (VL2, Net-Lord), DRR (NetShare), network directory service (NetLord, VL2, Portland, VICTOR, SEC2), and programmable hardware (CloudNaaS) that may not be easily supported by commodity switches. Thus, implementing those architectures can increase the overall cost of the network. Nevertheless, with hardware evolution and wide adoption of programmable hardware, it is not excluded that these technologies become common place in the near future.</p><p>Finally, we would like to mention that data centers managers tend to deploy commodity equipment, which are cheap and easily replaceable. Using this equipment is not always a synonym of lack of scalability. For instance, in the case of traditional data centers, commodity switches have to store MAC addresses of all hosted VMs. It induces a scalability issue because commodity switches often have a limited amount of resources (i.e., size of the FIB table) <ref type="bibr" target="#b21">[22]</ref>. However, the forwarding scheme proposed in NetLord requires commodity switches to store only MAC addresses of edge switches. The number of switches being much smaller than the number of VMs in a data center drastically improves scalability. In both conventional and NetLord architectures, commodity switches are used, however, the forwarding scheme makes the difference, hence there is no scalability problem in NetLord.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. QoS Support</head><p>QoS in virtual networks is achieved by allocating guaranteed bandwidth for each virtual link. Oktopus, SecondNet, Gatekeeper, and CloudNaaS provide guaranteed bandwidth allocation for each virtual network. On the other hand, Seawall and NetShare provide weighted fair-sharing of bandwidth among tenants; however, they do not provide guaranteed bandwidth allocation meaning that there is no predictable performance. Whereas the remaining architectures do not discuss QoS issues, we believe that it is possible to support QoS in these architectures by properly combining them with the ones that support bandwidth guarantee (e.g., incorporating Oktopus into NetLord).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Load-balancing</head><p>Load-balancing is a desirable feature for reducing network congestion while improving network resource availability and application performance. Among the architectures surveyed in the paper, SPAIN and NetLord (which relies on SPAIN) achieve load-balancing by distributing traffic among multiple spanning trees. To achieve load balancing and realize multipathing, Portland and VL2 rely on ECMP and VLB. Lastly, Diverter, VICTOR, and SEC2 are essentially addressing schemes that do not explicitly address load-balancing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Summary</head><p>Our comparison of different proposed architectures reveal several observations. First, there is no ideal solution for all the issues that should be addressed in the context of data center network virtualization. This is mainly because each architecture tries to focus on a particular aspect of data center virtualization. On the other hand, we believe that it is possible to combine the key features of some of the architectures to take advantage of their respective benefits. For example, it is possible to combine VICTOR and Oktopus to deploy virtualized data center with bandwidth guarantees while providing efficient support for VM migration. Second, finding the best architecture (or combination) requires a careful understanding of the performance requirements of the applications residing in the data centers. Thus, the issues discussed in this section require further research efforts in the context of different cloud environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. FUTURE RESEARCH DIRECTIONS</head><p>In this section, we discuss some of the key directions for future explorations regarding data center network virtualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Virtualized Edge Data Centers</head><p>Most of the existing studies so far on data center network virtualization have been focusing on large data centers containing several thousands of machines. Although large data centers enjoy economy-of-scale and high manageability due to their centralized nature, they have their inherent limitations when it comes to service hosting. In particular, economics factors dictate that there will be only a handful of large data centers built in locations where construction and operational (e.g. energy) costs are low <ref type="bibr" target="#b39">[40]</ref>. As a result, these data centers may be located far away from end users, resulting in higher communication cost and potentially sub-optimal service quality in terms of delay, jitter and throughput.</p><p>Motivated by this observation, recent proposals such as mist <ref type="bibr" target="#b40">[41]</ref>, EdgeCloud <ref type="bibr" target="#b41">[42]</ref>, micro-data centers <ref type="bibr" target="#b42">[43]</ref>, nanodata centers <ref type="bibr" target="#b43">[44]</ref> have been put forward to advocate building small-scale data centers for service hosting at the network edge (e.g. access networks), where services can be hosted close to the end users. In this paper we adopt the terminology of edge data centers to refer to small data centers located at network edge. While not as cost-efficient as large data centers, edge data centers offer several key benefits compared to large remote data centers <ref type="bibr" target="#b42">[43]</ref>: (1) They can offer better QoS for delay-sensitive applications such as video streaming, online gaming, web telephony and conferencing. (2) They can reduce network communication cost by reducing the traffic routed across network providers. <ref type="bibr" target="#b2">(3)</ref> The construction cost of edge data centers is lower compared to large remote data centers. In fact, many existing telecommunication and Internet Service Providers (ISP) are willing to leverage their existing infrastructure to provide value-added services using edge data centers <ref type="bibr" target="#b44">[45]</ref>. Therefore, it is envisioned that future cloud infrastructures will be multi-tiered, where edge data centers will complement remote data centers in providing high quality online services at low cost.</p><p>Similar to large data centers, virtualization is required in edge data centers for supporting VDCs from multiple tenants with diverse performance objectives and management goals. However, virtualizing edge data centers also imposes several new research challenges:</p><p>• For a service provider, one fundamental problem is how to best divide the service infrastructure between remote and edge centers to achieve the optimal tradeoff between performance and operational cost? This problem is commonly known as the service placement problem <ref type="bibr" target="#b45">[46]</ref>. Finding a solution to this problem is essential for service providers to use edge data center-based service architectures. This problem shares many similarities with the traditional replica placement problem <ref type="bibr" target="#b46">[47]</ref>. However, existing solutions have not studied the dynamic case, where demand and system conditions (e.g. resource price and network conditions) can change over time. In this case, if the placement configuration needs to be changed, it is also necessary to consider the cost of reconfiguration (such as VM migration) in the optimization model. • How to efficiently manage services hosted in multiple data centers? As there can be a large number of edge data centers, monitoring and controlling resources in such a large infrastructure have inherent challenges and can potentially incur a significant overhead. Minimizing this management overhead is a major issue worth investigation. We believe addressing the above research challenges will be crucial to the success of multi-tiered cloud infrastructures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Virtual data center embedding</head><p>Accommodating a high number of VDCs depends on a efficient mapping of virtual resources to physical ones. This problem is commonly referred to as embedding and has been the subject of extensive research in the context of network virtualization <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b50">[51]</ref>. Data center architectures like Second-Net <ref type="bibr" target="#b14">[15]</ref> and Oktopus <ref type="bibr" target="#b15">[16]</ref> have proposed heuristics to cope with the NP-hardness of the embedding problem. However, there are several other issues concerning the design of virtual data center embedding algorithms:</p><p>• In a virtualized data center, there are other resources besides physical servers that can be virtualized. They include routers, switches, storage devices, and security</p><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p><p>systems. Existing embedding solutions for data centers have only focused on achieving the optimal VM embedding to satisfy bandwidth and processing requirements. We believe the embedding algorithms for VDCs should consider requirements for other resources as well. According to <ref type="bibr" target="#b52">[53]</ref>, computing equipment of a typical 5000-square-foot data center, which includes processors, server power supplies, other server components, storage and communication equipment, consumes 52% of the total DC energy usage; supply systems consisting of the UPS (uninterruptible power supply), power distribution, cooling, lighting, and building switchgear consume 48%. Greenberg et al. <ref type="bibr" target="#b53">[54]</ref> report that a network of a data center consumes 10-20% of its total power. Designing "green" virtual data center embedding algorithms that take into account energy consumption will help administrators to reduce costs and comply with the new environmental concerns. In particular, network virtualization helps in reducing energy consumption through decreasing the number of physical routers/switches that need to be active by consolidating a large number of virtual resources on a smaller number of physical ones. However, despite recent effort on designing energy-aware data center networks <ref type="bibr" target="#b54">[55]</ref>, none of the existing embedding algorithms has considered energy cost. The main challenge in reducing energy consumption is how to jointly optimize the placement of both VMs and VNs for saving energy. • Fault-tolerance is another major concern in virtualized data centers. The failure of a physical link can cause disruption to multiple VDCs that share the link. In order to minimize the application performance penalty due to failures, it is necessary for the tenants to find embeddings that are fault tolerant. Existing work on survivable virtual network embedding <ref type="bibr" target="#b55">[56]</ref> represents an initial step towards this direction. • Finally, some tenants may wish to deploy VDCs across data centers from multiple geographical regions. This raises the problem of embedding VDCs across multiple administrative domains. In this context, devising a framework that finds an efficient embedding without sacrificing the autonomy of individual infrastructure providers becomes a challenging problem. Recent work such as Polyvine <ref type="bibr" target="#b56">[57]</ref> represents an initial effort for tackling this problem.</p><p>Finally, the problem of VDC embedding also raises the question of finding ideal data center physical topologies for VDC embedding. Even though the proposed data center architectures have relied on different network topologies such as Fat-Tree and Clos, it is unclear which topology is best suited for VDC embedding. For example, it has been reported that the SecondNet embedding scheme achieves high server and network utilization for a BCube topology <ref type="bibr" target="#b14">[15]</ref> than for a fat-tree topology. Thus, we believe it is important to analyze the effect of VDC embedding on the design of physical data center network topologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Programmability</head><p>Network programming is motivated by the desire to increase flexibility and innovation by decomposing network functions to facilitate the introduction of new protocols, services, and architectures. Simply stated, network programmability can be defined as the ability to run third party code on a network device (e.g., a router) in both the control plane and the data plane. Network programmability has recently received renewed attention in the research community. In particular, the concept of software defined networking (SDN) aims at providing a simple API for programming network control plane. Network programmability benefits from virtualization techniques regardless of the context considered (i.e., ISP network or DC). For example, running a customized code on a virtual node not only has no effect on other virtual nodes of the network (thanks to isolation), but also does not cause disruption in the physical substrate, which was a major concern for the adoption of network programmability. In the context of virtualized data centers, network programmability provides a modular interface for separating physical topologies from virtual topologies, allowing each of them to be managed and evolved independently. As we have already seen, many architectural proposals surveyed in the paper are relying on network programming technologies such as Openflow. However, in the context of virtualized multitenant data centers, network programmability needs to address a number of research challenges:</p><p>• Current data center network architecture proposals only allow for controlling layer 2 and layer 3 protocols. This design mandates the use of the same layer 2 and layer 3 protocols (e.g. IPv4 and Ethernet) by all tenants. Providing programming APIs for virtualization at different layers of the network stack will add significant flexibility to data center networks. • While programmable networking technologies offer management flexibilities to tenants and infrastructure providers, they also open up opportunities for malicious tenants to misuse the infrastructure. Infrastructure providers need to determine how to provide access and how much control to delegate to tenants so that, the tenants get a satisfactory level of flexibility in-terms of programming the network devices while ensuring safe and secured co-existence of multiple tenants. • Network vendors may offer non-standard, proprietary programming APIs. An interesting research challenge is to understand the impact of building a data center network infrastructure from heterogeneous equipments with different hardware level APIs. Introducing heterogeneity has both advantages and disadvantages. The major disadvantage is the administrative overhead introduced by the divergent interfaces, while the advantage is that some vendor-specific features may be desirable in certain situations. Currently, OpenFlow <ref type="bibr" target="#b57">[58]</ref> and its virtualization layer FlowVisor <ref type="bibr" target="#b58">[59]</ref> are the most prominently proposed technologies for achieving programmability in data center networks. OpenFlow is an abstraction layer that allows users to define the behaviour of networking switches by using special components called controllers. FlowVisor <ref type="bibr" target="#b58">[59]</ref> is a network virtualization layer that allows multiple controllers (one controller per tenant) to share a single OpenFlow switch. Recent research efforts have been carried to deploy OpenFlow in data center networks <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. One of the limitations of OpenFlow is scalability. Currently, OpenFlow adopts a centralized architecture where a single controller is responsible for managing all OpenFlow-enabled switches in the network. Since a large data center network typically serves million of flows simultaneously, an OpenFlow switch may become a performance bottleneck. There are some proposals that aim to overcome this issue. In particular, DevoFlow <ref type="bibr" target="#b60">[61]</ref> controls only over a subset of the flows (i.e., long lived "elephant" flows), and HyperFlow <ref type="bibr" target="#b38">[39]</ref> uses distributed controllers with a unified logical view. Similarly, the scalability of FlowVisor is also a subject needing further investigation, given the large number of tenants involved in a virtualized data center. A possible avenue for improving FlowVisor scalability is to determine the optimal number and placement of FlowVisor instances in a programmable data center network. Finally, other programmable platforms (e.g., active networks, mobile agents, and Script MIB) could also be evaluated in the context of virtualized data center networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network performance guarantees</head><p>Commercial data centers today are home to a vast number of applications with diverse performance requirements. For example, user-facing applications, such as web servers and real-time (e.g. gaming) applications, often require low communication latency, whereas data-intensive applications, such as MapReduce jobs, typically desire high network throughput. In this context, it is a challenging problem to design scalable yet flexible data center networks for supporting diverse network performance objectives. Data center network virtualization is capable of overcoming these challenges by dividing a data center network into multiple logical networks that can be provisioned independently to achieve desired performance objectives. For instance, many proposed architectures, such as SecondNet and Oktopus, proposed mechanisms to allocate guaranteed bandwidth to each virtual data center. However, providing strict bandwidth guarantee can lead to low utilization if tenants do not fully utilize the allocated bandwidth. On the other hand, weighted fair-sharing based solutions, such as Seawall and Gatekeeper, are capable of achieving high resource utilization. However, they do not provide hard resource guarantees to each virtual data center. There is an inherent conflict between maximizing network utilization and providing guaranteed network performance. Designing a bandwidth allocation scheme that finds a good trade-off between these two objectives is a key research problem in virtualized data center environments.</p><p>On the other hand, existing work on data center network virtualization has been primarily focusing on bandwidth allocation for achieving predictable throughputs. The issue of providing guaranteed delay is still an open problem, as it not only requires isolated bandwidth allocation, but also effective rate control mechanisms. For example, S 3 <ref type="bibr" target="#b61">[62]</ref> is a flow control mechanism that aims to meet flow deadlines. One particular challenge in data center environment is the TCP incast collapse problem <ref type="bibr" target="#b62">[63]</ref>, where simultaneous arrival of packets from a large number of short flows can overflow the buffer in network switches, resulting in significant increase in network delay. We believe any solution that provides delay guarantees in data center networks must also have the capability of handling TCP incast collapse. We believe the problem of providing delay guarantee in multi-tenant environments still needs further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Data center management</head><p>In a virtualized data center environment, infrastructure providers are responsible for managing the physical resources of the data center while service providers manage the virtual resources (e.g. computing, storage, network, I/O) allocated to their virtual data centers. An important advantage of virtualized data centers is that the physical resources are managed by a single infrastructure provider. This allows the infrastructure provider to have a full view of the system thus facilitating efficient resource allocation and handling of failures. However, there are still several challenges that need to be addressed in virtualized data centers including:</p><p>• Monitoring is a challenging task due to the large number of resources in production data centers. Centralized monitoring approaches suffer from low scalability and resilience. Cooperative monitoring <ref type="bibr" target="#b63">[64]</ref> and gossipping <ref type="bibr" target="#b64">[65]</ref> aim to overcome these limitations by enabling distributed and robust monitoring solutions for large scale environments. A key concern is to minimize the negative impact of management traffic on the performance of the network. At the same time, finding a scalable solution for aggregating relevant monitoring information without hurting accuracy is a challenge that needs to be tackled by monitoring tools designed for data centers. Lastly, providing customized and isolated views for individual service providers and defining the interplay between the monitoring systems of the Infrastructure providers and service providers also require further exploration. • Efficient energy management is crucial for reducing the operational cost of a data center. One of the main challenges towards optimal energy consumption is to design energy-proportional data center architectures, where energy consumption is determined by server and network utilization <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b65">[66]</ref>. ElasticTree <ref type="bibr" target="#b54">[55]</ref>, for example, attempts to achieve energy proportionality by dynamically powering off switches and links. In this respect, data center network virtualization can further contribute to</p><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p><p>reducing power consumption though network consolidation (e.g. through virtual network migration <ref type="bibr" target="#b66">[67]</ref>). However, minimizing energy consumption can come at the price of VDC performance degradation. Thus, designing energy-proportional data center architectures factoring in network virtualization, and finding good tradeoffs between energy consumption and VDC performance are interesting research questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Security</head><p>Security has always been an important issue of any network architecture. The issue is exacerbated in the context of virtualized data centers due to complex interactions between tenants and infrastructure providers, and among tenants themselves. Although the virtualization of both servers and data center networks can address some of the security challenges such as limiting information leakage, the existence of side channels and performance interference attacks, today's virtualization technologies are still far from being mature. In particular, various vulnerabilities in server virtualization technologies such as VMWare [68], Xen <ref type="bibr" target="#b67">[69]</ref>, and Microsoft Virtual PC and Virtual Server <ref type="bibr" target="#b68">[70]</ref> have been revealed in the literature. Similar vulnerabilities are likely to occur in programmable network components as well. Thus, not only do network virtualization techniques give no guaranteed protection from existing attacks and threats to physical and virtual networks, but also lead to new security vulnerabilities. For example, an attack against a VM may lead to an attack against a hypervisor of a physical server hosting the VM, subsequent attacks against other VMs hosted on that server, and eventually, all virtual networks sharing that server <ref type="bibr" target="#b69">[71]</ref>. This raises the issue of designing secure virtualization architectures immune to these security vulnerabilities.</p><p>In addition to mitigating security vulnerabilities related to virtualization technologies, there is a need to provide monitoring and auditing infrastructures, in order to detect malicious activities from both tenants and infrastructure providers. It is known that data center network traffic exhibits different characteristics than the traffic in traditional data networks <ref type="bibr" target="#b70">[72]</ref>. Thus, appropriate mechanisms may be required to detect network anomalies in virtualized data center networks. On the other hand, auditability in virtualized data centers should be mutual between tenants and infrastructure providers to prevent malicious behaviors from either party. However, there is often an overhead associated with such infrastructures especially in large-scale data centers. In <ref type="bibr" target="#b71">[73]</ref>, the authors showed that it is a challenge to audit web services in cloud computing environments without deteriorating application performance. We expect the problem to be further exacerbated when extending to network activities in a VDC. Much work remains to be done on designing scalable and efficient mechanisms for monitoring and auditing virtualized data centers.</p><p>Finally, in a multi-tenant data center environment, different tenants may desire different levels of security. This introduces the additional complexity of managing heterogeneous security mechanisms and policies. Furthermore, the co-existence and interaction of multiple security systems expected in a multitenant data center is an issue that has not been addressed before. For example, potential conflicts between firewalls and intrusion detection systems policies of infrastructure providers and service providers, need to be detected and solved <ref type="bibr" target="#b72">[74]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Pricing</head><p>Pricing is an important issue in multi-tenant data center environments, not only because it directly affects the income of the infrastructure provider, but also because it provides incentives for tenants to behave in ways that lead to desired outcomes, such as maximum resource utilization and application performance <ref type="bibr" target="#b0">[1]</ref>. Generally speaking, a well-designed pricing scheme should be both fair and efficient. Fairness means that identical good should be sold at identical price. Efficiency means the price of the good should lead to efficient outcomes (e.g. matching supply and demand). Today, infrastructure providers promise to provide resources to tenants in an on-demand manner, and charge tenants flat-rates for both VM and network usage. Despite being fair, this simple pricing scheme still suffers from several drawbacks. First, the cost of VMs and network for a single tenant can be dependent on each other. For example, poor network performance can prolong the running time of tenant jobs, resulting in increased VM usage cost <ref type="bibr" target="#b73">[75]</ref>. Data center network virtualization can address this problem by allocating guaranteed bandwidth for each VM <ref type="bibr" target="#b15">[16]</ref>. For virtual data centers with best-effort connectivity, the recent proposal of dominant resource pricing (DRP) <ref type="bibr" target="#b73">[75]</ref> seems to be a promising solution to eliminate the dependency between VM and network usage.</p><p>The second drawback of current data center pricing schemes is that they do not provide incentives for tenants to achieve desired outcomes. In particular, they do not (1) encourage purchase of resources when demand is low, and (2) suppress excessive demand (while giving priorities to important applications) when demand is high. A promising solution to this problem is to use market-driven pricing schemes, where resource price fluctuates according to supply and demand. In this perspective, Amazon EC2 spot instance service represents the first commercial endeavour towards fully market-driven pricing schemes. Similar techniques can also be used for virtualized data center networks, where resources prices for different service classes (e.g. guaranteed bandwidth, besteffort) vary according to resource demand. However, designing a market-driven resource allocation scheme that allocates multiple resource types (e.g. VMs and bandwidth) with different service quality guarantees is still a challenging problem.</p><p>While the discussion so far has been focusing on pricing resources inside data centers, the case for pricing cloud resources outside data centers is also a challenging one. In particular, when a tenant wishes to deploy a virtual data center across multiple data centers, it is necessary to develop mechanisms not only to help tenants decide appropriate embedding of VDCs across multiple networks, but also to allow both the tenant and infrastructure providers to negotiate for service quality and pricing schemes. Existing work such as V-mart <ref type="bibr" target="#b74">[76]</ref> represents initial efforts in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Data centers have become a cost-effective infrastructure for data storage and hosting large-scale network applications. However, traditional data center network architectures are ill-suited for future multi-tenant data center environments. Virtualization is a promising technology for designing scalable and easily deployable data centers that flexibly meet the needs of tenant applications while reducing infrastructure cost, improving management flexibility, and decreasing energy consumption.</p><p>In this paper, we surveyed the state of the art in data center network virtualization research. We discussed the proposed schemes from different perspectives, highlighting the trends researchers have been following when designing these architectures. We also identified some of the key research directions in data center network virtualization and discussed potential approaches for pursuing them.</p><p>Although current proposals improve scalability, provide mechanisms for load balancing, ensure bandwidth guarantees, there are challenging and important issues that are yet to be explored. Designing smart-edge networks, providing strict performance guarantees, devising effective business and pricing models, ensuring security and programmability, supporting multi-tiered and multi-sited data center infrastructures, implementing flexible provisioning and management interfaces between tenants and providers, and developing efficient tools for managing virtualized data centers are important directions for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Conventional data center network topology</figDesc><graphic coords="2,49.98,53.21,251.59,157.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Clos topology</figDesc><graphic coords="2,348.56,53.59,179.79,135.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Fat-tree topology (k = 4)</figDesc><graphic coords="3,154.26,53.43,305.78,145.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Virtualization of a data center.</figDesc><graphic coords="3,319.68,235.72,237.32,280.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. NetLord architecture</figDesc><graphic coords="6,73.27,53.22,467.80,166.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. VICTOR architecture.</figDesc><graphic coords="6,330.53,255.50,215.73,176.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. SEC2 architecture.</figDesc><graphic coords="7,316.14,53.14,244.48,157.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Abstractions in Oktopus: (a) virtual cluster; (b) virtual oversubscribed cluster.</figDesc><graphic coords="8,335.12,135.83,230.13,148.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. SecondNet architecture.</figDesc><graphic coords="9,57.18,53.32,237.42,178.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I USED ABBREVIATION</head><label>IABBREVIATION</label><figDesc></figDesc><table><row><cell>Acronym</cell><cell>Description</cell></row><row><cell>DC</cell><cell>Data Center</cell></row><row><cell>V M</cell><cell>Virtual Machine</cell></row><row><cell>V N</cell><cell>Virtual Network</cell></row><row><cell>V DC</cell><cell>Virtual Data Center</cell></row><row><cell>V LAN</cell><cell>Virtual Local Area Network</cell></row><row><cell>T oR</cell><cell>Top-of-Rack</cell></row><row><cell>AS</cell><cell>Aggregation Switch</cell></row><row><cell>CR</cell><cell>Core Router</cell></row><row><cell>InP</cell><cell>Infrastructure Provider</cell></row><row><cell>SP</cell><cell>Service Provider</cell></row><row><cell>IaaS</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Manuscript received 7 March 2012; revised 2 August 2012. This work was supported in part by the Natural Science and Engineering Council of Canada (NSERC) under the Smart Applications on Virtual Infrastructure (SAVI) Research Network, and in part by the World Class University (WCU) Program under the Korea Science and Engineering Foundation funded by the Ministry of Education, Science and Technology (Project No. R31-2008-000-10100-0). Proposal Scalability Fault-tolerance Deployability QoS Load balancing T raditional DC Low No High No No Diverter High Yes High No No NetLord High No Low No Yes V ICT OR Low Yes Low No No V L2 High Yes Low No Yes P ortLand High Yes Low No Yes SecondNet High Yes High Yes No SEC2 Low No Low No No CloudN aaS Low Yes Low Yes No TABLE IV QUALITATIVE COMPARISON OF THE PROPOSALS REGARDING MULTIPATHING Proposal Scalability Fault-tolerance Deployability QoS Load balancing T raditional DC Low No High No No SP AIN</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low Yes High No Yes NetLord High No Low No Yes V L2 High Yes Low No Yes P ortLand High Yes Low</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://aws.amazon.com/ec2/" />
		<title level="m">Amazon Elastic Compute Cloud (Amazon EC2)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How Google Works</title>
		<author>
			<persName><forename type="first">D</forename><surname>Carr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX OSDI</title>
		<meeting>USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2004-12">December 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wmware</surname></persName>
		</author>
		<ptr target="http://www.vmware.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Xen</surname></persName>
		</author>
		<ptr target="http://xen.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sharing the Data Center Network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandulaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CloudNaaS: A Cloud Networking Platform for Enterprise Applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sahu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SOCC</title>
		<meeting>ACM SOCC</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Survey of Network Virtualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="862" to="876" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Data Center: Load Balancing Data Center Services SRND</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Principles and Practices of Interconnection Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Towles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fat-Trees: Universal Networks for Hardware-Efficient Supercomputing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="892" to="901" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Scalable, Commodity Data Center Network Architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukissas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2008-08">August 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BCube: A High Performance, Server-centric Network Architecture for Modular Data Centers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Cost Comparison of Datacenter Network Architectures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CoNext</title>
		<meeting>ACM CoNext</meeting>
		<imprint>
			<date type="published" when="2010-11">November 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SecondNet: A Data Center Network Virtualization Architecture with Bandwidth Guarantees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CoNEXT</title>
		<meeting>ACM CoNEXT</meeting>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards Predictable Datacenter Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring ISP Topologies with Rocketfuel</title>
		<author>
			<persName><forename type="first">N</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="16" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic Energy-Aware Capacity Provisioning for Cloud Computing Environments</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Zhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/ACM International Conference on Autonomic Computing (ICAC)</title>
		<meeting>IEEE/ACM International Conference on Autonomic Computing (ICAC)</meeting>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">IEEE Standard for Local and Metropolitan Area Networks -Virtual Bridged Local Area Networks</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Std 802.1Q-2005</title>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SPAIN:COTS Data-Center Ethernet for Multipathing over Arbitrary Topologies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yalagandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM USENIX NSDI</title>
		<meeting>ACM USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diverter: A New Approach to Networking Within Virtualized Infrastructures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM WREN</title>
		<meeting>ACM WREN</meeting>
		<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NetLord: A Scalable Multi-Tenant Network Architecture for Virtualized Datacenters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yalagandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stiekes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pouffary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhancing Dynamic Cloud-based Services using Network Virtualization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM VISA</title>
		<meeting>ACM VISA</meeting>
		<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VL2: A Scalable and Flexible Data Center Network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pamboris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gatekeeper: Supporting Bandwidth Guarantees for Multi-tenant Datacenter Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guedes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WIOV</title>
		<meeting>WIOV</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">NetShare: Virtualizing Data Center Networks across Services</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<idno>CS2010-0957</idno>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Secure Cloud Computing with a Virtualized Network Infrastructure</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX HotCloud</title>
		<meeting>USENIX HotCloud</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Analysis of an Equal-Cost Multi-Path Algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hopps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
	<note>IETF RFC 2992</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Designing a Predictable Internet Backbone Network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang-Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM HotNets</title>
		<meeting>ACM HotNets</meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Designing a Predictable Internet Backbone with Valiant Load-Balancing</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IWQoS</title>
		<meeting>IWQoS</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Is Cloud Computing Really Ready for Prime Time?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leavitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="20" />
			<date type="published" when="2009-01">January 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">product data sheet0900aecd80322aeb</title>
		<ptr target="http://www.cisco.com/en/US/prod/collateral/switches/ps5718/ps6545/" />
		<imprint/>
	</monogr>
	<note>html</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BGP/MPLS IP Virtual Private Networks (VPNs)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rekhter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IETF RFC</title>
		<imprint>
			<biblScope unit="volume">4364</biblScope>
			<date type="published" when="2006-02">February 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An Algorithm for Distributed Computation of a Spanning Tree in an Extended LAN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Perlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="1985-09">September 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiprotocol Label Switching Architecture</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Callon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IETF RFC</title>
		<imprint>
			<biblScope unit="volume">3031</biblScope>
			<date type="published" when="2001-01">January 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient Fair Queuing Using Deficit Round-Robin</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shreedhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="385" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">1D-2004, IEEE Standard for Local and Metropolitan Area Networks, Media Access Control (MAC) Bridges</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Std</title>
		<imprint>
			<biblScope unit="volume">802</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">HyperFlow: a Distributed Control Plane for OpenFlow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tootoonchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganjalir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI INM/WREN</title>
		<meeting>NSDI INM/WREN</meeting>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Intelligent Placement of Datacenters for Internet Services</title>
		<author>
			<persName><forename type="first">Í</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guitart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICDCS</title>
		<meeting>IEEE ICDCS</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Ahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chemouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oueslati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Söllner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Welin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Content, Connectivity, and Cloud: Ingredients for the Network of the Future</title>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="62" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Network Edge Intelligence for the Emerging Next-Generation Internet</title>
		<author>
			<persName><forename type="first">S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Gregoire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="623" />
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On Delivering Embarrassingly Distributed Cloud Services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM HotNets</title>
		<meeting>ACM HotNets</meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Greening the Internet with Nano Data Centers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Valancius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laoutaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Massoulié</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CoNEXT</title>
		<meeting>ACM CoNEXT</meeting>
		<imprint>
			<date type="published" when="2009-12">December 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Next-Generation Managed Services: A Window of Opportunity for Service Providers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Mobley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CISCO Technical Report</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Service Placement in a Shared Wide-Area Platform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX ATEC</title>
		<meeting>USENIX ATEC</meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the Placement of Web Server Replicas</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Voelker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2001-04">April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking Virtual Network Embedding: Substrate Support for Path Splitting and Migration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="17" to="29" />
			<date type="published" when="2008-04">April 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Virtual Network Embedding with Coordinated Node and Link Mapping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INFOCOM</title>
		<meeting>INFOCOM</meeting>
		<imprint>
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Survivable Virtual Network Embedding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIP Networking</title>
		<meeting>IFIP Networking</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Topology-Awareness and Reoptimization Mechanism for Virtual Network Embedding</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M M K</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIP Networking</title>
		<meeting>IFIP Networking</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<ptr target="http://www.sigmetrics.org/sigmetrics2011/greenmetrics/CareyGreenMetricsKeynote060711.pdf" />
		<title level="m">Energy Efficiency and Sustainability of Data Centers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<ptr target="http://www.cisco.com/web/partners/downloads/765/other/" />
		<title level="m">Energy Logic: Reducing Data Center Energy Consumption by Creating Savings that Cascade Across Systems</title>
		<imprint/>
	</monogr>
	<note>Energy Logic Reducing Data Center Energy Consumption. pdf</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Cost of a Cloud: Research Problems in Data Center Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="73" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ElasticTree: Saving Energy in Data Center Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yiakoumis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Survivable Virtual Network Embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NETWORKING 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="40" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">PolyViNE</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM VISA</title>
		<meeting>ACM VISA</meeting>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">OpenFlow: Enabling Innovation in Campus Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="69" to="74" />
			<date type="published" when="2008-04">April 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Can the Production Network Be the Test-bed?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Appenzeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parulkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX OSDI</title>
		<meeting>USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Applying NOX to the Datacenter</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koponen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM HotNets</title>
		<meeting>ACM HotNets</meeting>
		<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">DevoFlow: Scaling Flow Management for High-Performance Network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tourrilhes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yalagandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Better never than Late: Meeting Deadlines in Datacenter Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">ICTCP: Incast Congestion Control for TCP</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CoNEXT</title>
		<meeting>ACM CoNEXT</meeting>
		<imprint>
			<date type="published" when="2010-11">November 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cooperative Monitoring for Internet Data Centers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IPCCC</title>
		<meeting>IEEE IPCCC</meeting>
		<imprint>
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Robust Monitoring of Network-wide Aggregates through Gossiping</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wuhib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clemm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Network Service Management</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="109" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Energy Efficiency in Data Centers and Cloud-based Multimedia Services: An Overview and Future Directions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IGCC</title>
		<meeting>IGCC</meeting>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Virtual Routers on the Move: Live Router Migration as a Network-Management Primitive</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biskeborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Merwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
		<ptr target="http://securitytracker.com/alerts/2008/Feb/1019493.html" />
	</analytic>
	<monogr>
		<title level="j">ACM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="231" to="242" />
			<date type="published" when="2008-08">August 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<ptr target="http://secunia.com/advisories/26986" />
		<title level="m">Xen vulnerability</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<ptr target="http://technet.microsoft.com/en-us/security/bulletin/MS07-049" />
		<title level="m">Virtual PC vulnerability</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Eliminating the Hypervisor Attack Surface for a More Secure Cloud</title>
		<author>
			<persName><forename type="first">J</forename><surname>Szefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CSS</title>
		<meeting>ACM CSS</meeting>
		<imprint>
			<date type="published" when="2011-10">October 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Understanding Data Center Traffic Characteristics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Logging in the Age of Web Services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chukavkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security and Privacy</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="82" to="85" />
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Conflict Classification and Analysis of Distributed Firewall Policies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Al-Shaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2069" to="2084" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">The Price Is Right: Towards Location-Independent Costs in Datacenters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multi-Provider Service Negotiation and Contracting in Network Virtualization</title>
		<author>
			<persName><forename type="first">F.-E</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/IFIP NOMS</title>
		<meeting>IEEE/IFIP NOMS</meeting>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
