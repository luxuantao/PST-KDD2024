<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pervasive and Mobile Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-12-03">3 December 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">George</forename><surname>Okeyo</surname></persName>
							<email>okeyo-g@email.ulster.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Mathematics</orgName>
								<orgName type="institution" key="instit1">Computer Science Research Institute</orgName>
								<orgName type="institution" key="instit2">University of Ulster</orgName>
								<address>
									<addrLine>Shore Road</addrLine>
									<postCode>BT37 0QB</postCode>
									<settlement>Newtownabbey</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liming</forename><surname>Chen</surname></persName>
							<email>l.chen@ulster.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Mathematics</orgName>
								<orgName type="institution" key="instit1">Computer Science Research Institute</orgName>
								<orgName type="institution" key="instit2">University of Ulster</orgName>
								<address>
									<addrLine>Shore Road</addrLine>
									<postCode>BT37 0QB</postCode>
									<settlement>Newtownabbey</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
							<email>h.wang@ulster.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Mathematics</orgName>
								<orgName type="institution" key="instit1">Computer Science Research Institute</orgName>
								<orgName type="institution" key="instit2">University of Ulster</orgName>
								<address>
									<addrLine>Shore Road</addrLine>
									<postCode>BT37 0QB</postCode>
									<settlement>Newtownabbey</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roy</forename><surname>Sterritt</surname></persName>
							<email>r.sterritt@ulster.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Mathematics</orgName>
								<orgName type="institution" key="instit1">Computer Science Research Institute</orgName>
								<orgName type="institution" key="instit2">University of Ulster</orgName>
								<address>
									<addrLine>Shore Road</addrLine>
									<postCode>BT37 0QB</postCode>
									<settlement>Newtownabbey</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pervasive and Mobile Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-12-03">3 December 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">3339D5F6AF488865C4665C7C0CAE4AEF</idno>
					<idno type="DOI">10.1016/j.pmcj.2012.11.004</idno>
					<note type="submission">Received 30 November 2011 Received in revised form 4 July 2012 Accepted 23 November 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Ontology Sensor data segmentation Time window Real-time activity recognition Ontological activity modelling Temporal information</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Approaches and algorithms for activity recognition have recently made substantial progress due to advancements in pervasive and mobile computing, smart environments and ambient assisted living. Nevertheless, it is still difficult to achieve real-time continuous activity recognition as sensor data segmentation remains a challenge. This paper presents a novel approach to real-time sensor data segmentation for continuous activity recognition. Central to the approach is a dynamic segmentation model, based on the notion of varied time windows, which can shrink and expand the segmentation window size by using temporal information of sensor data and activities as well as the state of activity recognition. The paper first analyzes the characteristics of activities of daily living from which the segmentation model that is applicable to a wide range of activity recognition scenarios is motivated and developed. It then describes the working mechanism and relevant algorithms of the model in the context of knowledge-driven activity recognition based on ontologies. The presented approach has been implemented in a prototype system and evaluated in a number of experiments. Results have shown average recognition accuracy above 83% in all experiments for real time activity recognition, which proves the approach and the underlying model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Ambient Assisted Living (AAL) is motivated by the need to support independent living, whereby technology is used to provide people with proactive services in their normal environments, e.g. at home. Smart Homes (SH) have emerged as a viable technology that can support individuals, such as the elderly and disabled, for independent and dignified living. To provide assistance for individual inhabitants of an SH, activity recognition is required to identify the task that the individual is currently undertaking. In addition, activity recognition also determines whether the individual has any difficulties completing tasks.</p><p>To perform activity recognition, three important tasks are undertaken, namely activity modeling, activity monitoring, and pattern recognition. During activity modeling, suitable computational models of activities are created and presented in a format that can be automatically processed by computer systems. Existing literature provides a number of modeling approaches that fall in two main categories: data-driven and knowledge-driven activity modeling. In data-driven activity modelling <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, activity models are learnt from pre-existing activity datasets. In knowledge-driven activity modelling <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, knowledge engineers and/or domain experts employ knowledge engineering techniques to specify activity models explicitly. The resulting knowledge bases capture and encode commonsense domain knowledge. The activity monitoring task captures an inhabitant's contextual information, e.g. location, time, objects used, and previous tasks performed, and which is then used to infer ongoing activities. Various monitoring techniques, such as dense sensing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, computer vision <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, and wearable sensors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, have been adopted for collecting contextual information. Finally, during pattern recognition, incoming sensor data is processed against the activity models to infer the ongoing activities. Analogous to activity modeling approaches, pattern recognition can be performed through either the data-driven or the knowledgedriven approach. Data-driven approaches <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> use machine learning techniques, typically statistical and probability analysis methods, to process sensor data against the activity models for pattern recognition. Conversely, knowledge-driven approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>] make use of knowledge-based inference techniques to infer ongoing activities. Usually, they take as input the available sensor data and process them against the predefined explicit activity models.</p><p>While vision-based activity monitoring has been widely used in security surveillance, dense sensor based activity monitoring has gained currency in SH environments due to privacy and ethical considerations. In such environments, sensors are attached to objects in the environment (e.g. fridges, cupboards, e.t.c.) and an inhabitant's interactions with these objects are monitored and used to identify the ongoing activities of daily living (ADLs). A key problem in dense sensor based activity recognition when sensors are activated along a timeline is how the sensor data are segmented so that the set of sensor interactions represents exactly a unique activity.</p><p>Recently, the ontology-based knowledge driven approach to activity recognition has attracted increasing attention. Ontology is essentially a formal, explicit specification of a shared conceptualization of a domain <ref type="bibr" target="#b18">[19]</ref>. It provides a vocabulary for modeling a domain by specifying the latter's objects and/or concepts, properties, and relationships. In this way, domain and prior knowledge can be exploited to predefine activity models, i.e., the so-called activity ontologies. Whenever sensor data are obtained, the approach determines the likely ADL by reasoning against the model through ontological inference. Nevertheless, existing works on ontology-based activity recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref> and similar work on knowledge-driven activity recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref> do not clearly articulate the mechanism about how and what sensor data are selected from a live data stream for performing activity inference. In some research experiments that support on-line continuous activity recognition, the experiments restart manually each time an ADL is identified. For the approach to be applicable to real-world use scenarios it is necessary that after an ADL is identified, the activity recognition process should continue on fresh sensor data and decide what to exclude from those already used in the previously identified ADL(s). Obviously, this is not a trivial task and requires the development of a suitable discriminating strategy. To this end, we develop a segmentation approach that makes use of temporal information associated with sensor data and temporal characteristics of an activity for real-time activity recognition. The approach addresses two important issues: 'segmentation and aggregation' and 'the conditions that trigger ontological reasoning'. Segmentation breaks down a sensor data stream into fragments that can be mapped to activity descriptions; while aggregation combines a finite collection of sensor data items available in a segment for activity inference.</p><p>The main purpose of this work is to develop a systematic approach to dynamic sensor data segmentation for real-time continuous activity recognition. The approach can dynamically decide an appropriate set of sensor data from a live sensor data stream for real-time activity recognition. Also, it is able to support continuous segmentation and aggregation along a timeline, thus allowing real-time ongoing activity recognition. As a result, this paper makes the following contributions. Firstly, we propose a time window based segmentation model that is applicable to a wide range of activity recognition scenarios. Secondly, we develop various mechanisms for dynamic manipulation of model parameters during activity recognition, such as the setting, shrinking, and expansion of the time window's length, thus adapting the segmentation model in terms of the way activities are performed. Thirdly, we integrate the dynamic sensor data segmentation approach into an ontology-based algorithm for real-time, continuous activity recognition. This provides a basis for the implementation of re-useable knowledge-driven algorithms and applications for real-time activity recognition. In addition, we develop a synthetic ADL data generator that can be used to quickly generate temporally-rich synthetic ADL data for evaluation of activity recognition algorithms. Finally, we evaluate the performance of the proposed model and algorithms in supporting real-time activity recognition. We believe the time window based segmentation model and associated algorithms in activity recognition provide a realistically scalable, reusable approach that can continuously recognize activities of different complexities in a Smart Home context. The research is based on typical ADL activities that an inhabitant can perform in the kitchen, lounge, and bathroom of a Smart Home, e.g. cooking, watching television, and showering.</p><p>The remainder of the paper is organized as follows. Section 2 outlines related work. Section 3 describes the proposed approach, including ontological activity modeling. Sensor data stream segmentation and analysis is described in Section 4 which includes formal time window based modeling, recognition algorithms, and mechanisms used to dynamically vary the time windows. The implementation of various components and evaluation of the approach is presented in Section 5. Finally, Section 6 summarizes the results and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section we first briefly review related work in activity recognition. Secondly, since this work is motivated by the need to segment a sensor data stream using temporal information, we also review papers that use temporal segmentation for activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Activity recognition</head><p>In <ref type="bibr" target="#b21">[22]</ref> the authors explain activity recognition as the process that allows an actor's behavior and their environment to be monitored and analyzed in order to infer the ongoing tasks. Activity recognition can be classified based on two criteria:</p><p>(1) how are the activities monitored?; (2) how are activities modeled, represented and subsequently processed to infer the ongoing activities?</p><p>Activity monitoring allows the context information associated with an activity to be captured for use in activity inference. Based on activity monitoring, there are two main categories of activity recognition: vision-based and sensor-based activity recognition. Vision-based activity recognition relies on visual sensing equipments that monitor an actor's behavior and associated environment changes <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. Techniques from computer vision are then used to analyze the obtained visual information to obtain features for use in pattern recognition. The main criticism leveled against the adoption of this approach is that it is intrusive and may interfere with the privacy of actors. In contrast, sensor-based activity recognition uses a variety of sensor technologies, e.g. wireless sensor networks, wearable sensors, radio frequency identification (RFID) and geographical positioning systems (GPS), to monitor and track the actor's behavior and environment <ref type="bibr" target="#b24">[25]</ref>. In the literature, several sensor technologies have been used for monitoring, e.g. wearable sensors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, object-based monitoring <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>, and combined wearable and object based sensing <ref type="bibr" target="#b28">[29]</ref>. In wearable sensor-based monitoring, sensors are attached to individuals and used to monitor activities related to human physical movements, e.g. climbing stairs, walking and typing. Conversely, in object-based monitoring common objects are embedded with sensors and the actor's interactions with these objects tracked. Activity inference is performed on these interactions. Sensor-based activity recognition is capable of addressing some of the privacy concerns associated with vision-based recognition. However, it is important to note that no approach can be said to be superior to the other. Instead, the nature of the application will dictate the choice of method to use and whether or not to combine both vision-based and sensor-based techniques.</p><p>Based on the second criteria (i.e., activity modeling, representation and inference), there are two main approaches to activity recognition: the so-called data-driven and knowledge driven activity recognition. Data-driven activity recognition uses state-of-the-art machine learning techniques that elicit activity models from existing datasets. Typically, probabilistic and statistical reasoning is used to perform activity inference. A number of techniques and tools have been investigated, e.g. hidden Markov models (HMM) <ref type="bibr" target="#b6">[7]</ref>, Dynamic Bayes Nets (DBNs) <ref type="bibr" target="#b6">[7]</ref>, naive Bayes <ref type="bibr" target="#b29">[30]</ref>, nearest neighbour <ref type="bibr" target="#b26">[27]</ref>, support vector machines (SVM) <ref type="bibr" target="#b11">[12]</ref>, conditional random fields (CRF) <ref type="bibr" target="#b30">[31]</ref>, and multiple eigenspaces <ref type="bibr" target="#b13">[14]</ref>. Also, in <ref type="bibr" target="#b31">[32]</ref>, the authors explored the concept of an adaptive activity recognition chain (adARC). Unlike other data-driven approaches that generate generic design-time activity models, adARC supports autonomous adaptation to allow activity models to be generated and adapted with continuous use. Data-driven activity recognition techniques, e.g. HMMs and DBNs, are considered better in handling noisy, uncertain and incomplete data. For instance, by using probabilities, heuristics about the domain used to deal with uncertainty, e.g. activity X is more likely than activity Y, can be captured and modeled. In addition approaches such as HMM, DBN and CRF that inherently support handling of temporal information provide better support for modeling and recognition of complex activity patterns, e.g. interweaved activities. The main criticism that has been made on these techniques is that it could become computationally expensive to learn activity models when there is a large diversity of activities. Furthermore, the learnt models may have to be revised due to variations in an actor's behavior and environment. In addition, probabilities may not be expressive enough to intuitively model certain domain knowledge concepts.</p><p>In contrast, knowledge-driven activity recognition is inspired by logical modeling and reasoning. It uses logic-based knowledge representation to model activities and sensor data and then exploits logical reasoning for activity inference. In the literature, several methods have been explored such as the use of event calculus <ref type="bibr" target="#b32">[33]</ref>, description logics and lattice theory <ref type="bibr" target="#b15">[16]</ref>, description logics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>, temporal reasoning and active databases <ref type="bibr" target="#b33">[34]</ref>, spatiotemporal reasoning <ref type="bibr" target="#b20">[21]</ref> and spatiotemporal and context reasoning <ref type="bibr" target="#b16">[17]</ref>. Given that knowledge-driven activity recognition is grounded on logic theory, the resulting activity models are semantically clear and elegant. In addition, it is easy to capture and model domain structure and heuristics. The main criticism is that these approaches handle uncertainty poorly. In addition, it is difficult to find the most optimal model of the activities and sensor data. Further, since learning ability is not inbuilt into these models, adaptive capability must be deliberately implemented to deal with variations in activities attributed to changes in the actor's behavior and their environment. Of particular interest is the use of ontologies in activity recognition -an area of growing interest <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. The key idea is to use ontologies to describe the actor's environment and provide semantics that automated systems can easily reason with. In <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, the authors use ontologies during activity modeling to describe items in the domain. The resulting models are processed for activity inference using independent algorithms, i.e. finite state machines <ref type="bibr" target="#b8">[9]</ref> and probabilistic inference <ref type="bibr" target="#b7">[8]</ref>, respectively. However, in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref> the authors describe a different approach that integrates both modeling and activity recognition into a unified framework. The presented approach models ADLs using web ontology language (OWL) <ref type="bibr" target="#b34">[35]</ref>-based ontologies and then use description-logic based reasoning <ref type="bibr" target="#b35">[36]</ref> to infer ongoing activities. They use ADL ontologies to represent explicit activity models by creating description based models of the activities and sensor data. By combining activity modeling and recognition in this way, agents can perform knowledge-based intelligent processing to infer activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Temporal segmentation in activity recognition</head><p>The issue of sensor data segmentation in knowledge-driven activity recognition has received little attention in existing work. For instance, in <ref type="bibr" target="#b19">[20]</ref> the authors present a knowledge-driven activity recognition approach but do not provide the details of the method for sensor data selection. Despite showing that ontology-based activity recognition is feasible, the absence of a suitable method for sensor data selection makes the presented method difficult to replicate. However, another knowledge-driven activity recognition method presented in <ref type="bibr" target="#b16">[17]</ref> uses competing hidden Markov models to segment a sensor data stream. The selected sensor data is used to perform spatiotemporal and context reasoning for activity recognition. They use a variable window length and the window moves over a sequence of observations. The main weakness of this approach is that it requires a pre-existing dataset to determine the optimal size of the time windows and the segmentation rules that it uses. Since the same individual or different individuals may perform the same activity in many different ways, this method will be difficult to reuse. In addition, the derived optimal window lengths have to be revised to deal with new situations.</p><p>The use of one minute time slices to evaluate the effectiveness of ontology-based activity recognition is presented in <ref type="bibr" target="#b17">[18]</ref>. Sets of sensor data are selected every minute for activity inference. The work is based on the van Kasteren dataset <ref type="bibr" target="#b36">[37]</ref> and the main limitation is that the ontology used is modeled on and closely tied to the dataset making it difficult to re-use. In addition, the fixed-size time slices used may lead to a huge computational expense since the activity inference engine is forced to periodically sample the data stream even when no new sensors have been activated. Furthermore, its ability to support real-time activity recognition has not been discussed. Ontologies and video are used for activity recognition in <ref type="bibr" target="#b8">[9]</ref>, whereby an ontology-based knowledge base supports the recognition of human activities from video sequences. The ontology models human activities, in terms of entities, environments and interactions, and creates semantic links between events and activities. Vision-based techniques are used to select the input data for activity inference based on a pre-existing dataset. Our work is modeled on a dense sensing framework, and as a result the computer vision based techniques used in <ref type="bibr" target="#b8">[9]</ref> are less suitable. However, the authors adopt a method to select the input data used in activity recognition which is comparable to the problem that we aim to address, i.e., to select a subset of sensor data for activity inference. From the foregoing, it is clear that in most knowledge-driven activity recognition work the method used to select sensor data is either non-existent or, at best, ad hoc. There is a need to develop a systematic approach that can be applicable in different knowledge-driven activity recognition approaches to help segment and then aggregate sensor data.</p><p>In the data-driven activity recognition community, the problem of sensor data segmentation has been widely explored <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. The notion of time windows is adopted to provide a basis for handling time-dependent data, e.g., the sensor data stream. However, some sensor data segmentation approaches use static sliding windows to segment the data stream <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37]</ref> while others use dynamically derived time window lengths <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. The notion of time slices is used in <ref type="bibr" target="#b36">[37]</ref> to derive segments used to perform activity recognition. In <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b11">[12]</ref>, a sliding window method is used to derive features used in activity inference by the proposed algorithms. The time windows used in <ref type="bibr" target="#b0">[1]</ref> are made to have a 50% overlap. The main criticism for static sliding windows is that incorrect lengths can truncate an activity instance or overlap activity instances leading to recognition failure. The work described in <ref type="bibr" target="#b31">[32]</ref> uses the concept of ''null-class'' rejection whereby the activity classifier provides probabilistic outputs which are applied to sliding windows. When the probability of an activity is below a learned threshold, it is considered a ''null-class'' and the activity is rejected. The sensor data stream is thus segmented and activities that are not considered ''null-class'' activities are reported. The main difference is that our work uses domain knowledge and temporal knowledge in segmentation while <ref type="bibr" target="#b31">[32]</ref> uses learned thresholds. To learn the threshold values for activities requires pre-existing datasets to be used. On the other hand, our work is motivated by a scenario where it is difficult to obtain pre-existing data sets that can represent varied activities. Due to the above issues, our work is closely related to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> whereby time window parameters are varied.</p><p>The work in <ref type="bibr" target="#b29">[30]</ref> uses temporal information (i.e., the average activity duration) to set different length values to the time windows at initialization; however, once a time window is activated its length cannot be dynamically modified. This can cause the time window to overlap the end of one activity and the beginning of the next one, thus leading to recognition failures. The notion of time contiguity in sensor data is used together with location context to segment sensor data from state change sensors in <ref type="bibr" target="#b37">[38]</ref>. Any noted changes in location context between two consecutive sensors are used to signify a break point. The break point then helps identify the start and end of segments on which activity inference is eventually performed by evidential fusion <ref type="bibr" target="#b39">[40]</ref>. This approach will work well when consecutive activities occur in different locations; however, segmenting the sensor data stream arising from the same location may prove difficult if the break point is not detected. Since recognition is only attempted on a segment after the start and end points are identified, this approach assumes that the user always performs activities correctly, making diagnosis that is necessary for activity assistance difficult or impossible.</p><p>Although our work is in the area of knowledge driven activity recognition, it is slightly similar to the work in <ref type="bibr" target="#b38">[39]</ref> since it uses dynamic windows. In <ref type="bibr" target="#b38">[39]</ref>, the length of the window is dynamically derived at runtime based on the occurrence of specified low-level events, e.g. the change of sensor state. The key difference with our work is that while they use primitive events to dynamically manipulate time window parameters, we use high level context information such as activity duration, and the current status of recognition resulting from a high-level activity inference event. As a result, our approach is able to utilize high quality knowledge in sensor data segmentation Following the above discussion, we contend that by capturing some temporal features of sensor data and by extension that of activities, the sensor data stream can be broken into segments for real-time activity recognition. We use dynamically varied time windows based on the temporal information of activities to support this segmentation and activity recognition is then performed on these segments. The main strengths of the proposed approach are that it is: (1) systematic; (2) simple, well-defined and easy to implement; and (3) not specific to any user or dataset; hence, can be replicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Real-time continuous activity recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ontological activity modeling</head><p>Ontological modeling allows the creation of logical activity models to formally conceptualize the Smart Home domain. Activity models are based on objects, environmental elements, events, and interrelationships (e.g. ''is-a'' and ''part-of'' relations) between activities. Ontological activity modeling encodes activities as ADLs and uses ontologies to represent this knowledge for use in activity recognition. The resulting activity models can be processed by an automated system, through semantic reasoning, to directly infer activities. The ADL ontologies capture the contextual information of activities in an SH domain.</p><p>Typically, inhabitants of a SH perform routine ADLs in specific locations, with certain objects, and at particular times. For instance, an inhabitant may prepare a glass of juice in the evening after dinner. This may involve the use of the fridge and the glass cupboard, both of which are located in the kitchen. This information is generally referred to as the context and can be associated with a specific activity. This contextual information, together with other information, e.g. the different ways a person performs the same activity, is part of domain knowledge. By using activity modeling, this domain knowledge can be represented as ADL ontologies.</p><p>Ontological modeling allows ADL activities to be structured in a hierarchical tree with the most specific ADL descriptions represented as leaf concepts -all leaf concepts have no child classes. Each concept is associated with a number of role (property) restrictions. All child concepts inherit all the roles of their parent concepts but may specify further constraints. In this way, ontological ADL models can facilitate progressive activity recognition of both generic and specific activities. A generic activity refers to an ADL class that has associated descendant classes. On the other hand, a specific activity (the socalled leaf activity) is an activity with no descendant classes in the ontology. For instance, 'make drink' is a generic activity, while its descendants 'make tea' and 'make coffee' are specific activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The approach for continuous activity recognition</head><p>Continuous, real-time activity recognition helps to identify ongoing ADLs as they occur, thus offering the possibility to provide timely assistance for SH inhabitants. In ontology-based activity recognition, when an ADL is performed along a timeline, the contextual information associated with the ADL is captured incrementally and subsumption reasoning is used to infer the ongoing ADL. At the initial stages of an ADL, subsumption reasoning may only classify the contextual information to a generic ADL class. However, as more contextual information is obtained over time, and reasoning is continuously performed, it would be possible to recognize the specific ongoing ADL.</p><p>In a dense sensing based SH, contextual information is captured through a variety of sensors, with each sensor representing a particular view of the prevailing situation. From the activated sensors, an agent can infer physical and contextual entities, e.g. objects, locations, times, and events. For example, a pressure sensor can be attached to the sofa in the lounge. Given that this knowledge is explicitly encoded in the ADL ontology, whenever this pressure sensor is activated it is possible to infer that the inhabitant is in the lounge and is sitting on the sofa. Consequently, this allows the inference of an activity that occurs in the lounge while sitting on the sofa, e.g. reading a book or watching television. Since most ADLs require the fusion of data from multiple sensors over time to infer high-level activities, it is necessary to first aggregate a collection of sensor activations in order to generate a situation at a particular time point during activity recognition. For a more detailed description of the ontology-based activity recognition approach, we refer the interested reader to <ref type="bibr" target="#b3">[4]</ref> due to space limitations.</p><p>While work in <ref type="bibr" target="#b3">[4]</ref> described the rationale and algorithm for semantic reasoning for activity recognition, it did not present any details about sensor data segmentation that is critical for continuous real-time activity recognition. In this paper, we focus on extending the ontology-based approach in <ref type="bibr" target="#b3">[4]</ref> with a sensor data segmentation mechanism so as to support realtime activity recognition. Fig. <ref type="figure" target="#fig_0">1</ref> shows the three-layer architecture for the extended approach, namely context selection, iterative action inference, and activity recognition layers.</p><p>Whenever activities are performed, the incoming sensor data is received as a sensor data stream and segmented in the context selection layer. Context selection refers to the process by which the stream is divided into a set of fragments using temporal segmentation. In temporal segmentation, the stream is analyzed along a temporal dimension using the temporal properties (of both sensor data and activities) captured by time windows. This ensures that only those sensor activations occurring within a given time window are included in the segment. To achieve its goal, this layer uses activity monitoring and dynamic segmentation components. The activity monitoring component allows sensor data to be received, while dynamic segmentation component implements the temporal segmentation algorithm. We model the notion of a time window as a data structure made up of a number of parameters. Section 4 provides the formal model of the time window mechanism, provides a detailed description, and the algorithm that utilizes it in activity recognition.</p><p>The iterative action inference layer analyzes the segments of the data stream that are generated in the context selection layer to identify a collection of actions (also called low-level activities) associated with the activated sensors. Typically, a time window's sensor activations are processed against the ADL ontology to determine the ongoing primitive action, e.g. 'cup is used'. This action can be represented as context information in the ontology by a property assertion that is equivalent to the description: 'hasContainer property associated with unknown ADL activity X has value cup'. This process is repeated for all sensor activations that have so far been received in the time window. A collection of such low-level (simple) activities may combine to constitute the activity description of one or more high-level (complex) activities. The activity inference layer is responsible both for the inference of ongoing activities and the initiation of dynamic modification of time window parameters. To this end, it is made up of two main components, namely aggregation and high-level inference components. The aggregation component collects the individual property assertions from the iterative action inference layer together to derive the overall description of the current activity. The resulting activity description is passed on to the high-level activity inference component. The high-level activity inference component consists of activity inference and time window manipulator sub-components. The activity inference component uses the activity description, ADL ontology, and ontological reasoning to infer the ongoing ADL, e.g. 'make tea'. If a specific ADL is inferred, the recognition process is considered successful and the result is reported. Otherwise, a generic ADL is reported and the system will wait for additional sensors to be activated before attempting recognition again. In this way, ongoing ADLs can be progressively inferred. The system can dynamically initiate the shrinking or expansion of time windows whenever necessary through the time window manipulator component. The mechanism for shrinking and expansion is described in the next section. To ensure perpetual, real-time activity recognition, the entire process continues to run with new time windows being continuously and dynamically generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sensor data segmentation and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Characterization of segmentation and recognition</head><p>A key factor in continuous real-time activity recognition is how to select the set of activated sensor data to be aggregated for activity classification. In a typical Smart Home, sensors will be continuously activated and the resulting sensor data sequence needs to be broken down into fragments that can be mapped to specific ADL activities. To segment a sensor data stream, this work presents a number of scenarios and configurations that can be considered. The scenarios are divided into two main categories: overlapping and non-overlapping time windows. In overlapping time windows, two or more distinct time windows can share some activated sensors. On the other hand, whenever non-overlapping time windows are used, no single activated sensor is shared by two or more time windows.</p><p>Under each category, there are four scenarios to be considered. The first scenario (Fig. <ref type="figure" target="#fig_3">2</ref> In this case, the lengths of newly created time windows are dynamically derived at run-time such that the length of any new window is a multiple of that of the initial window (i.e., given the initial window has length w 0 , any new window will have the length set to a * w 0 , where a is a positive real number). Regarding both scenarios, a key challenge is how to choose optimal sizes at runtime. The third and fourth scenarios allow time windows to be dynamically shrunk and/or expanded at runtime as a result of activity inference. Scenario three (Fig. <ref type="figure" target="#fig_3">2</ref>  From the scenarios provided, it is clear that although the task of segmenting a sensor data stream with time windows is complex, there are various methods that can be used to achieve it. However, choosing the most suitable method for segmentation is a non-trivial task. Providing support for the different configurations described requires careful design of the time windows together with an appropriate choice of the parameters and strategies for manipulation. In the next sections, we present a time window based approach and algorithms that model and implement the presented scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sensor data segmentation mechanism</head><p>This paper presents a mechanism that uses time windows to decide on which sensor activations to use for activity inference. The mechanism utilizes a sensor data segmentation model that is modeled by a time window data structure. To implement the different configurations shown in Fig. <ref type="figure" target="#fig_3">2</ref>, the time window data structure provides various parameters. Some parameters can be preset but can remain unchanged or be varied, while others are dynamically set at runtime. The time window model and its parameters are described in the next section.</p><p>To illustrate the use of the time window model, the scenario in Fig. <ref type="figure" target="#fig_3">2</ref>(e) that allows windows to be shrunk and/or expanded was selected. The lines marked in the form TW-N' indicate that the corresponding initial time window has been shrunk or expanded. In the diagram, we have two windows that are not modified (TW-1, TW-4), two that are shrunk (TW-0, TW-2), and one window (TW-3) that is expanded. In the current work, only non-overlapping time windows are investigated. This is because we consider non-overlapping time windows sufficient in segmenting sensor data in single activity scenarios. While overlapping time windows, as modeled in Fig. <ref type="figure" target="#fig_3">2</ref>(c)(d)(g) and (h), are relevant to composite activity scenarios, segmentation for composite activity recognition is beyond the scope of this paper. During expansion, the time window's length is extended so as to accept further sensor data. This occurs whenever additional sensor data is required to successfully infer an activity but the pending window length would be inadequate to cover the given activity's duration as provided in the activity ontologies. In addition, a window can be expanded whenever a generic activity has been identified but the pending window length is inadequate and thus requiring additional time to successfully infer any of the specific descendants of the activity.</p><p>Conversely, during shrinking the time window is truncated before its preset length is exhausted. Typically, as soon as an ADL is recognized, the ADL ontology is used to determine whether additional sensor activations should be anticipated or not. In addition, the identified activity's duration information available in the activity ontologies can also be used to determine if the time window should be truncated. If there are no further sensor activations expected or the duration has been exhausted, the current time window is closed and a fresh time window activated. Alternatively, if it requires additional sensor activations or the duration has not been exhausted, then the time window will continue to be active until either its preset length is exhausted or further sensor activations are obtained. In addition, an assistive system can be invoked to provide some interventions, e.g. prompts and suggestions, to the user in an Ambient Assisted Living (AAL) environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Formal time window modeling and manipulation</head><p>We propose a formal time window model whose characteristics and operation are described below. We define a number of parameters to describe how the time window is manipulated. Significant parameters include start time, end time, window length, the enclosed collection of sensor data, and overlap, shrinking and expansion capabilities. Other parameters are used to provide a means for manipulating the time window data structure. Some (dependent) parameters (e.g. end time) are assigned dynamically during recognition processes, while the independent ones (e.g. window length) are preset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Definitions</head><p>Let:</p><p>• α: the start time for a time window • ω: the end time for a time window • w: the length of a time window</p><p>• Ω α : a time window whose start time is α.</p><p>• Ψ : sensor data set. This is a data structure for storing the set of sensor data belonging to a given time window.</p><p>• A: a vector of activity labels assigned to the time window after activity inference.</p><p>• γ : reasoning start mode. Used to determine when to trigger activity inference.</p><p>• ρ: time window factor. Used to derive the size of a new time window from the initial time window.</p><p>• µ: sliding factor. Used to determine the size of the slide applied to the active time window to move it over the sensor data stream.</p><p>• δ: change factor. Used to determine the magnitude used to expand or shrink the length of a time window. We can define a time window, Ω α , as a 9-tuple with nine properties: Ψ , α, ω, w, γ , ρ, µ, δ, and A as shown in the expression below:</p><p>Ω α : ⟨Ψ , α, ω, w, γ , ρ, µ, δ, A⟩.</p><p>(</p><p>The end time, ω, can be computed from the start time, α, and window length, w, as shown below:</p><formula xml:id="formula_1">ω = α + w.<label>(2)</label></formula><p>Given that a sensor activation arriving at time, t, is denoted by sa t ; Ψ can be defined below:</p><p>Ψ : {sa t |α ≤ t ≤ ω, for all t}.</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Time window manipulation</head><p>A number of operations can be performed on the time window model, namely sizing, activation, deactivation, sliding, shrinking, expansion and overlapping.</p><p>Sizing: This sets the initial length of the time window. To determine the length of time windows, let the length of the initial time window Ω α-0 , be set to w 0 . The length of each time window is delimited by a minimum size, w min , and a maximum size, w max . The values of w min and w max are obtained from activity duration information that is derived from prior domain knowledge. For instance, w min is set to the duration of the shortest activity, while w max is set to that of the longest activity plus some slack time. Typically, given the initial time window, Ω α-0 , then the length of any new window, Ω α-i , can be assigned using the formula below:</p><formula xml:id="formula_2">w i = ρ * w 0 , w min ≤ w i ≤ w max , i = 1, 2, . . . , n. (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>The value of ρ is chosen such that the resulting time window length lies between w min and w max . To minimize computational complexity, both w min and w max are set constant for all time windows. In this paper, the value of ρ = 1 is chosen to set the default size of all time windows as equivalent to the initial time window. However, the default size is dynamically varied through the shrinking or expansion operations.</p><p>Activation/Deactivation: A Boolean flag, activated, is used to activate or deactivate a time window. It is set to true to indicate that the time window is active and false to show deactivation. By default, the flag is set to false and must be changed to true so as to use the time window model to segment a sensor data stream. Before the deactivation operation, the state of the time window must be logged in a suitable storage.</p><p>Sliding: The sliding operation allows the shifting of the current time window by some factor in order to derive a new time window. To determine the criteria for sliding, a sliding factor, µ, is defined. The sliding factor is a value that satisfies the constraint 0 &lt; µ ≤ 1. In this way we can obtain the size of the slide that needs to be applied to the current time window. The slide determines by how much the start time for the current time window, Ω α-i , is shifted forward to determine the start time of the new time window, Ω α-j . Let the slide to be applied to the current window to obtain the start time for the next time window to be denoted by Θ i . We can compute Θ i by using the following formula:</p><formula xml:id="formula_4">Θ i = µ * w i , i = 0, 1, 2, . . . , n.</formula><p>(</p><p>Given that the start time for the current time window is denoted by α i and that of the succeeding time window by α i+1 , we can apply the slide to derive α i+1 using the formula:</p><formula xml:id="formula_6">α i+1 = α i + Θ i .<label>(6)</label></formula><p>Overlapping: This refers to the process of having two or more time windows active at the same time. By choosing a sliding factor value less than one (µ &lt; 1) two time windows are made to overlap. A value of one (µ = 1) means that the time windows are successive and non-overlapping. Furthermore, by examining the time windows being created and activated we can identify two properties:</p><p>Property (1): Two time windows, Ω α-i and Ω α-j , i &lt; j, and Ω α-i starts before Ω α-j , are said to overlap in time if the start time, α j , of Ω α-j is less than the end time, ω i , of Ω α-i . This is denoted by the expression below:</p><formula xml:id="formula_7">α j &lt; ω i . (7)</formula><p>Property (2): Two time windows, Ω α-i and Ω α-j , i &lt; j, are said to overlap in activations if property (1) is true and the intersection between the data sets in the two time windows is non-empty, i.e., at least one sensor activation belongs to both time windows. The non-emptiness is denoted by the following expression:</p><formula xml:id="formula_8">Ψ i ∩ Ψ j ̸ = Ø. (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>Whenever property ( <ref type="formula" target="#formula_1">2</ref>) is satisfied, sensor activations can be used in two or more time windows during activity inference. This scenario can be used to facilitate the recognition based on complex activity patterns such as interleaved and concurrent activities.</p><p>Shrinking/Expansion: Given that the time window size is preset, an ADL may be identified before the expiry of the window. Whenever this happens, the time window length may be reduced dynamically (this is called shrinking). Conversely, whenever it can be established that the time window may expire before an ongoing ADL is conclusively identified, the window length can be increased (this is called expansion) to keep the time window active for a little longer. To perform the shrinking operation and to compute the new window length, w ′ i , we define the shrink time, st. Shrink time, dynamically derived at runtime, refers to the time at which the decision to shrink the current time window is made. The new window length, w ′ i , is then computed using the formula below:</p><formula xml:id="formula_10">w ′ i = st -α i .<label>(9)</label></formula><p>Similarly, to expand we define the length of expansion, exp. The new window length is then computed using the formula below: </p><formula xml:id="formula_11">w ′ i = w i + exp. (<label>10</label></formula><formula xml:id="formula_12">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Algorithms for continuous activity recognition</head><p>Once sensor activations are received, then by using the ADL ontology, each of them is converted into the corresponding ADL property assertion and added to a set of property assertions. At an appropriate time, the reasoning engine attempts to recognize the ongoing ADL. There are three modes that can be used to trigger reasoning. In the first mode, γ = 0 and each time a sensor is activated, activity recognition is performed. Using the second mode γ = 1 and reasoning occurs periodically at regular intervals during the length of the time window. The intervals can be set at configuration time. The activity inference engine should check the existence of new sensor activations before further attempts at reasoning. This requires that the current and previous sensor states are tracked to determine whether or not fresh activations have been obtained. Finally, using the third mode γ = 2 and reasoning occurs only at the expiry of the time window. The success of this mode depends entirely on optimal choice of time window lengths. At the deactivation of each time window, all sensor activations used within it are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Recognition algorithms</head><p>In order to support continuous, real-time activity recognition we modify the algorithm in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. To manipulate the time window data structure the ontology-based activity recognition algorithm is enhanced with temporal segmentation ability as shown in Figs. <ref type="figure" target="#fig_4">3</ref> and<ref type="figure" target="#fig_5">4</ref>. Fig. <ref type="figure" target="#fig_4">3</ref> shows the pseudo-code for the time-window based algorithm, and Fig. <ref type="figure" target="#fig_5">4</ref> shows the pseudo-code for the ontological reasoning component of the algorithm.</p><p>Three operation modes are proposed to support the manipulation of time windows and to demonstrate the impact of dynamic manipulation. The first is no-shrink-no-expand, whereby the time window is effectively static and the size is not reduced or extended at runtime. The second is shrink-only mode for which the length of a time window can be reduced but cannot be extended. Finally, in shrink-and-expand mode, the length of a time window can be extended, reduced or both. The shrink-only and shrink-and-expand modes show the use of dynamic time windows. The mode used can be specified at configuration time. The pseudo-code in Fig. <ref type="figure" target="#fig_5">4</ref> supports both shrinking and expansion whenever shrink-only or shrink-andexpand modes are selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Algorithm for shrinking time window</head><p>A time window can be shrunk under two conditions. Firstly, if all property assertions needed to describe a leaf activity have been specified, then the recognition system can truncate the current time window and spawn a new window. This is done by checking the activity description derived from the time window against the restrictions that have been defined for the given ADL class in the ontology. Secondly, the recognition system can choose to truncate the current time window if it determines that the ongoing activity has exhausted its duration and hence it is least likely to generate further sensor activations. Both cases have been captured by the pseudo-code in Fig. <ref type="figure" target="#fig_6">5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Algorithm for expanding time window</head><p>A time window can be expanded under two conditions. Firstly, given that a leaf activity has already been identified but the pending time window length is inadequate to complete the activity description, the window is expanded to allow additional activated sensors to be obtained. Secondly, given that a leaf activity has not been identified, information about the currently identified generic activity is used to determine how much additional time would be needed to recognize its subclass that has the longest duration. The pseudo-code in Fig. <ref type="figure" target="#fig_7">6</ref> depicts this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation and evaluation</head><p>We have applied the proposed approach to develop a SH-based activity recognition system. The system is implemented using Java language and a raft of semantic technologies and tools. Specifically, we developed ADL ontologies based on OWL-DL <ref type="bibr" target="#b34">[35]</ref> using Protégé editor <ref type="bibr" target="#b40">[41]</ref> as shown in Fig. <ref type="figure" target="#fig_8">7</ref>. The ADL ontology captures information about ADLs such as ADL concepts, hierarchical relationships among concepts, property restrictions for ADLs and contextual information, and sensor related concepts.</p><p>To support ontological reasoning we have used Pellet <ref type="bibr" target="#b41">[42]</ref> OWL reasoner, accessed through application programming interfaces (APIs) in Java, to provide reasoning capabilities for activity inference. In addition, we implemented the time window based segmentation model as part of an activity recognition module. Fig. <ref type="figure" target="#fig_9">8</ref> shows four system interfaces of the implemented system. Firstly, on the top-left it shows the interface that displays the set of all sensors that are currently  deployed in the environment. Secondly, the top-right of Fig. <ref type="figure" target="#fig_9">8</ref> the configuration window that is used to add sensors to SH environment, set the initial time window parameters, and to initialize the activity monitoring task. Thirdly, the dialogue for choosing whether to monitor sensor in real-time or to play back from a file is shown on the Finally, the list of all sensors that have been activated during a particular time window as well as the status of activity recognition is provided at the bottom-right of Fig. <ref type="figure" target="#fig_9">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment design</head><p>To evaluate and demonstrate the feasibility of the proposed approach, we developed a synthetic data generator that can be used to generate synthetic ADL data. This provides ADL data items that possess the necessary temporal information and allows us to quickly evaluate the feasibility of the developed approach before deployment in a real-world environment. Another advantage is that we are able to test the approach on different datasets. To facilitate data generation, we specified 'seed' ADL patterns at the start. Each seed ADL pattern is described by a sequence of ADLs. The synthetic ADL data generator then derives different permutations of these patterns. To select the permutation to use, it uses a random number generator. In this way, each permutation is given an equal chance of being considered an ADL pattern during dataset generation.</p><p>To generate the synthetic ADL data, eight typical ADLs related to meals (e.g. MakeTea, MakeCoffee, MakeChocolate, and MakePasta), hygiene (e.g. HaveBath, BrushTeeth, WashHands) and recreation (e.g. WatchTelevision) were used. In addition, to ensure that data is rich with useful temporal information, synthetic ADL data is generated corresponding to three time periods per day: morning (6am-9am), midday (12pm-2pm), and evening (6pm-10pm). To facilitate this, the time period to which the ADL can be performed is specified when adding possible seed ADLs to the synthetic data generator. Similarly, when specifying seed patterns the time period to which an ADL pattern belongs is provided. Finally, the transition time (in seconds) between ADLs is specified for each ADL pattern. For example, the pattern MakeTea-0, BrushTeeth-600, implies that MakeTea is the first ADL in the pattern, while the ADL BrushTeeth will occur 600 s after MakeTea is completed. Currently, we have made the assumption that only one ADL can be performed at the same time. As a result, sensors implying interleaved and concurrent ADLs cannot be activated during data generation.</p><p>For each ADL considered, we provide one or more patterns of sensor activations. Given that the same ADL may be performed in a variety of ways; these patterns depict the various ways. To incorporate more temporal meaning, each sensor in a pattern is activated after a given amount of time after the immediately preceding activation. By implication, this ensures that duration information of ADLs is included when synthetic ADL data is generated. The text SensorObj@n in a pattern means that the sensor object labeled SensorObj is activated n seconds after the preceding sensor object is activated. As an example, MakeTea is represented by the following patterns of activated sensors.</p><p>• Pattern#1: KitchenDoorObj@0, KettleObj@20, CupObj@180, TeaObj@20, MilkObj@20, SugarObj@20 (duration = 260 s) • Pattern#2: KitchenDoorObj@0, KettleObj@20, CupObj@180, TeaObj@20, MilkObj@20 (duration = 240 s) • Pattern#3: KitchenDoorObj@0, KettleObj@20, CupObj@180, MilkObj@20, TeaObj@20, SugarObj@20 (duration = 260 s) Each sensor pattern is captured in a collection data structure; with a collection available for each ADL. Similar sensor activation patterns are specified for the other ADLs too. To select the sensor pattern for ADL, the synthetic data generator uses a random number generator to randomly the index of the sensor pattern for the relevant ADL from the relevant collection. This eliminates bias and gives each pattern a fair chance being selected. To test the time window approach and associated algorithms, a simulation tool has been built to mimic the activation of sensors in a dense sensor based deployment. The simulation tool plays back the synthetic ADL data generated as described above and feeds the sensor data to the activity recognition system as if the sensor activations are occurring in real-time. As the data is played back, the recognition engine tries to identify the ongoing ADL and displays the status on the interface shown at the bottom-right of Fig. <ref type="figure" target="#fig_9">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Time-window model configuration</head><p>To carry out the experiments, the duration of the default time window is initially set to a value slightly greater than the longest ADL -in the experiments the longest ADL by duration is MakePasta. The reasoning start mode (γ ) is set to zero (0) so that activity inference is attempted each time sensor activation is obtained. The sliding factor (µ) is set to one <ref type="bibr" target="#b0">(1)</ref> to indicate that time windows are consecutive and non-overlapping. The time window factor (ρ) is set to one (1) so that each time window is by default the same size as the initial window. Finally, the change factor (δ) is computed at runtime during shrinking and expansion operations. Similarly, the other parameters (i.e. start time (α), end time (ω), sensor data set (ψ ) and the vector of activity labels (A) are dynamically computed at runtime).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Ground-true synthetic ADL data</head><p>To facilitate analysis, we generated synthetic ADL data for four weeks based on the eight ADLs above. The dataset contains a total of 154 ADL activities and a summary of the ADLs is provided in Table <ref type="table" target="#tab_0">1</ref>. Three variables are used to describe the dataset. These are: (1) % in-pattern ADLs-describes proportion of the ADL that appear in ADL patterns that have at least two ADLs; (2) % standalone ADLs-describes the proportion of the ADL that appear in single-ADL patterns and; (3) the total number of times a given ADL occurs in the dataset. Generally, ADLs that participate in many ADL patterns (i.e., MakeTea, MakePasta, BrushTeeth, HaveBath and WatchTelevision) have more instances. Conversely, those that appear in just one ADL pattern (i.e., MakeCoffee, MakeChocolate and WashHands) typically have just a few instances. Using the real-time activity recognition system, we played back these synthetic ADL data and present the results in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Experiment results</head><p>In order to evaluate the performance and therefore the feasibility of the approach, we used the metric accuracy. Accuracy measures the correctness of the algorithm, i.e. the ability of the algorithm to return correct results. We compute the accuracy of the recognition performance and provide the results in Table <ref type="table">2</ref>. The accuracy is computed from the values of true positive (tp), false positive (fp), true negative (tn), and false negative (fn) using the formula: accuracy = (tp + tn)/(tp + fp + tn + fn).</p><p>(11) We report results for three experiments and the first experiment was to evaluate recognition performance for static time windows, i.e., given that time windows cannot be shrunk or expanded. The results are shown in Table <ref type="table">2</ref>. The second experiment evaluated recognition performance when shrink-only is enabled. The results are presented in Table <ref type="table" target="#tab_1">3</ref>. Finally, the third experiment evaluated the performance given that shrink-and-expand is selected. The results are shown in Table <ref type="table">4</ref>. The second and third experiments relate to dynamically manipulated time windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Recognition accuracy</head><p>As can be seen in Table <ref type="table">2</ref>, the recognition accuracy of MakeTea, MakePasta, MakeCoffee, MakeChocolate and WashHands is quite encouraging and attests to the feasibility of the presented approach. However, it is important to note that the recognition accuracy for BrushTeeth, HaveBath and WatchTelevision is low compared to the other ADLs. This can be attributed to the fact that these ADLs occur in very few standalone patterns; instead they mostly appear in sequential patterns. Given that the time window does not dynamically vary once created, sensor data belonging to more than one ADL in the pattern may be merged within a time window, thus leading to poor recognition accuracy.</p><p>Results in Table <ref type="table">4</ref> show that there is a significant improvement on overall recognition accuracy. However, compared to Table <ref type="table">2</ref>, there is a reduction in the accuracy for MakePasta and a corresponding increase for BrushTeeth, HaveBath and WatchTelevision. Similarly, results in Table <ref type="table" target="#tab_1">3</ref> indicate that the overall recognition accuracy was highest compared to the Tables <ref type="table">2</ref> and<ref type="table">4</ref>. Just like in Table <ref type="table">4</ref>, the recognition accuracy of MakePasta reduced while that of BrushTeeth, HaveBath and WatchTelevision increased. However, despite the reduced recognition accuracy observed for MakePasta in both in Tables <ref type="table" target="#tab_1">3</ref> and<ref type="table">4</ref>, there is an overall improved average accuracy. The direct comparison of recognition accuracy per ADL is shown in Fig. <ref type="figure">9</ref>. In addition, Fig. <ref type="figure" target="#fig_0">10</ref> shows a direct comparison of average recognition accuracy.</p><p>In all experiments, the activity recognition system recognized all the instances of all standalone ADLs. The reduced recognition accuracy was only observed regarding the in-pattern instances. As a result, we have reason to believe that the performance of activity recognition regarding BrusthTeeth, HaveBath and WatchTelevision may have been affected by the fact that they occur with other ADLs as indicated in Table <ref type="table" target="#tab_0">1</ref>, and more so as subsequent ADLs in the ADL patterns. In addition, the transition times from one ADL to another in an ADL pattern could also have made it possible for sensor data belonging to two distinct ADLs to be aggregated into one description, resulting in non-recognition. Another reason is the fact that several variants of ADLs were generated in the dataset. Whenever shrinking was allowing, a window could be shrunk before all the sensor activations associated with an ADL are obtained, hence the activations that arrive later could be merged with subsequent activations thus causing recognition failures. In the no-shrink-no-expand case, the recognition failure could be attributed to the chosen time window sizes.</p><p>We believe that by handling transitions between adjacent activities the recognition accuracy can be improved. This explains better recognition accuracy when shrinking and expansion are allowed. However, to minimize the failures whenever shrinking and expansion are supported, we believe that explicit relationships between ADLs in a pattern should be defined. This should involve a characterization of additional temporal relationships (i.e. qualitative temporal relations) for the ADLs that could occur in patterns. An interesting finding from the comparison of experiment results in Fig. <ref type="figure" target="#fig_0">10</ref> is that shrink-only had the best recognition accuracy. The shrink-and-expand case is the next best performer. We attribute lower recognition accuracy of shrinkand-expand compared to shrink-only to the fact that the maximum duration was used to derive time window lengths, thereby favoring activities with longer duration at the expense of those with shorter durations. However, we believe that by incorporating information about how inhabitants perform activities that can be captured through continuous use will improve the recognition accuracy of both shrink-only and shrink-and-expand mode. In future we plan to integrate the activity learning and model evolution approaches described in <ref type="bibr" target="#b42">[43]</ref> so that feedback from how an inhabitant performs tasks is used by the recognition system for adaptation.</p><p>The average accuracy for all the experiments is above 83% using the provided dataset as well as with other datasets that we generated. This demonstrates that the approach is feasible in supporting real-time activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Computational complexity and deployment</head><p>Computational complexity for DL reasoning has been well studied with regards to decidability, completeness, and soundness of the reasoning algorithms <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. Due to the development of advanced tableau <ref type="bibr" target="#b44">[45]</ref> and hyper-tableau <ref type="bibr" target="#b45">[46]</ref> algorithms, the performances of existing DL reasoners, e.g. Fact++, RacerPro, Pellet, and HermiT, have substantially improved.</p><p>As a result, current semantic data infrastructures can support classifications and queries in seconds for knowledge-bases that contain millions of triples. As a result, we believe that due to these performance improvements, the ontology based solution will execute in good time and optimize memory use. Regarding deployment, while the current implementation is based on a stand-alone desktop environment, we believe that a service-oriented architecture would make it possible to deploy in other environments, e.g. smartphones, too. However, the development and evaluation of the deployment architecture is outside the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>This paper presented an approach based on dynamically varied time windows to support sensor data segmentation for use in continuous, real-time activity recognition. It characterizes activity recognition and sensor data segmentation from which it formally defines a time window based segmentation model. The paper has detailed the rationale and operation algorithms of the model in the context of knowledge-driven activity recognition. In addition, different scenarios regarding dynamic manipulation of time windows were discussed. The model allows rich temporal information associated with sensor data and activities to be exploited in real-time activity recognition. The implementation of a prototype to evaluate the approach was also described. The prototype consists of a synthetic ADL data generator, ADL ontology, sensor data simulator for ADL data playback, and a real-time activity recognition system. To establish the feasibility of this approach, this paper has presented evaluation results from experiments. Accuracy was chosen as an evaluation metric and the resulting average accuracy has demonstrated the feasibility of the approach. The average recognition accuracy was lowest, at 84%, when noshrink-no-expand mode was activated. It was highest, at over 91%, when shrink-only mode was enabled. This proved that it is beneficial to dynamically manipulate time windows at runtime. In future, we plan to investigate richer models for capturing temporal information to address more complex temporal relationships among sensor data and activities. This would address problems with low recognition accuracy noted especially during the recognition of activities that occur in patterns. Also, we plan to investigate the use of feedback or user profiles to aid the dynamic manipulation so as to improve performance. In addition, we plan to develop and evaluate a framework that deploys the recognition solution in a serviceoriented architecture able to work in varied Smart Home client devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of real-time activity recognition approach.</figDesc><graphic coords="6,147.21,53.13,253.80,222.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) &amp; (c)) uses fixed-sized time windows, whereby all time windows are created of the same size (i.e., given the initial time window has length w 0 , any newly created window will also have the length set to w 0 ). The second scenario (Fig.2(b) &amp; (d)) uses variable-sized time windows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(e) &amp; (g)) is a variation of scenario one because it uses fixed-sized time windows. Similarly, scenario four (Fig.2(f) &amp; (h)) is a variant of scenario two. A key challenge is the criteria for triggering shrinking or expansion of a time window. The resulting eight distinct configurations are depicted in Fig.2(a)-(h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Representation of sensor data segmentation scenarios. (a) Fixed size, no overlap (w 0 = w 1 = w 2 = w 3 ); (b) Dynamic sizing, no overlap; (c) Fixed sizing, with overlap (w 0 = w 1 = w 2 = w 3 ); (d) Dynamic sizing, with overlap; (e) Fixed sizing plus shrinking and/or expansion, no overlap (w 0 = w 1 = w 2 = w 3 = w 4 ); (f) Dynamic sizing plus shrinking and/or expansion, no overlap; (g) Fixed sizing plus shrinking and/or expansion, with overlap (w 0 = w 1 = w 2 = w 3 = w 4 ); (h) Dynamic sizing plus shrinking and/or expansion, with overlap.</figDesc><graphic coords="7,57.80,54.60,437.10,521.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Time-window segmentation based ontological activity recognition algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Ontological reasoning algorithm. Source: Adapted from [4].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Listing to shrink a time window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Listing to expand a time window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Fragment of ADL ontology.</figDesc><graphic coords="12,83.60,241.93,380.78,290.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. System configuration and status display interfaces.</figDesc><graphic coords="13,50.99,53.12,438.77,332.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Comparison of recognition accuracy per activity.</figDesc><graphic coords="16,165.71,338.61,242.31,117.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of synthetic ADL datasets.</figDesc><table><row><cell>ADL name</cell><cell cols="2">Description of dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>% Standalone ADLs</cell><cell></cell><cell>% In-pattern ADLs</cell><cell cols="2">Total instances</cell></row><row><cell>MakeTea</cell><cell>10%</cell><cell></cell><cell>90%</cell><cell>41</cell><cell></cell></row><row><cell>MakeCoffee</cell><cell>100%</cell><cell></cell><cell>0%</cell><cell>4</cell><cell></cell></row><row><cell>MakeChocolate</cell><cell>100%</cell><cell></cell><cell>0%</cell><cell>3</cell><cell></cell></row><row><cell>MakePasta</cell><cell>10%</cell><cell></cell><cell>90%</cell><cell>29</cell><cell></cell></row><row><cell>BrushTeeth</cell><cell>30%</cell><cell></cell><cell>70%</cell><cell>19</cell><cell></cell></row><row><cell>HaveBath</cell><cell>10%</cell><cell></cell><cell>90%</cell><cell>28</cell><cell></cell></row><row><cell>WashHands</cell><cell>100%</cell><cell></cell><cell>0%</cell><cell>6</cell><cell></cell></row><row><cell>WatchTelevision</cell><cell>10%</cell><cell></cell><cell>90%</cell><cell>24</cell><cell></cell></row><row><cell>Num. Of ADLs</cell><cell></cell><cell></cell><cell></cell><cell>154</cell><cell></cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Recognition accuracy without shrinking or expansion.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ADL</cell><cell cols="2">Values from dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TP</cell><cell>FP</cell><cell>TN</cell><cell>FN</cell><cell>Accuracy</cell></row><row><cell>MakeTea</cell><cell>39</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0.951</cell></row><row><cell>MakeCoffee</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>MakeChocolate</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>MakePasta</cell><cell>28</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0.966</cell></row><row><cell>BrushTeeth</cell><cell>14</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>0.737</cell></row><row><cell>HaveBath</cell><cell>19</cell><cell>0</cell><cell>0</cell><cell>9</cell><cell>0.679</cell></row><row><cell>WashHands</cell><cell>6</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>WatchTelevision</cell><cell>10</cell><cell>0</cell><cell>0</cell><cell>14</cell><cell>0.417</cell></row><row><cell>Average accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.844</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Recognition accuracy with only shrinking enabled.</figDesc><table><row><cell>ADL</cell><cell cols="2">Values from dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TP</cell><cell>FP</cell><cell>TN</cell><cell>FN</cell><cell>Accuracy</cell></row><row><cell>MakeTea</cell><cell>39</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0.951</cell></row><row><cell>MakeCoffee</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>MakeChocolate</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>MakePasta</cell><cell>26</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>0.897</cell></row><row><cell>BrushTeeth</cell><cell>17</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0.895</cell></row><row><cell>HaveBath</cell><cell>24</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>0.857</cell></row><row><cell>WashHands</cell><cell>6</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>WatchTelevision</cell><cell>18</cell><cell>0</cell><cell>0</cell><cell>6</cell><cell>0.750</cell></row><row><cell>Average accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.919</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Recognition accuracy with both shrinking and expansion enabled.</cell><cell></cell><cell></cell></row><row><cell>ADL</cell><cell cols="2">Values from dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TP</cell><cell>FP</cell><cell>TN</cell><cell>FN</cell><cell>Accuracy</cell></row><row><cell>MakeTea</cell><cell>39</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0.951</cell></row><row><cell>MakeCoffee</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>MakeChocolate</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>MakePasta</cell><cell>21</cell><cell>0</cell><cell>0</cell><cell>8</cell><cell>0.724</cell></row><row><cell>BrushTeeth</cell><cell>17</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0.895</cell></row><row><cell>HaveBath</cell><cell>24</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>0.857</cell></row><row><cell>WashHands</cell><cell>6</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.000</cell></row><row><cell>WatchTelevision</cell><cell>16</cell><cell>0</cell><cell>0</cell><cell>8</cell><cell>0.667</cell></row><row><cell>Average accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.887</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Activity recognition from user-annotated acceleration data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Intille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference, PERVASIVE 2004</title>
		<title level="s">Lecture Notes in Comput. Sci.</title>
		<imprint>
			<biblScope unit="volume">3001</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning situation models in a smart home</title>
		<author>
			<persName><forename type="first">O</forename><surname>Brdiczka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reignier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activity recognition for the smart hospital</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tentori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Favela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A knowledge-driven approach to activity recognition in smart homes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using ontologies in case-based activity recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Florida Artificial Intelligence Research Society Conference, FLAIRS-23</title>
		<imprint>
			<date type="published" when="2010-05-19">May 19, 2010-May 21, 2010</date>
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rule-based activity recognition framework: challenges, technique and learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Storf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 3rd International Conference on Pervasive Computing Technologies for Healthcare -Pervasive Health</title>
		<imprint>
			<date type="published" when="2009-04-01">2009. 2009. April 1, 2009-April 3, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition by aggregating abstract object usage</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth IEEE International Symposium on Wearable Computers</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Applying ontology and probabilistic model to human activity recognition from surrounding things</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kunito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Isoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Information Processing Society of Japan</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An ontology based approach for activity recognition from video</title>
		<author>
			<persName><forename type="first">U</forename><surname>Akdemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th ACM International Conference on Multimedia, MM &apos;08, October</title>
		<imprint>
			<date type="published" when="2008-10-31">2008-October 31, 2008</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="709" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine recognition of human activities: a survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1473" to="1488" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human activity recognition based on the blob features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hanqing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on multimedia and expo</title>
		<imprint>
			<date type="published" when="2009-06-28">2009. 2009. June 28, 2009-July 3, 2009</date>
			<biblScope unit="page" from="358" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable recognition of daily activities with wearable sensors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Blanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Symposium</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="50" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time recognition of physical activities and their intensities using wireless accelerometers and a heart rate monitor</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Haskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh IEEE International Symposium on Wearable Computers</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised discovery of structure in activity data using multiple eigenspaces, in: 2nd International Workshop on Locationand Context-Awareness, LoCA</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="151" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evidence fusion for activity recognition using the dempster-shafer theory of evidence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Information Technology and Applications in Biomedicine</title>
		<imprint>
			<date type="published" when="2009-11-04">2009. November 4, 2009-November 7, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A smart home agent for plan recognition of cognitively-impaired patients</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouzouane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computers</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marsland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Guesgen</surname></persName>
		</author>
		<title level="m">International Conference on Spatial Information Theory</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="9" to="20" />
		</imprint>
	</monogr>
	<note>Spatio-temporal and context reasoning in smart homes</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Is ontology-based activity recognition really effective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Riboni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pareschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Radaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoMoRea&apos;11, 8th IEEE Workshop on Context Modeling and Reasoning</title>
		<meeting>CoMoRea&apos;11, 8th IEEE Workshop on Context Modeling and Reasoning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A translation approach to portable ontology specifications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Acquisition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ontology-based activity recognition in intelligent pervasive environments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Web Information Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="410" to="430" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BehaviorScope: real-time remote human monitoring using sensor networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 International Conference on Information Processing in Sensor Networks, IPSN 2008</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="533" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Activity recognition: approaches, practices and trends</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Khalil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Activity Recognition in Pervasive Intelligent Environments</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Nugent</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Biswas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hoey</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam-Paris</addrLine></address></meeting>
		<imprint>
			<publisher>Atlantis Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision-based human tracking and activity recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bodor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th Mediterranean Conference on Control and Automation</title>
		<meeting>11th Mediterranean Conference on Control and Automation</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-camera human activity monitoring</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Drenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Somasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent and Robotic Systems: Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="5" to="43" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards a consistent methodology for evaluating activity recognition model performance</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L M</forename><surname>Van Kasteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J A</forename><surname>Kröse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pervasive Computing 2010 Workshop on how to do Good Research in Activity Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Activity classification using realistic data from wearable sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Parkka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ermes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Korpipaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mantyjarvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peltola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Activity and location recognition using wearable sensors</title>
		<author>
			<persName><forename type="first">Seon-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inferring activities from interactions with objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Fishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hahnel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wearable activity tracking in car manufacturing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stiefmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ogris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Troster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="42" to="50" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Activity recognition in the home using simple and ubiquitous sensors</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<biblScope unit="page" from="158" to="175" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extracting places and activities from GPS traces using hierarchical conditional random fields</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robotics Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="119" to="134" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The adARC pattern analysis architecture for adaptive human activity recognition systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Calatroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Troster</surname></persName>
		</author>
		<idno>08/01</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using event calculus for behaviour reasoning and assistance in a smart home</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mulvenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="81" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The use of temporal reasoning and management of complex events in smart homes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Augusto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Nugent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">778</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">OWL: a description logic based ontology language</title>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Principles and Practice of Constraint Programming -CP 2005</title>
		<imprint>
			<date type="published" when="2005-10-01">October 1, 2005-October 5, 2005</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Practical reasoning for expressive description logics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tobies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference, LPAR&apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="161" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accurate activity recognition in a home setting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Kasteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Ubiquitous Computing, UbiComp</title>
		<imprint>
			<date type="published" when="2008-09-21">2008. September 21, 2008-September 24, 2008</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Partitioning time series sensor data for activity recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Nugent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Information Technology and Applications in Biomedicine</title>
		<imprint>
			<date type="published" when="2009-11-04">2009. November 4, 2009-November 7, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A dynamic sliding window approach for activity recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ortiz Laguna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Olaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borrajo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on User Modeling, Adaptation and Personalization</title>
		<imprint>
			<date type="published" when="2011-07-11">2011. July 11, 2011-July 15, 2011</date>
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evidential fusion of sensor data for activity recognition in smart homes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mulvenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scotney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pervasive and Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="236" to="252" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The Protege Ontology Editor and Knowledge Acquisition System</title>
		<ptr target="http://protege.stanford.edu/" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Stanford Center for Biomedical Informatics Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pellet: OWL 2 Reasoner for Java</title>
		<author>
			<persName><forename type="first">Parsia</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="http://clarkparsia.com/pellet/" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ontology-enabled activity learning and model evolution in smart homes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Okeyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sterritt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 7th International Conference on Ubiquitous Intelligence and Computing</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Description logics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Baader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Knowledge Representation</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Van Harmelen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Lifschitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Porter</surname></persName>
		</editor>
		<imprint>
			<publisher>Elseiver</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="135" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A tableau decision procedure for SHOIQ</title>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hypertableau reasoning for description logics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shearer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="165" to="228" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
