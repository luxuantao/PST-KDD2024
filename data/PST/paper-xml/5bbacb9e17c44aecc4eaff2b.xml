<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEMI-SUPERVISED TRAINING FOR IMPROVING DATA EFFICIENCY IN END-TO-END SPEECH SYNTHESIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-08-30">30 Aug 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SEMI-SUPERVISED TRAINING FOR IMPROVING DATA EFFICIENCY IN END-TO-END SPEECH SYNTHESIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-08-30">30 Aug 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1808.10128v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tacotron</term>
					<term>text-to-speech</term>
					<term>semi-supervised learning</term>
					<term>pre-training</term>
					<term>data efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although end-to-end text-to-speech (TTS) models such as Tacotron have shown excellent results, they typically require a sizable set of high-quality &lt;text, audio&gt; pairs for training, which are expensive to collect. In this paper, we propose a semi-supervised training framework to improve the data efficiency of Tacotron. The idea is to allow Tacotron to utilize textual and acoustic knowledge contained in large, publiclyavailable text and speech corpora. Importantly, these external data are unpaired and potentially noisy. Specifically, first we embed each word in the input text into word vectors and condition the Tacotron encoder on them. We then use an unpaired speech corpus to pre-train the Tacotron decoder in the acoustic domain. Finally, we fine-tune the model using available paired data. We demonstrate that the proposed framework enables Tacotron to generate intelligible speech using less than half an hour of paired training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recent advances in end-to-end text-to-speech (TTS) have shown great promise. We are now able to produce natural prosody with high audio fidelity using a much simplified voice building pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. However, such models typically require a sizable dataset consisting of high-quality &lt;text, audio&gt; training pairs, which are expensive and timeconsuming to collect. Requiring large amounts of data also hinders their applicability in low-resource settings.</p><p>This work aims to improve the data efficiency for endto-end TTS training by leveraging large-scale, publicly available, and unpaired text and speech data. Unpaired data are plentiful and relatively easy to collect. Specifically, we propose a simple yet effective semi-supervised framework for training Tacotron <ref type="bibr" target="#b0">[1]</ref>, a recently proposed end-to-end TTS model. We propose to transfer the textual and acoustic representations learned from unpaired data to Tacotron in an unsupervised manner. This is then followed by a fine-tuning step using only a small amount of paired data to learn the alignment between the two representation domains.</p><p>In this preliminary study, we first identify the data requirement of a baseline Tacotron, i.e., the least amount of training data needed for a baseline Tacotron to produce intelligible speech. We then show that a Tacotron enhanced with the proposed framework is able to produce intelligible speech using less amount of data. Finally, we study different configurations for incorporating the framework. For evaluation, we perform both objective and subjective tests.</p><p>There exists previous work studying the application of unsupervised and weakly supervised learning for TTS <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Related to our work, for example, <ref type="bibr" target="#b6">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS <ref type="bibr" target="#b6">[7]</ref>. These studies consider learning methods within the traditional TTS paradigm, however. This work, by contrast, examines them within end-to-end TTS, and specifically targets the data efficiency problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED APPROACH</head><p>We use a baseline Tacotron architecture specified in <ref type="bibr" target="#b7">[8]</ref>, where we use a GMM attention <ref type="bibr" target="#b8">[9]</ref>, LSTM-based decoder with zoneout regularization <ref type="bibr" target="#b9">[10]</ref> and phoneme inputs derived from normalized text. We use Griffin-Lim <ref type="bibr" target="#b10">[11]</ref> as the inversion algorithm to convert the predicted spectrograms to waveforms, as our main focus is to enable Tacotron training on small data instead of producing high-fidelity audio. Using Griffin-Lim allows much faster experiment cycles.</p><p>The two main building blocks of Tacotron are the encoder and the attention-based decoder. At a high level, the encoder takes a source text as input and produces sequential representations of it; the decoder is then conditioned on the text representations to generate corresponding acoustic representations (spectrogram frames), which are then converted to waveforms. In the baseline Tacotron, the model is trained from scratch where all network weights are randomly initialized, and both the text and acoustic representations are learned from the given (parallel) training data. Below, we introduce our approach to inject external textual and acoustic knowledge to bootstrap the encoder and decoder, respectively. Fig. <ref type="figure">1</ref>. Illustration of conditioning encoder on pre-trained word vectors. The left side shows the locations of encoder input and encoder top, where the word vectors can be incorporated via a conditioning module. The right side illustrates the conditioning module, where we show two methods of conditioning. '&lt;sil&gt;' denotes silence (e.g. space). In method 2, the dash lines correspond to the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conditioning the encoder on pre-trained word vectors</head><p>The goal of the encoder is to extract robust sequential representations of text. However, for a baseline Tacotron, the only training signal comes from the text data in the &lt;text, audio&gt; pairs, and the extracted representations are usually not rich enough when there's only a small amount of text.</p><p>We propose to exploit the textual knowledge contained in large text corpora, which typically contain millions to billions of words. From these large text corpora, one can train real-valued word vectors that contain the meanings of the words <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> or language models that model grammatical and semantic context <ref type="bibr" target="#b13">[14]</ref>. These word vectors can be added as auxiliary to a TTS model to convey additional textual knowledge not learnable from the original text data.</p><p>To expose this additional knowledge to the encoder, we first embed each word in the input text into a word vector, and add the word vector sequence on one of two locations (illustrated in the left side of Figure <ref type="figure">1</ref>): "encoder input" representing the phoneme embedding sequence, or "encoder top" representing the final encoder output sequence. While both conditioning locations allow the encoder to access the pre-trained word vectors, the choice of the conditioning location is an important design choice, which we study in experiments. For convenience, we call both encoder input and encoder top features conditioning location features. Due to the fact that the word vectors and conditioning location features might have different time resolutions (different sequence lengths), below we propose two ways of combining them (illustrated in the right side of Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Word vectors concatenation</head><p>The first conditioning approach concatenates the word vector at the first phoneme of the corresponding word and replicates the word vector across all phonemes in the word. Take input text "Thank you" as an example. The phoneme inputs are 'th', 'a', 'ng', 'k', '&lt;sil&gt;', 'y', 'uu' (same rule applies to character inputs), where '&lt;sil&gt;' denotes silence (e.g. space). The word vector of "Thank" is appended to the phoneme embeddings of phonemes 'th', 'a', 'ng', and 'k'. This approach can be thought of as an hard attention mechanism where the alignment is pre-determined by the position mapping between words and their phonemes in the input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">A conditioning attention head</head><p>If we consider the phrase "Thank you", it's possible that the semantics of "Thank" can help the encoder to generate more robust representation for "you". However, the first approach never exposes the word vector of "Thank" to the encoder when it's processing the phoneme embeddings of "you". Our second approach attempts to resolve this by applying a separate attention head between word vectors and conditioning location features. It takes each conditioning location feature as the attention query to generate the corresponding context vector, which is a weighted sum of the word vectors. The context vector and the conditioning location feature are then concatenated together for further processing. This enables each conditioning location feature to extract and gather information it needs from all word vectors. In this work, we use a simple tanh based additive attention <ref type="bibr" target="#b14">[15]</ref>.</p><p>Since the encoder weights are still trained from scratch with random initialization, we refer to this approach as encoder conditioning for the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Decoder pre-training</head><p>In a baseline Tacotron system, the decoder needs to simultaneously learn acoustic representations and their alignments with the text representations extracted by the encoder. To reduce the workload of the decoder, we propose using an independent speech data source to pre-train the decoder, such that it is initialized by a pre-learned acoustic representation. During pre-training, the decoder acts as a next-step frame predictor with teacher forcing. Since the only objective is to predict an acoustic frame from the previous one, this step does not require text transcripts. In this stage, we simply keep the encoder weights frozen and replace the attention context vectors by zero vectors. This forces the decoder to learn an autoregressive model of acoustics at the frame level.</p><p>After the decoder is pre-trained, we fine-tune the entire model (including both encoder and decoder) using paired data. By pre-training, the decoder no longer needs to learn the acoustic representations from scratch and can thus focus more on learning the alignment between text and acoustic representations.</p><p>A potential source of error of our simple approach is that there is a model mismatch between decoder pre-training and model fine-tuning: during pre-training, the decoder is only conditioned on the previous frame; while during fine-tuning, it is additionally conditioned on the text representations from the encoder. Despite such a mismatch, we found decoder pre-training still helpful. In addition, we found that the pretrained Tacotron converges much faster than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We conduct experiments to demonstrate the effectiveness of our framework. We use an internal single-speaker US English dataset for training (fine-tuning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data requirements of the baseline Tacotron</head><p>To improve Tacotron's data efficiency, first we need to understand its limit. We'd like to answer the following question: what is the maximum amount of data N that could almost never successfully train a baseline Tacotron to produce intelligible speech? To find out N , we gradually decrease the amount of data used for training a baseline Tacotron from about 40 hours to about 12 minutes and listen to the synthesized speech on unseen phrases. As can be heard on our demo page, we estimated that using between 10 and 40 hours of data produces almost equally good synthesis, and using between 3 and 10 hours of data causes minor degradation but still sounds very good. However, when there are only about 24 minutes of data, the model fails to produce intelligible speech. When there are only 12 minutes of data, the model outputs gibberish that is impossible to understand. It's important to note that the transcripts in the 12 minutes data already cover all phonemes, therefore the failure is not simply due to phoneme coverage.</p><p>Therefore, in the next section, we focus on demonstrating the effectiveness of our semi-supervised framework using only 24 minutes of paired data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results on small data</head><p>Our encoder conditioning and decoder pre-training approaches can be applied to Tacotron independently or jointly. We denote the model that only incorporates encoder conditioning as T-Enc, model that only incorporates decoder pre-training as T-Dec, model that incorporates both as T-Enc-Dec, and the baseline Tacotron as T-Base.</p><p>We measure the synthesis quality using both objective and subjective tests. For the objective metric, we use mel cepstral distortions (MCD) <ref type="bibr" target="#b15">[16]</ref>, which measures the distance between synthesis and ground truth in the mel cepstrum spacethe smaller the better. We use an evaluation set containing about 30 minutes (631 sentences) of unseen data. We found Table <ref type="table">1</ref>.</p><p>MCD between ground-truth audio and synthesis from 7 Tacotron variants (lower is better). For T-Enc, we include both the results of using NNLM (1st row) and W2V (2nd row) as the word embedding module; concatenation/attention and input/top denote the conditioning method and location, respectively. The best result is marked in bold. that our MCD results correlate well with our subjective perception. For subjective measurements, we ran a series of sideby-side preference tests using 1000 unseen phrases of different lengths.</p><p>For encoder conditioning, we used a neural network language model (NNLM) <ref type="bibr" target="#b13">[14]</ref> trained on English Google News 200B corpus from TensorFlow Hub as the word embedding module. The module maps each word to a 128-dimensional vector. We also tried word2vec (W2V) <ref type="bibr" target="#b16">[17]</ref> trained on the same corpus as the word embedding module.</p><p>For decoder pre-training, we used VCTK <ref type="bibr" target="#b17">[18]</ref>, a publicly available corpus containing 44 hours of speech from 109 speakers, the majority of which have British accents. Note that there is an accent mismatch between the decoder pretraining (multiple speakers with British accents) and finetuning (single speaker with US accent) datasets. As mentioned above, we only use the speech signals in VCTK but not their transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">MCD objective tests</head><p>The MCD results are shown in Table <ref type="table">1</ref>. We first compare the four configurations of encoder conditioning. Here we include the results of both NNLM and W2V word embeddings. We can see that models using NNLM always outperform their W2V counterpart. We speculate that this is because W2V only conveys word meanings but not the contextual or structural information, which is modeled in NNLM.</p><p>In terms of conditioning locations, we see that conditioning at encoder top always outperforms conditioning at encoder input. While feeding word vectors to the early parts of the network seems intuitive (as in encoder input conditioning), we believe it is not the best choice in the low-resource setting. If the encoder weights learned from small data are noisy, for example, they may "distort" well-trained word vectors. Therefore, conditioning word vectors at a higher layer (e.g. encoder top) may lead to better generalization.</p><p>In terms of conditioning method, we find that the simple concatenation method always outperforms using a separate attention head. We also attribute this to the limited training data: although a separate attention head offers more flexibility for learning the alignment, it also introduces more trainable parameters. In summary, the best configuration for encoder conditioning is to directly concatenate word vectors obtained from a pre-trained NNLM at the encoder top. We used this configuration for T-Enc for the rest of the experiments.</p><p>From Table <ref type="table">1</ref> we can see that T-Enc, T-Dec, and T-Enc-Dec all achieve much lower MCD than T-Base. Among them, T-Dec achieves the best result. However, T-Enc, T-Dec, and T-Enc-Dec achieve similar MCD results <ref type="bibr">(12.46, 12.09, 12.27, respectively)</ref>. As shown in side-by-side comparisons below, the raters did not strongly prefer one over the other two, either.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Side-by-side subjective tests</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results on other amounts of data</head><p>We also compare T-Base, T-Enc, T-Dec, and T-Enc-Dec trained on other amounts of data. In Figure <ref type="figure" target="#fig_1">2</ref>, each curve corresponds to a Tacotron variant, showing the relationship between the amount of paired data used for training that Tacotron variant and the MCD between ground-truth audio and synthesis from it. We can see that the largest gap between T-Base and the three semi-supervised systems occurs when using only 12 minutes (0.5 shards) of paired data. The gap keeps decreasing when the amount of data increases. This phenomenon is somewhat expected, because with more paired data, Tacotron relies less on external knowledge for learning representations and alignments. However, semisupervised Tacotron consistently achieves lower MCD than the baseline, which may indicate benefits beyond better data efficiency (e.g. improved prosody). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS AND DISCUSSIONS</head><p>We have proposed a semi-supervised training framework for improving data efficiency in end-to-end TTS. Our framework leverages large-scale, publicly available, and unpaired text and speech data to provide additional textual and acoustic knowledge to the Tacotron encoder and decoder, respectively. We have shown that our framework makes end-to-end TTS feasible in small-data regime. Specifically, a semi-supervised trained Tacotron can produce intelligible speech using just 24 minutes of paired training data. This promising result also provides some guiding principles for future data collection efforts for both single and multi-speaker TTS. While we used Tacotron as the TTS model in this study, we believe the framework is generally applicable to other end-to-end TTS models. This is only a preliminary work, and there is still much to be investigated. For example, we've been using phoneme inputs in this work and we'd like to understand the performance tradeoffs on grapheme inputs. For leveraging textual knowledge, instead of simply conditioning with word vectors, a likely more effective method is to initialize the entire encoder with a pre-trained bidirectional NNLM <ref type="bibr" target="#b18">[19]</ref>. For decoder pre-training, the model mismatch during pre-training and fine-tuning can be further studied. An analysis on what kind of information are extracted from external data and how they are actually used by Tacotron is also an important future work. Lastly, since the main focus of this work is to make end-to-end TTS feasible in small-data regime instead of producing high-fidelity audio, we only used Griffin-Lim as the waveform synthesizer. To produce high-fidelity speech with very little paired data, we still need to address the problem of adapting neural vocoders in the semi-supervised setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-by-side (SxS) preference tests, comparing T-Base against T-Enc, T-Base against T-Dec, T-Enc against T-Dec, and T-Enc-Dec against T-Dec. As we can see from the table, both T-Enc and T-Dec significantly outperform T-Base: in both tests, raters strongly preferred them over the baseline by more than 60%. Interestingly, the raters considered T-Dec, T-Enc, and T-Enc-Dec similarly preferable. The results of SxS tests are consistent to those of MCD objective tests, and both demonstrate the effectiveness of our semi-supervised framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. MCD results by increasing the amount of paired data.</figDesc><graphic url="image-2.png" coords="4,342.14,72.00,189.92,141.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of SxS subjective tests based on a 7-point rating scale. We report both rater preferences (in percentage) and p-values for each comparison.</figDesc><table><row><cell>Competing pair</cell><cell></cell><cell cols="2">Preference (%)</cell><cell>p-value</cell></row><row><cell></cell><cell cols="3">Former Latter Neutral</cell><cell></cell></row><row><cell>T-Base vs. T-Enc</cell><cell>3.3</cell><cell>65.1</cell><cell>31.6</cell><cell>1.07e-84</cell></row><row><cell>T-Base vs. T-Dec</cell><cell>3.2</cell><cell>61.8</cell><cell>35.0</cell><cell>3.47e-83</cell></row><row><cell>T-Enc vs. T-Dec</cell><cell>16.1</cell><cell>18.2</cell><cell>65.7</cell><cell>0.256</cell></row><row><cell>T-Enc-Dec vs. T-Dec</cell><cell>17.0</cell><cell>17.9</cell><cell>65.1</cell><cell>0.630</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>shows the results of the four side</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGEMENTS</head><p>The authors thank Daisy Stanton, Eric Battenberg, Soroosh Mariooryad, Yinfei Yang, and the Machine Hearing and Google Brain teams for their helpful feedback and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tacotron: Towards end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Clarinet: Parallel wave generation in end-to-end text-to-speech</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07281</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised learning for text-tospeech synthesis</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Watts</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>The University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining a vector space representation of linguistic context with a deep neural network for text-to-speech synthesis</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Workshop on Speech Synthesis</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sentence-level control vectors for deep neural network speech synthesis</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>King</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word embedding for recurrent neural network based TTS synthesis</title>
		<author>
			<persName><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing RNNs by randomly preserving hidden activations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mel-cepstral distance measure for objective speech quality assessment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kubichek</surname></persName>
		</author>
		<editor>PacRim</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirsten</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Donald</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
