<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DUET: A Compiler-Runtime Subgraph Scheduling Approach for Tensor Programs on a Coupled CPU-GPU Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
							<email>minjiaz@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zehua</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingqin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DUET: A Compiler-Runtime Subgraph Scheduling Approach for Tensor Programs on a Coupled CPU-GPU Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>DNNs</term>
					<term>Efficient Inference</term>
					<term>Compiler</term>
					<term>Heterogeneous Execution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) are currently the foundation for many artificial intelligence tasks. Existing DL frameworks and compilers often focus on optimizing DL inference speed against CPUs and GPUs in isolation while missing the opportunities to reap the benefits of aggregated computation power from both CPU and GPU. We show that there are DNNs that exhibit complex computation patterns, and different components might be suitable for executing on different types of devices to maximize performance gains. Based on this observation, we present a DNN inference engine, called DUET, that explores potential concurrent execution opportunities on heterogeneous CPU-GPU architecture for DNN inference. In particular, we introduce (i) a coarse-grained partitioning strategy that divides a DNN computation graph into subgraphs that retain high computational granularity with relatively low communication volume, (ii) a compiler-aware profiling method to include DL compiler optimization into the loop to improve scheduling decisions, and (iii) a greedy-correction subgraph scheduling algorithm that automatically maps the DNN computation to CPU and GPU without input from model developers. We evaluate DUET against several DNNs that exhibit complex model structures and compare its performance against existing DL frameworks and the state-ofthe-art DNN compiler. The experiment results show that DUET is much faster than existing DL frameworks and obtains 1.5-2.3 times and 1.3-6.4 times speed-ups against the optimized code by the state-of-the-art DNN compiler on GPU and CPU alone, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The development of deep neural networks (DNNs) is driving an explosion in multiple artificial intelligence domains, such as computer vision, natural language processing, and speech recognition. However, the ability of DNN comes at the cost of high computational complexity. In order to achieve high efficiency for DNN on existing general-purpose hardware such as CPU and GPU, popular deep learning (DL) frameworks such as TensorFlow <ref type="bibr" target="#b6">[10]</ref> and PyTorch <ref type="bibr" target="#b28">[32]</ref> resort to incorporate highly optimized kernels via hardware vendors such as Nvidia cuDNN <ref type="bibr" target="#b10">[14]</ref> or Intel MKL-DNN <ref type="bibr" target="#b1">[3]</ref> as backend. However, when a tensor operator is not supported by the pre-optimized vendor library, the computation efficiency decreases dramatically. To keep up with the pace of the fast innovations in DNNs, major players in the industry develop ? Work performed as a Microsoft intern. * Equal contribution.</p><p>highly optimized in-house operators <ref type="bibr" target="#b40">[43]</ref>, <ref type="bibr" target="#b44">[47]</ref>. However, this raises challenges because of the increasing complexity of tensor operations in DNNs and the volatility of DL algorithms. The complexity further increases when multiple versions of the same model have to be optimized to deploy on different hardware platforms.</p><p>Motivated by improving the agility of optimizing DNNs, there has been a great interest in developing automated frameworks to handle the unprecedented amount of innovations. Notably, recent research has developed neural network compilers, such as XLA <ref type="bibr" target="#b5">[9]</ref>, Halide <ref type="bibr" target="#b31">[35]</ref>, Glow <ref type="bibr" target="#b34">[38]</ref>, Tensor Comprehension <ref type="bibr" target="#b37">[41]</ref>, and TVM <ref type="bibr" target="#b8">[12]</ref>. Many of them use static analysis to find pipelined operations that can be fused together for improved performance and generate platformdependent code efficiently for models trained through popular DL frameworks.</p><p>While DL compilers produce highly optimized code for DNNs, existing works often focus on optimizing DNNs for CPUs, GPUs, and other accelerators in isolation. However, servers with coupled CPUs and GPUs are now ubiquitous in data centers and cloud environments. From a practical point of view, combining CPUs and GPU means that the computation power provided by the CPU and the GPU can be aggregated to improve DNN performance. Despite the potential advantages of this strategy, there is, to the best of our knowledge, no DL compiler that can reap the benefit of this approach.</p><p>On another aspect, while the standard DNNs consist of a linear task dependence chain with more or less homogeneous components, e.g., ResNet <ref type="bibr" target="#b13">[17]</ref> and VGG <ref type="bibr" target="#b35">[39]</ref>, there is a large number of DNNs that exhibit more complex model structures and diverse computation patterns. For example, some models exhibit higher fan-outs <ref type="bibr" target="#b25">[29]</ref>, <ref type="bibr" target="#b27">[31]</ref>, <ref type="bibr" target="#b41">[44]</ref>, implying more potential for parallel execution, while others contain sub-networks that have very different characteristics <ref type="bibr" target="#b9">[13]</ref>, where some components are more suitable to execute on one type of device than the other, implying potential benefits for heterogeneous execution. Existing DL compilers lack efficiency in handling these DNNs. For example, they often employ a Operatorsin-Sequence scheduling, where an operator runs, presumably using multiple threads, but one operator starts running only after the previous one finishes. They also lack the necessary abstractions to support hardware-conscious inference execu-tion over multiple devices to exploit all the resources available on a single server.</p><p>To address these limitations, we propose DUET, a DNN inference engine, which supports heterogeneity-conscious and compiler-aware DNN inference on a coupled CPU-GPU architecture. Our main contributions are the following:</p><p>? We make the case for heterogeneity-and compiler-aware DNN inference and present DUET, an engine design for concurrent execution of DNN computation on heterogeneous hardware. ? We introduce a coarse-grained partitioning strategy that allows the partitioned subgraphs to retain high computational granularity with relatively low communication volume.</p><p>? We show that involving the DL compiler in the heterogeneity optimization loop is beneficial for improving scheduling decisions. ? We introduce a greedy-correction subgraph scheduling algorithm, which automatically partitions the work of DNN inference between the CPU and GPU without input from model developers.</p><p>DUET builds on top of a unified graph-level IR and TVM <ref type="bibr" target="#b8">[12]</ref>, which is framework agnostic. It also makes the hardware implementation and the scheduling invisible to DL/ML practitioners, avoiding the developing cost of algorithms specialized for heterogeneous hardware. If a model does not have much intrinsic parallelism and cannot have performance improvements through the subgraph scheduling, DUET falls back to the original best-performing single device execution. The experimental results show that DUET achieves 1.3-6.4 times speed-ups on CPU and 1.5-2.3 times speed-ups on GPU, respectively, on three DNNs -Wide-and-Deep <ref type="bibr" target="#b9">[13]</ref>, Siamese Network <ref type="bibr" target="#b27">[31]</ref>, and MT-DNN <ref type="bibr" target="#b25">[29]</ref>, against the stateof-the-art DNN compiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DNN Inference</head><p>The general life-cycle of DNNs from its birth to deployment comprises two major stages. The first stage is the designing and the training of a DNN by a model scientist, with the primary goal of achieving the highest feasible accuracy. The second stage is the deployment of the pre-trained DNN to a target hardware (often on GPU or CPUs <ref type="bibr" target="#b16">[20]</ref>, <ref type="bibr" target="#b40">[43]</ref>, <ref type="bibr" target="#b44">[47]</ref>) to benefit end users, often done by a deployment engineer. These two stages are iterative processes: model scientists iterate until it reaches the target performance in terms of accuracy, whereas the deployment engineers iterate until the inference speed satisfies a latency SLA (e.g., often a few milliseconds per query). These two stages are most often separate processes, and optimizing the performance of DNNs to meet stringent latency targets can be very time-consuming. Therefore, the goal of DNN inference optimization is to minimize the model inference time while improving optimization agility to accelerate the overall deployment cycle. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DNN Compilation</head><p>There has been recent work on optimizing DNN performance through DL compilers, which emit optimized code that runs the model efficiently on a target hardware <ref type="bibr" target="#b5">[9]</ref>, <ref type="bibr" target="#b8">[12]</ref>, <ref type="bibr" target="#b31">[35]</ref>, <ref type="bibr" target="#b34">[38]</ref>, <ref type="bibr" target="#b37">[41]</ref>. These DL compilers work in the context of high-level DNN specifications, provided by deep learning frameworks such as TensorFlow <ref type="bibr" target="#b6">[10]</ref>, PyTorch <ref type="bibr" target="#b28">[32]</ref>, and MXNet <ref type="bibr" target="#b7">[11]</ref>. The optimization passes are applied at different stages of the compilation process. Fig. <ref type="figure" target="#fig_0">1</ref> shows a typical processing flow of these DL compilers, which consists of five layers: 1) front-end, 2) intermediate representation (IR), 3) graph-level optimization, 4) low-level optimization, and 5) back-end.</p><p>The front-end transforms high-level DSL of DNNs into compiler-specific IRs. These IRs are usually in the form of data flow graphs, in which each node represents a tensor operator, and each edge denotes the data dependency between operators. Based on these IRs, graph-level optimizations can be applied to fuse operations and optimize data layouts. Lowlevel optimizations perform hardware-dependent optimizations (e.g., tiling size, vectorization) against the fused operators to improve data locality and utilization of the target hardware (e.g., CPU or GPU). Finally, the back-end is responsible for generating hardware-dependent executable instructions using LLVM (for CPU) or CUDA (for GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CHALLENGES AND MOTIVATIONS</head><p>This section first discusses the challenges of performing DL inference on both CPU and GPU efficiently, and then it presents several studies that guided the design of the approach introduced in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Challenges</head><p>First, the CPU and GPU have very different device characteristics. CPU has a smaller number but faster cores, which typically have out-of-order execution with sophisticated branch predictions and deep cache hierarchies for reducing memory access latency. In contrast, GPU has many but slower cores, which are typically in-order and hide memory latency by switching between hardware threads. This dissimilarity leads to significant differences in their execution performance. Given that different NN components may have different computation patterns during inference, some components may execute significantly faster on one device than the other. As a result, executing even a small amount of work on the slower device may hurt performance. Moreover, although there is a big improvement in PCIe bandwidth, CPU-GPU communication may still create a performance bottleneck.  Second, the DNN execution is heavily affected by the model characteristics, framework, compiler optimization, and the hardware (i.e., the HW/SW stack). Without accurate information about a NN component's behavior on a target hardware, it is hard to optimally divide work between the CPU and GPU automatically. Previous work use FLOPs to approximate the execution time of operators <ref type="bibr" target="#b36">[40]</ref>. However, FLOPs is often an inaccurate proxy as operations with the same FLOPs can result in very different latencies on CPU and GPU, and factors such as compiler optimizations may change the execution time significantly as well. Recent work proposes to use profiling to characterize the performance of NN models <ref type="bibr" target="#b24">[28]</ref>. However, there have been fewer studies on how to leverage the profiled statistics to make informed DNN heterogeneous execution decisions.</p><p>Third, existing DL compilers often contain inefficiencies in parallel execution, making executing on both CPU and GPU challenging. For example, we observe that TVM <ref type="bibr" target="#b8">[12]</ref>, the state-of-the-art DL compiler, employs a sequential execution schedule of computation graphs, where the executable operators are executed synchronously in topological order. This scheduling strategy generally works well for models with a sequential chain of tasks, such as ResNet <ref type="bibr" target="#b13">[17]</ref>, VGG <ref type="bibr" target="#b35">[39]</ref>, and SqueezeNet <ref type="bibr" target="#b19">[23]</ref>. However, the structure of DNNs is more diverse and complex than just a sequential chain. For example, Fig. <ref type="figure" target="#fig_1">2</ref> shows the structure of a Wide-and-Deep network <ref type="bibr" target="#b43">[46]</ref>, which combines convolutional neural network (CNN), recurrent neural network (RNN), feed-forward neural network (FFN), etc. for heterogeneous contents encoding, and Fig. <ref type="figure" target="#fig_2">3</ref> shows a multi-task DNN model <ref type="bibr" target="#b25">[29]</ref> used for natural language understanding. Both contain independent submodules, yet existing DNN frameworks often miss the opportunity of executing these independent components concurrently without violating dependencies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Opportunities</head><p>Despite the aforementioned challenges, we still identify opportunities for heterogeneous computation of DNN inference on both CPU and GPU.</p><p>First, DNN computation exhibits diverse patterns, and GPU is not always faster than CPU for DNN inference. Although DNN training is often conducted on GPUs, which is throughput-oriented, CPU sometimes provides competitive performance compared to GPU for DNN inference. During inference, latency is the most important metric. The batch size at inference is often small (e.g., one or at most a few), which limits the amount of parallelism that can be leveraged by the massive cores on GPU. Furthermore, operators that contain sequential dependencies, such as recurrent neural networks (e.g., LSTM <ref type="bibr" target="#b15">[19]</ref>, GRU <ref type="bibr" target="#b11">[15]</ref>), are also difficult to parallelize on GPU across sequential steps. Figure <ref type="figure" target="#fig_3">4</ref> shows an example of the execution timeline of the Wide-and-Deep <ref type="bibr" target="#b9">[13]</ref> model using TVM on both CPU and GPU. As shown, although GPU takes less time to execute the model, the RNN execution time on GPU is much longer than on CPU. Simply executing the model on CPU does not work either because the CNN computation is extremely slow on CPU. Existing DL frameworks do not explicitly optimize for execution on both CPU and GPU, resulting in sub-optimal performance in this case.</p><p>Second, it is possible to make good use of aggregated computation capacity without incurring too much CPU-GPU inter-device communication delay. To validate the performance characteristics of the CPU-GPU communication, we use a micro-benchmark to measure the bandwidth and latency of CUDA point-to-point bulk transfer with respect to different message sizes. We conduct the experiment on a machine with Intel Xeon Gold 6152 CPU, Titan V GPU, connected through PCIe 3.0. From the results shown in Figure <ref type="figure" target="#fig_4">5</ref>, the latency increases almost linearly as the message size increases. This increased communication latency can limit the amount of communication links between CPU and GPU. However, given that the latency delay (e.g., from passing inputs/outputs of tensor operators) is often less than tens of milliseconds, which is orders of magnitude smaller than many NN operator (e.g., LSTM, CNN) execution time, we may minimize the impact from the communication overhead by retaining a relatively high computational granularity.</p><p>Finally, coarse-grained partitioning allows the DL compiler to perform more graph-level optimizations that improve single device efficiency. Graph-level optimizations have been demonstrated to be one of the best ways to achieve lower execution time on a single device. As an example, the fusion pass fuses multiple operators in the computation graph and generates a rewritten graph with fused ops to improve the temporal data locality and computation intensity. By exploiting the observation that graph-level optimizations help improve single device efficiency, we can minimize the effect of heterogeneity by partitioning the NN computation into coarsegrained subgraphs that may still benefit from the compiler graph-level optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMPILER-AWARE HETEROGENEOUS DNN INFERENCE</head><p>DUET allows DNN inference to take advantage of heterogeneous hardware present in modern servers by encapsulating heterogeneity and CPU-GPU parallelism. DUET is composed of three major parts, as shown in Figure <ref type="figure" target="#fig_5">6</ref>. The input of DUET is a pre-compiled DNN model. The first part is a coarsegrained graph partitioner, which divides the DNN computation graph into multiple subgraphs that still allow DL compiler to apply graph-level optimizations. The second part is a compileraware profiler, which is responsible for providing the runtime execution statistics of subgraphs based on compiler optimized code on target devices. The third part is a profiling-based online subgraph scheduler, which maps subgraphs to their specialized hardware based on the actual run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Coarse-Grained Multi-Phase Graph Partitioning</head><p>DNN inference computations is often transformed into compiler-specific IRs, in the form of directed acyclic graphs (DAG). For a given DAG G, each node v i ? G is an operator (e.g., matmul, softmax) in the DNN, and each edge (v i , v j ) ? G establishes a dependency between the output of operator v i and the input of operator v j . A valid execution  schedule of the DAG determines an execution order of its nodes that satisfies all the dependencies. In this work, we consider those valid schedules that are composed of phases: A phased schedule executes a DAG in a sequence of phases S 1 , S 2 , S 3 , ..., S t , ..., where each phase S t represents a non-overlapping subset of nodes and S = t S t consists of all nodes. There is a total ordering between phases such that if t &lt; t , then all nodes in S t must be executed before S t . We further divide a phase into two categories: i) If a phase consists of a subgraph with a sequential chain of operators, we call it a sequential phase. ii) Otherwise, if a phase contains multiple independent subgraphs, we call it a multi-path phase. Phases can be either sequential or multipath, and phase type(S i ) != phase type(S i+1 ). Figure <ref type="figure" target="#fig_6">7</ref> shows an example of a schedule with three phases, where S 1 and S 3 are sequential phases and S 2 is a multi-path phase.</p><p>According to the definition of the phased schedule, DUET partitions a DAG into multiple subgraphs <ref type="foot" target="#foot_0">1</ref> . A schedule S on a coupled CPU-GPU architecture includes a mapping for each subgraph to either CPU or GPU while satisfying dependencies. Our goal is to find a parallelization schedule S such that the per inference time is minimized.</p><p>When doing partitioning, we note that there are cases where multiple nodes consume the same input, i.e., a shared node in the DAG. We handle this situation by creating replicated placeholders in different branches but let them all point to the same input stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Profiler for Compiler-Optimized Subgraphs</head><p>Existing DL frameworks such as TensorFlow provides profilers to profile model execution time. However, these profilers assume a general, non-optimized compiler, and the profiled statistics are quite different from the true statistics from compiler optimized code, which do not help make informed decisions. Low-level profilers are provided by NVIDIA's nvprof [4] or Intel's VTune [2]. However, these are at the hardware and kernel execution level, which do not easily map to the execution of DNN subgraphs.</p><p>To obtain accurate execution time of each subgraph in the DAG, we take an end-to-end and build a profiler for compiler-optimized subgraphs. For a given subgraph, the profiler builds a micro-benchmark by treating that subgraph as a standalone DNN model and going through the DL compilation pipeline, including generating the target-dependent code through the back-end. The profiler then runs each microbenchmark on both CPU and GPU for several runs and records the information, including start and end time, the input/output data size, as well as the device running it. The execution time helps improve the scheduling decision. The input/output data size helps analyze the communication overhead. We note that profiling is only done during the offline phase and is therefore a one-time cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Greedy-Correction Subgraph Scheduling and Mapping</head><p>Given the profiling results, the next step decides how to map each subgraph to CPU and GPU. To better tackle the unpredictable variations at run time, we introduce a profilingbased online scheduling scheme: greedy-correction, where DUET prioritizes the subgraphs in the critical path of the DAG and then corrects the decision based on the actual running time. The approach consists of three steps: Step 1. Placing the critical path on the fastest device(s). The critical path is the path in the dataflow graph that has the longest computation time from source to sink vertex. Speeding up the processing of the critical path, therefore, would speed up the overall computation time of the dataflow graph. For each subgraph in the sequential phase, DUET prioritizes to select the device (e.g., between CPU and GPU) that the subgraph has a faster execution time to be on the critical path. For subgraphs within a multi-path phase, we select the shortest execution time between CPU and GPU as the cost of a subgraph, and add the subgraph that has maximum cost in that phase to the critical path.</p><p>Step 2. Greedily placing remaining subgraphs to CPU and GPU. At this step, we sort the subgraphs by their execution time on different devices. Then, for each independent subgraph in a multi-path layer, we make the placement decision in the order of the sorted subgraphs list. In each iteration, we place the subgraph to the device that minimizes the increase of the critical path.</p><p>Step 3. Correcting the placement decision by considering the communication cost. The first two steps provide an initial placement, but it might not be optimal because it does not consider the potential added communication overhead. In this step, we refine the previous decisions by performing series of experiments to understand which subgraph should be placed on which device, and how to arrange the computations so that the communication is optimized. In particular, we use iterative refinement to fine-tune the placement, in a way that swapping the subgraphs from one device to another to minimize the execution time. Figure <ref type="figure" target="#fig_7">8</ref> shows an example of an initial subgraph placement of GPU = {1,3,6} and CPU = {2,4,5}. The algorithm maintains and improves a schedule, In one pass, the algorithm selects a pair of subgraphs <ref type="bibr" target="#b3">(6,</ref><ref type="bibr" target="#b2">5)</ref> in GPU and CPU so that switching the paired subgraph from one side of the device to the other will optimize the performance (e.g., by avoiding some excessively high communication overhead between CPU and GPU). Notes that one of the subgraphs could be empty, which represents moving only a single subgraph to the other side. The correction step terminates when x round of swaps are performed without decreasing the execution time.</p><p>Note that we perform the third step for each multi-path layer, so it may need to run multiple times as we may have several multi-path layers in a model. Algorithm 1 provides the details of the subgraph scheduling. We remark that this idea of subgraph refinement is similar to the Kernighan-Lin refinement in existing literature on graph partitioning, which dates back to 1970's <ref type="bibr" target="#b23">[27]</ref>. Different from KL refinement, which finds equalsized subsets with the minimal edge-cut, the criterion we use is to minimize latency. We also note that it is possible to analytically decide the placement strategy based on the profiled subgraph computation and communication cost, similar to the dynamic programming based method <ref type="bibr" target="#b20">[24]</ref>. However, profiling communication in existing DL frameworks often introduces estimation errors, due to potential inefficiencies or unexpected behavior <ref type="bibr" target="#b29">[33]</ref>. Therefore, we take an approach that refines the subgraph placement based on actual end-to-end latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Executor</head><p>Once the scheduling decision has been made, DUET instantiates an executor to run the decided schedule, as shown in Figure <ref type="figure">9</ref>. The executor spawns two child processes to run compiled subgraphs concurrently on CPU and GPU<ref type="foot" target="#foot_1">2</ref> . Each process works in a busy loop: it polls for input data from its own synchronization queue, executes the corresponding subgraph, and triggers the subgraph's dependencies. The synchronization queue is implemented as a shared memory queue for high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTATION</head><p>DUET is built on top of TVM <ref type="bibr" target="#b8">[12]</ref>. The main reasons for choosing TVM are its wide adoptability for DL inference optimization and its support for multiple DL frameworks. However, the implementation can also be migrated to other deep learning compilers.</p><p>TVM uses Relay as an intermediate representation, which is a pure, expression-oriented language and employs the BNF Grammar <ref type="bibr" target="#b33">[37]</ref>, as shown in Listing 1. To facilitate graph partitioning and to debug, we perform a translation of this representation to an adjacency-list representation, as shown in Figure <ref type="figure" target="#fig_0">10</ref>. In particular, we iterate the Relay IR using the visitor pattern and obtain the inputs/outputs of each operator to build a graph with adjacency-lists. We apply phased partitioning against this adjacency-list graph, and we translate the subgraphs back to a sequence of Relay statements, which can be readily optimized through the TVM compiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setup</head><p>Environment. Our evaluation is conducted on a server with a 2.10 GHz Intel(R) Xeon(R) Gold 6152 CPU processor and an NVidia TITAN V GPU, connected through PCIe V3.0 interconnect. The server has 128GB RAM, running 64-bit Linux Ubuntu 16.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>Subgraph scheduling algorithm Input: A set of subgraphs obtained through the graph partitioning method described in Section IV-A. Output: Fine-tuned subgraphs placement S cpu and S gpu Step 1: Placing the critical path on the fastest device.</p><p>Step 2: Greedily placing the remaining subgraphs to CPU and GPU.</p><p>Step 3: Refining the subgraph placement decision. For each multi-path phase, assume that the subgraphs are separated into sets S cpu (subgraphs placed to CPU) and S gpu (subgraphs placed to GPU), T old ? measure latency(S cpu , S gpu ) do gain ? 0 S cpu ? S cpu , S gpu ? S gpu while S cpu = ? or S gpu = ? do find swapping pairs of subgraphs (s i ? S cpu , s j ? S gpu or movement of individual subgraph) that maximize the reduction of the expected latency</p><formula xml:id="formula_0">S cpu ? S cpu -s i , S gpu ? S gpu -s j T new ? measure latency(S cpu -s i +s j , S gpu -s j + s i ) if T new &lt; T old then S cpu ? S cpu -s i + s j S gpu ? S gpu + s i -s j T old ? T new end if gain ? max(T old -T new , gain)</formula><p>end while while gain &gt; 0 Workloads. The experiments compare the inference speed on three neural networks. The first one is Wide-and-Deep Network <ref type="bibr" target="#b9">[13]</ref>, which is trained with wide linear layers and deep neural networks together and can simultaneously have the benefits of memorization and generalization as well as heterogeneous contents encoding, with a lot of applications in recommender systems <ref type="bibr" target="#b12">[16]</ref>, <ref type="bibr" target="#b18">[22]</ref>, <ref type="bibr" target="#b45">[48]</ref>. We choose the opensourced PyTorch implementation based on <ref type="bibr">[7]</ref>, which has a structure that consists of wide linear layer, FFN, RNN, and CNN, as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The second is Siamese Network <ref type="bibr" target="#b27">[31]</ref>, which is a neural network with two independent RNN branches used for similarity ranking (e.g., similarities between queries and passages). We choose the TensorFlow implementation from <ref type="bibr" target="#b0">[1]</ref>.</p><p>The third one we use is a Transformer-based neural network called MT-DNN <ref type="bibr" target="#b25">[29]</ref>. The model is used for natural language understanding. It has a shared layer that consists of a lexicon encoder and a multi-layer bidirectional Transformer encoder. It then has an arbitrary number of task-specific output layers, which are independent from each other, as shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Table <ref type="table" target="#tab_1">I</ref> shows the model parameters used for the evaluation. We choose batch size of 1 to represent a common case in DNN inference. For RNNs, sequence lengths refer to maximum sequence length. To make reliable measurement, we run each  configuration 5000 times to report average and tail latency, with the warm-up time excluded.</p><p>Comparison framework. We compare the performance with TVM, which is widely accepted as the state-of-the-art compiler for DNN inference, on both CPU and GPU. We also include comparison with the original PyTorch <ref type="bibr" target="#b28">[32]</ref> or Tensor-Flow <ref type="bibr" target="#b6">[10]</ref> implementation. We let the framework to decide the appropriate number of threads used for computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DNN Inference Performance Comparison</head><p>Figure <ref type="figure" target="#fig_9">11</ref> shows the execution time of different models by PyTorch/TensorFlow, TVM, and DUET. We make the following observations. First, DUET achieves 1.5-2.3 times and 1.3-15.9 times speed-ups compared with TVM-GPU and TVM-CPU, respectively. DUET achieves speed-ups because it optimizes by leveraging the computation power from both CPU and GPU. Second, DUET is significantly faster than the inference time using existing DL frameworks, achieving 2.1-8.4(on GPU) times and 2.3-18.8(on CPU) times speedups than TensorFlow/PyTorch on GPU and CPU, respectively. DUET offers much higher performance improvements than DL frameworks, because it combines heterogeneous execution with DL compiler optimizations to maximize the gains on CPU-GPU.</p><p>Computation cost breakdown and placement decisions. To see why DUET obtains speed-ups compared with using just GPU or CPU, Table <ref type="table" target="#tab_1">II</ref> shows the computation cost (column 3 and 4) and final scheduling decision (column 5 and 6) of subgraphs (column 2) from the three models. The computation cost is collected through the DL compileraware profiler (Section IV-B), where a fixed, small number of profiling runs (e.g., 500) is sufficient to obtain statistically stable measurement. As shown, in the first line of Table <ref type="table" target="#tab_1">II</ref>, For Wide-and-Deep, the RNN subgraph takes 2.4ms on CPU but 6.4ms on GPU, while the CNN subgraph takes 14.9ms on CPU but only 0.9ms on GPU. Due to this heterogeneity compute pattern, running the model either on CPU or GPU entirely does not lead to the optimal latency. In contrast, DUET exploits hardware heterogeneity and place subgraphs to their suitable hardware, which minimizes the overall end-to-end latency.</p><p>Tail latency. For online inference, tail latency is as important, if not more, as the mean latency. To see if DUET provides steady speed-ups, we collect the 50th (P50), 99th (P99), and 99.9th (P99.9) percentile latency at batch size 1 from running TVM-GPU and DUET on the three models, as shown in Figure <ref type="figure" target="#fig_10">12</ref>. The results show that in most cases, the P99 and P99.9 latencies only increase moderately, and DUET obtains 1.3-2.4 times and 1.1-2.1 times speed-ups against TVM-GPU at P99 and P99.9, respectively. The speed-ups at P99.9 is slightly smaller, especially for MT-DNN, because the CPU-GPU interconnect communication adds additional performance variation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison of Scheduling Algorithms</head><p>In this part, we evaluate the effectiveness of DUET's subgraph scheduling algorithm by comparing the following schemes:</p><p>? Random: randomly assigns a subgraph to devices.</p><p>? Round-Robin: assigns subgraphs to CPU and GPU alternatively.</p><p>? Random + Correction: first randomly assigns subgraphs and then performs the correction (step 3) as described in Section IV-C. ? Greedy + Correction: our scheduling algorithm described in Section IV-C. We use Wide-and-Deep as an example. Fig. <ref type="figure" target="#fig_11">13</ref> presents the model execution time from the above schedules. We observe that both Round-robin and Random scheduling yield relatively higher execution time than the two correction-based scheduling algorithms. This is expected as the former two schedule subgraphs in an arbitrary topological order, where a global optimization strategy cannot be imposed. In contrast, the two correction-based scheduling algorithms yield much lower execution time, because they make subgraph placement decisions by taking into account the execution time of subgraphs and GPU-CPU communication cost. We choose greedy-correction because the greedy placement provides a good initialization for the correction algorithm, which requires fewer iterations for the correction algorithm to converge. To verify the correctness of our scheduling algorithm, we enumerate all possible schedules (which may not be always feasible given that finding the optimal schedule is NP-hard) to find the exact optimal schedule (Ideal). We empirically show that the greedy-correction methods finds the optimal schedule, at least when the number of subgraphs is relatively small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation of Model Variations</head><p>In practice, model scientists may constantly experiment with new model architectures, e.g., varying depths of a model, and it is important to have the agility to adapt to different architectures for DNN inference optimization. In this part, we evaluate how DUET performs when the model architecture of Wide-and-Deep changes.</p><p>Varying the stacked RNN layers. Figure <ref type="figure" target="#fig_12">14</ref> shows the comparison of the execution time, varying the number of stacked RNN to have 1, 2, 4, 8 layers. Compared with TVM-GPU and TVM-CPU, DUET achieves 2.3-2.5 times and 2.9-9.8 times speed-ups, respectively. The execution time of all configurations increases as the number of RNN layers increases, but the execution time on GPU increases more substantially. This is because RNN is relatively slow to execute on GPU. With GPU only, RNN computation becomes a dominant part of the execution time, creating a performance bottleneck as the RNN layers keep increasing. On the other hand, both TVM-CPU and DUET have a relatively slower increase in execution time as the stacked RNN increases its depths. However, DUET achieves a much lower execution time because it maps the CNN computation, which is slow to run on GPU, to GPU, resulting in overall reduced execution time.</p><p>Varying the CNN depths. Figure <ref type="figure" target="#fig_13">15</ref> shows the execution time of the same network, but varying the depth (e.g., 18, 34, 50, 101) of the ResNet encoder. This time, TVM-CPU observes a much larger latency increase as the ResNet increases its depth. This is because ResNet dominates the total execution on CPU. For DUET, the execution time remains almost the same when the depth of the ResNet is relatively small (e.g., <ref type="bibr" target="#b14">18,</ref><ref type="bibr" target="#b20">24)</ref>. This is because when CNN is shallow, the RNN computation on the CPU side dominates the total execution time, which can hide the computation of CNN on the GPU side. As the depth of ResNet keeps increasing, the  Varying the FFN depths. Figure <ref type="figure" target="#fig_14">16</ref> reports the comparison of the execution time, varying the number of hidden layers in FFN. As shown, the execution time does not change much as we increase the FFN depths. This is because FFN consists of mostly GEMMs, which have been highly optimized on both CPU and GPU. As a result, FFN only takes a very small amount of execution time despite its hidden depth increases.</p><p>Varying the batch sizes. Another factor that may affect the effectiveness of DUET is the batch size of the input data. Since TVM does not support dynamic batch size yet, we freeze the model with a fixed batch size range from 2, 4, 8, 16, 32. Overall, as shown in Figure <ref type="figure" target="#fig_15">17</ref>, the speed-ups from DUET are more pronounced when the batch size is small (e.g., 1.5 times speed-up at batch size 2) but gradually diminish as the batch size increases, compared with TVM-GPU. This is expected, because GPU is overall more suitable for large batch execution. However, as discussed earlier, batch size is often rather small for inference scenarios due to the stringent latency target.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Applicability to Traditional Models</head><p>So far, we have evaluated DNN workloads with complex structures that exhibit heterogeneity where existing DNN frameworks are less efficient to optimize. One may concern about how DUET would perform for existing models that have been well-optimized on a specific hardware. We conduct experiments on ResNet <ref type="bibr" target="#b13">[17]</ref> and the results are shown in Table <ref type="table" target="#tab_3">III</ref>. As shown, DUET offers the same performance as the best performing baseline, which is TVM-GPU in this case. This is expected, because not only ResNet has a relatively sequential structure but it also consists mostly of CNNs, which have been heavily optimized by TVM on GPUs. Given that the model is mostly sequential and does not present much heterogeneity, its partitioned subgraphs cannot be executed in parallel efficiently because it introduces additional communication overhead and the CPU does not make CNNs run faster. In this case, DUET falls back to the single-device execution mode and simply chooses the device where the model runs the fastest.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>DUET offers a solution that enables (i) heterogeneous execution, with compiler-aware subgraph scheduling, for (2) DNN inference. As such, we discuss the related work from these two independent research directions.</p><p>Heterogeneous DNN computation. Prior work studies using CPU memory as an extension of GPU memory to increase memory capacity for DNN workloads <ref type="bibr" target="#b14">[18]</ref>, <ref type="bibr" target="#b17">[21]</ref>, <ref type="bibr" target="#b21">[25]</ref>, <ref type="bibr" target="#b30">[34]</ref>, <ref type="bibr" target="#b32">[36]</ref>, <ref type="bibr" target="#b39">[42]</ref>. However, most of these work target at optimizing the training process, such as improving the training throughput with larger model/batch sizes, whereas DUET focuses on optimizing the DNN inference, where latency is the most important metric and batch size is often just 1.</p><p>Mirhoseini et. al. <ref type="bibr" target="#b26">[30]</ref> proposed to use reinforcement learning to learn efficient operator schedules for model parallelism. However, the scheduling is performed at the operator-level and assumes a general, non-optimized compiler, which prevents many graph-level compiler optimizations such as fusion. The use of RL, which requires to train a complex policy network with hyperparameter tuning, also makes it difficult to apply in practice. In contrast, DUET schedules computation at the subgraph-level, which simplifies the design space, while still allowing the DNN compiler to apply the majority of graphlevel optimizations, increasing the computation efficiency on a single device.</p><p>In a more general context, heterogeneous computing has been studied to make well-orchestrated use of heterogeneous hardware to execute various application <ref type="bibr" target="#b22">[26]</ref>, <ref type="bibr" target="#b42">[45]</ref>. While DUET is inspired by those prior works, unlike them, it is specially tailored for reducing the execution time of DNN inference.</p><p>DNN inference. There has been work on optimizing DNN inference through platforms, libraries, and compile-time strategies. Several platforms have been built to facilitate the deployment of DNN models, such as TensorFlow Serving <ref type="bibr" target="#b4">[8]</ref>, Ten-sorRT <ref type="bibr" target="#b2">[5]</ref>, ONNXRuntime <ref type="bibr" target="#b3">[6]</ref>. To the best of our knowledge, these platforms do not support heterogeneous DNN inference yet, and DUET can be integrated with these platforms to exploit concurrent execution opportunities on multiple devices.</p><p>There are libraries for accelerating DNN inference for a specific type of hardware, such as cuDNN <ref type="bibr" target="#b10">[14]</ref> and MKL-DNN <ref type="bibr" target="#b1">[3]</ref>. DUET can be combined with these libraries by incorporating them as a back-end. Finally, DUET serves as a middleware in between DL frameworks and a DNN compiler <ref type="bibr" target="#b5">[9]</ref>, <ref type="bibr" target="#b8">[12]</ref>, <ref type="bibr" target="#b34">[38]</ref>, <ref type="bibr" target="#b37">[41]</ref>, which allows an existing DNN to benefit from hardware heterogeneity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Although DL compilers produce highly optimized code for individual hardware devices, a drawback is that they miss opportunities to allow DNNs to benefit from the aggregated computation power of CPU and GPU. We introduce DUET, a DNN inference engine that allows DNNs to explore potential concurrent execution opportunities on coupled CPU-GPU architecture. Powered by the coarse-grained graph partitioning, compiler-aware profiling, and a profiling-based online subgraph scheduling algorithm, DUET greatly decreases the end-to-end inference latency for DNNs that exhibit complex model structures and diverse computation patterns. We hope this work will encourage additional studies of heterogeneous execution on the DNN model online serving.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The compilation pipeline for DNNs.</figDesc><graphic url="image-1.png" coords="2,354.82,50.54,165.38,136.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of Wideand-Deep network for heterogeneous contents encoding.</figDesc><graphic url="image-2.png" coords="3,49.45,146.41,115.76,118.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of MT-DNN for natural language understanding.</figDesc><graphic url="image-3.png" coords="3,176.15,161.66,111.04,96.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The execution timeline of Wide-and-Deep models on GPU (upper) and CPU (lower).</figDesc><graphic url="image-4.png" coords="3,311.98,50.54,252.00,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Communication cost between CPU and GPU device.</figDesc><graphic url="image-5.png" coords="4,66.49,50.54,216.00,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The DUET Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The multi-phase execution schedule.</figDesc><graphic url="image-12.png" coords="5,83.89,50.54,181.20,129.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The greedy-correction scheduling for a case of concurrently executing multiple subgraphs on CPU and GPU.</figDesc><graphic url="image-13.png" coords="5,313.95,50.54,247.13,115.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. The heterogeneous execution engine.</figDesc><graphic url="image-14.png" coords="7,48.96,59.04,159.08,85.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The end-to-end latency of different frameworks on Wide&amp;Deep, Siamese and MT-DNN model.</figDesc><graphic url="image-16.png" coords="8,66.49,50.54,216.00,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The comparison of 55th, 99th, 99.9th percentile latencies between TVM(GPU) and DUET (CPU-GPU) on Wide-and-Deep, Siamese network, and MT-DNN.</figDesc><graphic url="image-17.png" coords="8,66.49,222.02,216.00,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The comparison of execution time with different scheduling algorithms.</figDesc><graphic url="image-18.png" coords="8,316.55,50.54,241.92,120.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Inference latency and speedup on Wide&amp;Deep Model with different layers in the RNN component.</figDesc><graphic url="image-19.png" coords="9,66.49,50.54,216.00,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Inference latency and speedup on Wide&amp;Deep Model with different layers(18/34/50/101) in the CNN component.</figDesc><graphic url="image-20.png" coords="9,66.49,233.52,216.00,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Inference latency and speedup on Wide&amp;Deep Model with different config in the Deep component.</figDesc><graphic url="image-21.png" coords="9,329.51,50.54,216.00,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Comparison of different configurations with different batch sizes.</figDesc><graphic url="image-22.png" coords="9,329.51,228.48,216.00,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>PyTorch-CPU PyTorch-GPU TVM-CPU TVM-GPU DUET Time(ms)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>MODEL PARAMETERS OF WIDE-AND-DEEP, SIAMESE, MT-DNN.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III THE</head><label>III</label><figDesc>END-TO-END LATENCY ON </figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Please note that it is possible to have a nested partition of subgraphs. However, doing so will decrease the computation granularity and incur more CPU-GPU communication overhead. For simplicity, we assume a one-level partition scheme, and we leave multi-level partitioning as future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Please note that it is possible to further improve the performance by allowing multiple subgraphs to execute concurrently within one device (e.g., CPU). For simplification, we assume a sequential execution of subgraphs on CPU, because the small number of cores can be largely occupied by most subgraphs.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors appreciate the anonymous IPDPS reviewers for providing very constructive and useful feedback, which has significantly helped improving the quality of this paper.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/dhwajraj/deep-siamese-text-similarity" />
		<title level="m">Deep LSTM siamese network for text similarity</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">(r)</forename><surname>Intel</surname></persName>
		</author>
		<ptr target="https://github.com/01org/mkl-dnn" />
		<title level="m">Math Kernel Library for Deep Neural Networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia Tensorrt</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Onnxruntime</surname></persName>
		</author>
		<ptr target="https://github.com/microsoft/onnxruntime" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tensorflow</forename><surname>Serving</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/serving/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="https://www.tensorflow.org/performance/xla/" />
		<title level="m">The Accelerated Linear Algebra Compiler Framework</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tensor-Flow: A System for Large-scale Machine Learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI &apos;16</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI &apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<title level="m">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TVM: an automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Carlsbad, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-08">2018. October 8-10, 2018. 2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wide deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zakaria</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemal</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient Primitives for Deep Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>?aglar G?lc ?ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deepfm: An end-to-end wide &amp; deep learning framework for CTR prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<idno>CoRR, abs/1804.04950</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autotm: Automatic tensor movement in heterogeneous memory systems using integer linear programming</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jawad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Trika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lowe-Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GRNN: low-latency and scalable RNN inference on gpus</title>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mawhirter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference</title>
		<editor>
			<persName><forename type="first">George</forename><surname>Candea</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Robbert</forename><surname>Van Renesse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christof</forename><surname>Fetzer</surname></persName>
		</editor>
		<meeting>the Fourteenth EuroSys Conference<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-03-25">2019. March 25-28, 2019. 2019</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping</title>
		<author>
			<persName><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1341" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trec: an efficient recommendation system for hunting passengers with deep neural networks</title>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiujun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">S-1</biblScope>
			<biblScope unit="page" from="209" to="222" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and &lt;1MB Model Size</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring hidden dimensions in parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2279" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Layer-centric memory reuse and data migration for extreme-scale deep learning on many-core architectures</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofeng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive work placement for query processing on heterogeneous computing resources</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Karnagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Habich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Lehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="733" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient heuristic procedure for partitioning graphs</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Kernighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic ?benchmark generation to compute&quot; lower-bound&quot; latency and inform optimizations of deep learning models on gpus</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Dakkak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><surname>Benanza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The microsoft toolkit of multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Awa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Asli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">July 5-10, 2020. 2020</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Device placement optimization with reinforcement learning</title>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">2017. August 2017. 2017</date>
			<biblScope unit="page" from="2430" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning text similarity with Siamese recurrent networks</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Neculoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Rotaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14">2019. 2019, 8-14 December 2019. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluating characteristics of CUDA communication primitives on high-bandwidth interconnects</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Dakkak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Hashash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Hsin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering</title>
		<editor>
			<persName><surname>Varsha Apte</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Di</forename><surname>Antinisca</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marin</forename><surname>Marco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jos?</forename><surname>Litoiu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Merseguer</surname></persName>
		</editor>
		<meeting>the 2019 ACM/SPEC International Conference on Performance Engineering<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-04-07">2019. April 7-11, 2019. 2019</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Capuchin: Tensor-based gpu memory management for deep learning</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hulin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiliang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="891" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;13</title>
		<meeting>the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="519" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">vdnn: Virtualized deep neural networks for scalable, memory-efficient neural network design</title>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relay: a new IR for machine learning frameworks</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIG-PLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 2nd ACM SIG-PLAN International Workshop on Machine Learning and Programming Languages<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">June 18-22, 2018. 2018</date>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
	<note>MAPL@PLDI 2018</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Jongsoo Park, Artem Rakhov, and Misha Smelyanskiy. Glow: Graph lowering compiler techniques for neural networks</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleem</forename><surname>Abdulrasool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Summer</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Dzhabarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Levenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Olesen</surname></persName>
		</author>
		<idno>CoRR, abs/1805.00907</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Flops as a direct optimization objective for learning sparse neural networks</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/1811.03060</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Tensor comprehensions: Frameworkagnostic high-performance machine learning abstractions</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno>abs/1802.04730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Superneurons: Dynamic gpu memory management for training deep neural networks</title>
		<author>
			<persName><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiwen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;18</title>
		<meeting>the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="41" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Machine learning at facebook: Understanding inference at the edge</title>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sy</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21">2017. July 21-26, 2017. 2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">When computing meets heterogeneous cluster: Workload assignment in graph computation</title>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Big Data, Big Data</title>
		<meeting><address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-10-29">2015. October 29 -November 1, 2015. 2015</date>
			<biblScope unit="page" from="154" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<editor>
			<persName><forename type="first">Ankur</forename><surname>Teredesai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R?mer</forename><surname>Rosales</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evimaria</forename><surname>Terzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</editor>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-04">2019. August 4-8, 2019. 2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deepcpu: Serving rnn-based deep learning models 10x faster</title>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 USENIX Annual Technical Conference, USENIX ATC 2018</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Benjamin</forename><surname>Gunawi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Reed</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">July 11-13, 2018. 2018</date>
			<biblScope unit="page" from="951" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
