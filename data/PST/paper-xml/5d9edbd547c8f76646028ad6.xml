<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On-line Building Energy Optimization using Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Elena</forename><surname>Mocanu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Constantin</forename><surname>Decebal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Phuong</forename><forename type="middle">H</forename><surname>Mocanu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Antonio</forename><forename type="middle">Liotta</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Webber</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Madeleine</forename><surname>Gibescu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">J</forename><forename type="middle">G</forename><surname>Slootweg</surname></persName>
						</author>
						<title level="a" type="main">On-line Building Energy Optimization using Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">170E2E00385A745F41D92E9544F7B1F6</idno>
					<idno type="DOI">10.1109/TSG.2018.2834219</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSG.2018.2834219, IEEE Transactions on Smart Grid IEEE TRANSACTIONS ON SMART GRID 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSG.2018.2834219, IEEE Transactions on Smart Grid</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Reinforcement Learning</term>
					<term>Demand Response</term>
					<term>Deep Neural Networks</term>
					<term>Smart Grid</term>
					<term>Strategic Optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unprecedented high volumes of data are becoming available with the growth of the advanced metering infrastructure. These are expected to benefit planning and operation of the future power system, and to help the customers transition from a passive to an active role. In this paper, we explore for the first time in the smart grid context the benefits of using Deep Reinforcement Learning, a hybrid type of methods that combines Reinforcement Learning with Deep Learning, to perform on-line optimization of schedules for building energy management systems. The learning procedure was explored using two methods, Deep Q-learning and Deep Policy Gradient, both of them being extended to perform multiple actions simultaneously. The proposed approach was validated on the large-scale Pecan Street Inc. database. This highly-dimensional database includes information about photovoltaic power generation, electric vehicles as well as buildings appliances. Moreover, these on-line energy scheduling strategies could be used to provide realtime feedback to consumers to encourage more efficient use of electricity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HERE is an energy transition underway since the start of the millennium, comprised primarily of a push towards replacing large, fossil-fuel plants with renewable and distributed generation. It results in increased uncertainty and complexity in both the business transactions and in the physical flows of electricity in the smart grid. Because the built environment is the largest user of electricity, a deeper look at building energy consumption holds a promise for improving energy efficiency and sustainability. Understanding such individual consumption behavior based on the knowledge transfer from the fusion of extensive data collected from the Advanced Metering Infrastructure (AMI) is an essential step E. Mocanu is with the Department of Electrical Engineering and Department of Mechanical Engineering, Eindhoven University of Technology, Eindhoven, 5600 MB, The Netherlands (e-mail:e.mocanu@tue.nl) D.C. Mocanu is with the Department of Mathematics and Computer Science, Eindhoven University of Technology, Eindhoven, 5600 MB, The Netherlands (e-mail: d.c.mocanu@tue.nl) P.H. Nguyen, M. Gibescu and J.G. Slootweg are with the Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, 5600 MB, The Netherlands (e-mail:{p.nguyen.hong; m.gibescu; j.g.slootweg}@tue.nl) A. Liotta is with the Data Science Centre, University of Derby, UK (e-mail: a.liotta@derby.ac.uk) M.E. Webber is with the Department of Mechanical Engineering, The University of Texas at Austin, Austin, TX 78712-1591, USA. (e-mail: webber@mail.utexas.edu) to optimize building energy consumption and consequently the effects of its use.</p><p>This work is motivated by the hypothesis that an optimal resource allocation of end-user patterns based on daily smart electrical device profiles could be used to smoothly reconcile differences in future energy consumption patterns and the supply of variable sources such as wind and solar <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. It is expected that a cost minimization problem could be solved to activate real-time price responsive behavior <ref type="bibr" target="#b3">[4]</ref>. A widerange of methods have been proposed to solve the building energy and cost optimization problems, including linear and dynamic programing, heuristic methods such as Particle Swarm Optimization (PSO), game theory, fuzzy methods and so on <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Therein, both centralized and decentralized solutions exist, but they fail to consider on-line solutions for large-scale, real databases <ref type="bibr" target="#b5">[6]</ref>. More concretely, any time when an optimization is needed, these methods have to compute completely or partially all the possible solutions and to choose the best one. This procedure is time consuming. In the big data era, more and more machine learning methods appear to be suitable to overcome this limitation by automatically extracting, controlling and optimizing the electrical patterns. This can be done by performing successive transformation of the historical data to learn powerful machine learning models to cope with the high uncertainty of the electrical patterns. Then, these models will be capable of generalization and they could be exploited in an on-line manner (i.e. few milliseconds) to minimize the cost or the energy consumption in newly encountered situations. Among all these machine learning models, the ones belonging to the Reinforcement Learning (RL) area are the most suitable for the cost minimization problem, as they are capable to learn an optimal behavior, while the global optimum is not known.</p><p>Thus, in the remaining of this paper we focus on RL methods, such as Q-learning <ref type="bibr" target="#b7">[8]</ref>, and their latest developments. The building environment is modeled using a Markov Decision Process <ref type="bibr" target="#b8">[9]</ref> and it can be used to find the best long-term strategies. Prior studies showed that RL methods are able to solve stochastic optimal control problems <ref type="bibr" target="#b9">[10]</ref> in the power system area as well as an energy consumption scheduling problem <ref type="bibr" target="#b10">[11]</ref> with dynamic pricing <ref type="bibr" target="#b11">[12]</ref>. A batch reinforcement learning method was introduced in <ref type="bibr" target="#b12">[13]</ref> to schedule a cluster of domestic electric water heaters, and further on applied for smart home energy management <ref type="bibr" target="#b13">[14]</ref>. Owing to the curse of dimensionality, these methods fail for large-scale problems. More recently, there has been a revival of interest in combining deep learning with reinforcement learning. Lately, in 2015, an application of Q-learning to deep learning has been successful at playing Atari2600 games at expert human levels <ref type="bibr" target="#b14">[15]</ref>. In 2016, another one has defeated for the first time in history the world champion at the game of Go. Complementary with our work, Francois-Lavet et al. has proposed the use of Deep Q-learning for storage scheduling in microgrids <ref type="bibr" target="#b15">[16]</ref>. The above methods represent the starting point of a new research area, known as Deep Reinforcement Learning (DRL), which has evolved through the intersection of reinforcement learning and neural networks. At the same time, in our previous work, we showed that Reinforcement Learning using Deep Belief Networks for continuous states estimation can successfully perform unsupervised energy prediction <ref type="bibr" target="#b16">[17]</ref>.</p><p>Our contribution: In this paper, inspired by the above research developments, we propose for the first time the use of the Deep Policy Gradient method, as part of Deep Reinforcement Learning algorithms, in the large-scale physical context of smart grid -smart building, as follows.</p><p>• We propose for the first time an approach to optimize directly on-line the building energy consumption and the cost.</p><p>• We propose a new way to adapt DRL algorithms to the smart grid context, with the aim of conceiving a fast algorithm to learn the electrical patterns and to optimize on-line either the building energy consumption or the cost.</p><p>• We investigate two DRL algorithms, namely Deep Qlearning (DQN) <ref type="bibr" target="#b14">[15]</ref> and Deep Policy Gradient (DPG). • DPG in its current state-of-the-art form is capable to take just one action at a specific time. As in the building context multiple actions have to be taken at the same moment, we propose a novel gradient method to enhance DPG with the capability of handling multiple actions simultaneously. We evaluate our proposed methods on the PecanStreet database at both the building and aggregated level. In the end, we prove that our proposed methods are able to efficiently cope with the inherent uncertainty and variability in the generation of renewable energy, as well as in the peoples' behavior related with their use of electricity (i.e. charging of electric vehicles). Specifically, we show that the enhanced DPG is more appropriate to solve peak reductions and cost minimization problems than DQN.</p><p>The remaining of this paper is organized as follows. Section II describes the problem formulation and Section III we introduce the background and preliminary concepts. Section IV describes our proposed method followed by implementation details in Section V. Results and discussions are provided in Section VI. Finally, we conclude with some directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM FORMULATION</head><p>In this context, we aim to reduce load peaks as well as to minimize the cost of energy. 1 Let B denote the set of 1 Please note that the main symbols and notation used in this paper are summarized in the Appendix, Table <ref type="table">V</ref>. buildings, such that B i ∈ B, ∀i ∈ N representing the index of the building analyzed. The total building energy consumption E i is a sum over all power generation P + and consumption in a specific interval of time ∆t. Therein, based on the shifting capabilities of appliances present in a building we differentiate between flexible power P - d , e.g. electric devices d ∈ {1, .., m i }, and fixed consumption P -. a) Cost minimization problem: In this paper, we assume two price components over the space of B, such that λ - t is the price value set by the utility company for the timeslot t and λ + t represents the price value at which the utility company buys energy from end-users at time-slot t. Therefore, the optimal cost associated with customer i at time t for an optimization time horizon T can be calculated as min</p><formula xml:id="formula_0">T t=1 (λ + t n i=1 P + i,t -λ - t n i=1 (P - i,t + mi d=1 a i,d,t P - i,d,t ))<label>(1) s.t.</label></formula><p>T t=1</p><formula xml:id="formula_1">P - i ∆t = E i , ∀i ∈ N, ∀t ∈ N,<label>(2)</label></formula><formula xml:id="formula_2">T t=1 P - d ∆t = E d , ∀d ∈ N, ∀t ∈ N,<label>(3)</label></formula><formula xml:id="formula_3">a i,d,t = {1, 0}, ∀a ∈ A, ∀i ∈ N, ∀d ∈ N, ∀t ∈ N,<label>(4)</label></formula><formula xml:id="formula_4">P + i,t , P - i,t , P - i,d,t ≥ 0, ∀t = [1 : T ] ∈ N,<label>(5)</label></formula><formula xml:id="formula_5">λ + t , λ - t ≥ 0, ∀t = [1 : T ] ∈ N.<label>(6)</label></formula><p>where a i,d,t = 1 if the electrical device is on at that specific moment in time, and 0 otherwise. Please note that, in our proposed method, computing a i,d,t is equivalent with the estimation of the actions (see Fig. <ref type="figure" target="#fig_0">1</ref>). b) Peak reduction problem: In the special case of constant price, for electricity generation and consumption, with λ + t = λ - t , the cost minimization problem becomes a peak reduction problem, defined as min</p><formula xml:id="formula_6">T t=1 n i=1 P + i,t - n i=1 (P - i,t + mi d=1 a i,d,t P - i,d,t )<label>(7)</label></formula><p>Consequently, the constraints following Eq. 1 will remain valid for both problems. However, based on the differences between different types of electrical devices the full range of constraints is larger as explained in the next sections. c) Electrical device constrains: We are assuming three types of consumption profiles. Firstly, we consider the time-scaling load. In respect to this we confine our analysis to the air conditioning load (d AC ), as a representative part of a larger set of electrical devices in every building which could be switched on-off for a limited number of times during an optimization horizon, e.g. lights, television, refrigerator. Prior studies show that short-term air conditioning curtailments have a negligible effect on end-user comfort <ref type="bibr" target="#b17">[18]</ref>. Secondly, we include the time-shifting load, also called deferrable load, that must consume a minimum amount of power over a given time interval. Therein, we model the dishwasher (d DW ) as an uninterruptible load, which requires a number of consecutive time steps. Finally the electric vehicle(d EV ) was considered as both a time scaling and shifting load. A more rigorous formulation of the building electrical components and their associated constrains could be found in <ref type="bibr" target="#b18">[19]</ref>. In our case, a complementary probabilistic perspective over the time dependent devices constrains a d,t give us the following assumptions:</p><p>A 1: For all d, with P - d time-scaling loads, there ∃δ d ∈ R + constants over the optimization horizon such that</p><formula xml:id="formula_7">t P - d ≤ δ d if p(P - d = 0|t) ∈ (0, 1] t P - d = δ d if p(P - d = 0|t) = 0<label>(8)</label></formula><p>where p(P - d = 0|t) is the probability of the electrical device d to be active at any moment in time t, for all t = [1 : T ] ∈ N.</p><p>A 2: For all d, with P - d time-shifting loads, there ∃δ d constants such that t P d = δ d , for all t = [1 : T ] ∈ N.</p><p>Observation 1: In this paper, P + (e.g. PV generation) is considered a non-curtailable resource.</p><p>Observation 2: All electrical vehicles, d EV , and their associated consumption P - d , were considered as time scaling and shifting loads working under the conditions imposed by Assumption 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BACKGROUND AND PRELIMINARIES</head><p>In this section, we provide a brief overview of reinforcement learning, Markov decision formalism, and deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reinforcement Learning</head><p>In a Reinforcement Learning (RL) <ref type="bibr" target="#b8">[9]</ref> context, an agent learns to act using a (Partial Observable) Markov Decision Process (MDP) formalism. MDPs are defined by a 4-tuple S, A, T • (•, •), R • (•, •) , where:</p><p>• S is the state space, ∀s ∈ S, • A is the action space, ∀a ∈ A, • T : S × A × S → [0, 1] is the transition function given by the probability that by choosing action a in state s at time t, the system will arrive at state s at time t + 1, such that p a (s, s ) = p(s t+1 = s |s t = s, a t = a), and</p><formula xml:id="formula_8">• R : S ×A×S → R is the reward function, were R a (s, s )</formula><p>is the immediate reward received by the agent after it performs the transition to state s from state s.</p><p>The agent aims to optimize a stochastic policy π : S × A × R → R + . Under structure assumption of the environment (i.e. finite states and actions) the Markov decision problem is typically solved using dynamic programing. However, in our built environment, the model has a large (continuous) states space. Therein, the state space is given by the building energy consumption and price at every moment in time, while the action space is highly dependent on the electric device constrains. The success of every action a is measured by a reward r. Learning to act in an environment will make the agent to choose actions to maximize future rewards. The value function Q π (s, a) is an expected total reward in state s using action a under a policy π. Currently, one of the most popular reinforcement learning algorithm is Q-learning <ref type="bibr" target="#b7">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Neural Networks</head><p>The topology of a Deep Neural Network (DNN) architecture is based on multiple layers of neurons. In general, a neuron is a non-linear transformation of the linear sum of its inputs. The first layer models directly the data. A hidden layer in the neural network architecture is build as an array of neurons taking the inputs from the previous layer. The activation function of a neuron on top of k stacked layers in the architecture is using composite functions, such as</p><formula xml:id="formula_9">x ⊗ h 1 ⊗ h 2 ⊗ • • • ⊗ h k .</formula><p>In 2011, it was shown that supervised training of a very deep neural network with hard non-linearities is faster if the hidden layers are composed of Rectified Linear Units (ReLU) <ref type="bibr" target="#b19">[20]</ref>. Recently, the logistic sigmoid and the hyperbolic tangent activation are outperformed by ReLU <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Formally, ReLU is defined as a function f (x i ) = max(0, x i ), where x i is its input. However, to avoid a non-zero gradient when the hidden units are not active, we used a slightly relaxed form proposed in <ref type="bibr" target="#b22">[23]</ref>, given by</p><formula xml:id="formula_10">f (x i ) = x i if x i &gt; 0, ∀i ∈ N ηx i if x i ≤ 0, ∀i ∈ N (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>where η is a coefficient controlling the slope of the negative part. If η = 0 then Eq. 9 becames ReLU. Still, deep learning methods applied to the power system are in an incipient phase. In 2014, Conditional Restricted Boltzmann Machine (CRBM) were used to increase the energy prediction accuracy <ref type="bibr" target="#b23">[24]</ref>. Later on, for the energy prediction problem various deep learning methods have been successfully explored and extended <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>. For example, Ryu et al.</p><p>[28] used the Long Short-Term Memory (LSTM) method for building energy prediction. Another paper related is the work of Marino et al. <ref type="bibr" target="#b28">[29]</ref>, which is followed by the work of Manic et al. <ref type="bibr" target="#b29">[30]</ref>. Further on, the Multilayer Perceptron (MLP) enhanced with deep learning capabilities was proposed in <ref type="bibr" target="#b30">[31]</ref>. Possible benefits of deep learning methods in terms of superior accuracy start to be investigated also for other types of signals in smart grids, such as wind forecast <ref type="bibr" target="#b31">[32]</ref>, as well as for event detection (classification) problem <ref type="bibr" target="#b32">[33]</ref>. One special case of using a DNN is in deep reinforcement learning where the input is given by the states of an MDP and the output represents the actions of the MDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED METHOD</head><p>In this section, we propose the use of Deep Reinforcement Learning (DRL) as an on-line method to perform optimal building resource allocation at different levels of aggregation. The general architecture of our proposed method is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. DRL (RL combined with DNNs of k hidden layers) can learn to act better than the standard RL by automatically extracting patterns, such as those of electricity consumption. Overall, we can represent the DNN method, from a very general perspective, as a black box model with good generalization capabilities over a given input distribution as follows: <ref type="bibr" target="#b9">(10)</ref> In the remaining of this section we will introduce two DRL methods, namely Deep Q-learning (DQN) and Deep Policy Gradient (DPG).</p><formula xml:id="formula_12">Input ----→ data DN N (k) Output -----------→ Data estimation</formula><formula xml:id="formula_13">DRL =      Input ----→ states DN N (k) Output -----→ Q(s,a) Deep Q-learning Input ----→ states DN N (k) Output -----→ p(a|s)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Policy Gradient</head><p>In contrast to value-based methods (e.g. DQN), policy-based model free methods (e.g. DPG) directly parameterize the policy π(a|s; θ) and update the parameters θ by performing, typically approximate, gradient ascent on the expected long-term reward <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Q-learning (DQN)</head><p>Learning in DRL is done as follows. The DNN is trained with a variant of the Q-learning algorithm, using stochastic gradient descent to update its parameters <ref type="bibr" target="#b14">[15]</ref>. Firstly, the value-function from the standard RL algorithm is replaced by a deep Q-network with parameters θ, given by the weights and biases of DNN, such that Q(s, a, θ) ≈ Q π (s, a). This approximation is used further to define the objective function by mean-squared error in Q-values</p><formula xml:id="formula_14">L(θ) = E r+γ max at+1 Q(s t+1 , a t+1 , θ)-Q(s t , a t , θ) 2<label>(11)</label></formula><p>Leading to the following Q-learning gradient </p><formula xml:id="formula_15">∂L(θ) ∂θ = E r + γ max at+1 Q(s t+1 , a t+1 , θ)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Policy Gradient (DPG)</head><p>Recently, it has been shown that policy gradient methods are able to decrease the time needed for convergence in continuous games <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. From an architectural perspective, the neurons from the output layer of the DNN (with parameters θ) corresponding to DPG, instead of estimating the Q(s t , a, θ), ∀a ∈ A as in DQN, they estimate the probability to take action a in a specific state s t , such as p(a|s t , θ), ∀a ∈ A. This offers a clear advantage to DPG over DQN when there is a need to perform multiple actions simultaneously, as all actions can be sampled and executed simultaneously in the game using their own probability.</p><p>In the policy gradient context, the approximate optimization problem defined in Eq. 1 or Eq. 7 is an equivalent of maximizing the total expected reward of a parameterized model under a policy π, as follows</p><formula xml:id="formula_16">maximize E x∼p(x|θ) [R|π]<label>(13)</label></formula><p>In the DPG context, the parameterized model is the DNN. Thus, the DNN becomes a probability density function over its inputs (the game states), i.e. f (x), leading Eq. 13 to the following optimization problem</p><formula xml:id="formula_17">maximize E x∼p(x|θ) [f (x)]<label>(14)</label></formula><p>As shown in <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, the unbiased gradient estimation uses f (x) as a score function yielding</p><formula xml:id="formula_18">θ E x [f (x)] = θ dx p(x|θ)f (x) = dx θ p(x|θ)f (x) = dx p(x|θ) θ p(x|θ) p(x|θ) f (x) = dx p(x|θ) θ log p(x|θ)f (x) = E x [f (x) θ log p(x|θ)]<label>(15)</label></formula><p>where ∂ ∂θ = θ denote the first-order partial derivative over the output data. Intuitively, to solve the gradient of Eq. 15, first we have to take samples of x i ∼ p(x|θ) and to compute the estimated gradient, such that ĝθ i = f (x i ) θ log p(x i |θ). Moving in the ĝi direction increases the log-probability of that particular sample x i proportional with the reward associated with it, f (x i ). In other words, this practically shows how good is that sample. As in policy gradient the reward is available at the end of a game, these samples are collected in a trajectory, i.e. τ = (s 0 , a 0 , r 0 , . . . , s T -1 , a T -1 , r T -1 ). To compute the gradient of a trajectory, we need to calculate and differentiate the density p(τ |θ) with respect to θ as follows:</p><formula xml:id="formula_19">p(τ |θ) = p(s 0 ) T -1 t=0 [π(a t |s t , θ)p(s t+1 |s t , a t )]<label>(16)</label></formula><p>By taking the log-probability of Eq. ( <ref type="formula" target="#formula_19">16</ref>), we obtain</p><formula xml:id="formula_20">log p(τ |θ) = log p(s 0 ) + T -1 t=0 log π(a t |s t , θ)<label>(17)</label></formula><p>+ log p(s t+1 |s t , a t )</p><p>Taking the derivative of Eq. ( <ref type="formula" target="#formula_20">17</ref>) with respect to θ leads to</p><formula xml:id="formula_21">∂ ∂θ log p(τ |θ) = ∂ ∂θ T -1 t=0 log π(a t |s t , θ)<label>(18)</label></formula><p>Finally, we can write the gradient update ĝθ τ for parameters θ after considering a trajectory τ as</p><formula xml:id="formula_22">ĝθ τ ∝ R τ ∂ ∂θ T -1 t=0 log π(a t |s t , θ)<label>(19)</label></formula><p>V. IMPLEMENTATION DETAILS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning and Validation</head><p>To train the DQN and DPG models we have used as a starting point an off-line database. First, we created the built environment game. In this game for each day we did not alter the base load, but for the assumed flexible loads we considered many possibilities (even if they did not happen reality). We evaluated each possibility. We gave a positive reward to it if it was close to our optimization goal. If not, we assigned to that possibility a negative reward. At the begin of the learning there are a lot of random choices, but in time (many iterations), the reinforcement learning model converges and will learn to choose just the possibilities which are close to the optimization goal. Finally, we did not optimize the "existing" off-line database, but we obtained an alternative optimized version of it, which would be better if this strategy would have been used in reality.</p><p>The main advantage of (deep) reinforcement learning is that after the model is trained completely on such off-line database, it can be exploited on-line in a real environment. If this online environment has slightly changed from the "training" environment, the reinforcement learning model can learn by default these changes and adapt dynamically its behavior to achieve the best possible performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>Aiming to have a fair comparison between DQN and DPG, the architecture of the deep neural networks used is similar for both models and it has the following characteristics. Each reinforcement learning state (encoded in the input layer), is given by a time-window of two consecutive time steps. Thus, in the case of the peak reduction problem the input layer has 11 neurons, i.e. time step t, and base load, PV, AC state, EV and dishwasher at t-1 and t. This leads to a very large number of states for one building. Furthermore, these continuous states will linearly increase with the number of buildings.</p><p>Please note that with the exception of base load and generation which are fixed, the other state components are given by the dynamically adapted values (the ones obtained during the learning process) and not the initial ones measured by the smart meters. For the cost reduction problem, the input layer has an extra neuron which is used to encode the ToU tariff. The number of layers and the number of neurons in each layer was carefully chosen, by performing a small trial and error procedure. Furthermore, the networks have three layers of hidden neurons, each layer having 100 neurons with Rectifier Linear Units (ReLU) as activation function.</p><p>The output layer differs for DQN and DPG. For DQN the output layer has 8 neurons, each neuron representing the Q-value of a combined action. Each combined action is a possible combination of the actions of the three flexible devices 2 , i.e. stop air conditioner (a 1 ), electric vehicle on/off (a 2 ), dishwasher on/off (a 3 ). By contrast, the DPG output layer has just three neurons, each neuron representing a device action. More precisely, it gives the probability to perform the action associated with the flexible device for the specific input state. This is a clear advantage of DPG over DQN as it scales linearly with the number of flexible devices.</p><p>Hyper-parameters settings: In all experiments performed, the learning rate is set to α = 10 -2 , the discount factor to γ = 0.99, and η = 0.01. We train the models for 5000 episodes, where an episode is composed by 20 randomly chosen days. The weights update is performed after every two episodes. The final policy is kept as the output of the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The reward vectors for DRL</head><p>Regarding the multi-objective optimization problems solved in this paper, an accurate reward function is computed at the end of the day, instead of at each time step of the day. Thus, we derived a simple multiple-task joint reward with three reward components: Component 1: For all τ = (s t , a t , r t ) the reward vectors will be able to control the actions of the three types of flexible consumption, and therefore the total shiftable and scalable load in a household m d=1 a t P - d,t , using differentiated rewards</p><formula xml:id="formula_23">r a1 =      -n a + 1 if n a + 1 &gt;<label>10</label></formula><formula xml:id="formula_24">ζ 1 if n a + 1 ∈ [1, 10]; ζ 2 if n a + 1 &lt; 1 r a3 =      -n a + 3 if n a + 3 &gt; 2 ζ 1 if n a + 3 ∈ [1, 2] ζ 2 if n a + 3 &lt; 1 r a2 = -4|n a t 2 -n a + 2 | if n a + 2 = n a t 2 , ∀n a t 2 ∈ N n a + 2 if n a + 2 = n a t 2 , ∀n a t 2 ∈ N<label>(20)</label></formula><p>where</p><formula xml:id="formula_25">n a + 1 , n a + 2</formula><p>, and n a + 3 represent how many times the action corresponding with the flexible device is performed, and n a t 2 is the targeted number of loads per day for the electric car. The 2 The number of neurons in the output layer of DQN is exponentially correlated with respect to the number of flexible devices.</p><p>1949-3053 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Component 2: Controlling the total energy consumption defined in Eq. 7 is done as follows r = -3ζ 1 -1 if max( P -) &gt; max(P -) -3ζ 2 +4[max(P -)-max( P -)] otherwise <ref type="bibr" target="#b20">(21)</ref> Further on, shifting the consumption through the time when there is more generation Eq. 6.</p><formula xml:id="formula_26">r = ζ1 2 -|min( P -)| if P -&lt; 0 -ζ2 2 otherwise (<label>22</label></formula><formula xml:id="formula_27">)</formula><p>The control of AC under the A2, Eq. 8 is given by</p><formula xml:id="formula_28">r = ζ1 8 + 2[max( P - AC ) -max(P - AC )] if P -&lt; 0 -ζ2 10 otherwise (<label>23</label></formula><formula xml:id="formula_29">)</formula><p>Component 3: Controlling the total cost C, defined in Eq. 1.</p><formula xml:id="formula_30">r = 5| C -C| if C &lt; C -3ζ 1 -1 otherwise<label>(24)</label></formula><p>The agent must learn multiple tasks consecutively with the goal of optimizing performance across all previously learned tasks. So, we used for solving Eq. 7 the Component 1 and 2 of the reward, while for Eq. 1 the Component 1 and 3.</p><p>The joint reward components could be easily generalized to perform an arbitrary number of tasks. However, the range intervals for n a + 1 and n a + 3 considered in Eq. 20 as well as the positive and negative coefficients (i.e. ζ 1 and ζ 2 ) used in Eq. 20-24 are dependent on the application. Also in Eq. 20 the range of n a + 1 and n a + 3 may be enlarged if comfort limits are relaxed. Algorithm 1 exemplifies on DPG, how DRL can be implemented. We have implemented both methods, DQN and DPG, in Python.</p><p>Algorithm 1 Deep Policy Gradient (DPG) -estimating Eq.13 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND DISCUSSION</head><p>In this section, we validate our proposed methods and we analyze their performance on a large real-world database recorded by Pecan Street.Inc. First, the database is described. Then, numerical results are given for both problems, i.e. peak reduction and cost minimization for various number of buildings.</p><p>A. Data set characteristics 1) Buildings pattern: To validate our proposed method we used the Pecan Street dataset. The disaggregated customer energy data contains up to 90 million unique electricity consumption records per day, which are used in order to build specific device patterns. Figure <ref type="figure">3</ref>(a), 3(b), and 3(c) show three different building patterns averaged over the year 2015 with 15 minutes resolution. In these patterns the solar generation uncertainty as well as the people behavior characteristics are notable, e.g. even if all three buildings have an electric vehicle, just in the case of the third building, Fig. <ref type="figure">3(c)</ref>, it is used frequently. In our experiments, we have used the data between 27th October 2012 and 3rd September 2016.</p><p>2) Price data: We use the time-of-use (ToU) tariff provided by the local grid operator Austin Energy for customers who live inside the City of Austin, Texas <ref type="foot" target="#foot_0">3</ref> . The λ -summer rates are composed by on-peak, mid-peak and off-peak hours and the winter tariff has the mid-peak and off-peak hours components. There is also a difference between weekend and working-days tariff. Additionally, the self-generating customers are receiving an amount that is being paid by the utility for solar generation, called the value of solar tariff (VOST).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline reinforcement learning</head><p>Before going into the assessment of the proposed methods, we perform a comparison of their behavior with a widely known baseline reinforcement learning method, i.e. Qlearning. We considered the peak reduction problem. To be able to perform Q-learning on it we discretized the values of the continuous states in two integer numbers per state dimension. This leads to 2048 discrete states. This discretization offered poor generalization capabilities to Q-learning and, in fact, it was not able to learn properly the problem constraints, as reflected in Figure <ref type="figure" target="#fig_3">2</ref>. Remarkable, the DPG method learns to stay within the constraints-bounds. Perhaps, a higher granularity would help Q-learning generalize better, but if we would consider for example 3 integer numbers per state dimension this would lead to a number of 177147 (3 11 ) discrete states.</p><p>In general, reinforcement learning cannot handle properly continuous states due to the fact that it has to discretize them. Then a matrix with Q-values of the (discretized state, action) pairs is stored in the computer memory. Still, this matrix has an exponential memory footprint with respect to the number of actions and the discretization steps, and it cannot be handled properly by general purpose hardware. For 1949-3053 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.   <ref type="formula" target="#formula_24">20</ref>). The results are computed using all testing episodes. The gray areas show the bounds for each action. Remarkable, DPG always learns a good behavior, while Q-learning never does. instance, just 11 dimensions for the state space (as in our case) and 10 discretization steps lead to 100 billions discretized states. Thus, the only option is to employ artificial neural networks to store the (state, action) Q-values. This can be done either directly, as in DQN, or indirectly, as it happens in DPG. However, this does not make sense to be considered as even in the case analyzed by us with 2048 discrete states the training time of Q-learning was almost double than the one of DQN and DPG (order of minutes for one building). For these reasons, further on, we analyze just the performance of DPG and DQN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Numerical results -Peak reduction problem</head><p>The numerical results in terms of peak reduction at the single building level are showed in Table <ref type="table" target="#tab_2">I</ref> and Figure <ref type="figure">3</ref> for three different buildings (B I , B II and B III ), over one year with 15 minutes resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Numerical results -Cost minimization problem</head><p>The results for the cost minimization problem are summarized in Table <ref type="table" target="#tab_2">II</ref>. The difference between the cost minimization solutions obtained for every building correlated with their average electrical patterns (see Figure <ref type="figure">3</ref>) give as a first indication   <ref type="figure">3</ref>, a secondary advantage of solving the cost minimization problem is its impact on solving of the peak reduction problem also. Therein, the best results in terms of both, peak reduction and cost reduction, are obtained for building B III using DPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Scalability and learning capabilities of DRL</head><p>To test whether good estimations occur in practice and at scale, we investigate the performance of our proposed methods, in three cases with different numbers of customers using data from the Pecan Street smart grid test-bed. Specifically, we are investigating and analyzing the corresponding results using DQN and DPG methods for 10, 20 and 48 buildings, respectively. Tables <ref type="table" target="#tab_4">III</ref> and<ref type="table" target="#tab_2">IV</ref> show that our proposed approach is scalable for both, peak reduction and cost minimization, respectively. More than that, they show that at the aggregated level, when more customers are taking the cost minimization problem into consideration, this solves implicitly also the peak reduction problem. Same as at the building level, DPG is more stable than DQN, and achieves a better performance. Overall, in the case of the cost reduction problem for 48 buildings, DPG reduces the peak with 26.3% and minimizes the cost with 27.4%, while DQN reduces the peak with just 9.6% and minimizes the cost with just 14.1%. To visualize how DPG performs, in Figure <ref type="figure">4</ref> we depict the unoptimized and the optimized annualized energy costs for each of the 48 buildings. We can observe that the buildings behave very differently and in some cases DPG is capable to halve the yearly cost, while in other cases it succeeds to reduce the cost with just a few percentage points.</p><p>Convergence capabilities of DPG: The convergence is assessed through many iterations over episodes. For example, the learning capabilities of DPG method in terms of peak reduc- tion and their corresponding reward function for a building are showed in Figure <ref type="figure" target="#fig_5">5</ref>. Each episode represents an average value over 20 randomly chosen days. Initially, we may observe that the reward increases fast, while after about 1000 episodes the reward, as expected, increases much slower. Therefore, after approximatively 1000 episodes the average peak value and the optimized average peak value using the Deep Policy Gradient method converge. Still, the long-term reward expectation, as was expressed in Eq. ( <ref type="formula" target="#formula_16">13</ref>), is increasing until approximatively 2500 episodes.</p><p>Computational time requirements: Both DRL variants have the advantage of handling naturally much larger continuous state spaces, leading to better performance. In comparison with heuristic methods (e.g. PSO), after DRL learns how to act, it can make decisions (e.g. choosing the optimal control action) in a few milliseconds, while PSO needs to re-run the costly optimization process for every decision.    On-line versus off-line learning performance: One of the most important characteristics of all deep reinforcement learning methods is their ability to learn in an on-line manner. To illustrate this, in Figure <ref type="figure" target="#fig_6">6</ref> it is shown how the DPG performance is continuously improving as more real-time data is collected. This case study compares the DPG learning capabilities in an off-line learning case, where all training data are used from the beginning, with a practical application environment, where increasing amounts of training data are accumulating over time. Note that although the reward function for the case of off-line learning is initially dominating the one for on-line learning, they become equivalent after about 80% of the data has been served, or 3500 episodes. However, further studies could be done in order to compare the performance of on-line versus off-line methods. In this case, an offline method may be seen as a combination of one prediction method and one optimization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this paper, we proposed the use of Deep Reinforcement Learning, as a hybrid method which combines Reinforcement Learning with Deep Learning, with the aim of conceiving optimization for the scheduling of electricity consuming devices in residential buildings and aggregations of buildings. We have shown that a single agent, empowered with a suitable learning algorithm, can solve many challenging tasks. We proposed two optimization methods, Deep Q-learning and Deep Policy Gradient, to solve the same sequential decision problems at both the building level and the aggregated level. Also, we show the advantages of these methods in solving these tasks in comparison with a well-known Reinforcement Learning method, i.e. Q-learning. At both levels, we showed that Deep Policy Gradient is more suited to perform on-line scheduling of energy resources than Deep Q-learning. We explored and validated our proposed methods using the large Pecan Street database. Both proposed methods are able to successfully perform either the minimization of the energy cost or the flattening of the net energy profile. For the minimization of the energy cost, a variable electricity price signal is investigated to incentivize customers to shift their consumption to low-price, off-peak periods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. APPENDIX</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The closed loop general architecture of Deep Reinforcement Learning, built as a combination of Reinforcement Learning and Deep Neural Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSG.2018.2834219, IEEE Transactions on Smart Grid IEEE TRANSACTIONS ON SMART GRID 6 choice of ζ 1 and ζ 2 coefficients was based on a trial and error procedure. The obtained values are ζ 1 = 40 and ζ 2 = -50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 3. (a), (b) and (c) represent different building electrical patterns averaged over one year (solid line) with 15 minute resolution, followed by their standard deviation (shadow area). Their yearly average optimization results using Deep Policy Gradient method for the peak reduction problem are depicted in Fig.2 (d), (e) and (f) whether the cost minimization results are showed in Fig.2. (g), (h) and (i).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Learning to satisfy the constraints bounds on one building (Equation20). The results are computed using all testing episodes. The gray areas show the bounds for each action. Remarkable, DPG always learns a good behavior, while Q-learning never does.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Learning capabilities of Deep Policy Gradient method in terms of peak reduction and their corresponding reward function for a building (i.e. Fig.2 Building I). Every episode represents an average value over 20 random days.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. On-line versus off-line learning capabilities of Deep Policy Gradient method in terms of reward function for a building (i.e. Fig.2 Building I). Note that the right side y-axis shows the amount of training data served to the DPG. Every episode represents an average value over 20 random days.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>t</head><label></label><figDesc>The price value of P - SThe set of states, ∀s ∈ S. AThe set of actions, ∀a ∈ A. T Transition probability function, T :S × A × S → [0, 1] R The reward function, R : S × A × S → R Q The quality matrix, Q : S × A → R π represents the stochastic policy, π : S × A × R → R + θrepresents the set of parameters in Deep Belief Network k represents the number of layers in Deep Belief Network η represents a coefficient controlling the activation function τ represents a trajectory, τ = (st, at, rt) g, ĝ represents the gradient and the estimated gradient na represents the index of the action, na ∈ {a 1 , a 2 , a 3 }. ζ 1 , ζ 2 represents the coefficients controlling the reward function and by the European Union's Horizon 2020 project INTER-IoT (grant number 687283).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Usually, this standard Q-learning algorithm used in synergy with neural networks oscillates or diverges, mainly because data are sequential. To overcome this limitation of correlated data and non-stationary distributions, we use an experience replay mechanism which randomly samples previous minibatch of transitions (s t , a t , r t , s t+1 ) from the dataset D, and therefore smooths the training distribution over many historical data. It is straightforward to integrate the above Deep RL approach into Eq.1. The binary action vector a t ∈ A is augmented to max at+1 Q(s t+1 , a t+1 , θ) and therefore</figDesc><table><row><cell>m d=1 a t P -d,t is optimally controlled. Specifically, rather than</cell></row><row><cell>enforcing the constraints on the time window required by a</cell></row><row><cell>specific device d and the comfort of end-users considered</cell></row><row><cell>in A1, Eq.8, our idea is to encapsulate them in the reward</cell></row><row><cell>function, r t (λ + t , λ -t , P -i,d ), which is further detailed in Section</cell></row><row><cell>IV.A.</cell></row></table><note><p>-Q(s t , a t , θ) ∂Q(s t , a t , θ) ∂θ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>: initialize model: DNN with random weights θ 3: initialize game: first state s from a random day 4: for iteration = 1 to arbitrary number do</figDesc><table><row><cell>5:</cell><cell>sample actions p(a1, a2, a3|θ, s) with DNN</cell></row><row><cell>6:</cell><cell>collect probabilities p(a1, a2, a3|θ, s) in A</cell></row><row><cell>7:</cell><cell>collect hidden neurons values in H from DNN</cell></row><row><cell>8:</cell><cell>collect s in S</cell></row><row><cell>9:</cell><cell>execute actions a1, a2, a3 and move to next state s</cell></row><row><cell>10:</cell><cell>collect reward r in R from game</cell></row><row><cell>11:</cell><cell>if episode is finished then</cell></row><row><cell>12:</cell><cell>compute discounted rewards R d from R</cell></row><row><cell>13:</cell><cell>estimate gradients from A  *  R d , θ, S, and H (Eq. 19)</cell></row><row><cell>14:</cell><cell>update θ with the estimated gradient</cell></row><row><cell>15:</cell><cell>empty A, R, S, and H</cell></row><row><cell>16:</cell><cell>end if</cell></row><row><cell>17:</cell><cell>if current day ends then</cell></row><row><cell>18:</cell><cell>reset game: first state s from a random day</cell></row><row><cell>19:</cell><cell>end if</cell></row><row><cell>20:</cell><cell>set s = s</cell></row><row><cell cols="2">21: end for</cell></row></table><note><p>1: initialize model: hyper-parameters (α, γ, ζ) 2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I DAILY</head><label>I</label><figDesc>PEAK VALUE AT THE BUILDING LEVEL (B) AVERAGED OVER ONE YEAR WITH 15 MINUTES RESOLUTION VERSUS OPTIMIZED PEAK VALUE USING DQN AND DPG METHODS</figDesc><table><row><cell></cell><cell></cell><cell>B I</cell><cell></cell><cell></cell><cell>B II</cell><cell cols="2">B III</cell></row><row><cell></cell><cell cols="7">Method Mean St.dev. Mean St.dev. Mean St.dev.</cell></row><row><cell></cell><cell></cell><cell>(µ)</cell><cell>(Σ)</cell><cell>(µ)</cell><cell>(Σ)</cell><cell>(µ)</cell><cell>(Σ)</cell></row><row><cell>Peak [kW]</cell><cell>-</cell><cell>3.81</cell><cell>1.72</cell><cell>3.77</cell><cell>2.32</cell><cell>4.55</cell><cell>1.52</cell></row><row><cell>Optimized</cell><cell>DQN</cell><cell>2.72</cell><cell>1.45</cell><cell>2.72</cell><cell>1.21</cell><cell>3.59</cell><cell>1.41</cell></row><row><cell>peak [kW ]</cell><cell>DPG</cell><cell>2.55</cell><cell>1.36</cell><cell>2.49</cell><cell>1.13</cell><cell>3.12</cell><cell>1.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III PEAK</head><label>III</label><figDesc>REDUCTION -DAILY OPTIMIZATION RESULTS AT DIFFERENT LEVELS OF AGGREGATION AVERAGE OVER ONE YEAR WITH 15 MINUTES RESOLUTION USING DQN AND DPG METHODS</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Number of buildings</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell>20</cell><cell>48</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Method Mean St.dev. Mean St.dev. Mean St.dev.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(µ)</cell><cell>(Σ)</cell><cell>(µ)</cell><cell>(Σ)</cell><cell>(µ)</cell><cell>(Σ)</cell></row><row><cell cols="3">Peak [kW]</cell><cell>-</cell><cell cols="3">59.79 6.12 124.72 10.28 281.88 14.32</cell></row><row><cell cols="7">Optimized DQN 49.67 5.62 106.84 7.49 238.12 12.98</cell></row><row><cell cols="6">peak [kW ] DPG 41.74 5.08</cell><cell>93.83</cell><cell>7.29 213.01 12.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE IV</cell></row><row><cell cols="7">COST REDUCTION -DAILY OPTIMIZATION RESULTS AT DIFFERENT</cell></row><row><cell cols="7">LEVELS OF AGGREGATION AVERAGE OVER ONE YEAR WITH 15 MINUTES</cell></row><row><cell></cell><cell></cell><cell cols="5">RESOLUTION USING DQN AND DPG METHODS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Number of buildings</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>20</cell><cell>48</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Method Mean St.dev. Mean St.dev. Mean St.dev.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(µ)</cell><cell>(Σ)</cell><cell>(µ)</cell><cell>(Σ)</cell><cell>(µ)</cell><cell>(Σ)</cell></row><row><cell cols="3">Peak [kW]</cell><cell>-</cell><cell cols="3">59.79 6.12 124.72 10.28 281.88 14.32</cell></row><row><cell cols="2">Peak</cell><cell></cell><cell cols="4">DQN 54.85 5.93 116.72 9.24 254.67 13.21</cell></row><row><cell cols="7">reduction [kW ] DPG 44.91 4.80 92.41 7.74 207.73 11.48</cell></row><row><cell cols="3">Cost [$/day]</cell><cell>-</cell><cell cols="3">57.79 20.90 118.03 30.01 231.27 38.76</cell></row><row><cell cols="3">Minimized</cell><cell cols="4">DQN 47.71 17.83 93.68 24.18 198.51 32.67</cell></row><row><cell cols="3">cost [$/day]</cell><cell cols="4">DPG 44.35 16.01 82.71 21.48 167.70 28.62</cell></row><row><cell></cell><cell>5000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unoptimized cost</cell></row><row><cell>[$/year] Annualized energy cost</cell><cell>1000 2000 3000 4000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Optimized cost</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Buildings [#]</cell></row><row><cell cols="7">Fig. 4. Yearly savings per buildings when cost optimization is performed</cell></row><row><cell cols="7">at the aggregated level on 48 buildings using Deep Policy Gradient (DPG)</cell></row><row><cell cols="2">method.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1949-3053 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSG.2018.2834219, IEEE Transactions on Smart Grid</figDesc><table><row><cell>0</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell>4000</cell><cell>5000</cell></row><row><cell></cell><cell></cell><cell cols="2">Episodes [#]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The set of natural, real, and positive real numbers , , Sum, product, and integralp[•], p[•], P [•] Probability value/vector/matrix p(a|b)The conditional probability of a given b N (µ, σ 2 )The price value of P +</figDesc><table><row><cell></cell><cell>TABLE V</cell></row><row><cell></cell><cell>SYMBOLS AND NOTATIONS</cell></row><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>N, R, R +</cell><cell></cell></row><row><cell></cell><cell>The normal distribution</cell></row><row><cell>E[•]</cell><cell>Expected value operator</cell></row><row><cell>f , ∂[•]</cell><cell>The gradient of f ; The partial derivative of [•]</cell></row><row><cell>∝</cell><cell>Proportionality</cell></row><row><cell>f ⊗ g</cell><cell>Composite functions</cell></row><row><cell>ã</cell><cell>Estimated value of a</cell></row><row><cell>Notation</cell><cell>Description</cell></row><row><cell>t, T</cell><cell>Time slot and time horizon</cell></row><row><cell>D</cell><cell>Dataset</cell></row><row><cell>X</cell><cell>The training set, ∀X ∈ D</cell></row><row><cell>B</cell><cell>Set of buildings</cell></row><row><cell>i</cell><cell>index of buildings, such that B i ∈ B, ∀i ∈ N</cell></row><row><cell>d</cell><cell>index of electrical devices, d ∈ {1, .., m i }.</cell></row><row><cell>P +</cell><cell>Power generation</cell></row><row><cell>P -</cell><cell>Power consumption</cell></row><row><cell>P -d λ + t</cell><cell>Power consumption per device</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>http://austinenergy.com/wps/portal/ae/residential/rates/residential-electricrates-and-line-items (Last visit: 30 October 2016)</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1949-3053 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSG.2018.2834219, IEEE Transactions on Smart Grid</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computational methods for residential energy cost optimization in smart grids: A survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>St-Hilaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kunz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimization models and methods for demand-side management of residential users: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barbato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Capone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energies</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5787" to="5824" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decentralized multi-period economic dispatch for real-time flexible demand management</title>
		<author>
			<persName><forename type="first">E</forename><surname>Loukarakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Dent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="672" to="684" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal residential load control with price prediction in real-time electricity pricing environments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Mohsenian-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leon-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="133" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coordinated operation of a neighborhood of smart households comprising electric vehicles, energy storage and distributed generation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Paterakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Erdinc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Pappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bakirtzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P S</forename><surname>Catalão</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2736" to="2747" />
			<date type="published" when="2016-11">Nov 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on deman response programs in smart grids: Pricing methods and optimization algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vardakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zorba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Verikoukis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comunication Surveys Tutorials</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="178" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comfort-constrained demand flexibility management for building aggregations using a decentralized approach</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Kling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Smart Cities and Green ICT Systems (SMARTGREENS)</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Technical note: Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning, 1st ed</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reinforcement learning versus model predictive control: A comparison on a power system problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glavic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Capitanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2009-04">April 2009</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="517" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Residential demand response using reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levorato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Conference on Smart Grid Communications (SmartGridComm)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="409" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic pricing and energy consumption scheduling with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2187" to="2198" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Demand response of a heterogeneous cluster of electric water heaters using batch reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ruelens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Claessens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vandael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iacovella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vingerhoets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Belmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Power Systems Computation Conference (PSCC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch reinforcement learning for smart home energy management</title>
		<author>
			<persName><forename type="first">H</forename><surname>Berlink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H R</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence, ser. IJCAI&apos;15</title>
		<meeting>the 24th International Conference on Artificial Intelligence, ser. IJCAI&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning solutions for energy microgrids management</title>
		<author>
			<persName><forename type="first">V</forename><surname>Franc ¸ois-Lavet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Taralla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonteneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Workshop on Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised energy prediction in a smart grid context using reinforcement crossbuilding transfer learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Kling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy and Buildings</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="646" to="655" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Measuring short-term air conditioner demand reductions for operations and settlement</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Eto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>LBNL</publisher>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic network energy management via proximal message passing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kraning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lavaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="126" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On rectified linear units for speech processing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="3517" to="3521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Comparison of machine learning methods for estimating energy consumption in buildings</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Probabilistic Methods Applied to Power Systems</title>
		<meeting>the 13th International Conference on Probabilistic Methods Applied to Power Systems<address><addrLine>Durham, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning for estimating building energy consumption</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Kling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sustainable Energy, Grids and Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for household load forecasting -a novel pooling Deep RNN</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Demand forecasting at low aggregation levels using factored conditional restricted Boltzmann machine</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Power Systems Computation Conference (PSCC)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural network based demand side short term load forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energies</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building energy load forecasting using deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IECON</title>
		<imprint>
			<biblScope unit="page" from="7046" to="7051" />
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>in 42nd Annual Conference of the</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Intelligent buildings of the future: Cyberaware, deep learning powered, and human interacting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Manic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rodriguez-Andina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rieger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Industrial Electronics Magazine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="32" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning versus traditional machine learning methods for aggregated energy demand prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Paterakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stappers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Alst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Innovative Smart Grid Technologies</title>
		<meeting>the 7th IEEE International Conference on Innovative Smart Grid Technologies</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning based ensemble approach for probabilistic wind power forecasting</title>
		<author>
			<persName><forename type="first">H.-Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="56" to="70" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Supplement C</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time detection of false data injection attacks in smart grid: A deep learning-based intelligent mechanism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2505" to="2516" />
			<date type="published" when="2017-09">Sept 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1602.01783</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Highdimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1506.02438</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reinforcement learning of motor skills with policy gradients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="682" to="697" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online multitask learning for policy gradient methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bou-Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML</title>
		<meeting>the 31th International Conference on Machine Learning, ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June, 2014</date>
			<biblScope unit="page" from="1206" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
