<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Behavioural Biometrics in VR Identifying People from Body Motion and Relations in Virtual Reality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ken</forename><surname>Pfeuffer</surname></persName>
							<email>ken.pfeuffer@unibw.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Bundeswehr University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><forename type="middle">J</forename><surname>Geiger</surname></persName>
							<email>matthias.geiger@campus.lmu.de</email>
							<affiliation key="aff1">
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarah</forename><surname>Prange</surname></persName>
							<email>sarah.prange@hm.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Applied Sciences Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lukas</forename><surname>Mecke</surname></persName>
							<email>lukas.mecke@hm.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Applied Sciences Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Buschek</surname></persName>
							<email>daniel.buschek@ifi.lmu.de</email>
							<affiliation key="aff1">
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Alt</surname></persName>
							<email>florian.alt@unibw.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Bundeswehr University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">NY</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Behavioural Biometrics in VR Identifying People from Body Motion and Relations in Virtual Reality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1641FC19869423254F50F8CACAB3A513</idno>
					<idno type="DOI">10.1145/3290605.3300340</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Virtual Reality</term>
					<term>Behavioural Biometrics</term>
					<term>Motion</term>
					<term>Relation</term>
					<term>Proprioception</term>
					<term>Adaptive UIs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Every person is unique, with individual behavioural characteristics: how one moves, coordinates, and uses their body. In this paper we investigate body motion as behavioural biometrics for virtual reality. In particular, we look into which behaviour is suitable to identify a user. This is valuable in situations where multiple people use a virtual reality environment in parallel, for example in the context of authentication or to adapt the VR environment to users' preferences. We present a user study (N=22) where people perform controlled VR tasks (pointing, grabbing, walking, typing), monitoring their head, hand, and eye motion data over two sessions. These body segments can be arbitrarily combined into body relations, and we found that these movements and their combination lead to characteristic behavioural patterns. We present an extensive analysis of which motion/relation is useful to identify users in which tasks using classification methods. Our findings are beneficial for researchers and practitioners alike who aim to build novel adaptive and secure user interfaces in virtual reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Human-centered computing → Virtual reality;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Every person is unique with individual behavioural characteristics. A particularly salient one is physical body motion: how one moves, coordinates, and uses body segments in relation to each other <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>. In virtual reality systems, body motion is captured to provide a more immersed experience, ranging from head position and angle, via hand movements, to eye tracking and full body tracking capabilities <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>. The increasing proliferation of VR devices and the rapid technological advances will soon allow large-scale inferences about user activity, movements, and behaviour to be made. This research explores how motion data can be useful for behavioural biometrics to identify users in VR.</p><p>Implicit biometric identification of users in VR opens many opportunities. Authentication can be realized through behavioural biometrics, hence minimizing interruptions usually caused by explicit authentication schemes <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>. As the user's identity can be continuously assessed in the background, potential unauthorised access can be detected (and the device locked) as the original user is still logged in.</p><p>Furthermore, usability can be enhanced. For example, in situations where devices are shared among people, the system can detect a new user and adapt the UI to their preferences or offer personalised content. For example, if within a family an adult is identified from the first movements, an overview of the day's work could be shown, whereas when a child is identified, their favourite cartoon may be triggered.</p><p>Therefore, we focus on how body motion, and the relations between body segments may be useful as behavioural biometric in VR. This is interesting as many body movements are not specific to one segment, such as the hand, but often coordinate with other parts of the body such as the arm relative to body posture or hands to each other, involving motor control, proprioception, or hand-eye coordination <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>. Research showed that combinations of biometric features can enhance identification accuracy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21</ref>]. Yet, there are numerous combinations of body motion features.</p><p>In this paper, we first analyse body motion and how movements may relate to each other to identify body feature sets of interest. We then conduct a user study to collect motion data, particularly head, hands, and eye movement that can be captures in current VR devices such as the HTC Vive, Oculus Rift, or FOVE. The study involves common VR interactions including pointing, grabbing, walking, and typing as fundamental tasks to interact with virtual content. Lastly we evaluate the motion data with machine learning methods to analyse various body motion combinations with regards to how accurate they allow user identification and authentication. We present detailed results of which motion is useful to identify users in which tasks. For this reason, our main contribution is in understanding which body relations are more accurate to describe individual user behaviour.</p><p>Contribution Statement. First, we provide a theoretical analysis of body motion theories as basis for behavioural biometrics. Second, we report on a user study (N=22) in which we collected detailed motion recordings of hand, head, and eye movements during different tasks. Third, we contribute an analysis that reveals body features suitable for user identification as well as authentication and assess accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Behavioural biometrics research explores what kind behaviour may be unique to a user. The goal is to enable systems to identify and authenticate users -usually in an implicit manner. Systems can become more resistant to unauthorised access, they can improve usability by avoiding physical or digital tokens such as a key or a password <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>, or they can adapt their UI to the user. The procedure can run in the background as users interact normally without requiring to explicitly provide identifiable information, such as a fingerprint or face ID. On current smartphones, researchers, for example, showed how touch motion behaviour can be used to identify users <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Use of body motions as behavioural biometrics has a long tradition in security related research <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b48">49]</ref>. Gait has been extensively researched in the context of various possible features as users inhere unique walking patterns <ref type="bibr" target="#b47">[48]</ref>. Research on anthropomeasures includes body motion as a whole, e.g. by using depth sensors such as the Kinect, for person identification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref>. Estimation of body height, shoulder breadth, or stride information from cameras can be useful to design systems that identify users form multiple biometric sources <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. A user's eye movement information such as saccadic vigor, acceleration cues, and response to visual stimuli have shown promising results on user authentication <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>These works show that various body movements are accurate measures for person identification, which motivates us to explore these in virtual reality systems.</p><p>Researchers investigated behavioural biometrics for mobile, wearable, VR or AR devices. Kupin et al. assessed taskdriven biometric authentication and showed that using a ball-throwing task, 14 users could be accurately identified <ref type="bibr" target="#b24">[25]</ref>. <ref type="bibr">Mustafa et al.</ref> showed how head pointing motion from Google Cardboard sensors can be used to identify users <ref type="bibr" target="#b35">[36]</ref>; other researchers focused on head nodding to music <ref type="bibr" target="#b26">[27]</ref> and explicit head gestures from Google Glass <ref type="bibr" target="#b45">[46]</ref>. GaitLock can authenticate users during walking tasks from the on-board inertial measurement units of a Google Glass to protect VR and AR systems <ref type="bibr" target="#b39">[40]</ref>. Ashbya et al. show how electroencephalography in response to mental imagery tasks allow accurate user authentication on low-cost headworn sensors <ref type="bibr" target="#b1">[2]</ref>. Brain Password leverages brain responses in relation to visual stimuli as biometric features for smart headwear <ref type="bibr" target="#b28">[29]</ref>. Prior work focuses on specific aspects of the user's body motion, which we complement with a more holistic view on the combination of body motions.</p><p>Our work also contributes to the notion of soft biometrics -behavioural traits that may not be unique features, but can complement primary identifiers or be used temporarily <ref type="bibr" target="#b19">[20]</ref>. Reid and Nixon introduced a method to gather relative measures based on comparative human descriptions. This allowed subjects in video data to be accurately retrieved <ref type="bibr" target="#b37">[38]</ref>. Dantcheva et al. 's survey provides an exhaustive review of research on different user characteristics, demonstrating their use for person identification from soft biometrics <ref type="bibr" target="#b10">[11]</ref>.</p><p>The literature shows that body motions are promising features with most papers focusing on single body segments or sensors. As the body provides a plethora of motions as potential features that are potentially related, we investigate body motion and spatial relations to identify users in VR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BODY MOTION &amp; BEHAVIOURAL FEATURES</head><p>Consideration of body motion and their coordination dates back to the Roman author Pollio's thoughts on the correlations of ideal human proportions <ref type="bibr" target="#b36">[37]</ref>, illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, to which every person deviates in some way. This creates the potential of using body relations as biometrics.</p><p>In HCI, the notion of Reality Based Interaction emphasises that human-computer systems should take advantage of pre-existing knowledge of reality; of relevance here are the themes of Body Awareness &amp; Skills and Environmental Awareness &amp; Skills <ref type="bibr" target="#b18">[19]</ref>. We discuss body relation theories interesting for biometrics. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the theories of body motion we consider. We apply these theories in context of motion types available in current VR systems:  <ref type="bibr" target="#b36">[37]</ref> indicate correlations of the body useful as user identifiers.</p><p>• Head: the intertial motion sensors of the VR headset allow estimation of head position (HeadPos), direction (HeadRay) and rotation. • Hands: controllers allow estimating hand motion for both dominant (DH) and non-dominant hands (NDH). One can consider each hand's position and pointing ray separately (DHPos, NDHPos, DHRay, NDHRay). • Gaze: by using eye trackers, we can track the direction (ray) where the user is looking in the virtual scene.</p><p>Motor Control (Figure <ref type="figure" target="#fig_1">2a</ref>)</p><p>In neuroscience research, Massion emphasises two types of motor control that users can perform to fulfill a goal, considered here as stabilisation and movement <ref type="bibr" target="#b31">[32]</ref>. Stabilisation occurs when humans maintain a reference value against external and internal disturbances, for example,. to stabilise body posture against wind. Movement denotes the displacement of body segments toward a goal, for example, when reaching out or grasping an object. Most motor acts have both types of motor control combined, such as when raising an arm, leg muscles are activated for postural control <ref type="bibr" target="#b3">[4]</ref>. VR interactions can involve both types of the motor tasks. For example, in a pointing task the user may first stabilise their body to an optimal posture for the manual act, and then perform the pointing movement. Thus, to describe a pointing task more comprehensively to better capture individual user behaviour, it may be relevant to include body position too (here we consider the HMD location as approximation of body position). Head position itself may be a good feature, as every user will have a different body height. Thus, particular feature combinations can be derived from:</p><p>• Position of body segment (head, hands) • Movement/Motion of body segment (head, hands) Proprioception (Figure <ref type="figure" target="#fig_1">2b</ref>) Proprioception is the ability of humans to sense the position, orientation, and movement of their limbs, muscles, and joints in relation to each other <ref type="bibr" target="#b13">[14]</ref>. In HCI, it has been suggested to enhance the UI, for example, for wearables <ref type="bibr" target="#b29">[30]</ref> or virtual reality interfaces <ref type="bibr" target="#b33">[34]</ref>. Users frequently take advantage of their proprioceptive senses, with the body providing a physical reference frame in which to operate direct, precise, and potentially eyes-free. For manipulation, holding an object in the hand provides a good sense of its position and greater sense of control. Conversely, understanding and controlling an object can be more difficult when using non-linear mappings between hand and object <ref type="bibr" target="#b7">[8]</ref>. Another example of proprioception are Guiard's observations on bimanual asymmetry <ref type="bibr" target="#b15">[16]</ref>. The NDH tends to precede the action of the DH, and acts as the spatial frame of reference (potentially supported by proprioceptive senses). For example, in writing, the NDH is orienting the paper while the DH is writing.</p><p>In the context of behavioural biometrics, how do the relative movements between segments of the users body contribute to biometric behaviour in contrast to absolute measures? We can sense the triangular reference frame that users span out with their hands to their head, i.e. how they move their hands relative to the body root position. Furthermore, we investigate two manual tasks -grabbing (grab target directly in hand) with a linear mapping and pointing (users point with the controller to a remote target) -as an example for a non-linear mapping. Particular features are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Spatial relation between left hand and body (head) • Spatial relation between right hand and body (head) • Spatial relation between DH and NDH</head><p>View Relation (Figure <ref type="figure" target="#fig_1">2c</ref>) User interactions are often guided by the visual stimuli available to the user. For example, eye-hand coordination describes the user's spatial coordination of the their eye gaze in relation to their hands. The eyes can direct the hands to a target, visually identifying relevant information for the hands to act <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>. On the other hand, manual action of the user may guide the visual, for example. when drawing a line the eyes follow the hand's interaction closely. In the context of walking, the user visually inspects the space around them to navigate through paths and avoid obstacles.</p><p>We aim to see whether users inhere distinct eye-to-body relations. Users with less experience in manual tasks may be subject to a more visual approach, where they want to closely follow their interaction toward the target. Experienced persons can guide themselves eyes-free and interact, for example, simply by muscle memory; other users may be somewhere in between. Therefore, as features we add: World Reference &amp; Target (Figure <ref type="figure" target="#fig_1">2d</ref>) User behaviour can be influenced by external factors of the surrounding world. In VR/AR systems, digital information and manipulation targets can be spatially located relative to their view, body, or the surrounding world <ref type="bibr" target="#b5">[6]</ref>. For instance, in a manipulation task, the user reaches out and manipulates an object in space, and how they reach may depend on each user's individual strategy. As users visually acquire the target before manual action <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44]</ref>, eye movements may similarly contribute to user strategy. This related to understanding biometric user behaviour in the context of Environmental Awareness &amp; Skills when acting toward objects relative in physical space <ref type="bibr" target="#b18">[19]</ref>. Based on this, we explore how spatial relations between segments of the user and the target in world space may be useful as biometric behaviour. In particular:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Spatial relation between eye gaze and head ray • Spatial relation between eye gaze and controller ray</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Spatial relation between hand and target • Spatial relation between eye gaze and target • Spatial relation between head direction and target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STUDY</head><p>In this study we gather user motion data (head, hands, eyes) in common VR interactions across two sessions. The goal is to investigate and better understand user behaviour based on various motion and assess their biometric quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Study Design</head><p>Our goal was to find a few tasks that represent generic tasks in VR. We selected four tasks with the help of a review of related work and current applications in VR. We now describe the tasks. For details on the layout see Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>We conducted a within-subjects study design with repeated measures, over two sessions with at least 3 days in between. The conditions are specific to each task.</p><p>Pointing (Figure <ref type="figure" target="#fig_3">3a</ref>). Pointing at a target is a fundamental task as most VR systems employ raycasting controllers. Here a pointing tasks with the user hovering over a target with the controller ray, at which its color changes from dark blue to light blue for indicating this input state. A target selection is valid if the trigger click and release events are both executed in target hover state. After a valid selection, the selected target is hidden and the next target is shown to the user. Grabbing (Figure <ref type="figure" target="#fig_3">3b</ref>). Grabbing is another in elemental task in VR, where users can directly move their hand (represented by the controller) to a target for selection. The selection finishes when any part of the controller is within the target (feedback given by target highlighting) and the user clicks and releases. Two conditions are used: big and small targets.</p><p>For both pointing and grabbing tasks, arrangement of targets is based on the multi-directional tapping task described in the ISO9241-9 standard <ref type="bibr" target="#b17">[18]</ref> adapted to 13 targets based on Teather and Stuerzlinger's evaluation of 3D pointing <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. 13 spherical targets are ordered in a circular arrangement for each condition (2 m / 4 m distance to user position), and users select them by alternating clockwise around the circle.</p><p>• Target size condition: small (0.15 cm) and big (0.30 cm) • Distance between targets: 0.7 m • Target positions: 13 • Repetitions: 10 • Overall: 260 trials per session Walking (Figure <ref type="figure" target="#fig_3">3c</ref>). Users navigate through the virtual space and physical walking is a prominent method to move in the local space. In this context, gait has been shown as a promising feature <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b47">48]</ref>. We complement this work with a perspective on body user motion.</p><p>In this task users are walking from a start to an end position. The end position is visualised by an outline of a 2 meter high door. When the user's head position collides with the layer, the task is finished and the next layer visualised. The user turns around and walks to it. To fit the tasks within the tracking range of the VIVE lighthouse, the path of the tasks were designed accordingly. Two paths were chosen that each include 4 straight line movements (2 short and 2 long lines). The two paths were counterbalanced. Typing (Figure <ref type="figure" target="#fig_3">3d</ref>). Typing behaviour has been shown to be a promising feature for mobile devices <ref type="bibr" target="#b9">[10]</ref>. In VR, an efficient typing method is tracked by hand-held raypointing controllers <ref type="bibr" target="#b40">[41]</ref>, which we chose for our investigation.</p><p>We designed a typical typing task where the user sees keyboard, target sentence, and textfield on a screen 3.75 m in front of the user. The keyboard was 4 × 1.6 m large and each key 0.4 m 2 . 15 sentences were randomly selected from MacKenzie and Soukoreff's phrase sets <ref type="bibr" target="#b30">[31]</ref>. The average sentence length was 25.8 letters (SD=3.14). The key is selected when users hover over it and click (button_down event). When users type a wrong letter, they can correct it. A trial finishes when all letters were written. The user can make a break or continue with the next by clicking the touchpad.</p><p>• Sentences: 15 • Letters per sentence: 25.8 (SD=3.14)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Each user performed two sessions with a pause in between of at least 3 days. A session started with a brief introduction by the experimenter and a demographic questionnaire. Then, the user performed the tasks. An eye tracker calibration was conducted before each task. The user performed blocks of tasks (for example, 13 targets in succession), after which the user had the option to take a break. On average, each session of the study took approximately one hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>The VR setup is shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Log</head><p>During all tasks, the VR application logs position, rotation, velocity and angular velocity of each device (HMD, controllers) as well as the current gaze point and collision points of the devices' raycasts (HMD, hand controllers, gaze) related to the area of interaction with a sample rate of 100 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>A total of 22 paid participants (4 female, 3 left-handed) took part in the study. On average they were 26.05 years old (SD: 3.81, range: 19-35 years) and had an average height of 179.9 cm (SD: 8,66). They were either students from the local university or employees with mixed background. 6 participants wore glasses, 2 wore contact lenses and 14 had no constraints regarding eye-sightedness. Regarding experience in virtual reality on a scale from 1 (no experience) to 5 (very experienced), users were little experienced (M: 2.1, SD: 0.86).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DATA OVERVIEW</head><p>To understand the study data, we plot a few tasks. Figure <ref type="figure" target="#fig_6">5</ref> shows the task completion times for each task/session. Across sessions, there is a learning behaviour. Note that the typing task is accomplished when entering the whole sentence, hence the longer times task completion times.</p><p>To further investigate the learning rate, we plotted task completion times over the conditions of the study. Here we focus on the pointing and grabbing tasks, as they have comparable times. Note, that the other tasks showed similar trends. Figure <ref type="figure" target="#fig_7">6</ref> shows increased task completion times for the first trial. We also plotted the number of targets over time and found an increased time for the first trial as evidence for learning effects.</p><p>For body relations, we looked at the average distance that users had between the main devices, i.e. head, dominant, and non-dominant hand positions (Figure <ref type="figure" target="#fig_8">7</ref>). It shows how users exhibit different distances depending on the task. Moreover, it highlights user characteristics such as arm length and body height that are also relevant factors for body relations.</p><p>Considering target and view based relations, we focus on the two manual tasks to assess the relations from the user to the target (see Figure <ref type="figure" target="#fig_9">8</ref>). First, as expected, users look closely  at the target to accomplish the task. In the grabbing task, users on average look a bit further away from the targetthis is probably a result of (muscle) memory which supports eyes free interaction. As users directly move their hands to the targets, users can remember the spatial locations and aim with less visual control. This is also evident in the target to DH position distance which is substantially lower in the grabbing task, i.e. users are closer to the target with their hand (by design of the task).</p><p>Notably, the target to DHRay is at odds with the expectation, as we would expect users to point closely to the target. We think this makes sense, as the ray position is averaged over the time of the task and at the beginning of each task users point quite far off the target location. Figure <ref type="figure" target="#fig_10">9</ref> shows the relation of three visual distances over time. We used a logarithmic scale to show the extreme distances that the controller's ray and the target span initially. It confirms that the ray comes closer when finishing the task (averaging over all tasks including fast users still resulted in a high distance). The figure also shows how the user's gaze comes closer to the target. This is interesting for the grabbing task where users visually acquire the target and the hand follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MODELLING AND EVALUATION METHOD</head><p>We train Random Forest and Support Vector Machine (SVM) classifiers using the scikit-learn Machine Learning library for Python. We use default values for the hyperparameters (apart from trees=100 and C=1e4).</p><p>We filtered out the 3 left-handed users, the first two runs, and first two targets to avoid learning effects. For task typing, we filtered another user as the eye tracker did not work. For pointing and grabbing: 11 targets × 8 runs × 2 conditions = 176 targets; for walking: 8 runs × 2 conditions × 2 conditions = 32 targets; for typing: 13 targets; all per session per user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Features</head><p>We generate features by preprocessing the raw time series data from all sensors. We split the data by session, task, repetition, and action (e.g., by targets for the pointing task). In addition, for each device (controllers, HMD), the virtual reality system provides vectors for velocity, angular velocity, and rotation, which we include as features, too.</p><p>To assess the categories from the body motion theories introduced in our concept section, we provide the following feature categories as listed in Table <ref type="table" target="#tab_0">1:</ref> • Individual: these represent individual sensor types, e.g., DH (Motion) stands for the controller in the dominant hand. 'Motion' denotes the relative movement of the device, derived by the meter length of the path that the device moved from start to end of the task. • Move+Stabilise: This category combines two Motion types into one feature set. It is used to mutually investigate effects of motion between body segments. • Distance: In this category, the Euclidean distance is used between two positions in the 3D virtual space. • Target/Distance: These feature sets use the Euclidean distance from the device to the target position. For the walking task, the center of the door is the target. • Target (visual angle): The visual angle is computed between the target position and the position of another device; the metric is degrees of visual angle.</p><p>• View (visual angle): This category includes all visual angle measures between user-related features, such as the angle between the ray span between the forward ray of the HMD and the user's eye gaze direction. • All (category): Includes all features of one category.</p><p>• All: Includes all features of all categories.</p><p>For each of the resulting feature subseries we compute the mean, min, max, and standard deviation of each sensor. For example, we get the four value types of the head motion during the time frame in which participant 1 aimed at the 12 o'clock target in her first repetition of the pointing task in her first session. This means that each line in Table <ref type="table" target="#tab_0">1</ref> includes four data types that describe the feature or feature set.</p><p>These values define the feature vector of one such action. We use these feature vectors as training instances. The label for each vector is given by the corresponding participant ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>We test several feature sets motivated by interesting comparisons, such as the relative importance of different tracking devices (e.g., controllers, headset, gaze). In addition, we report on optimised feature sets per task. These optimised sets are found with a wrapper-based feature selection approach <ref type="bibr" target="#b23">[24]</ref>, which uses a greedy search through the space of feature combinations based on the performance of the actual classifier. We refer the reader to the related work for details. We train the model on the data from the first session and test on the data from the second session. This corresponds to a scenario in which user enrolment happens once on one day and verification happens on other days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identification</head><p>In the identification scenario, we train a Random Forest classifier on the described training instances and labels. The resulting model makes a prediction for a new feature vector based on interactions which participant p i from the set of all participants P performed. This corresponds to use cases in which the system is supposed to identify a user from a set of known users. We report the accuracy here (F1 scores available in supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identification vs. Group Size</head><p>Of interest is also the influence of the size of the user group from which to identify the interacting user. In general, the classification problem becomes harder if there are more people to distinguish, since, e.g., behaviour that might be characteristic among five users might not be unique among ten.</p><p>Thus, we evaluate classification performance across group sizes as follows: For each group size д (from 2 to 19), we draw 100 random subsets of д participants from the set of all participants. For each such draw, we then evaluate the classifier on this subset of participants as described before. We report the average performance across the 100 draws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>We report classification accuracy -the number of correct classification decisions divided by the total number of decision (i.e. number of test cases). We report this performance measure since it is commonly used and easy to interpret. Accuracy is adequate in all our evaluation schemes since we always have balanced test sets (i.e. same number of test cases for each user/class). Thus, results can be compared to the baseline of guessing: 1 out of 19 users =5.26% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>Overall, the best body motion to identify and authenticate users are head motions, and distances between the devices. Table <ref type="table" target="#tab_0">1</ref> shows the results for each feature and combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identification</head><p>Overall we reach accuracies of about 40% to identify users across sessions, for each task. In the following, we report on individual aspects of the results.</p><p>Relative distances are most accurate overall: Overall the set of features of relative distances between DH, NDH, and Head showed the highest accuracy to identify users. This shows that proprioception indicates promising features to identify users which is aligned with the task-specific analysis of body relations shown in Figure <ref type="figure" target="#fig_8">7</ref> (except for grabbing, where the relations consistently received low scores).</p><p>Head motion is second accurate overall: As a single set of features from the HMD, head motion consistently scored over 30% accuracy and represents a practical source of motion information if controllers are not available.</p><p>Pointing is better than grabbing for hand movement: In grabbing, users have to move to the same absolute positions, thus hand movements resulted in less accurate results. In comparison, pointing tasks allow users to exhibit more freedom in the manual movement.</p><p>Head movement is suitable for walking tasks: We find that head movement can better describe individual user walking than hand movements. This confirms prior work that found gait is well modelled using headwear <ref type="bibr" target="#b39">[40]</ref>.</p><p>View relations are similarly accurate as head motion for grabbing tasks: In the grabbing task, all view-based feature sets (30.59%) reached almost the accuracy of all head motion features (31.25%). This can be explained by the design of the task, where the targets were spread across a large area in front of the user, necessitating frequent changes of view.</p><p>Best Feature Sets. We used the described feature selection algorithm for finding best combinations among all features  per task. To do so, we separated each individual category feature into X/Y/Z. Furthermore, we include relations between DHPos, DHRay, NDHPos and NDHRay. Distance and View feature categories are unchanged. The resulting feature sets as presented in Table <ref type="table" target="#tab_1">2</ref> achieved a better accuracy than our systematic tests (cf. Table <ref type="table" target="#tab_0">1</ref>), thus showing that further combinations can improve the system. The largest improvement is for the pointing task from 41.39% to 63.55%. This feature set integrates many body relations of Distance and View, thus indicating that proprioception and view based positions provide strong features. The same is true for Grabbing where less features were found to be relevant -yet distance and visual angle are among them.</p><p>For walking, the features are all related to the user's head motion, aligning with the understanding that head movement is a good identifier of gait. The View-based properties span large visual angles between the HMD forward vector and the user's hands (usually held waist down at walking).</p><p>Here we think the maximum viewing angles between head and controllers are more distinctly pronounced than their respective distances or positions, and selected as features.</p><p>For typing, an increased accuracy is achieved by combining Head, Distance, and features. This correlates with the individual accuracy results as reported in Table <ref type="table">However</ref>, the feature set optimisation algorithm found a better combination than our systematic approach.</p><p>Identification vs. Group Size. We plotted the identification accuracy across user group results in Figure <ref type="figure" target="#fig_11">10</ref>. We used each of the combined feature sets per category ('all' in Table <ref type="table" target="#tab_0">1</ref>), and additionally the best feature group found with the feature selection algorithm. Over all tasks, the diagrams show that with increasing group size, the accuracy decreases at a logarithmic rate. For the pointing task, the best feature set (pink line) decreases at a substantially lower rate and remains above 60%, whereas some of the other feature sets decrease to under 30% of user identification. Similarly, the best feature set also achieves best scores in the grabbing task, although the rate of decrease is similar to other feature sets. In the walking and typing tasks, three feature sets are on a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>Our main contribution is the detailed analysis on user identification accuracy of a plethora of body relations, ranging from motion, distances between segments, and how users visually perceive relations between gaze, head direction, and hand positions. Our work allows the relative biometric value of these body features to be understood, pointing to what feature is more unique among users. This leads to a better understanding of how body motion, proprioception, and internal/external reference points to the user's body are related to each other. Researchers can assess and compare these features depending on which part may be of relevance to their future work. We also want to highlight the main insights:</p><p>Body distances are most accurate: The higher accuracy of body relations and distances suggests that proprioception <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref> and hand coordination <ref type="bibr" target="#b15">[16]</ref> may be stronger biometrics than individual features, as focused on in prior work.</p><p>Interestingly, one could question whether this can be attributed to absolute biometrics such as arm and body size, but that does not explain why features that consider eye gaze information are so prominent in our feature optimization results (Table <ref type="table" target="#tab_1">2</ref>). It indicates that the biometric value also arises from how users coordinate aspects of the visual and manual abilities, which demands further study.</p><p>Pointing is more accurate than grabbing: in pointing, users can exhibit more diverse movement. In grabbing, all users have to move their hand to the same targets, thus creating less individual movement. In conclusion, pointing is more suitable as a biometric.</p><p>The dominant hand provides strong features: When pointing, users are free to, e.g., move their hand in direction of the target, or alternatively to make small wrist rotations. These strategies lead to more individual features, and support recent results on dominant hand motion authentication <ref type="bibr" target="#b24">[25]</ref>.</p><p>Our modelling approach reached a maximum of 63% accuracy, which may seem low for practical use. At the same time, our work provides a first fundamental investigation of these biometric features in the VR context. We show that accuracy improves with fewer users (Figure <ref type="figure" target="#fig_11">10</ref>): user identification with our used model can be particularly useful for small user groups such as families or companies, for example, to realise some user adaptation of the system.</p><p>For more users, we see several ways to improve the modelling. First, we could evaluate further classifiers, features, and variations and combinations thereof. A more substantial extension could investigate other treatments of the temporal aspect of the data, including, for example, extracting features for rolling windows over time instead of aggregation per action. Moreover, we could investigate the frequency domain of features instead of the time domain. Finally, considering that our identification models vastly outperform chance level by about a factor of ten, we could investigate potential improvements via aggregation of evidence over time (for example, decision after observing multiple actions). We thus see interesting opportunities for investigating these optimisations in future work.</p><p>Beyond accuracy, we contribute to the understanding of human motion for user identification. Our concept section and Figure <ref type="figure" target="#fig_1">2</ref> provide a systematic overview of how previous work has considered motion and derives relations between body segments. The list of results in Table <ref type="table" target="#tab_0">1</ref> promotes discussion on the different body motions and how suitable they are as predictors for individual human motion. Moreover it characterizes 4 fundamental interaction tasks with regards to which motion features are valuable to identify users, showing overall good predictors across tasks (for example, distance or head), as well as which specific combinations scored highest (Table <ref type="table" target="#tab_0">1</ref>). Depending on available sensors and tasks, a developer can use our results to make an informed decision.</p><p>Our results are based on a user study, which was rigorously designed but should be considered carefully with the following limitations. First, the tasks were designed to represent typical VR interactions. This may naturally involve bias factors. In walking to a target, users may adjust stride length. Furthermore, users may get tired from repetitive manual tasks, such as interacting with a design application where users are continuously drawing or a game where users point to shoot. These are factors common in everyday interactions and we intentionally did not control these factors but call for future studies to better understand potential bias factors.</p><p>The results may also be affected by our choice of users. Authentication systems benefit from testing with more participants than the 19 we had. This was sufficient to show that the accuracy stagnated with an increasing number of users (Figure <ref type="figure" target="#fig_11">10</ref>), but testing with more users is relevant to really assess the potential for an authentication system. Additionally, features that involve body relations may be affected by physical attributes such as the user height and arm length. However, that does not explain why features that consider visual information are so prominent in our feature optimization results (Table <ref type="table" target="#tab_1">2</ref>). It indicates that the biometric value also arises from how users coordinate aspects of the visual and manual abilities, which demands further study. Finally, our user set does not balance gender well. We expect that more diversity might increase user identification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>Our work provides an exploration of a broad range of body relations, that points to the possibility for continuously identifying and authenticating users in current VR systems and, hence, overcoming the limitations of explicit authentication mechanisms <ref type="bibr" target="#b14">[15]</ref>. Features such as head motion and distances between head and the controllers, which have shown higher recognition accuracy than other features, can be integrated right now as a soft biometric identifier to enhance the default authentication method. In the short-term future, further explorations into motion relations beyond hands and head may reveal better features. Also more advanced machine learning methods, such as deep learning, may make the system more accurate. In the long run, motion tracking methods may be able to capture whole body motion as well as the space around the user in realtime and exploit this for user identification and general user behavioural analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">ACKNOWLEDGMENTS</head><p>This research was supported by the German Research Foundation (DFG), Grant No. AL 1899/2-1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Da Vinci's illustration of human proportions<ref type="bibr" target="#b36">[37]</ref> indicate correlations of the body useful as user identifiers.</figDesc><graphic coords="3,101.85,92.65,144.15,113.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Body based motion and relations useful as biometric features.</figDesc><graphic coords="4,107.80,92.65,396.40,95.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Target distance condition: close (2 m) and far (4 m) • Distance between targets: 2m • Target positions: 13 • Repetitions: 10 • Overall: 260 trials per session</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Study design and a VR screenshot of the user's perspective for each task, respectively.</figDesc><graphic coords="5,53.80,92.66,504.38,154.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>It is based on an HTC Vive (HMD, two controllers, optical tracking) which is equipped with an additional eye tracker (Pupil Labs). The VR application is implemented in C# in Unity 3D on a Windows 10 PC (Intel Core i7-6700, 32GB RAM, NVIDIA GeForce 1080). The eye tracker's accuracy was measured in a 9-point accuracy test at the study end (M=5.46 • , SD=9.36 • ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A user during the study with the VIVE VR headset.</figDesc><graphic coords="5,335.97,589.69,204.19,114.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Task completion times for each task and session.</figDesc><graphic coords="6,53.80,92.65,240.25,143.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Learning rate for pointing and grabbing tasks.</figDesc><graphic coords="6,53.80,262.62,240.24,139.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Distances between head, (non) dominant hand.</figDesc><graphic coords="6,317.96,92.65,240.24,135.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: View based features for two tasks.</figDesc><graphic coords="6,317.96,264.95,240.24,141.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Visual angle over task time.</figDesc><graphic coords="7,53.80,92.65,240.26,123.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Identification accuracy across user group sizes, for each task.</figDesc><graphic coords="9,53.80,92.65,504.51,104.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Pointing (63.55%) Grabbing (45.84%) max(DH AngVel), avg(NDH AngVel) avg(DH Rotation), avg(Dist NDH, HEAD), max(Distance NDH, HEAD), avg(View DHRay, NDHPos) max(Distance DH, NDH), avg(View HMDRay, NDHPos) avg(Distance DH, HMD), min(View HMDRay, DHPos), max(View HMDRay, DHPos), avg(View HMDRay, DH Pos) Walking (49.67%) Typing (54.27%) min(Head Vel) avg(NDH Rotation) max(Head AngVel) avg(Head Rotation) max(Distance DH, HEAD) max(Distance DH Head) max(View HMDRay, NDHRay) max(View DHRay, DHPos) min(View HMDRay, NDHPos) avg(View Gaze, NDHPos) avg(View HMDRay, NDHPos)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy results of features and feature sets of body motion categories (* denote best per task; bold and cursive denote category 1st and 2nd best, respectively).</figDesc><table><row><cell></cell><cell>Point</cell><cell>Grab</cell><cell>Walk</cell><cell>Type</cell></row><row><cell>Guessing</cell><cell>5.26</cell><cell>5.26</cell><cell>5.26</cell><cell>5.56</cell></row><row><cell>Individual</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DH (Motion)</cell><cell>17.11</cell><cell>13.97</cell><cell>13.32</cell><cell>12.82</cell></row><row><cell>DH (Velocity)</cell><cell>21.59</cell><cell>18.36</cell><cell>14.97</cell><cell>21.79</cell></row><row><cell>DH (AngVel)</cell><cell>21.08</cell><cell>24.22</cell><cell>14.31</cell><cell>18.8</cell></row><row><cell>DH (Rotation)</cell><cell>20.07</cell><cell>26.44</cell><cell>22.04</cell><cell>25.64</cell></row><row><cell>DH (All)</cell><cell>29.25</cell><cell>30.08</cell><cell>22.37</cell><cell>33.33</cell></row><row><cell>NDH (Motion)</cell><cell>8.76</cell><cell>14</cell><cell>17.43</cell><cell>14.96</cell></row><row><cell>NDH (Velocity)</cell><cell>12.05</cell><cell>18.96</cell><cell>16.94</cell><cell>18.8</cell></row><row><cell>NDH (AngVel)</cell><cell>17.17</cell><cell>15.85</cell><cell>15.62</cell><cell>15.81</cell></row><row><cell>NDH (Rotation)</cell><cell>26.88</cell><cell>23.33</cell><cell>12.01</cell><cell>25.21</cell></row><row><cell>NDH (All)</cell><cell>22.22</cell><cell>25.57</cell><cell>22.7</cell><cell>31.2</cell></row><row><cell>Head (Motion)</cell><cell>12.59</cell><cell>14.14</cell><cell>19.08</cell><cell>13.68</cell></row><row><cell>Head (Velocity)</cell><cell>17.34</cell><cell>17.82</cell><cell>22.04</cell><cell>28.21</cell></row><row><cell>Head (AngVel)</cell><cell>21.62</cell><cell>21.68</cell><cell>20.39</cell><cell>27.78</cell></row><row><cell>Head (Rotation)</cell><cell>32.21</cell><cell>27.81</cell><cell>28.12</cell><cell>34.62</cell></row><row><cell>Head (All)</cell><cell>33.67</cell><cell cols="2">31.25* 39.47</cell><cell>37.61</cell></row><row><cell>All</cell><cell>33.34</cell><cell>27.03</cell><cell>39.31</cell><cell>38.03</cell></row><row><cell>Move+Stabilise</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DH, NDH</cell><cell>17.55</cell><cell>18.27</cell><cell>19.24</cell><cell>20.09</cell></row><row><cell>DH, Head</cell><cell>23.44</cell><cell>19.8</cell><cell>20.39</cell><cell>24.79</cell></row><row><cell>NDH, Head</cell><cell>13.04</cell><cell>15.76</cell><cell>22.53</cell><cell>22.65</cell></row><row><cell>DH, NDH, Head</cell><cell>23.03</cell><cell>18</cell><cell>24.84</cell><cell>30.77</cell></row><row><cell>All</cell><cell>22.19</cell><cell>18.57</cell><cell>24.84</cell><cell>24.79</cell></row><row><cell>Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DH,NDH</cell><cell>18.09</cell><cell>10.53</cell><cell>12.66</cell><cell>18.38</cell></row><row><cell>DH,Head</cell><cell>22.79</cell><cell>10.44</cell><cell>25</cell><cell>25.64</cell></row><row><cell>NDH, Head</cell><cell>22.22</cell><cell>13.28</cell><cell>20.72</cell><cell>10.26</cell></row><row><cell cols="2">DH,Head; NDH,Head 29.04</cell><cell>10.94</cell><cell>33.88</cell><cell>35.04</cell></row><row><cell>All</cell><cell cols="2">41.39* 14.86</cell><cell cols="2">43.42* 44.44*</cell></row><row><cell>Target/Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target, DH</cell><cell>10.89</cell><cell>12.05</cell><cell>16.61</cell><cell>8.12</cell></row><row><cell>Target, NDH</cell><cell>11.84</cell><cell>13.19</cell><cell>11.51</cell><cell>11.11</cell></row><row><cell>Target, Head</cell><cell>7.24</cell><cell>9.15</cell><cell>25</cell><cell>7.69</cell></row><row><cell>All</cell><cell>20.96</cell><cell>19.29</cell><cell>32.07</cell><cell>14.53</cell></row><row><cell>Target (viual angle)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target, HMDRay</cell><cell>6.13</cell><cell>7.72</cell><cell>8.72</cell><cell>8.97</cell></row><row><cell>Target, Gaze</cell><cell>10.11</cell><cell>11.27</cell><cell>10.36</cell><cell>9.4</cell></row><row><cell>Target, DHRay</cell><cell>9.75</cell><cell>13.1</cell><cell>12.01</cell><cell>19.23</cell></row><row><cell>Target, NDHRay</cell><cell>10.11</cell><cell>9.03</cell><cell>15.79</cell><cell>11.54</cell></row><row><cell>All</cell><cell>15.85</cell><cell>21.32</cell><cell>15.3</cell><cell>20.09</cell></row><row><cell>View</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HMDRay, Gaze</cell><cell>7.33</cell><cell>7.39</cell><cell>8.55</cell><cell>6.84</cell></row><row><cell>HMDRay, DHRay</cell><cell>13.04</cell><cell>11.63</cell><cell>14.47</cell><cell>14.53</cell></row><row><cell>HMDRay, DHPos</cell><cell>15.67</cell><cell>11.27</cell><cell>13.49</cell><cell>18.38</cell></row><row><cell>HMDRay, NDHRay</cell><cell>09.09</cell><cell>13.82</cell><cell>15.79</cell><cell>19.23</cell></row><row><cell>HMDRay, NDHPos</cell><cell>16.81</cell><cell>17.34</cell><cell>13.32</cell><cell>22.22</cell></row><row><cell>Gaze, DHRay</cell><cell>9.84</cell><cell>8.13</cell><cell>9.21</cell><cell>8.97</cell></row><row><cell>Gaze, DHPos</cell><cell>10.18</cell><cell>7.98</cell><cell>8.47</cell><cell>8.33</cell></row><row><cell>Gaze, NDHRay</cell><cell>9.51</cell><cell>9.63</cell><cell>9.05</cell><cell>5.98</cell></row><row><cell>Gaze, NDHPos</cell><cell>7</cell><cell>7.18</cell><cell>7.57</cell><cell>9.4</cell></row><row><cell>All</cell><cell>21.35</cell><cell>30.59</cell><cell>22.53</cell><cell>19.66</cell></row><row><cell>All</cell><cell>26.64</cell><cell>27.3</cell><cell>39.64</cell><cell>44.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Feature optimization results.similar top score level: Individual All, Distance All, and the best set found with the feature selection algorithm.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Person Identification Using Anthropometric and Gait Data from Kinect Sensor</title>
		<author>
			<persName><forename type="first">Virginia</forename><forename type="middle">O</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">M</forename><surname>Araujo</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2887007.2887067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI&apos;15)</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI&apos;15)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="425" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-cost electroencephalogram (EEG) based authentication</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Tenore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Vogelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Engineering (NER), 2011 5th International IEEE/EMBS Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="442" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">User identification and authentication using multi-modal behavioral biometrics</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Kyle O Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilbert</forename><forename type="middle">L</forename><surname>Okolica</surname></persName>
		</author>
		<author>
			<persName><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="77" to="89" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the control elements of voluntary movements</title>
		<author>
			<persName><surname>Ve Belenkii</surname></persName>
		</author>
		<author>
			<persName><surname>Vs Gurfinkel</surname></persName>
		</author>
		<author>
			<persName><surname>Paltsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biofizika</title>
		<imprint>
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Viewinvariant estimation of height and stride for gait recognition</title>
		<author>
			<persName><forename type="first">Chiraz</forename><surname>Benabdelkader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Biometric Authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="155" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatial information displays on a wearable computer</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Bowskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Morphett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="24" to="31" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SilentSense: Silent User Identification via Touch and Movement Behavioral Biometrics</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2500423.2504572</idno>
		<ptr target="https://doi.org/10.1145/2500423.2504572" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual International Conference on Mobile Computing &amp; Networking (MobiCom &apos;13)</title>
		<meeting>the 19th Annual International Conference on Mobile Computing &amp; Networking (MobiCom &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="187" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making Nested Rotations Convenient for the User</title>
		<author>
			<persName><forename type="first">G</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Britton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Lipscomb</surname></persName>
		</author>
		<author>
			<persName><surname>Pique</surname></persName>
		</author>
		<idno type="DOI">10.1145/800248.807394</idno>
		<ptr target="https://doi.org/10.1145/800248.807394" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;78)</title>
		<meeting>the 5th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;78)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="222" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Burdea</surname></persName>
		</author>
		<author>
			<persName><surname>Coiffet</surname></persName>
		</author>
		<title level="m">Virtual reality technology</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving Accuracy, Applicability and Usability of Keystroke Biometrics on Mobile Touchscreen Devices</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Buschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>De Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Alt</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702252</idno>
		<ptr target="https://doi.org/10.1145/2702123.2702252" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1393" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag of Soft Biometrics for Person Identification</title>
		<author>
			<persName><forename type="first">Antitza</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmelo</forename><surname>Velardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Angela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Luc</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName><surname>Dugelay</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-010-0635-7</idno>
		<ptr target="https://doi.org/10.1007/s11042-010-0635-7" />
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="739" to="777" />
			<date type="published" when="2011-01">2011. Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Touch Me Once and I Know It&apos;s You!: Implicit Authentication Based on Touch Screen Patterns</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>De Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Brudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Hussmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208544</idno>
		<ptr target="https://doi.org/10.1145/2207676.2208544" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="987" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating Behavioral Biometrics for Continuous Authentication: Challenges and Metrics</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Eberz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kasper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Lenders</surname></persName>
		</author>
		<idno type="DOI">10.1145/3052973.3053032</idno>
		<ptr target="https://doi.org/10.1145/3052973.3053032" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security (ASIA CCS &apos;17)</title>
		<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security (ASIA CCS &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">Martinovic. 2017</date>
			<biblScope unit="page" from="386" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Handbook of phenomenology and cognitive science</title>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Schmicking</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seamless and Secure VR: Adapting and Evaluating Established Authentication Systems for Virtual Reality</title>
		<author>
			<persName><forename type="first">Ceenu</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinus</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Hussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Usable Security Mini Conference</title>
		<meeting>the Usable Security Mini Conference<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USEC&apos;17). Internet Society</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Asymmetric Division of Labor in Human Skilled Bimanual Action: The Kinematic Chain as a Model</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Guiard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Motor Behavior</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="486" to="517" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bodyprint: Biometric User Identification on Mobile Devices Using the Capacitive Touchscreen to Scan Body Parts</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senaka</forename><surname>Buthpitiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Knaust</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702518</idno>
		<ptr target="https://doi.org/10.1145/2702123.2702518" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3011" to="3014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><surname>Iso Iso</surname></persName>
		</author>
		<title level="m">9241-9 Ergonomic requirements for office work with visual display terminals (VDTs)-Part 9: Requirements for non-keyboard input devices</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>International Organization for Standardization</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reality-based Interaction: A Framework for post-WIMP Interfaces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leanne</forename><forename type="middle">M</forename><surname>Girouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Hirshfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orit</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><forename type="middle">Treacy</forename><surname>Shaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Solovey</surname></persName>
		</author>
		<author>
			<persName><surname>Zigelbaum</surname></persName>
		</author>
		<idno type="DOI">10.1145/1357054.1357089</idno>
		<ptr target="https://doi.org/10.1145/1357054.1357089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;08)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Soft biometric traits for personal recognition systems</title>
		<author>
			<persName><forename type="first">Sarat</forename><forename type="middle">C</forename><surname>Anil K Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Dass</surname></persName>
		</author>
		<author>
			<persName><surname>Nandakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="731" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring human height using calibrated cameras</title>
		<author>
			<persName><forename type="first">Erno</forename><surname>Jeges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Istvan</forename><surname>Kispal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltan</forename><surname>Hornak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human System Interactions, 2008 Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="755" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eye-hand coordination in object manipulation</title>
		<author>
			<persName><forename type="first">Göran</forename><surname>Roland S Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Westling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bäckström</surname></persName>
		</author>
		<author>
			<persName><surname>Randall Flanagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="6917" to="6932" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introducing touchstroke: keystroke-based authentication system for smartphones. Security and Communication Networks</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Kambourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Damopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="542" to="554" />
		</imprint>
	</monogr>
	<note>Dimitrios Papamartzivanos, and Emmanouil Pavlidakis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">H</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Task-Driven Biometric Authentication of Users in Virtual Reality (VR) Environments</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kupin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natasha</forename><forename type="middle">Kholgade</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-05710-7_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-05710-7_5" />
	</analytic>
	<monogr>
		<title level="m">Multi-Media Modeling -25th International Conference, MMM 2019, Thessaloniki</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01-08">2019. January 8-11, 2019</date>
			<biblScope unit="page" from="55" to="67" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Method for tracking and displaying user&apos;s spatial position and orientation, a method for representing virtual reality for a user, and systems of embodiment of such methods</title>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">548</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Nurakhmed Nurislamovich Latypov and Nurulla Nurislamovich Latypov</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Whose move is it anyway? Authenticating smart wearable devices using unique head movement patterns</title>
		<author>
			<persName><forename type="first">Sugang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenren</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Lindqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macro</forename><surname>Gruteser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pervasive Computing and Communications (PerCom)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining eye and hand in search is suboptimal</title>
		<author>
			<persName><forename type="first">Hanneke</forename><surname>Liesker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeroen Bj</forename><surname>Smeets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental brain research</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="395" to="401" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Brain Password: A Secure and Truly Cancelable Brain Biometrics for Smart Headwear</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><forename type="middle">Woo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services</title>
		<meeting>the 16th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="296" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Proprioceptive Interaction</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><surname>Jonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Baudisch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702613.2732490</idno>
		<ptr target="https://doi.org/10.1145/2702613.2732490" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA &apos;15)</title>
		<meeting>the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="175" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Phrase Sets for Evaluating Text Entry Techniques</title>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">William</forename><surname>Soukoreff</surname></persName>
		</author>
		<idno type="DOI">10.1145/765891.765971</idno>
		<ptr target="https://doi.org/10.1145/765891.765971" />
	</analytic>
	<monogr>
		<title level="m">CHI &apos;03 Extended Abstracts on Human Factors in Computing Systems (CHI EA &apos;03)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="754" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Movement, posture and equilibrium: interaction and coordination</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Massion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in neurobiology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="35" to="56" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Everything you need to know about biometric identification. Personal Identification News</title>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometric Industry Directory</title>
		<imprint>
			<date type="published" when="1988">1988. 1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Moving objects in space: exploiting proprioception in virtual-environment interaction</title>
		<author>
			<persName><forename type="first">Frederick</forename><forename type="middle">P</forename><surname>Mark R Mine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><forename type="middle">H</forename><surname>Brooks</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><surname>Sequin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 24th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person Identification Using Full-body Motion and Anthropometric Biometrics from Kinect Videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Munsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzheng</forename><surname>Temlyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33885-4_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-33885-4_10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Computer Vision -Volume Part III (ECCV&apos;12)</title>
		<meeting>the 12th International Conference on Computer Vision -Volume Part III (ECCV&apos;12)<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsure How to Authenticate on Your VR Headset?: Come on, Use Your Head!</title>
		<author>
			<persName><forename type="first">Tahrima</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Matovu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Serwadda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Muirhead</surname></persName>
		</author>
		<idno type="DOI">10.1145/3180445.3180450</idno>
		<ptr target="https://doi.org/10.1145/3180445.3180450" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics (IWSPA &apos;18)</title>
		<meeting>the Fourth ACM International Workshop on Security and Privacy Analytics (IWSPA &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">De architectura</title>
		<author>
			<persName><forename type="first">Vitruvius</forename><surname>Pollio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<publisher>Dover Publications</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using comparative human descriptions for soft biometrics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (IJCB), 2011 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Biometric recognition via eye movements: Saccadic vigor and acceleration cues</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Rigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Komogortsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shadmehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception (TAP)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GaitLock: Protect virtual and augmented reality headsets using gait</title>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selection-based Text Entry in Virtual Reality</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Speicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">Maria</forename><surname>Feit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Krüger</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174221</idno>
		<ptr target="https://doi.org/10.1145/3173574.3174221" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI &apos;18)</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems (CHI &apos;18)<address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">647</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Robert</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teather</forename></persName>
		</author>
		<title level="m">Evaluating 3D pointing techniques</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pointing at 3D targets in a stereo head-tracked virtual environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Teather</surname></persName>
		</author>
		<author>
			<persName><surname>Stuerzlinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D User Interfaces (3DUI), 2011 IEEE Symposium</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An Evaluation of an Eye Tracker As a Device for Computer Input</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harutune</forename><forename type="middle">H</forename><surname>Mikaelian</surname></persName>
		</author>
		<idno type="DOI">10.1145/29933.275627</idno>
		<ptr target="https://doi.org/10.1145/29933.275627" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI/GI Conference on Human Factors in Computing Systems and Graphics Interface (CHI &apos;87)</title>
		<meeting>the SIGCHI/GI Conference on Human Factors in Computing Systems and Graphics Interface (CHI &apos;87)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="183" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A definition of biometrics</title>
		<author>
			<persName><surname>Wayman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Biometric Test Center Collected Works</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="23" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Glassgesture: Exploring head gesture interface of smart glasses</title>
		<author>
			<persName><forename type="first">Shanhe</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengrui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Communications, IEEE INFOCOM 2016-The 35th Annual IEEE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Continuous Authentication Using Eye Movement Response of Implicit Visual Stimuli</title>
		<author>
			<persName><forename type="first">Yongtuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><forename type="middle">Tung</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiankun</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3161410</idno>
		<ptr target="https://doi.org/10.1145/3161410" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 4, Article</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2018-01">2018. Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A survey of advances in biometric gait recognition</title>
		<author>
			<persName><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maodi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="150" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A survey of on-line signature verification</title>
		<author>
			<persName><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
