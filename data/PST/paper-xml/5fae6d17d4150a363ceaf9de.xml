<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Zongjie</forename><surname>Cao</surname></persName>
							<email>zjcao@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ship Detection in Large-Scale SAR Images Via Spatial Shuffle-Group Enhance Attention</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Communication Engi-neering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35CABDA076B8019E44CAB42AE7C1628D</idno>
					<idno type="DOI">10.1109/TGRS.2020.2997200</idno>
					<note type="submission">received February 24, 2020; revised April 22, 2020; accepted May 20, 2020.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CenterNet</term>
					<term>large-scale scene</term>
					<term>ship detection</term>
					<term>spatial shuffle-group enhance</term>
					<term>synthetic aperture radar (SAR)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ship target detection using large-scale synthetic aperture radar (SAR) images has important application in military and civilian fields. However, ship targets are difficult to distinguish from the surrounding background and many false alarms can occur due to the influence of land area. False alarms always occur with ship target detection because most of the area in large-scale SAR images are treated as background and clutter, and the ship targets are considered unevenly distributing small targets. To address these issues, a ship detection method in large-scale SAR images via CenterNet is proposed in this article. As an anchor-free method, CenterNet defines the target as a point, and the center point of the target is located through key point estimation, which can effectively avoid the missing detection of small targets. At the same time, the spatial shuffle-group enhance (SSE) attention module is introduced into CenterNet. Through SSE, the stronger semantic features are extracted while suppressing some noise to reduce false positives caused by inshore and inland interferences. The experiments on the public SARship-data set show that the proposed method can detect all targets without missed detection with dense-docking targets. For the ship targets in large-scale SAR images from Sentinel 1, the proposed method can also detect targets near the shore and in the sea with high accuracy, which outperforms the methods like faster R-convolutional neural network (CNN), single-shot multibox detector (SSD), you only look once (YOLO), feature pyramid network (FPN), and their variations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>large-scale scenery data available <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Thus, ship target detection in large-scale SAR images becomes a particularly important traditional task <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Traditional SAR ship target detection algorithms are mainly divided into three stages: area selection, feature extraction, and classification <ref type="bibr" target="#b4">[5]</ref>. The primary purpose of area selection is to locate the targets. Feature extraction is based on region selection to extract classification features that can accurately express the target area, which is the core of target detection. Commonly used features include scale invariant feature transform (SIFT) <ref type="bibr" target="#b5">[6]</ref> and histogram of oriented gradient (HOG) <ref type="bibr" target="#b6">[7]</ref>. These methods must use gray value, scale size, or other characters to manually design features and classifiers. These features perform poorly for ship detection in inshore areas and it is difficult to rule out false positives, such as icebergs and islands. At the same time, remodeling is required for different scenes, which causes low detection efficiency and high false alarm rate <ref type="bibr" target="#b7">[8]</ref>. In addition, speckle noise and motion blur in SAR images may cause unnecessary differences between ships, which make it difficult for traditional SAR ship detection <ref type="bibr" target="#b8">[9]</ref>. At present, the constant false alarm rate (CFAR) detection algorithm is the most widely used and effective detection method in the theoretical research of SAR image ship detection. However, the CFAR algorithm is greatly affected by the background statistical area. Generally speaking, increasing the background statistical area increases the accuracy of the description, but this may result in larger background clutter changes and increase the false alarms rates <ref type="bibr" target="#b9">[10]</ref>. Thus, it is difficult to solve the issues of ship target detection using the traditional methods for large-scale SAR images <ref type="bibr" target="#b10">[11]</ref>.</p><p>With the development of neural networks, target detection technology based on deep learning has become the mainstream direction in the field of target detection. An increasing number of target detection methods based on deep learning have been proposed, through which higher accuracy can be obtained. Especially in recent years, the end-to-end networks have been proposed, in which design features do not need to be manually designed, and in turn, enhances the generalization ability <ref type="bibr" target="#b11">[12]</ref>. The emergence of AlexNet <ref type="bibr" target="#b12">[13]</ref> has epoch-making significance, making it possible for convolutional neural networks (CNNs) to go deeper. As a result more in-depth research on target detection networks have been conducted to produce new algorithms. These algorithms mainly include: two-stage detection algorithms represented by faster R-CNN <ref type="bibr" target="#b13">[14]</ref> and single-stage detection algorithms represented by you only look once (YOLO) <ref type="bibr" target="#b14">[15]</ref> and single shot multibox detector (SSD) <ref type="bibr" target="#b15">[16]</ref>. To solve the shortcomings of fewer shallow features of the target on the final feature map, a feature fusion method feature pyramid network (FPN) structure is proposed <ref type="bibr" target="#b16">[17]</ref>.</p><p>Ship detection in SAR images based on CNNs has also had great breakthroughs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. For instance, Li et al. <ref type="bibr" target="#b7">[8]</ref> proposed an improved faster R-CNN method. In <ref type="bibr" target="#b0">[1]</ref>, a squeeze and excitation rank faster R-CNN method was proposed for ship detection in SAR images. Kang et al. <ref type="bibr" target="#b19">[20]</ref> put forward a contextual region-based method for SAR ship detection. Jiao et al. <ref type="bibr" target="#b20">[21]</ref> densely connected multiscale neural network to solve the problem of multiscale and multiscene SAR ship detection. All above methods are based on anchors. The matching mechanism of the anchor makes the frequency of extreme scales (large and small targets) to be matched lower than that of other moderately sized targets. But the size of the ship targets varies widely in large-scale SAR images. Therefore, the anchor-based method is not suitable for ship target detection in large-scale SAR images.</p><p>Recently, researchers have focused on the research of anchor free methods, such as CornetNet <ref type="bibr" target="#b21">[22]</ref> and ExtremeNet <ref type="bibr" target="#b22">[23]</ref>. CornerNet completely abandons the anchor concept and uses the method of detecting points to identify targets for the first time. CornerNet detects the upper-left and lower-right corners of the bounding box to determine the position and size of the target. However, the semantic information in the top left and bottom right is relatively weak and difficult to detect. At the same time, two points that belong to the same target need to be combined with postprocessing. ExtremeNet uses the best keypoint estimation framework to find extreme points by predicting four multipeak heatmaps for each target class. In addition, ExtremeNet uses the center heatmap of each class to predict the target center. However, the speed is slow and more points need to be detected and the detection effect is not significantly improved. Too much attention to the edge can easily cause missed inspections and false alarms. Center-Net <ref type="bibr" target="#b23">[24]</ref> is improved on the above basis, and the detection is based on the center point of the target. CenterNet only needs to extract a center point for each object without grouping or postprocessing. Therefore, it can effectively reduce missed detections in large-scale SAR images.</p><p>In large-scale SAR images, the proportion of the target area is usually very small, which means that ship detection on large-scale SAR images is considered as small object detection. When mapped to the final feature map, small objects have little information for location refinement and classification, which naturally reduces detection performance. Therefore, the CenterNet-based target detection method is proposed in this article and is used to solve the problem of detecting ships docking side by side on the shore. However, there are still many false alarms in the near shore and inland areas.</p><p>Generally, ship detection in large-scale SAR images has many disturbances, such as land, islets, and sea clutter, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. To reduce possible false alarms, attention mechanisms have been introduced to target detection network. Attention mechanisms have made important breakthroughs in image interpretation, natural language processing, and other fields. Attention mechanisms have also been helpful in the improvement of network model performance in recent years <ref type="bibr" target="#b24">[25]</ref>. It is also functions similar to the perception mechanism of the human brain and human eye by focusing on local information mechanisms <ref type="bibr" target="#b25">[26]</ref>. The results are usually presented in the form of probability graphs or probability eigenvectors. In principle, attention models are mainly divided into three types: spatial attention model, channel attention model, and spatial and channel mixed attention model. SENet <ref type="bibr" target="#b26">[27]</ref> proposes the squeeze and excite (SE) module, which essentially performs attention operations on channels. But in addition to the channel, another important dimension in the convolutional feature map needs to be considered: space. Channels and spaces mix attention to get more detailed semantic features.</p><p>Based on above analysis, the spatial shuffle-group enhance (SSE) attention module is proposed to get semantic features that better represent target features. SSE divides the feature map into G groups along the channel dimension. Then the channels shuffle to enhance the relationship between the groups. The attention model SSE is combined with the anchor-free CenterNet network. In this way, SSE enables feature maps with similar semantics to better extract the regional features of the targets. Experiments show that the method proposed in this article has better detection performance and high accuracy on ship detection in large-scale SAR images.</p><p>The rest of this article is organized as follows: Section II introduces the proposed method based on CenterNet <ref type="bibr" target="#b23">[24]</ref>; In Section III, the details of experiments on the data set named SAR-ship-data set and the analysis of the results are exhibited; finally, the conclusion is presented in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHOD</head><p>The overall process framework shown in Fig. <ref type="figure" target="#fig_1">2</ref> mainly consists of rough land and sea classification and ship detection by CenterNet with SSE attention module. In this section, the overall process and key technical points of the proposed method are introduced, and then each branch of the proposed method is described in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ship Detection Framework in Large-Scale Images</head><p>As SAR imaging technology has developed, the SAR image resolution has improved gradually, and data sets have become gradually more enriched. In large-scale SAR images, the proportion of ship targets in the entire image is very small. At the same time, the image size is large (more than one million pixels) and cannot be directly input into existing deep neural networks. Therefore, before sending the SAR image of the large scene into the neural network, a sliding window is needed to divide the SAR image of the large scene into small slices. Sliding window is a preprocessing process in the ship target detection process of large scenes. A fixed window size (400 × 400) without overlapping is used for sliding windows. However, pixel-by-pixel detection produces more false alarms, which wastes computing resources. A rough classification method of sea and land is proposed to reduce the waste of computing resources and to reduce the false alarms inland before detection.</p><p>In large-scale SAR images, the size of the ship targets varies greatly. Especially the detection of small ship targets is difficult. Researchers have proposed many detection frameworks for small targets <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. These methods mainly focus on implementing multiscale frameworks or using reinforcement methods to make the network pay more attention to small targets. Compared with other target detection frameworks, CenterNet is an anchor-free detection network based on center points <ref type="bibr" target="#b23">[24]</ref>, which greatly reduces the amount of calculation. And the down sampling factor of CenterNet's output resolution is 4, which is relatively small when compared with other target detection frameworks (Mask-Rcnn is at least 16 and SSD is at least 16). Thus, CenterNet is chosen as the target detection network framework in this article.</p><p>Although CenterNet fuses shallow features with deep features, it cannot discriminate well for some false alarms in large-scale SAR images, such as land and islands. Thus, obtaining more advanced semantic features is essential to eliminate false alarms. Feature learning and grouping in convolutional networks have been primary areas of research in recent years. Numerous studies have previously studied the spatial components of CNNs. They all try to enhance the representation ability of CNNs by improving the quality of the spatial coding of the entire feature level.</p><p>In recent years, grouping channels has been applied to reduce the amount of computation. Inspired by spatial group-wise enhance (SGE) module <ref type="bibr" target="#b29">[30]</ref>, the SSE attention module proposed in this article is used to simultaneously integrate attention into channels and spaces. The channels are grouped to reduce the amount of calculation, and then are shuffled to enhance the connection between the groups. In the space of the group, global features and critical features are combined to obtain stronger semantic information. The important information of the feature makes the feature more expressive of the target characteristic. Thus, it can extract higher level semantic features, and reduce false alarms.</p><p>When processing on a large scene image, the large scene image is cropped to a small scale (e.g., 400 × 400) without overlapping to handle the large input size and work within the computer's limited memory. The patches of large-scale SAR images are roughly classified by sea and land. The patches that contain only land (such as the last patch in Fig. <ref type="figure" target="#fig_1">2</ref>) are screened out and not sent to the heavy detector. The rest patches are sent to the detector to obtain the detection results of the ship target. Finally, all the patches are mapped back to the original image through the positional information to obtain the final detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CenterNet</head><p>Most current target detectors list potential target locations and classify each target. This type of methods rely heavily on the design of hyperparameters such as anchors and thresholds, which are computationally intensive. CenterNet models objects as a single point, finds the center point by keypoint estimation, and uses the image information of the center point to get the size and position of the targets. There is no need to set anchors in advance, which greatly reduces the amount of network parameters and calculations.</p><p>CenterNet has many advantages for being applied to SAR ship detection. First, it has no artificial thresholds for foreground and background classification, which reduces requirements for positive and negative samples of the data set. At the same time, the difficulty of detecting small targets due to the anchor matching mechanism is reduced. What is more, the center point can be regarded as a single agnostic shape anchor, where only local peaks are extracted in the keypoint heatmap, and nonmaximum suppression (NMS) is not needed. Detection speed is greatly accelerated without NMS, since NMS is extremely time consuming. At the same time, the anchor-based target detection method requires NMS to obtain the final detection result after obtaining candidate regions. In the process, densely docked ship targets are likely to be suppressed. As a result, densely docked ship targets are missed. CenterNet is no need for NMS, and only returns a bounding box based on the center point of the target, the detection of densely docked ship targets does not affect each other.</p><p>In addition, compared with traditional target detection (output stride of 16), CenterNet uses a higher output resolution (output stride of 4), which can better retain the characteristics of small targets. CenterNet uses a full convolutional network with encoding and decoding. Up sampling uses transposed convolution, which can better restore semantic information and position information of the images. At the same time, CenterNet is end-to-end differentiable, simple and fast.</p><p>In large-scale SAR images, the targets occupy a small proportion of the entire image, so better feature extraction is particularly important. Therefore, CenterNet-based object detection method is adopted for ship detection. CenterNet inputs images to the full convolutional network to generate a heatmap, and then screens to get the final target center point. Image features at the peak can predict height and width of the target bounding box.</p><p>The specific implementation principle is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. First, assume I ∈ R W ×H ×3 is a patch with input width H and height W . Obeying Ŷ ∈ [0, 1] (W/R)×(H /R)×C distribution, where R is the output stride size and C is the number of keypoint categories, keypoint heatmap is obtained. If Ŷ = 1, it corresponds to a detected key point; if Ŷ = 0, this is the background. If (x 1 , y 1 , x 2 , y 2 ) is the bounding box of the target, its center point is at p = ((x 1 + x 2 )/2, (y 1 + y 2 )/2). The other information of the target is obtained from the image information of the key points. All the center points are predicted by Ŷ and then regressed to get the target size. The value at the key point is used as its confidence information, and regression at its position is used to get the bounding box size. The position coordinates are</p><formula xml:id="formula_0">⎛ ⎜ ⎜ ⎝ xi + δ xi - ŵi 2 , ŷi + δ ŷi - ĥi 2 , xi + δ xi + ŵi 2 , ŷi + δ ŷi + ĥi 2 ⎞ ⎟ ⎟ ⎠<label>(1)</label></formula><p>where (δ xi , δ ŷi ) = Ôx i , ŷi is the offset prediction in the down sampling process. ( ŵi , ĥi ) = Ŝx i , ŷi is the prediction of the target size. However, CenterNet only detects the center point and obtains minimum target information, which results in more false positives for ship target detection in SAR images. To solve this problem, SSE is applied to the network so that the network can focus on more advanced semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Shuffle-Group Enhance Attention</head><p>In <ref type="bibr" target="#b29">[30]</ref>, a SGE module was proposed, which is the first-generation source to use the similarity between local and global as an attention mask. Furthermore, SGE has good interpretability for the enhancement of semantic representation. The article believes that a complete feature is composed of many subfeatures, and these subfeatures are distributed in the form of groups in the features of each layer. The SGE attention module obtains the importance of each subfeature by generating an attention factor for each spatial position in each group, where each group can learn and suppress noise in a targeted manner. This attention factor is determined only by the similarity between global and local features within each group. SGE is a lightweight attention module that obtains high-level semantics, which can significantly improve target detection performance. However, these subfeatures were processed in the same way, and they were affected by background noise, which lead to incorrect classification and positioning results. When multiple groups of convolutions are superimposed together, the output of a certain channel is only derived from a small number of input channels, thereby reducing the information flow and information representation capabilities between channel groups.</p><p>In <ref type="bibr" target="#b30">[31]</ref>, an effective CNN named Shufflenet for mobile devices was proposed, which is mainly used for compression and acceleration of deep networks. Modern CNNs contain multiple repetitive modules. As the number of network layers deepens and the number of channels increases sharply, the convolution kernel of the convolutional layer increases, and the spatial dimension decreases accordingly. At the same time, the feature map becomes decreases with the convolution pooling operation. As a result, channels are becoming more and more important in deep networks. Group convolution can solve the limitation between limited channels. By ensuring that each convolution operation is performed only on the corresponding input channel group, group convolution can significantly reduce calculation loss. If group convolution is allowed to receive different input data, the input and output channels will become fully correlated.</p><p>Mainly inspired by the above research, the SSE module is proposed in this article. The idea of channel shuffle in model compression is applied to our attention module. The overall framework of the proposed attention module (SSE) is shown in Fig. <ref type="figure" target="#fig_3">4</ref>. SSE divides the C channels and H ×W convolutional feature map into G groups along the channel dimension. When the group shuffle is applied to increase the correlation between the groups, SSE shuffles the output channels of the previous layer, divides them into G groups again, and outputs to the next layer, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. SSE can aggregate more relevant   features between channels to improve detection performance, and some false alarms can be eliminated. Meanwhile, SSE generates an attention factor at each spatial location within each group to learn more advanced semantic information, which produces enhanced feature maps <ref type="bibr" target="#b29">[30]</ref>.</p><p>In deep networks, channels become more important. Group convolution can reduce the amount of calculations significantly <ref type="bibr" target="#b22">[23]</ref>, and its schematic is shown in Fig. <ref type="figure" target="#fig_5">6</ref>. It can be seen from Fig. <ref type="figure" target="#fig_5">6</ref> that each group learns different features by a fewer parameters. Group convolution is to group input feature maps, and then convolve each group separately. Suppose the size of the input feature map is C × H × W , and the number of output feature maps is N. If channels are divided into G groups, the size of each convolution kernel is (C/G) × K × K , and the number of convolution kernels is still N. The convolution kernel is only convolved with the feature maps of the same group. The total parameter amount is N ×(C/G)× K × K , and the total parameter amount is reduced to the original (1/G). Group convolution can be regarded as sparsely structured. The size of each convolution kernel changes from</p><formula xml:id="formula_1">C × K × K to (C/G) × K × K . The remaining (C -(C/G)) × K × K</formula><p>parameters can be regarded as 0, which can sometimes result in even better results while still reducing the amount of parameters.</p><p>Channel shuffle operation can be completed by the following steps <ref type="bibr" target="#b30">[31]</ref>: first, divide the channels of a convolutional layer into G groups, where each group has n = (C/G) channels. Then, reshape the output channel dimension into (G, n) and transpose into (n, G). Next, flatten and redivide into G groups.</p><p>Due to noise, it is difficult for a CNN to obtain favorable feature distributions. To solve the problem, the overall information of the entire group is used to enhance the learning of semantic features in critical regions. The spatial enhance operation can be completed by the following steps: global statistical features are used to approximate the semantic vector g of this group of learning representations through the spatial average function</p><formula xml:id="formula_2">g = 1 m m i=1 x i (2)</formula><p>where x = {x 1,...,m }, x i ∈ (C/G) , m = H × W is the vector representation of the group at each position in space.</p><p>Next, a corresponding importance coefficient c i is generated for each feature. The coefficient is obtained by a simple dot product, which measures the similarity between the global semantic feature g and the local feature</p><formula xml:id="formula_3">x i c i = g • x i . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>At the same time, to prevent deviations in coefficients between various samples, c in space is normalized</p><formula xml:id="formula_5">μ c = 1 m m j c j (<label>4</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">σ c = 1 m m j c j -μ c 2 (5) ĉi = c i -μ c σ c + ε (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where ε is a constant added for numerical stability.</p><p>To ensure that the normalization inserted in the network can represent identity conversion, a set of parameters γ , β is introduced for each coefficient c i to scale and transform the normalized value <ref type="bibr" target="#b31">[32]</ref>. a i = γ ĉi + β. Finally, to obtain the enhanced feature vector xi , the original x i is scaled spatially by the generated importance coefficient a i through the sigmoid function threshold</p><formula xml:id="formula_9">xi = x i • σ (a i ). (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>All enhanced features make up the final feature map. The enhanced feature map can better exclude the interference of the coast on ship target detection. This greatly improves the accuracy of detection and reduces false alarms on the shore. Therefore, CenterNet with SSE solves the problem of detecting ships docking side by side at the shore.</p><p>The SSE module can be embedded in existing network structures the same as the SE module. The SSE module is placed after the last BatchNorm layer of each bottleneck of CenterNet, as shown in Fig. <ref type="figure">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>To evaluate the proposed method, experiments are implemented on public data set named SAR-ship-data set <ref type="bibr" target="#b32">[33]</ref>. First, the implementations such as data set, evaluation criteria, and detailed experimental settings will be introduced. Next, the role of SSE is mainly verified, and the current ship target detection methods are compared to verify the effectiveness of the proposed method in this article. At last, the effectiveness of the proposed method in large-scale scenes is verified. 2) Evaluation Metric: To evaluate the detection performance of the network, AP, AP 50 , AP 75 , AP s , AP m , and AP l are used as the metrics following the evaluation protocol on the COCO data set. Average precision (AP) is the area under the precision-recall curve. Mean average precision (mAP) is the average of multiple categories of AP. mAP is the most important one in the target detection algorithm. There is only one category of ships in this article, so mAP = AP. Generally speaking, the better the classifier, the higher the AP value. In this article, AP is average precision at Intersection over Union (IoU) = 0.50:0.05:0.95 (primary challenge metric), and AP 50 is AP at IoU = 0.5 (PASCAL Visual Object Classes (VOC) metric), Similarly, AP 75 is AP at IoU = 0.75 (strict metric). AP s , AP m , and AP l represent AP for small objects, medium objects, and large objects, respectively <ref type="bibr" target="#b33">[34]</ref>. The most important metric is AP 50 . The precision and recall are as follows: where TP (true positives) is the number of correctly detected ships and FP (false positives) is the number of ships incorrectly classified as positive, and FN (false negative) is the number of ships incorrectly classified as negative.</p><formula xml:id="formula_11">Precision = TP TP + FP (8) Recall = TP TP + FN (9)</formula><p>AP is defined as</p><formula xml:id="formula_12">AP = 1 0 p(r )dr (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where p represents precision, r represents recall, and p is a function that takes r as a parameter, which is equal to taking the area under the curve. In addition, the training time is applied to the comparison to show the time advantage of the method proposed in this article.</p><p>3) Settings: The proposed method is implemented on pytorch 0.4.1, CUDA 9.0 and CUDNN 7.1 with Intel Core i7-6700HQ CPU and a Nvidia Geforce GTX 1080Ti GPU. Data set is divided by random sampling, the ratio of training set, verification set, and test set is 8:1:1. There are 35 055 chips for training and 4382 chips for verification and test, respectively. The input resolution of 512 × 512 is applied, which will yield an output resolution of 128 × 128 for all the models. The operations of random flips, random scaling (between 0.6 and 1.3), crop and color dithering are used for data enhancement, and the Adam operation is used to optimize the overall goal. The learning rate is set to 1.25e-4, and the batch size is set to 16. The backbone of the CenterNet detection network uses full layer convolutional network deep layer aggregation (DLA) <ref type="bibr" target="#b34">[35]</ref>. The DLA network is an image classification network with hierarchical skip connections. DLA is modified with bottleneck residual blocks and deformable convolution. The down sampling layer of DLA-34 is initialized using ImageNet pretraining, while the up sampling layer is initialized randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ship Detection in SAR-Ship-Data Set 1) Effect of SSE:</head><p>In this section, the role of the attention module proposed in this article is verified. When training the networks, the same parameter settings are used for comparison. When detecting on large-scale images, slicing is necessary, so it is also valid to verify the model in small images. At present, the scope of comparison and reference in the detection of ship targets in large scenes is relatively small, and considering the problem of image display in this article, the effectiveness of the method is verified in the small images first. As shown in Table <ref type="table" target="#tab_0">I</ref>, after adding the attention module the AP 50 reached 94.7%, which is 2.3% higher than before the addition. At the same time, the AP for small targets improved about 6%.</p><p>As shown in Fig. <ref type="figure" target="#fig_7">8</ref> As a note, a rectangular box in this article always represents a ship target. The ship targets in [Fig. <ref type="figure" target="#fig_7">8(b)</ref>] is used as a reference, it can be seen from [Fig. <ref type="figure" target="#fig_7">8(c)</ref>] that CenterNet has good detection results, but there are many false alarms. After adding SSE to the network structure, the detection of false positives decreases, as shown in [Fig. <ref type="figure" target="#fig_7">8(d)</ref>] and the detection rate improves, as shown in Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Comparison With Other Methods:</head><p>To verify the performance of our method, a comparative analysis using different methods is conducted, followed by a detailed explanation of the experimental results. As illustrated in Fig. <ref type="figure" target="#fig_8">9</ref>, it can be seen from the experiment results that, the method proposed in this article could effectively reduce false alarms when compared with other methods. At the same time, it has a better detection performance for dense-docking ships than other existing network models.</p><p>First, the method proposed in this article is compared with the ship target detection methods in the field of optics. As shown in Table <ref type="table" target="#tab_0">II</ref>, the proposed method obtains the highest mAP. In Fig. <ref type="figure" target="#fig_8">9</ref>, the comparison of the effects of several methods can be seen. [Fig. <ref type="figure" target="#fig_8">9(a)</ref>] is the ground truth. [Fig. <ref type="figure" target="#fig_8">9(b)</ref>] shows the results with method of faster R-CNN <ref type="bibr" target="#b32">[33]</ref>. It can  be seen from [Fig. <ref type="figure" target="#fig_8">9</ref>(b)] that, there are many false alarms in the detection results, such as islands and land. At the same time, there are many missed detections for densely docked ships. [Fig. <ref type="figure" target="#fig_8">9(c)</ref>] is the result with the SSD-300 method <ref type="bibr" target="#b32">[33]</ref>, in which there are many missed targets. [Fig. <ref type="figure" target="#fig_8">9(d)]</ref> shows the results with the YOLOv3 method, which also shows many false alarms and has very low confidence. [Fig. <ref type="figure" target="#fig_8">9</ref>(e)] shows the results using the proposed method. It can be seen from [Fig. <ref type="figure" target="#fig_8">9</ref>(e)] that, the method proposed in this article could eliminate some false alarms and for densely docked vessels, and it has a better detection effect than all previous methods, without additional processing. These benefit from the feature map enhanced by the attention module SSE, and CenterNet gets the target bounding box by center point regression, without the need of NMS. At the same time, it is worth mentioning that, small ship targets with fewer target pixels may be lost with the deepening of the network structure. The reason is that other CNN methods require a large amount of down sampling during the convolution process to reduce redundancy and reduce calculations. The method in this article only needs four times down sampling, which can detect more small ships.</p><p>The comparison experiments with the current SAR ship detection methods based on FPN were also implemented. An improved FPN structure was proposed in <ref type="bibr" target="#b35">[36]</ref>, with constructing the FPN through horizontal connections, and using the scale conversion layer to densely connect each feature map from top to bottom. Horizontal connections inject semantic information into high-resolution feature maps. In <ref type="bibr" target="#b36">[37]</ref>, the attention module convolutional block attention module (CBAM) was applied in FPN, and embedded in faster R-CNN. Both the methods in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b36">[37]</ref> are proposed for ship targets detection in SAR images, and the detection performance is shown in Table <ref type="table" target="#tab_0">III</ref>. From Table <ref type="table" target="#tab_0">III</ref>, it can be seen that, the method proposed in this article still has the highest mAP and the fastest detection speed compared with the main current ship detection methods.  The second advantage is that since a large and complex background area may cause more false positives, these false positives are first filtered out by the preclassification process. For example, land construction areas have pixel characteristics similar to ships. Even if the preclassification is not correctly classified, subsequent tests can still correct the final test results. Although the preclassification method of sea and land can effectively reduce the interference of the land area to the ship target detection, it is dependent on the data set. If more images in the data set contain only sea areas, land and sea preclassification becomes redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ship Detection in</head><p>2) Detection: The method proposed in this article is comprehensively evaluated on the slice of Sentinel 1 and some results are shown in Table <ref type="table" target="#tab_2">IV</ref>. In Table <ref type="table" target="#tab_2">IV</ref>, Pre represents preclassification. In this section, the necessity of preclassification and the effectiveness of SSE are verified. The results show that when preclassification and SSE are joined with the detector, the precision can reach 90.6%. When detecting the objects without SSE and preclassification, too many false positives appeared, resulting in low detection efficiency. The proposed method achieves the superior results on performance and speed.</p><p>From Table <ref type="table" target="#tab_2">IV</ref>, it can be seen that the proposed method is efficient for ship detection in large-scale SAR images.   DLA enables the inference time of the network less than other large models, such as ResNet-101, and extracts powerful features for object detection. Although the real-time detection of large-scale SAR images still has a long way to go, the proposed method has made some contributions.</p><p>To verify the performance of our method in large-scale SAR images, the method proposed in this article is compared with other ship target detection methods. As shown in Table <ref type="table" target="#tab_3">V</ref>, the proposed method obtains the highest precision. It can be seen that the method in this article has better detection performance. At the same time, the proposed method has a better performance for shorelines or other more complicated scenes, and the method further reduced false positives. As shown in Fig. <ref type="figure" target="#fig_12">10</ref>, [Fig. <ref type="figure" target="#fig_12">10(a)</ref>] is the result without the SSE attention module and preclassification. First, it can be seen that there are many false alarms inshore. Many bright areas in the inshore area are similar to ship targets, thus causing severe false alarms. This problem has always existed in the development of ship target detection. Secondly, the residential areas in the inland area also caused some interference. For ship targets, the detection of inland areas is a waste of computing resources. The enlarged red rectangle on the left in [Fig. <ref type="figure" target="#fig_12">10(a)</ref>] indicates that it has good performance in offshore. However, the red rectangle on the right shows that it has many false alarms inshore and cannot handle interference information well. [Fig. <ref type="figure" target="#fig_12">10(b)</ref>] is the detection result with SSE attention module and preclassification. Without reducing the detection rate, false positives are significantly reduced. It is seen from the red oval box in Fig. <ref type="figure" target="#fig_12">10</ref> that preclassification is necessary and critical. Preclassification can not only reduce the calculation and improve the detection speed, but also effectively reduce the false alarms in the inland area.</p><p>Some other ship detection results in large-scale SAR images are shown in Fig. <ref type="figure" target="#fig_13">11</ref>. As can be seen from Fig. <ref type="figure" target="#fig_13">11</ref>, the method proposed in this article can effectively reduce false alarms. All the above experimental results show that, for ship detection in large scene SAR images, the method proposed in this article has advantages in both detection performance and detection efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>A detection method based on CenterNet with SSE attention module is proposed in this article, to solve the ship detection problem in large-scale SAR images. CenterNet is different from the anchor-based methods in that it locates the center point of the target through key point estimation, which can avoid missing detection of ship target in large-scale SAR image effectively. Moreover, the SSE attention module is introduced into CenterNet, which can simultaneously integrate attention in channels and spaces. With SSE, the CenterNet can integrate more advanced semantic features, to reduce the false alarms caused by inshore and inland interference.</p><p>Experimental results on SAR-ship-data set show that, the mAP of ship detection in large-scale SAR images via the proposed method reaches 94.7%, which can prove that the proposed method achieves a better performance than other state-of-the-art ship detection methods, such as faster R-CNN, SSD, YOLO, FPN, and their variations. But it also needs to be noted that, although the proposed method has better detection performance and can effectively reduce false alarms, it cannot completely eliminate all false alarms for complex background, which requires further analysis and research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Large-scale SAR image with many disturbances.</figDesc><graphic coords="2,314.99,58.61,244.94,136.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall framework of the proposed method for ship detection.</figDesc><graphic coords="3,48.95,101.09,66.02,66.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Model an object as the center point of its bounding box. The bounding box size and other object properties are inferred from the keypoint feature at the center.</figDesc><graphic coords="4,50.99,58.49,246.14,76.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Structure of SSE module. Semantically enhanced feature maps are obtained through this attention module.</figDesc><graphic coords="5,130.43,228.29,57.98,57.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Channel shuffle operation in convolutional layer. (a) Input and output channels are fully related. (b) Each output channel only relates to the input channels within the group.</figDesc><graphic coords="5,131.39,336.05,57.98,57.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Standard convolution and group convolution. (a) Normal convolutional layer. (b) Convolutional layer with three groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.</head><label></label><figDesc>Fig. Position of SSE in the bottleneck module.A. Implementations 1) Data Sets: SAR-ship-data set is a multisource, multimode SAR ship data set published by the Chinese Academy of Sciences. It contains 102 Gaofen-3 images and 108 Sentinel-1 images that are used to create 59 535 ships in 43 819 ship chips<ref type="bibr" target="#b32">[33]</ref>. The information of the original SAR image is more abundant, and the resolution, incident angle, polarization method, and imaging mode are all very different. This information provides beneficial conditions for ship detection in SAR images based on deep learning.For ship target detection in large scene SAR images, some typical slices from Sentinel 1 are applied, which have different polarization methods, vertical transmission and vertical reception (VV) and vertical transmission and horizontal reception (VH). The original pixel values of the three largescale SAR images shown in the article are 12 075 × 8351, 4417 × 7749, and 8560 × 8297.2) Evaluation Metric: To evaluate the detection performance of the network, AP, AP 50 , AP 75 , AP s , AP m , and AP l are used as the metrics following the evaluation protocol on the COCO data set. Average precision (AP) is the area under the precision-recall curve. Mean average precision (mAP) is the average of multiple categories of AP. mAP is the most important one in the target detection algorithm. There is only one category of ships in this article, so mAP = AP. Generally speaking, the better the classifier, the higher the AP value. In this article, AP is average precision at Intersection over Union (IoU) = 0.50:0.05:0.95 (primary challenge metric), and AP 50 is AP at IoU = 0.5 (PASCAL Visual Object Classes (VOC) metric), Similarly, AP 75 is AP at IoU = 0.75 (strict metric). AP s , AP m , and AP l represent AP for small objects, medium objects, and large objects, respectively<ref type="bibr" target="#b33">[34]</ref>. The most important metric is AP 50 . The precision and recall are as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Effect of SSE module. (a) is the original images. (b) is the ground truth. (c) is the detection results of CenterNet without SSE. (d) is the detection results of CenterNet with SSE.</figDesc><graphic coords="7,53.99,349.01,120.74,86.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Effect of the proposed module. (a) is the ground truth. (b) is the detection results with faster R-CNN. (c) is the detection results with SSD-300. (d) is the detection results with YOLOv3. (e) is the detection results with the proposed method.</figDesc><graphic coords="8,53.99,432.53,97.10,83.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>, [Fig. 8(a)] are the original images, [Fig. 8(b)] is the ground truth, and [Fig. 8(c) and (d)] represent test results with and without SSE, respectively. The red rectangle in [Fig. 8(b)] represents the real ship targets and the purple rectangle in [Fig. 8(c) and (d)] represents detected ship targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Authorized licensed use limited to: University of Exeter. Downloaded on June 18,2020 at 01:04:03 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Large-Scale SAR Images 1) Preprocessing: With the improvement of SAR image resolution, people are more interested in solving the detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Results in large-scale SAR images. (a) is the result without SSE and preclassification. There are many false alarms in nearshore and inland. (b) is the result with SSE and preclassification.</figDesc><graphic coords="10,80.99,575.33,220.10,94.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Some other ship detection results in large-scale SAR images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETECTION</head><label>I</label><figDesc>PERFORMANCE OF CENTERNET WITH AND WITHOUT SSE</figDesc><table><row><cell>TABLE II</cell></row><row><cell>COMPARISON WITH THE TYPICAL DETECTION NETWORKS</cell></row><row><cell>IN DEEP LEARNING</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV EFFECT</head><label>IV</label><figDesc>OF PRECLASSIFICATION AND SSE ON LARGE-SCALE SAR IMAGES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V COMPARISON</head><label>V</label><figDesc>WITH OTHER METHODS IN LARGE-SCALE SAR IMAGES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: University of Exeter. Downloaded on June 18,2020 at 01:04:03 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61801098 and Grant 61971101, in part by the Shanghai Aerospace Science and Technology Innovation Fund under Grant SAST2018-079, and in part by the Science and Technology on Automatic Target Recognition Laboratory (ATR) Fund under Grant 6142503190201.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Squeeze and excitation rank faster R-CNN for ship detection in SAR images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="751" to="755" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A novel algorithm for ship detection in SAR imagery based on the wavelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lopez-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mallorqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="205" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from Google Earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building development monitoring in multitemporal remotely sensed image pairs with stochastic birth-death dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="50" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection and discrimination of ship targets in complex background from spaceborne ALOS-2 SAR images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="536" to="550" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SAR-SIFT: A SIFT-like algorithm for SAR images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A CFAR detection algorithm for generalized gamma distributed background in high-resolution SAR images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="806" to="810" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ship detection in SAR images based on an improved faster R-CNN</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SAR Big Data Era</title>
		<meeting>SAR Big Data Era</meeting>
		<imprint>
			<date type="published" when="2017-11">Nov. 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New hierarchical saliency filtering for fast ship detection in high-resolution SAR images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="351" to="362" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A bilateral CFAR algorithm for ship detection in SAR images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1536" to="1540" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An intensity-space domain CFAR method for ship detection in HR SAR images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">LeNet-5, Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/lenet" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Target classification using the deep convolutional networks for SAR images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4806" to="4817" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object detection with deep learning: A review</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3212" to="3232" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contextual region-based convolutional neural network with multilayer fusion for SAR ship detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">860</biblScope>
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A densely connected end-to-end neural network for multiscale and multiscene SAR ship detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="20881" to="20892" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<ptr target="http://arxiv.org/abs/1904.07850" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis</title>
		<meeting>IEEE/CVF Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A coupled convolutional neural network for small and densely clustered ship detection in SAR images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">42301</biblScope>
			<date type="published" when="2019-04">Apr. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple and efficient network for small target detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="85771" to="85781" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Spatial group-wise enhance: Improving semantic feature learning in convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09646</idno>
		<ptr target="http://arxiv.org/abs/1905.09646" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis</title>
		<meeting>IEEE/CVF Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A SAR dataset of ship detection for deep learning under complex backgrounds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">765</biblScope>
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis</title>
		<meeting>IEEE/CVF Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scale-transferrable pyramid network for multiscale ship detection in SAR images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dense attention pyramid networks for multi-scale ship detection in SAR images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8983" to="8997" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
