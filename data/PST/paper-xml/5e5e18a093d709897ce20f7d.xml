<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPHAF: A FLOW-BASED AUTOREGRESSIVE MODEL FOR MOLECULAR GRAPH GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
							<email>chenceshi@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
							<email>mkxu@apex.sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
							<email>zhaocheng.zhu@umontreal.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">Mila -Québec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>mzhangcs@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
							<email>wnzhang@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">Mila -Québec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">HEC Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">CIFAR AI Research Chair</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPHAF: A FLOW-BASED AUTOREGRESSIVE MODEL FOR MOLECULAR GRAPH GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68% chemically valid molecules even without chemical knowledge rules and 100% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization. 1 * Equal contribution, with order determined by flipping a coin. Work was done during internship at Mila.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Designing novel molecular structures with desired properties is a fundamental problem in a variety of applications such as drug discovery and material science. The problem is very challenging, since the chemical space is discrete by nature, and the entire search space is huge, which is believed to be as large as 10 33 <ref type="bibr" target="#b30">(Polishchuk et al., 2013)</ref>. Machine learning techniques have seen a big opportunity in molecular design thanks to the large amount of data in these domains. Recently, there are increasing efforts in developing machine learning algorithms that can automatically generate chemically valid molecular structures and meanwhile optimize their properties. Specifically, significant progress has been achieved by representing molecular structures as graphs and generating graph structures with deep generative models, e.g., Variational Autoencoders (VAEs) <ref type="bibr" target="#b15">(Kingma &amp; Welling, 2013)</ref>, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">(Goodfellow et al., 2014)</ref> and Autoregressive Models <ref type="bibr">(Van Oord et al., 2016)</ref>. For example, <ref type="bibr" target="#b12">Jin et al. (2018)</ref> proposed a Junction Tree VAE (JT-VAE) for molecular structure encoding and decoding. <ref type="bibr" target="#b1">De Cao &amp; Kipf (2018)</ref> studied how to use GANs for molecular graph generation. <ref type="bibr" target="#b45">You et al. (2018a)</ref> proposed an approach called Graph Convolutional Policy Network (GCPN), which formulated molecular graph generation as a sequential decision process and dynamically generates the nodes and edges based on the </p><formula xml:id="formula_0">- - - - - RVAE - - - - - - GCPN - - - - - MRNN - - - - - GraphNVP - - - - - - GraphAF - - - - -</formula><p>existing graph substructures. They used reinforcement learning to optimize the properties of generated graph structures. Recently, another very related work called MolecularRNN (MRNN) <ref type="bibr" target="#b32">(Popova et al., 2019)</ref> proposed to use an autoregressive model for molecular graph generation. The autoregressive based approaches including both GCPN and MRNN have demonstrated very competitive performance in a variety of tasks on molecular graph generation.</p><p>Recently, besides the aforementioned three types of generative models, normalizing flows have made significant progress and have been successfully applied to a variety of tasks including density estimation <ref type="bibr" target="#b2">(Dinh et al., 2016;</ref><ref type="bibr" target="#b28">Papamakarios et al., 2017)</ref>, variational inference <ref type="bibr" target="#b17">(Kingma et al., 2016;</ref><ref type="bibr" target="#b23">Louizos &amp; Welling, 2017;</ref><ref type="bibr" target="#b34">Rezende &amp; Mohamed, 2015)</ref>, and image generation <ref type="bibr" target="#b16">(Kingma &amp; Dhariwal, 2018)</ref>. Flow-based approaches define invertible transformations between a latent base distribution (e.g. Gaussian distribution) and real-world high-dimensional data (e.g. images and speech). Such an invertible mapping allows the calculation of the exact data likelihood. Meanwhile, by using multiple layers of non-linear transformation between the hidden space and observation space, flows have a high capacity to model the data density. Moreover, different architectures can be designed to promote fast training <ref type="bibr" target="#b28">(Papamakarios et al., 2017)</ref> or fast sampling <ref type="bibr" target="#b17">(Kingma et al., 2016)</ref> depending on the requirement of different applications. We conduct extensive experiments on the standard ZINC <ref type="bibr" target="#b11">(Irwin et al., 2012)</ref> dataset. Results show that the training of GraphAF is significantly efficient, which is two times faster than the state-of-theart model GCPN. The generated molecules are 100% valid by incorporating the chemical rules during generation. We are also surprised to find that even without using the chemical rules for valency checking during generation, the percentage of valid molecules generated by GraphAF can be still as high as 68%, which is significantly higher than existing state-of-the-art GCPN. This shows that GraphAF indeed has the high model capability to learn the data distribution of molecule structures. We further fine-tune the generation process with reinforcement learning to optimize the chemical properties of generated molecules. Results show that GraphAF significantly outperforms previous state-of-the-art GCPN on both property optimization and constrained property optimization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A variety of deep generative models have been proposed for molecular graph generation recently <ref type="bibr" target="#b40">(Segler et al., 2017;</ref><ref type="bibr" target="#b27">Olivecrona et al., 2017;</ref><ref type="bibr" target="#b36">Samanta et al., 2018;</ref><ref type="bibr" target="#b26">Neil et al., 2018)</ref>. The RVAE model <ref type="bibr" target="#b24">(Ma et al., 2018)</ref> used a variational autoencoder for molecule generation, and proposed a novel regularization framework to ensure semantic validity. <ref type="bibr" target="#b12">Jin et al. (2018)</ref> proposed to represent a molecule as a junction tree of chemical scaffolds and proposed the JT-VAE model for molecule generation. For the VAE-based approaches, the optimization of chemical properties is usually done by searching in the latent space with Bayesian Optimization <ref type="bibr" target="#b12">(Jin et al., 2018)</ref>. <ref type="bibr" target="#b1">De Cao &amp; Kipf (2018)</ref> used Generative Adversarial Networks for molecule generation. The state-of-the-art models are built on autoregressive based approaches <ref type="bibr" target="#b45">(You et al., 2018a;</ref><ref type="bibr" target="#b32">Popova et al., 2019)</ref>. <ref type="bibr" target="#b45">(You et al., 2018a)</ref> formulated the problem as a sequential decision process by dynamically adding new nodes and edges based on current sub-graph structures, and the generation policy network is trained by a reinforcement learning framework. Recently, <ref type="bibr" target="#b32">Popova et al. (2019)</ref> proposed an autoregressive model called MolecularRNN to generate new nodes and edges based on the generated nodes and edge sequences. The iterative nature of autoregressive model allows effectively leveraging chemical rules for valency checking during generation and hence the proportion of valid molecules generated by these models is very high. However, due to the sequential generation nature, the training process is usually slow. Our GraphAF approach enjoys the advantage of iterative generation process like autoregressive models (the mapping from latent space to observation space) and meanwhile calculates the exact likelihood corresponding to a feedforward neural network (the mapping from observation space to latent space), which can be implemented efficiently through parallel computation.</p><p>Two recent work-Graph Normalizing Flows (GNF) <ref type="bibr" target="#b22">(Liu et al., 2019)</ref> and GraphNVP <ref type="bibr" target="#b25">(Madhawa et al., 2019)</ref>-are also flow-based approaches for graph generation. However, our work is fundamentally different from their work. GNF defines a normalizing flow from a base distribution to the hidden node representations of a pretrained Graph Autoencoders. The generation scheme is done through two separate stages by first generating the node embeddings with the normalizing flow and then generate the graphs based on the generated node embeddings in the first stage. By contrast, in GraphAF, we define an autoregressive flow from a base distribution directly to the molecular graph structures, which can be trained end-to-end. GraphNVP also defines a normalizing flow from a base distribution to the molecular graph structures. However, the generation process of GraphNVP is one-shot, which cannot effectively capture graph structures and also cannot guarantee the validity of generated molecules. In our GraphAF, we formulate the generation process as a sequential decision process and effectively capture the sub-graph structures via graph neural networks, based on which we define a policy function to generate the nodes and edges. The sequential generation process also allows incorporating the chemical rules. As a result, the validity of the generated molecules can be guaranteed. We summarize existing approaches in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AUTOREGRESSIVE FLOW</head><p>A normalizing flow <ref type="bibr" target="#b19">(Kobyzev et al., 2019)</ref> defines a parameterized invertible deterministic transformation from a base distribution E (the latent space, e.g., Gaussian distribution) to real-world observational space Z (e.g. images and speech). Let f : E → Z be an invertible transformation where ∼ p E ( ) is the base distribution, then we can compute the density function of real-world data z, i.e., p Z (z), via the change-of-variables formula:</p><formula xml:id="formula_1">p Z (z) = p E f −1 θ (z) det ∂f −1 θ (z) ∂z .<label>(1)</label></formula><p>Now considering two key processes of normalizing flows as a generative model: (1) Calculating Data Likelihood: given a datapoint z, the exact density p Z (z) can be calculated by inverting the transformation f , = f −1 θ (z);</p><p>(2) Sampling: z can be sampled from the distribution p Z (z) by first sample ∼ p E ( ) and then perform the feedforward transformation z = f θ ( ). To efficiently perform the above mentioned operations, f θ is required to be invertible with an easily computable Jacobian determinant. Autoregressive flows (AF), originally proposed in <ref type="bibr" target="#b28">Papamakarios et al. (2017)</ref>, is a variant that satisfies these criteria, which holds a triangular Jacobian matrix, and the determinant can be computed linearly. Formally, given z ∈ R D (D is the dimension of observation data), the autoregressive conditional probabilities can be parameterized as Gaussian distributions:</p><formula xml:id="formula_2">p(z d |z 1:d−1 ) = N (z d |µ d , (α d ) 2 ), where µ d = g µ (z 1:d−1 ; θ), α d = g α (z 1:d−1 ; θ),<label>(2)</label></formula><p>where g µ and g α are unconstrained and positive scalar functions of z 1:d−1 respectively to compute the mean and deviation. In practice, these functions can be implemented as neural networks. The affine transformation of AF can be written as:</p><formula xml:id="formula_3">f θ ( d ) = z d = µ d + α d • d ; f −1 θ (z d ) = d = z d − µ d α d .<label>(3)</label></formula><p>The Jacobian matrix in AF is triangular, since ∂zi ∂ j is non-zero only for j ≤ i. Therefore, the determinant can be efficiently computed through D d=1 α d . Specifically, to perform density estimation, we can apply all individual scalar affine transformations in parallel to compute the base density, each of which depends on previous variables z 1:d−1 ; to sample z, we can first sample ∈ R D and compute z 1 through the affine transformation, and then each subsequent z d can be computed sequentially based on previously observed z 1:d−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GRAPH REPRESENTATION LEARNING</head><p>Following existing work, we also represent a molecule as a graph G = (A, X), where A is the adjacency tensor and X is the node feature matrix. Assuming there are n nodes in the graph, d and b are the number of different types of nodes and edges respectively, then A ∈ {0, 1} n×n×b and X ∈ {0, 1} n×d . A ijk = 1 if there exists a bond with type k between i th and j th nodes.</p><p>Graph Convolutional Networks (GCN) <ref type="bibr" target="#b3">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b5">Gilmer et al., 2017;</ref><ref type="bibr" target="#b13">Kearnes et al., 2016;</ref><ref type="bibr" target="#b18">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b39">Schütt et al., 2017)</ref> are a family of neural network architectures for learning representations of graphs. In this paper, we use a variant of Relational GCN (R-GCN) <ref type="bibr" target="#b37">(Schlichtkrull et al., 2018)</ref> to learn the node representations (i.e., atoms) of graphs with categorical edge types. Let k denote the embedding dimension. We compute the node embeddings H l ∈ R n×k at the l th layer of R-GCN by aggregating messages from different edge types:</p><formula xml:id="formula_4">H l = Agg ReLU { D− 1 2 i Ẽi D− 1 2 i H l−1 W l i } i ∈ (1, . . . , b) ,<label>(4)</label></formula><p>where E i = A [:,:,i] denotes the i th slice of edge-conditioned adjacency tensor, Ẽi = E i + I, and</p><formula xml:id="formula_5">Di = k Ẽi [j, k]. W (l) i</formula><p>is a trainable weight matrix for the i th edge type. Agg(•) denotes an aggregation function such as mean pooling or summation. The initial hidden node representation H 0 is set as the original node feature matrix X. After L message passing layers, we use the the final hidden representation H L as the node representations. Meanwhile, the whole graph representations can be defined by aggregating the whole node representations using a readout function <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>, e.g., summation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED METHOD</head><p>4.1 GRAPHAF FRAMEWORK Similar to existing works like GCPN <ref type="bibr" target="#b45">(You et al., 2018a)</ref> and MolecularRNN <ref type="bibr" target="#b32">(Popova et al., 2019)</ref>, we formalize the problem of molecular graph generation as a sequential decision process. Let G = (A, X) denote a molecular graph structure. Starting from an empty graph G 1 , in each step a new node X i is generated based on the current sub-graph structure G i , i.e., p(X i |G i ). Afterwards, the edges between this new node and existing nodes are sequentially generated according to the current graph structure, i.e., p(A ij |G i , X i , A i,1:j−1 ). This process is repeated until all the nodes and edges are generated. An illustrative example is given in Fig. <ref type="figure" target="#fig_0">1(a)</ref>.</p><p>GraphAF is aimed at defining an invertible transformation from a base distribution (e.g. multivariate Gaussian) to a molecular graph structure G = (A, X). Note that we add one additional type of edge between two nodes, which corresponds to no edge between two nodes, i.e., A ∈ {0, 1} n×n×(b+1) . Since both the node type X i and the edge type A ij are discrete, which do not fit into a flow-based model, a standard approach is to use Dequantization technique <ref type="bibr" target="#b2">(Dinh et al., 2016;</ref><ref type="bibr" target="#b16">Kingma &amp; Dhariwal, 2018)</ref> to convert discrete data into continuous data by adding real-valued noise. We follow this approach to preprocess a discrete graph G = (A, X) into continuous data z = (z A , z X ): We present further discussions on dequantization techniques in Appendix A. Formally, we define the conditional distributions for the generation as:</p><formula xml:id="formula_6">z X i = X i + u, u ∼ U [0, 1) d ; z A ij = A ij + u, u ∼ U [0, 1) b+1 .<label>(5)</label></formula><formula xml:id="formula_7">p(z X i |G i ) =N (µ X i , (α X i ) 2 ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">µ X i = g µ X (G i ), α X i = g α X (G i ), p(z A ij |G i , X i , A i,1:j−1 ) = N (µ A ij , (α A ij ) 2 ), j ∈ {1, 2, . . . , i − 1},<label>(7)</label></formula><p>where</p><formula xml:id="formula_9">µ A ij = g µ A (G i , X i , A i,1:j−1 ), α A ij = g α A (G i , X i , A i,1:j−1 ),</formula><p>where g µ X , g µ A and g α X , g α A are parameterized neural networks for defining the mean and standard deviation of a Gaussian distribution. More specifically, given the current sub-graph structure G i , we use a L-layer of Relational GCN (defined in Section 3.2) to learn the node embeddings H L i ∈ R n×k , and the embedding of entire sub-graph hi ∈ R k , based on which we define the mean and standard deviations of Gaussian distributions to generate the nodes and edges respectively: R-GCN:</p><formula xml:id="formula_10">H L i = R-GCN(G i ), hi = sum(H L i ); Node-MLPs: g µ X = m µ X ( hi ), g α X = m α X ( hi ); Edge-MLPs: g µ A = m µ A ( hi , H L i,i , H L i,j ), g α A = m α A ( hi , H L i,i , H L i,j ),<label>(8)</label></formula><p>where sum denotes the sum-pooling operation, and H L i,j ∈ R k denotes the embedding of the j-th node in the embeddings H L i . m µ X , m α X are Multi-Layer Perceptrons (MLP) that predict the node types according to the current sub-graph embedding. and m µ A , m α A are MLPs that predict the types of edges according to the current sub-graph embedding and node embeddings.</p><p>To generate a new node X i and its edges connected to existing nodes, we just sample random variables i and ij from the base Gaussian distribution and convert it to discrete features. More specifically,</p><formula xml:id="formula_11">z X i = i α X i + µ X i , i ∈ R d ; z A ij = ij α A ij + µ A ij , j ∈ {1, 2, . . . , i − 1}, ij ∈ R b+1 ,<label>(9)</label></formula><p>where is the element-wise multiplication. In practice, a real molecular graph is generated by taking the argmax of generated continuous vectors, i.e.,</p><formula xml:id="formula_12">X i = v d argmax(z X i ) and A ij = v b+1 argmax(z A ij )</formula><p>, where v p q denotes a p dimensional one-hot vector with q th dimension equal to 1. Let = { 1 , 2 , 21 , 3 , 31 , 32 , . . . , n , n1 , . . . , n,n−1 }, where n is the number of atoms in the given molecule, GraphAF defines an invertible mapping between the base Gaussian distribution and the molecule structures z = (z A , z X ). According to Eq. 9, the inverse process from z = (z A , z X ) to can be easily calculated as:</p><formula xml:id="formula_13">i = z X i − µ X i 1 α X i ; ij = z A ij − µ A ij 1 α A ij , j ∈ {1, 2, . . . , i − 1},<label>(10)</label></formula><p>where 1 α X i and 1</p><p>α A ij denote element-wise reciprocals of α X i and α A ij respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EFFICIENT PARALLEL TRAINING</head><p>In GraphAF, since f : E → Z is autoregressive, the Jacobian matrix of the inverse process f −1 : Z → E is a triangular matrix, and its determinant can be calculated very efficiently. Given a minibatch of training data G, the exact density of each molecule under a given order can be efficiently computed by the change-of-variables formula in Eq. 1. Our objective is to maximize the likelihood of training data.</p><p>During training, we are able to perform parallel computation by defining a feedforward neural network between the input molecule graph G and the output latent variable by using masking.</p><p>The mask drops out some connections from inputs to ensure that R-GCN is only connected to the sub-graph G i when inferring the hidden variable of node i, i.e., i , and connected to sub-graph G i , X i , A i,1:j−1 when inferring the hidden variable of edge A ij , i.e., ij . This is similar to the approaches used in MADE <ref type="bibr" target="#b4">(Germain et al., 2015)</ref> and MAF <ref type="bibr" target="#b28">(Papamakarios et al., 2017)</ref>. With the masking technique, GraphAF satisfies the autoregressive property, and at the same time p(G) can be efficiently calculated in just one forward pass by computing all the conditionals in parallel.</p><p>To further accelerate the training process, the nodes and edges of a training graph are re-ordered according to the breadth-first search (BFS) order, which is widely adopted by existing approaches for graph generation <ref type="bibr" target="#b46">(You et al., 2018b;</ref><ref type="bibr" target="#b32">Popova et al., 2019)</ref>. Due to the nature of BFS, bonds can only be present between nodes within the same or consecutive BFS depths. Therefore, the maximum dependency distance between nodes is bounded by the largest number of nodes in a single BFS depth. In our data sets, any single BFS depth contains no more than 12 nodes, which means we only need to model the edges between current atom and the latest generated 12 atoms.</p><p>Due to space limitation, we summarize the detailed training algorithm into Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VALIDITY CONSTRAINED SAMPLING</head><p>In chemistry, there exist many chemical rules, which can help to generate valid molecules. Thanks to the sequential generation process, GraphAF can leverage these rules in each generation step. Specifically, we can explicitly apply a valency constraint during sampling to check whether current bonds have exceeded the allowed valency, which has been widely adopted in previous models <ref type="bibr" target="#b45">(You et al., 2018a;</ref><ref type="bibr" target="#b32">Popova et al., 2019)</ref>. Let |A ij | denote the order of the chemical bond A ij . In each edge generation step of A ij , we check the following valency constraint for the i th and j th atoms:</p><formula xml:id="formula_14">j |A ij | ≤ Valency(X i ) and i |A ij | ≤ Valency(X j ).<label>(11)</label></formula><p>If the newly added bond breaks the valency constraint, we just reject the bond A ij , sample a new ij in the latent space and generate another new bond type. The generation process will terminate if one of the following conditions is satisfied: 1) the graph size reaches the max-size n, 2) no bond is generated between the newly generated atom and previous sub-graph. Finally, hydrogens are added to the atoms that have not filled up their valencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GOAL-DIRECTED MOLECULE GENERATION WITH REINFORCEMENT LEARNING</head><p>So far, we have introduced how to use GraphAF to model the data density of molecular graph structures and generate valid molecules. Nonetheless, for drug discovery, we also need to optimize the chemical properties of generated molecules. In this part, we introduce how to fine-tune our generation process with reinforcement learning to optimize the properties of generated molecules.</p><p>State and Policy Network. The state is the current sub-graph, and the initial state is an empty graph. The policy network is the same as the autoregressive model defined in Section 4.1, which includes the process of generating a new atom based on the current sub-graph and generating the edges between the new atom and existing atoms, i.e., p (X i |G i ) and p (A ij |G i , X i , A i,1:j−1 ). The policy network itself defines a distribution p θ of molecular graphs G. If there are no edges between the newly generated atom and current sub-graph, the generation process terminates. For the state transition dynamics, we also incorporate the valency check constraint.</p><p>Reward design. Similar to <ref type="bibr">GCPN You et al. (2018a)</ref>, we also incorporate both intermediate and final rewards for training the policy network. A small penalization will be introduced as the intermediate reward if the edge predictions violate the valency check. The final rewards include both the score of targeted-properties of generated molecules such as octanol-water partition coefficient (logP) or drug-likeness (QED) <ref type="bibr" target="#b0">(Bickerton et al., 2012)</ref> and the chemical validity reward such as penalties for molecules with excessive steric strain and or functional groups that violate ZINC functional group filters <ref type="bibr" target="#b11">(Irwin et al., 2012)</ref>. The final reward is distributed to all intermediate steps with a discounting factor to stabilize the training.</p><p>In practice, we adopt Proximal Policy Optimization (PPO) <ref type="bibr" target="#b38">(Schulman et al., 2017)</ref>, an advanced policy gradient algorithm to train GraphAF in the above defined environment. Let G ij be the shorthand notation of sub-graph</p><formula xml:id="formula_15">G i ∪ X i ∪ A i,1:j−1 . Formally, L(θ) = − E G∼p θ E i min r i (θ)V (G i , X i ), clip (r i (θ), 1 − , 1 + ) V (G i , X i ) + E j min r ij (θ)V (G ij , A ij ), clip (r ij (θ), 1 − , 1 + ) V (G ij , A ij ) ,<label>(12)</label></formula><p>where r i (θ) = p θ (Xi|Gi) p θ old (Xi|Gi) and r ij (θ) = |Gij ) are ratios of probabilities output by old and new policies, and V (state, action) is the estimated advantage function with a moving average baseline to reduce the variance. More specifically, we treat generating a node and all its edges with existing nodes as one step and maintain a moving average baseline for each step. The clipped surrogate objective prevents the policy from being updated to collapse for some extreme rewards.</p><formula xml:id="formula_16">p θ (Aij |Gij ) p θ old (Aij</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENT SETUP</head><p>Evaluation Tasks. Following existing works on molecule generation <ref type="bibr" target="#b12">(Jin et al., 2018;</ref><ref type="bibr" target="#b45">You et al., 2018a;</ref><ref type="bibr" target="#b32">Popova et al., 2019)</ref>, we conduct experiments by comparing with the state-of-the-art approaches on three standard tasks. Density Modeling and Generation evaluates the model's capacity to learn the data distribution and generate realistic and diverse molecules. Property Optimization concentrates on generating novel molecules with optimized chemical properties. For this task, we fine-tune our network pretrained from the density modeling task to maximize the desired properties. Constrained Property Optimization is first proposed in <ref type="bibr" target="#b12">Jin et al. (2018)</ref>, which is aimed at modifying the given molecule to improve desired properties while satisfying a similarity constraint.</p><p>Data. We use the ZINC250k molecular dataset <ref type="bibr" target="#b11">(Irwin et al., 2012)</ref> for training. The dataset contains 250, 000 drug-like molecules with a maximum atom number of 38. It has 9 atom types and 3 edge types. We use the open-source chemical software RDkit <ref type="bibr" target="#b21">(Landrum, 2016)</ref> to preprocess molecules. All molecules are presented in kekulized form with hydrogen removed.</p><p>Baselines. We compare GraphAF with the following state-of-the-art approaches for molecule generation. JT-VAE <ref type="bibr" target="#b12">(Jin et al., 2018</ref>) is a VAE-based model which generates molecules by first decoding a tree structure of scaffolds and then assembling them into molecules. JT-VAE has been shown to outperform other previous VAE-based models <ref type="bibr" target="#b20">(Kusner et al., 2017;</ref><ref type="bibr" target="#b6">Gómez-Bombarelli et al., 2018;</ref><ref type="bibr" target="#b42">Simonovsky &amp; Komodakis, 2018)</ref>. GCPN is a state-of-the-art approach which combines reinforcement learning and graph representation learning methods to explore the vast chemical space. MolecularRNN (MRNN), another autoregressive model, uses RNN to generate molecules in a sequential manner. We also compare our model with GraphNVP <ref type="bibr" target="#b25">(Madhawa et al., 2019)</ref>, a recently proposed flow-based model. Results of baselines are taken from original papers unless stated.</p><p>Implementation Details. GraphAF is implemented in PyTorch <ref type="bibr" target="#b29">(Paszke et al., 2017)</ref>. The R-GCN is implemented with 3 layers, and the embedding dimension is set as 128. The max graph size is set as 48 empirically. For density modeling, we train our model for 10 epochs with a batch size of 32 and a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NUMERICAL RESULTS</head><p>Density Modeling and Generation. We evaluate the ability of the proposed method to model real molecules by utilizing the widely-used metrics: Validity is the percentage of valid molecules among all the generated graphs. Uniqueness is the percentage of unique molecules among all the generated molecules. Novelty is the percentage of generated molecules not appearing in training set.</p><p>Reconstruction is the percentage of the molecules that can be reconstructed from latent vectors. We calculate the above metrics from 10,000 randomly generated molecules.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows that GraphAF achieves competitive results on all four metrics. As a flow-based model, GraphAF holds perfect reconstruction ability compared with VAE approaches. Our model also achieves a 100% validity rate since we can leverage the valency check during sequential generation. By contrast, the validity rate of another flow-based approach GraphNVP is only 42.60% due to its one-shot sampling process. An interesting result is that even without the valency check during generation, GraphAF can still achieve a validity rate as high as 68%, while previous state-of-the-art approach GCPN only achieves 20%. This indicates the strong flexibility of GraphAF to model the data density and capture the domain knowledge from unsupervised training on the large chemical dataset. We also compare the efficiency of different methods on the same computation environment, a machine with 1 Tesla V100 GPU and 32 CPU cores. To achieve the results in Table <ref type="table" target="#tab_2">2</ref>, JT-VAE and GCPN take around 24 and 8 hours, respectively, while GraphAF only takes 4 hours.</p><p>To show that GraphAF is not overfitted to the specific dataset ZINC250k, we also conduct experiments on two other molecule datasets, QM9 <ref type="bibr" target="#b33">(Ramakrishnan et al., 2014)</ref> and MOSES <ref type="bibr" target="#b31">(Polykovskiy et al., 2018)</ref>. QM9 contains 134k molecules with 9 heavy atoms, and MOSES is much larger and more challenging, which contains 1.9M molecules with up to 30 heavy atoms. Table <ref type="table" target="#tab_3">3</ref> shows that GraphAF can always generate valid and novel molecules even on the more complicated dataset.</p><p>Furthermore, though GraphAF is originally designed for molecular graph generation, it is actually very general and can be used to model different types of graphs by simply modifying the node and edge generating functions Edge-MLPs and Node-MLPs (Eq. 8). Following the experimental setup of Graph Normalizing Flows (GNF) <ref type="bibr" target="#b22">(Liu et al., 2019)</ref>, we test GraphAF on two generic graph datasets: COMMUNITY-SMALL, which is a synthetic data set containing 100 2-community graphs, and EGO-SMALL, which is a set of graphs extracted from Citeseer dataset <ref type="bibr" target="#b41">(Sen et al., 2008)</ref>. In practice, we use one-hot indicator vectors as node features for R-GCN. We borrow open source scripts from GraphRNN <ref type="bibr" target="#b46">(You et al., 2018b)</ref> to generate datasets and evaluate different models. For evaluation, we report the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b8">(Gretton et al., 2012)</ref> between generated  and training graphs using some specific metrics on graphs proposed by <ref type="bibr" target="#b46">You et al. (2018b)</ref>. The results in Table <ref type="table" target="#tab_4">4</ref> demonstrate that when applied to generic graphs, GraphAF can still consistently yield comparable or better results compared with GraphRNN and GNF. We give the visualization of generated generic graphs in Appendix D.</p><p>Property Optimization. In this task, we aim at generating molecules with desired properties. Specifically, we choose penalized logP and QED as our target property. The former score is logP score penalized by ring size and synthetic accessibility, while the latter one measures the druglikeness of the molecules. Note that both scores are calculated using empirical prediction models and we adopt the script used in <ref type="bibr" target="#b45">(You et al., 2018a)</ref> to make results comparable. To perform this task, we pretrain the GraphAF network for 300 epochs for likelihood modeling, and then apply the RL process described in section 4.4 to fine-tune the network towards desired chemical properties. Detailed reward design and hyper-parameters setting can be found in Appendix C. Following existing works, we report the top-3 scores found by each model. As shown in Table <ref type="table" target="#tab_5">5</ref>, GraphAF outperforms all baselines by a large margin for penalized logP score and achieves comparable results for QED. This phenomenon indicates that combined with RL process, GraphAF successfully captures the distribution of desired molecules. Note that we re-evaluate the properties of the top-3 molecules found by MolecularRNN, which turn out to be lower than the results reported in the original paper. Figure <ref type="figure" target="#fig_1">2</ref>(a) and 2(b) show the molecules with the highest score discovered by our model. More realistic molecules generated by GraphAF with penalized logP score ranging from 5 to 10 are presented in Figure <ref type="figure" target="#fig_4">6</ref> in Appendix E.</p><p>One should note that, as defined in Sec 4.4, our RL process is close to the one used in previous work GCPN <ref type="bibr" target="#b45">(You et al., 2018a)</ref>. Therefore, the good property optimization performance is believed to come from the flexibility of flow. Compared with the GAN model used in GCPN, which is known to suffer from the mode collapse problem, flow is flexible at modeling complex distribution and generating diverse data (as shown in Table <ref type="table" target="#tab_3">2 and Table 3</ref>). This allows GraphAF to explore a variety of molecule structures in the RL process for molecule properties optimization.</p><p>Constrained Property Optimization. The goal of the last task is to modify the given molecule to improve specified property with the constraint that the similarity between the original and modified molecule is above a threshold δ. Following <ref type="bibr" target="#b12">Jin et al. (2018)</ref> and <ref type="bibr" target="#b45">You et al. (2018a)</ref>, we choose to optimize penalized logP for 800 molecules in ZINC250k with the lowest scores and adopt Tanimoto similarity with Morgan fingerprint <ref type="bibr" target="#b35">(Rogers &amp; Hahn, 2010)</ref> as the similarity metric.</p><p>Similar to the property optimization task, we pretrain GraphAF via density modeling and then finetune the model with RL. During generation, we set the initial states as sub-graphs randomly sampled from 800 molecules to be optimized. For evaluation, we report the mean and standard deviation of the highest improvement and the corresponding similarity between the original and modified molecules in Table <ref type="table" target="#tab_6">6</ref>. Experiment results show that GraphAF significantly outperforms all previous approaches and almost always succeeds in improving the target property. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed GraphAF, the first flow-based autoregressive model for generating realistic and diverse molecular graphs. GraphAF is capable to model the complex molecular distribution thanks to the flexibility of normalizing flow, as well as generate novel and 100% valid molecules in empirical experiments. Moreover, the training of GraphAF is very efficient. To optimize the properties of generated molecules, we fine-tuned the generative process with reinforcement learning. Experimental results show that GraphAF outperforms all previous state-of-the-art baselines on the standard tasks.</p><p>In the future, we plan to train our GraphAF model on larger datasets and also extend it to generate other types of graph structures (e.g., social networks).     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed GraphAF model. (a) Illustration of the generative procedure. New nodes or edges are marked in red. Starting from an empty graph and iteratively sample random variables to map them to atom/bond features. The numbered first three steps correspond to the maps in the bottom figure of Fig. 1(b). (b) Computation graph of GraphAF. The left side are the nodes and edges and the right are latent variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Molecules generated in property optimization and constrained property optimization tasks. (a) Molecules with high penalized logP scores. (b) Molecules with high QED scores. (c) Two pairs of molecules in constrained property optimization for penalized logP with similarity 0.71(top) and 0.64(bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2(c) visualizes two optimization examples, showing that our model is able to improve the penalized logP score by a large margin while maintaining a high similarity between the original and modified molecule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Visualizations of training graphs and generated graphs of EGO-SMALL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Molecule samples with high penalized logp score generated by GraphAF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: More results on constrained property optimization for penalized logP score. Numbers beside the arrow denote similarity and improvement of the given molecule pair respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Previous state-of-the-art algorithms for molecular graph generation. The comparison of training is only conducted between autoregressive models.</figDesc><table><row><cell>Name</cell><cell>Generative Model VAE GAN RNN Flow One-shot Iterative Sequential Parallel Sampling Process Training Process</cell></row><row><cell>JT-VAE</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different models on density modeling and generation. Reconstruction is only evaluated on latent variable models. Validity w/o check is only evaluated on models with valency constraints. Result with † is obtained by running GCPN's open-source code. Results with ‡ are taken from Popova et al. (2019).</figDesc><table><row><cell>Method</cell><cell cols="5">Validity Validity w/o check Uniqueness Novelty Reconstruction</cell></row><row><cell>JT-VAE</cell><cell>100%</cell><cell>-</cell><cell>100%  ‡</cell><cell>100%  ‡</cell><cell>76.7%</cell></row><row><cell>GCPN</cell><cell>100%</cell><cell>20%  †</cell><cell>99.97%  ‡</cell><cell>100%  ‡</cell><cell>-</cell></row><row><cell>MRNN</cell><cell>100%</cell><cell>65%</cell><cell>99.89%</cell><cell>100%</cell><cell>-</cell></row><row><cell cols="2">GraphNVP 42.60%</cell><cell>-</cell><cell>94.80%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>GraphAF</cell><cell>100%</cell><cell>68%</cell><cell>99.10%</cell><cell>100%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of density modeling and generation on three different datasets. For property optimization, we perform a grid search on the hyperparameters and select the best setting according to the chemical scoring performance. We use Adam<ref type="bibr" target="#b14">(Kingma &amp; Ba, 2014)</ref> to optimize our model. Full training details can be found in Appendix C.</figDesc><table><row><cell>Method</cell><cell cols="5">Validity Validity w/o check Uniqueness Novelty Reconstruction</cell></row><row><cell>ZINC250k</cell><cell>100%</cell><cell>68%</cell><cell>99.10%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>QM9</cell><cell>100%</cell><cell>67%</cell><cell>94.51%</cell><cell>88.83%</cell><cell>100%</cell></row><row><cell>MOSES</cell><cell>100%</cell><cell>71%</cell><cell>99.99%</cell><cell>100%</cell><cell>100%</cell></row><row><cell cols="2">learning rate of 0.001.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison between different graph generative models on general graphs with MMD metrics. We follow the evaluation scheme of GNF<ref type="bibr" target="#b22">(Liu et al., 2019)</ref>. Results of baselines are also taken from GNF.</figDesc><table><row><cell>Method</cell><cell cols="6">COMMUNITY-SMALL Degree Cluster Orbit Degree Cluster EGO-SMALL</cell><cell>Orbit</cell></row><row><cell>GraphVAE</cell><cell>0.35</cell><cell>0.98</cell><cell>0.54</cell><cell>0.13</cell><cell>0.17</cell><cell></cell><cell>0.05</cell></row><row><cell>DEEPGMG</cell><cell>0.22</cell><cell>0.95</cell><cell>0.4</cell><cell>0.04</cell><cell>0.10</cell><cell></cell><cell>0.02</cell></row><row><cell>GraphRNN</cell><cell>0.08</cell><cell>0.12</cell><cell>0.04</cell><cell>0.09</cell><cell>0.22</cell><cell></cell><cell>0.003</cell></row><row><cell>GNF</cell><cell>0.20</cell><cell>0.20</cell><cell>0.11</cell><cell>0.03</cell><cell>0.10</cell><cell></cell><cell>0.001</cell></row><row><cell>GraphAF</cell><cell>0.18</cell><cell>0.20</cell><cell>0.02</cell><cell>0.03</cell><cell>0.11</cell><cell></cell><cell>0.001</cell></row><row><cell>GraphRNN(1024)</cell><cell>0.03</cell><cell>0.01</cell><cell>0.01</cell><cell>0.04</cell><cell>0.05</cell><cell></cell><cell>0.06</cell></row><row><cell>GNF(1024)</cell><cell>0.12</cell><cell>0.15</cell><cell>0.02</cell><cell>0.01</cell><cell>0.03</cell><cell cols="2">0.0008</cell></row><row><cell>GraphAF(1024)</cell><cell>0.06</cell><cell>0.10</cell><cell>0.015</cell><cell>0.04</cell><cell>0.04</cell><cell></cell><cell>0.008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the top 3 property scores of generated molecules.</figDesc><table><row><cell>Method</cell><cell></cell><cell>1st</cell><cell cols="2">Penalized logP 2nd 3rd</cell><cell>Validity</cell><cell>1st</cell><cell>2nd</cell><cell>QED 3rd</cell><cell>Validity</cell></row><row><cell cols="2">ZINC (Dataset)</cell><cell>4.52</cell><cell>4.30</cell><cell cols="5">4.23 100.0% 0.948 0.948 0.948 100.0%</cell></row><row><cell cols="2">JT-VAE (Jin et al., 2018)</cell><cell>5.30</cell><cell>4.93</cell><cell cols="5">4.49 100.0% 0.925 0.911 0.910 100.0%</cell></row><row><cell cols="2">GCPN (You et al., 2018a)</cell><cell>7.98</cell><cell>7.85</cell><cell cols="5">7.80 100.0% 0.948 0.947 0.946 100.0%</cell></row><row><cell cols="3">MRNN 1 (Popova et al., 2019) 8.63</cell><cell>6.08</cell><cell cols="5">4.73 100.0% 0.844 0.796 0.736 100.0%</cell></row><row><cell>GraphAF</cell><cell></cell><cell cols="7">12.23 11.29 11.05 100.0% 0.948 0.948 0.947 100.0%</cell></row><row><cell>12.23</cell><cell>11.29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11.05</cell><cell>10.83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of results on constrained property optimization.</figDesc><table><row><cell>δ</cell><cell>Improvement</cell><cell>JT-VAE Similarity</cell><cell cols="2">Success Improvement</cell><cell>GCPN Similarity</cell><cell>Success</cell><cell>Improvement</cell><cell>GraphAF Similarity</cell><cell>Success</cell></row><row><cell cols="4">0.0 1.91 ± 2.04 0.28 ± 0.15 97.5%</cell><cell cols="2">4.20 ± 1.28 0.32 ± 0.12</cell><cell>100%</cell><cell cols="2">13.13 ± 6.89 0.29 ± 0.15</cell><cell>100%</cell></row><row><cell cols="4">0.2 1.68 ± 1.85 0.33 ± 0.13 97.1%</cell><cell cols="2">4.12 ± 1.19 0.34 ± 0.11</cell><cell>100%</cell><cell cols="2">11.90 ± 6.86 0.33 ± 0.12</cell><cell>100%</cell></row><row><cell cols="4">0.4 0.84 ± 1.45 0.51 ± 0.10 83.6%</cell><cell cols="2">2.49 ± 1.30 0.47 ± 0.08</cell><cell>100%</cell><cell cols="3">8.21 ± 6.51 0.49 ± 0.09 99.88%</cell></row><row><cell cols="4">0.6 0.21 ± 0.71 0.69 ± 0.06 46.4%</cell><cell cols="2">0.79 ± 0.63 0.68 ± 0.08</cell><cell>100%</cell><cell cols="3">4.98 ± 6.49 0.66 ± 0.05 96.88%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The scores reported here are recalculated based on top 3 molecules presented in the original paper(Popova  et al.,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2019" xml:id="foot_1">) using GCPN's script.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank Min Lin, Meng Qu, Andreea Deac, Laurent Dinh, Louis-Pascal A. C. Xhonneux and Vikas Verma for the extremely helpful discussions and comments. This project is supported by the Natural Sciences and Engineering Research Council of Canada, the Canada CI-FAR AI Chair Program, and a collaboration grant between Microsoft Research and Mila. Ming Zhang is supported by National Key Research and Development Program of China with Grant No. 2018AAA0101900/2018AAA0101902 as well as Beijing Municipal Commission of Science and Technology under Grant No. Z181100008918005. Weinan Zhang is supported by National Science Foundation of China (61702327, 61772333, 61632017, 81771937) and Shanghai Sailing Program (17YF1428200).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A DISCCUSIONS ON DEQUANTIZATION TECHNIQUES The dequantization techniques allow mapping the discrete data into the continuous one by adding a small noise to each dimension. By adding noise from U [0, 1), we can ensure that the range of different categories will not overlap. For example, after dequantization, the value of 1-entry in the one-hot vector lies in [1, 2) while the 0-entry lies in [0, 1). Therefore, we can map the dequantized continuous data back to the discrete one-hot data by easily performing the argmax operation in the generation process. Theoretically, as shown in <ref type="bibr" target="#b43">Theis et al. (2016)</ref>; <ref type="bibr" target="#b10">Ho et al. (2019)</ref>, training a continuous density model on uniform dequantized data can be interpreted as maximizing a lower bound on the log-likelihood for the original discrete data. Mathematically, this statement holds for both image data and binary/categorical data.</p><p>Furthermore, as suggested in <ref type="bibr" target="#b10">Ho et al. (2019)</ref>, instead of adding random uniform noise to each discrete data for dequantization, a more advanced dequantization technique is to treat the noise as a hidden variable and use variational inference to infer the optimum noise added to each discrete data, which we would like to explore in our future work. Convert mol to G = (A, X) with BFS re-ordering 5:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PARALLEL TRAINING ALGORITHM</head><p>for i = 1, ..., N do 6:</p><p>))</p><p>10:</p><p>))</p><p>15:</p><p>end for 16:</p><p>end for 17:</p><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENT DETAILS</head><p>Network architecture. The network architecture is fixed among all three tasks. More specifically, the R-GCN is implemented with 3 layers and the embedding dimension is set as 128. We use batch normalization before graph pooling to accelerate the convergence and choose sum-pooling as the readout function for graph representations. Both node MLPs and edge MLPs have two fullyconnected layers equipped with tanh non-linearity.</p><p>Density Modeling and Generation. To achieve the results in Table <ref type="table">2</ref>, we train GraphAF on ZINC250K with a batch size of 32 on 1 Tesla V100 GPU and 32 CPU cores for 10 epochs. We optimize our model with Adam with a fixed learning rate of 0.001.</p><p>Property Optimization. For both property optimization and constrained property optimization, we first pretrain a GraphAF network via the density modeling task for 300 epochs, and then finetune the network toward desired molecular distribution through RL process. Following are details about the reward design for property optimization. The reward of each step consists of step-wise validity rewards and the final rewards discounted by a fixed factor γ. The step-wise validity penalty is fixed as -1. The final reward of a molecule m includes both property-targeted reward and chemical validation reward. We adopt the same chemical validation rewards as GCPN. We define propertytargeted reward as follows:</p><p>γ is set to 0.97 for QED optimization and 0.9 for penalized logP optimization respectively. We fine-tune the pretrained model for 200 iterations with a fixed batch size of 64 using Adam optimizer.</p><p>We also adopt a linear learning rate warm-up to stabilize the training. We perform the grid search to determine the optimal hyperparameters according to the chemical scoring performance. The search space is summarised in Table <ref type="table">7</ref>. Constrained Property Optimization. We first introduce the way we sample sub-graphs from 800 ZINC molecules. Given a molecule, we first randomly sample a BFS order and then drop the last m nodes in BFS order as well as edges induced by these nodes, where m is randomly chosen from {0, 1, 2, 3, 4, 5} each time. Finally, we reconstruct the sub-graph from the remaining nodes in the BFS sequence. Note that the final sub-graph is connected due to the nature of BFS order. For reward design, we set it as the improvement of the target score. We fine-tune the pretrained model for 200 iterations with a batch size of 64. We also use Adam with a learning rate of 0.0001 to optimize the model. Finally, each molecule is optimized for 200 times by the tuned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D VISUALIZATION OF GENERATED GENERIC GRAPHS</head><p>We present visualizations of graphs from both the training set and generated graphs by GraphAF in Figure <ref type="figure">3</ref> and Figure <ref type="figure">4</ref>. The visualizations demonstrate that GraphAF has strong ability to model different graph structures in the generic graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MORE MOLECULE SAMPLES</head><p>We present more molecule samples generated by GraphAF in the following pages. Figure <ref type="figure">5</ref> presents 50 molecules randomly sampled from multivariate Gaussian, which justify the ability of our model to generate novel, realistic and unique molecules. From Figure <ref type="figure">6</ref> we can see that our model is able to generate molecules with high and diverse penalized logP scores ranging from 5 to 10. For constrained property optimization of penalized logP score, as shown by Figure <ref type="figure">7</ref>, our model can either reduce the ring size, remove the big ring or grow carbon chains from the original molecule, improving the penalized logP score by a large margin.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying the chemical beauty of drugs</title>
		<author>
			<persName><surname>Richard Bickerton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gaia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémy</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Sorel Muresan</surname></persName>
		</author>
		<author>
			<persName><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature chemistry</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Learning</title>
				<meeting>the 34th International Conference on Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamín</forename><surname>Sánchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/ho19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning. PMLR</title>
				<meeting>the 36th International Conference on Machine Learning. PMLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zinc: a free tool to discover chemistry for biology</title>
		<author>
			<persName><forename type="first">Teague</forename><surname>John J Irwin</surname></persName>
		</author>
		<author>
			<persName><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Michael M Mysinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">G</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3nd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09257</idno>
		<title level="m">Normalizing flows: Introduction and ideas</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grammar variational autoencoder</title>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics software</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13177</idno>
		<title level="m">Graph normalizing flows</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiplicative normalizing flows for variational bayesian neural networks</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constrained generation of semantically valid graphs via regularizing variational autoencoders</title>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7113" to="7124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Graphnvp: An invertible flow model for generating molecular graphs</title>
		<author>
			<persName><forename type="first">Kaushalya</forename><surname>Madhawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katushiko</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Nakago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Abe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11600</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring deep recurrent models with reinforcement learning for molecule design</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwin</forename><forename type="middle">H S</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Guasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sellwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, Workshop Track Proceedings</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Molecular de-novo design through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ola</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimation of the size of drug-like chemical space based on gdb-17 data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Polishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Madzhidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varnek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="675" to="679" />
			<date type="published" when="2013-08">Aug 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Polykovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zhebrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Golovanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oktai</forename><surname>Tatanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Belyaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rauf</forename><surname>Kurbanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksey</forename><surname>Artamonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Aladinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Veselov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Kadurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Nikolenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zhavoronkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12823</idno>
		<title level="m">Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Molecularrnn: Generating realistic molecular graphs with optimized properties</title>
		<author>
			<persName><forename type="first">Mariya</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junier</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexandr</forename><surname>Isayev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13372</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Pavlo O Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><surname>Anatole Von Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rezende</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Journal of chemical information and modeling</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="742" to="754" />
		</imprint>
	</monogr>
	<note>Extended-connectivity fingerprints</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Designing random graph models using variational autoencoders with applications to chemical design</title>
		<author>
			<persName><forename type="first">Bidisha</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abir</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05283</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generating focused molecule libraries for drug discovery with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Marwin Hs</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Kogej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Tyrchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">P</forename><surname>Waller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="131" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Galileo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><surname>Eliassi-Rad</surname></persName>
		</author>
		<ptr target="http://www.cs.iit.edu/˜ml/pdfs/sen-aimag08.pdf" />
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="412" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<ptr target="http://arxiv.org/abs/1511.01844" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page" from="6410" to="6421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
