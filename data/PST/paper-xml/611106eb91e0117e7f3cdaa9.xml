<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rebooting Virtual Memory with Midgard</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siddharth</forename><surname>Gupta</surname></persName>
							<email>siddharth.gupta@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Atri</forename><surname>Bhattacharyya</surname></persName>
							<email>atri.bhattacharyya@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Yunho</forename><surname>Oh</surname></persName>
							<email>yunho.oh@skku.edu</email>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
							<email>abhishek@cs.yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
							<email>babak.falsafi@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Payer</surname></persName>
							<email>mathias.payer@epfl.ch</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Sungkyunkwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rebooting Virtual Memory with Midgard</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>virtual memory</term>
					<term>address translation</term>
					<term>memory hierarchy</term>
					<term>virtual caches</term>
					<term>datacenters</term>
					<term>servers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computer systems designers are building cache hierarchies with higher capacity to capture the ever-increasing working sets of modern workloads. Cache hierarchies with higher capacity improve system performance but shift the performance bottleneck to address translation. We propose Midgard, an intermediate address space between the virtual and the physical address spaces, to mitigate address translation overheads without program-level changes.</p><p>Midgard leverages the operating system concept of virtual memory areas (VMAs) to realize a single Midgard address space where VMAs of all processes can be uniquely mapped. The Midgard address space serves as the namespace for all data in a coherence domain and the cache hierarchy. Because real-world workloads use far fewer VMAs than pages to represent their virtual address space, virtual to Midgard translation is achieved with hardware structures that are much smaller than TLB hierarchies. Costlier Midgard to physical address translations are needed only on LLC misses, which become much less frequent with larger caches. As a consequence, Midgard shows that instead of amplifying address translation overheads, memory hierarchies with large caches can reduce address translation overheads.</p><p>Our evaluation shows that Midgard achieves only 5% higher address translation overhead as compared to traditional TLB hierarchies for 4KB pages when using a 16MB aggregate LLC. Midgard also breaks even with traditional TLB hierarchies for 2MB pages when using a 256MB aggregate LLC. For cache hierarchies with higher capacity, Midgard's address translation overhead drops to near zero as secondary and tertiary data working sets fit in the LLC, while traditional TLBs suffer even higher degrees of address translation overhead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>We propose, design, and evaluate Midgard, 1 an experiment in future-proofing the virtual memory (VM) abstraction from performance and implementation complexity challenges in emerging big-memory systems.</p><p>VM simplifies the programming model by obviating the need for programmer-orchestrated data movement between memory devices and persistent storage <ref type="bibr" target="#b8">[9]</ref>, offers "a pointer is a pointer everywhere" semantics across multiple CPU cores <ref type="bibr" target="#b49">[50]</ref> and accelerators (e.g., GPUs <ref type="bibr" target="#b48">[49]</ref>, FPGAs <ref type="bibr" target="#b32">[33]</ref>, NICs <ref type="bibr" target="#b40">[41]</ref>, ASICs <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b50">[51]</ref>). Also, VM is the foundation of access control and memory protection mechanisms ubiquitous to modern computer systems security.</p><p>Unfortunately, VM is today plagued with crippling performance and complexity challenges that undermine its programmability benefits. The central problem is that computer architects are designing systems with increasingly highercapacity cache hierarchies and memory. The latter improves system performance in the face of big-data workloads <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b49">[50]</ref>, as evidenced by recent work on die-stacking, chiplets, DRAM caches <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b60">[61]</ref>, and non-volatile byte-addressable memories <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, they also shift the performance bottleneck to virtual-to-physical address translation, which can consume as much as 10-30% of overall system performance <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>.</p><p>Systems architects are consequently designing complex address translation hardware and Operating System (OS) support that requires significant on-chip area and sophisticated heuristics. The complex hardware and OS support pose verification burdens despite which design bugs still abound <ref type="bibr" target="#b35">[36]</ref>. Individual CPU cores (and recent accelerators) integrate large two-level TLB hierarchies with thousands of entries, separate TLBs at the first level for multiple page sizes, and skew/hashrehash TLBs at the second level to cache multiple page sizes concurrently <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b53">[54]</ref>. These in turn necessitate a staggering amount of OS logic to defragment memory to create 'huge pages' <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b66">[67]</ref> and heuristics to determine when to create, break, and migrate them <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Because huge page heuristics can lead to performance pathologies and hence are not a panacea, processor vendors also integrate specialized MMU cache per core to accelerate the page table walk process <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Specialized per-core TLBs and MMU cache in turn necessitate sophisticated coherence protocols in the OS (i.e., shootdowns) that are slow and buggy, especially with the adoption of asynchronous approaches to hide shootdown overheads at higher core and socket counts in modern servers <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>We circumvent these problems by asking the following questions: Larger cache hierarchies (i.e., L1-LLCs) traditionally amplify VM overheads, but could they actually mitigate VM overheads if we redirect most translation activity to them rather than to specialized translation hardware? This question is inspired in part by prior work on virtual cache hierar-chies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref> and in-cache address translation <ref type="bibr" target="#b64">[65]</ref>, which reduce address translation pressure by deferring the need for physical addresses until a system memory access. Unfortunately, they also compromise programmability because of problems with synonyms/homonyms, and reliance on inflexible fixed-size segments. While approaches like single address space OSes <ref type="bibr" target="#b31">[32]</ref> tackle some of these problems (i.e., removal of synonyms/homonyms), they require recompilation of binaries to map all data shared among processes into a unified virtual address space. What is needed is a programmertransparent intermediate address space for cache hierarchies -without the shortcomings of homonyms, synonyms, or fixed-size segments -that requires a lightweight conversion from virtual addresses and a heavier translation to physical addresses only when accessing the system memory.</p><p>We propose the Midgard abstraction as this intermediate address space by fusing the OS concept of virtual memory areas (VMAs) into hardware for the first time. Applications view memory as a collection of a few flexibly sized VMAs with certain permissions. We show that it is possible to create a single intermediate Midgard address space where VMAs of various processes can be uniquely mapped. This unique address space serves as a namespace for all data in a coherence domain and cache hierarchies. All accesses to the cache hierarchy must be translated from the program's virtual address to a Midgard address. However, the latter can be accomplished using translation structures that are much smaller than TLBs because there are far fewer VMAs (∼10) than pages in real-world workloads. Translation from Midgard to physical addresses can be filtered to only situations where an access misses in the coherent cache hierarchy. Therefore, instead of amplifying address translation overheads, larger cache hierarchies can now be leveraged to reduce them.</p><p>We quantify Midgard's performance characteristics over cache hierarchies ranging in size from tens of MBs to tens of GBs and show that even modest MB-scale SRAM cache hierarchies filter the majority of memory accesses, leaving only a small fraction of memory references for translation from Midgard to physical addresses. We characterize VMA counts as a function of dataset size and thread count and confirm that low VMA counts mean a seamless translation from virtual to Midgard addresses. Using average memory access time (AMAT) analysis, we show that LLC capacities in the tens of MBs comfortably outperform traditional address translation and that at hundreds of MBs, they even outperform huge pages. In our evaluation, Midgard reaches within 5% of address translation overhead of conventional 4KB-page TLB hierarchies for a 16MB LLC and breaks even with 2MBpage TLB hierarchies for a 256MB LLC. Unlike TLB hierarchies exhibiting higher overhead with larger cache hierarchies, Midgard's overhead drops to near zero as secondary and tertiary data working sets fit in the cache hierarchies. Finally, we show how, even for pessimistic scenarios with small LLCs, Midgard can be augmented with modest hardware assistance to achieve competitive performance with traditional address translation.</p><p>This paper is the first of several steps needed to demonstrate a fully working system with Midgard. In this paper, we focus on a proof-of-concept software-modeled prototype of key architectural components. Future work will address the wide spectrum of topics needed to realize Midgard in real systems, including (but not restricted to) OS support, verification of implementation on a range of architectures, and detailed circuit-level studies of key hardware structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. VIRTUAL MEMORY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Virtual Memory Abstraction</head><p>Modern OSes and toolchains organize the program virtual address space into VMAs, which are large contiguous regions representing various logical data sections (e.g., code, heap, stack, bss, mapped files). Each VMA consists of a base and bound address, a set of permissions, and other optional properties. VMAs adequately represent the program's logical properties without actually tying them to the underlying physical resources. Modern programs regularly use a few hundred VMAs, but only a handful (i.e., ∼10) are frequently accessed <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b66">[67]</ref>, and thus constitute the working set.</p><p>The OS is responsible for allocating physical memory to the program and creating mappings from virtual to physical addresses. While a trivial approach might be to map a VMA to an identically sized contiguous region of physical memory, the latter results in external fragmentation. Moreover, as physical memory is a constrained resource, the OS targets optimizing towards efficient memory capacity management. Therefore, current systems divide virtual and physical memory into small, fixed-size regions called pages and frames, respectively. And then, the systems map virtual pages to physical frames, effectively utilizing physical memory capacity. A page becomes the smallest unit of memory allocation, and each VMA's capacity is forced to be a page-size multiple by the OS. Typically, programs can continue using the VMA abstraction without directly encountering the page-based management of physical memory by the OS.</p><p>Current VM subsystems combine access control with translation. Each memory access needs to undergo a permission check or access control. Additionally, locating a page in the physical memory device requires virtual to physical address translation. A page table stores the virtual to physical page mappings for a process. While VMAs dictate the permission management granularity, the permission bits are duplicated for each page and stored in the page tables. TLBs are hardware structures that cache the page table entries to provide fast access and perform both access control and address translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Address Translation Bottleneck</head><p>With the growth in modern data-oriented services, programs are becoming more memory intensive. As online services hosted in the datacenter generate data rapidly, the memory capacity of servers operating on these datasets is growing commensurately already reaching terabytes <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b55">[56]</ref>.</p><p>Unfortunately, scaling physical memory to terabytes results in linear growth in translation metadata. With 4KB pages, 1TB of physical memory requires 256M mappings. Even with high locality (e.g., 3% of the data captures most of the memory accesses in server workloads <ref type="bibr" target="#b60">[61]</ref>), the system requires frequent lookups to 8M mappings. While modern systems already provide thousands of TLB entries per core <ref type="bibr" target="#b25">[26]</ref>, we still fall short by three orders of magnitude. Moreover, we cannot provision thousands of TLB entries for every heterogeneous logic that comes in various sizes (e.g., small cores, GPUs, and accelerators). This mismatch in requirements and availability makes virtual memory one of the most critical bottlenecks in memory scaling.</p><p>Each TLB miss incurs a long-latency page table walk. Larger memory capacity often means larger page tables and walk latency, thus further increasing the TLB miss overhead. Intel is already shifting to 57-bit virtual address spaces, which require 5-level page table walks, increasing both page table walk latency and traffic <ref type="bibr" target="#b21">[22]</ref>. While there have been proposals to parallelize page table walks for reducing latency <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b54">[55]</ref>, the incurred latency remains a performance bottleneck. Overall, core-side long-latency operations are a problem and undermine the hierarchical structure of memory as complex operations should be delegated to the bottom of the hierarchy.</p><p>Huge pages were introduced to increase TLB reach. Unfortunately, huge pages are difficult to use transparently in programs without causing internal fragmentation <ref type="bibr" target="#b13">[14]</ref>. More importantly, letting programs guide page allocation further erodes the VM abstraction. Ideally, programs using large VMAs should experience iso-performance as using huge pages without explicitly requesting them. Finally, huge pages add alignment constraints to a VMA's base address.</p><p>Furthermore, as a result of the end of DRAM scaling <ref type="bibr" target="#b52">[53]</ref>, the memory is becoming increasingly heterogeneous. Migrating pages dynamically among heterogeneous devices (e.g., DRAM, Persistent Memory, High-Bandwidth Memory, and Flash) for performance gains requires page mapping modifications, which results in expensive, global TLB shootdowns to invalidate the corresponding TLB entries from all cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Virtual Cache Hierarchies and Single Address Space OSes</head><p>Prior work observes that access control is required for every memory access, while translation to physical addresses is only required for physical memory accesses. However, traditional systems translate a virtual address to a physical address and use it as an index in the cache hierarchy because physical addresses form a unique namespace (Figure <ref type="figure" target="#fig_0">1a</ref>). This  early translation forces the core-side TLBs to operate at page granularity, while pages are only required for efficient capacity management on the memory side.</p><p>Virtual cache hierarchies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref> aim to delay this translation to the memory side, thus removing the complex TLB hierarchy from the critical path of every memory access and away from the core. The process-private virtual addresses are used as a namespace to index the cache hierarchies (Figure <ref type="figure" target="#fig_0">1b</ref>). Virtual hierarchies are difficult to incorporate in a modern system because of the complexity of resolving synonyms and homonyms across virtual addresses and implementing OS-level access control. While there have been numerous proposals for mechanisms and optimizations to implement virtual cache hierarchies, their implementation complexity remains high, thus preventing mainstream adoption.</p><p>Single address space operating systems <ref type="bibr" target="#b31">[32]</ref> avoid synonyms and homonyms by design, thus making virtual cache hierarchies easier to adopt. Instead of process-private virtual address spaces, these OSes divide a single virtual address space among all the processes, identify shared data among processes and represent them with unique virtual addresses. Unfortunately, implementing a single address operating system requires significant modifications to programming abstractions, which is a key obstacle to its adoption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE MIDGARD ABSTRACTION</head><p>Midgard is based on three conceptual pillars. First, Midgard enables the placement of the coherent cache hierarchy in a namespace (Figure <ref type="figure" target="#fig_0">1c</ref>) that offers the programmability of traditional VM. Second, Midgard quickly translates from virtual addresses to this namespace, permitting access control checks along the way and requiring significantly fewer hardware resources than modern per-core TLBs and MMU cache hierarchies. Third, translating between Midgard addresses and physical addresses requires only modest augmentation of modern OSes. Midgard filters heavyweight translation to physical addresses to only those memory references that miss in the LLC. Sizing LLCs to capture workload working sets also naturally enables better performance than traditional address translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Building Midgard atop Virtual Memory Areas</head><p>Midgard injects VMAs directly into hardware to realize the address space decoupling shown in Figure <ref type="figure" target="#fig_1">2</ref>. Instead of a single layer of mapping from per-process virtual pages to a system-wide set of physical frames, logic between CPUs and the cache hierarchy, which we call the "front side", maps perprocess virtual pages to a system-wide Midgard address space in the unit of VMAs. Logic after the cache hierarchy, which we call the "back side", maps VMA-size units from the Midgard address space to page-size physical frames.</p><p>Translating at the granularity of VMAs is beneficial because real-world workloads use orders of magnitude fewer VMAs of unrestricted size than pages of fixed size. Whereas traditional VM relies on large specialized TLB and MMU cache hierarchies and multi-level page tables, Midgard incorporates much smaller hardware and OS-level translation tables.</p><p>Midgard's ability to defer translation until LLC misses means that relatively costly translations at the page-level granularity can be eliminated except for LLC misses. Our evaluation shows that modestly sized LLCs in the 10s of MBs eliminate the need for heavyweight translations to physical frames for more than 90% of the memory references of modern graph processing workloads. The integration of large multi-GB eDRAM and DRAM caches <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b60">[61]</ref> means that, unlike traditional TLB-based translation, Midgard's LLC filtering of translation requests future-proofs VM's performance.</p><p>Midgard obviates the need to compromise programmability. Thanks to the indirection to a unique namespace, Midgard mitigates the homonym and synonym problems and access control limitations of virtual cache hierarchies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref> and in-cache address translation <ref type="bibr" target="#b64">[65]</ref>. Midgard avoids modifying existing programming abstractions or recompiling binaries as in single address space OSes <ref type="bibr" target="#b31">[32]</ref> or virtual block interfaces <ref type="bibr" target="#b17">[18]</ref>. Unlike prior work restricting VM abstractions to fixed-size segments <ref type="bibr" target="#b68">[69]</ref>, Midgard uses the existing OS-level variable VMA sizes in a flexible manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Operating System Support for Midgard</head><p>Midgard requires that the OS is augmented to map VMAs in per-process virtual address spaces to Midgard memory areas (MMAs) in a single system-wide Midgard address space. The OS must also maintain the tables for VMA to MMA mappings and for MMA mappings to physical frame numbers.</p><p>Sizing the Midgard address space: All per-process VMAs are mapped to a single Midgard address space (Figure <ref type="figure" target="#fig_1">2</ref>) without synonyms or homonyms; the OS must deduplicate shared VMAs across various processes. VMAs occupy a contiguous region in virtual and Midgard address spaces, and therefore growing a VMA in the virtual address space requires growing its corresponding MMA. To grow, MMAs must maintain adequate free space between one another to maximize their chances of growth without collision and their ensuing (costly) relocation. Fortunately, in practice, many MMAs are partly backed by physical memory and only some are fully backed. In other words, as long as the Midgard address space is adequately larger than the physical address space (e.g., 10-15 bits in our study), MMAs from thousands of processes can be practically accommodated. In the less common case where growing MMAs collide, the OS can either remap the MMA to another Midgard address, which may require cache flushes or splitting the MMA at the cost of tracking additional MMAs.</p><p>Tracking VMA to MMA mappings: We add a data structure in the OS, which we call a VMA Table <ref type="table">,</ref> for VMA to MMA mappings to achieve V2M translation in the front side. Many data structures can be used to implement the VMA Table <ref type="table">.</ref> Possibilities include per-process VMA Tables, like traditional per-process radix page tables, or system-wide VMA Tables like traditional inverted page tables.</p><p>In our Midgard prototype, we use compact per-process data structures that realize arbitrarily sized VMAs spanning ranges of virtual addresses. Akin to recent work on range tables <ref type="bibr" target="#b27">[28]</ref>, we implement VMA Tables with B-tree structures. Each VMA mapping requires a base and a bound virtual address that captures the size of the VMA. VMA mappings also need an offset field which indicates the relative offset between the position of the VMA in the virtual address space and the position of the MMA that it points to in the Midgard address space. Since the base, bound, and offset are all page-aligned, they require 52 bits of storage each in VMA Table entries for 64-bit virtual and Midgard address spaces. VMA Table entries also need permission bits for access control. Each VMA mapping is therefore roughly 24 bytes. Even relatively high VMA counts that range in the ∼100s can be accommodated in just a 4KB page, making the VMA Table far smaller than traditional page tables. We leave a detailed study of VMA Table implementations for future work <ref type="bibr" target="#b54">[55]</ref>.</p><p>Tracking MMA to physical frame number mappings: We also add a data structure, which we call a Midgard Page Table, in the OS to map from pages in MMAs to physical frame numbers to support M2P translation in the backside. Unlike the VMA Table, Midgard Page Table needs to map at the granularity of pages in the VMAs to physical frame numbers. Although alternative designs are possible, we use a radix page table <ref type="bibr" target="#b37">[38]</ref>. These mappings can be created either at memory allocation time or lazily akin to demand paging. When a page from the MMA is not mapped to a physical frame, lookups in the cache hierarchy will miss. The latter will prompt an M2P translation, which will in turn prompt a page fault, at which point control will be vectored to the OS, which will either lazily create an MMA to physical frame number mapping, initiate demand paging, or signal a segmentation fault.</p><p>Although not necessary for correctness, Midgard's performance also benefits from optimizations where the pages holding the Midgard Page Table are allocated contiguously. As explained subsequently in Section III-C, contiguous allocation allows M2P translations to short-circuit lookups of multiple levels of the radix tree used to implement the Midgard Page Table, similar in spirit to the operation of modern percore hardware paging structure caches <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Contiguous allocation is based on the insight that radix page tables are only sparsely utilized. We fully expand the radix tree and lay out even unmapped pages in a contiguous manner as shown in Figure <ref type="figure">3b</ref>, enabling short-circuited M2P walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hardware Support for Midgard</head><p>Midgard requires hardware support within the CPU and memory hierarchy for correct and efficient operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Registers to the root of VMA and Midgard Page Table:</head><p>The VMA Table is mapped into the Midgard address space and exposed to each CPU using per-core VMA Table <ref type="table">Base</ref> Registers which store the Midgard address of the root node of the B-tree. In contrast, a pointer to the physical address of the root of the Midgard Page Hardware support for page faults: Page faults due to M2P translation failures signal an exception to the core from which the memory request originated. To maintain precise exceptions, the faulting instruction needs to be rolled back before the exception handler can be executed. Such rollbacks are easier to implement for loads as they are synchronous operations and block until data arrives, than for stores that are asynchronous operations. Modern out-of-order processors retire stores from the reorder buffer once their values and addresses are confirmed. In other words, stores wait for completion in the store buffer while the out-of-order execution moves ahead. If the M2P translation for such a store fails, current speculation techniques cannot roll back the store, leading to imprecise exceptions. Midgard necessitates hardware extensions to buffer speculative state in the pipeline to cover the store buffer <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b61">[62]</ref>. For each store in the store buffer, we need to record the previous mappings to the physical register file, permitting rollback to those register mappings in case of an M2P translation failure. We leave details of such mechanisms for future work.</p><p>Updating the Midgard Page Table <ref type="table">:</ref> The Midgard Page Table entries (like conventional page tables) track access and dirty bits to identify recently used or modified pages. While access bits in TLB-based systems can be updated on a memory access, modern platforms opt for setting the access bit only upon a page walk and a TLB entry fill. Midgard updates the access bit upon an LLC cache block fill and the corresponding page walk. Because access bits present approximate access recency information and are reset periodically by the OS, coarse-grained updates to the access bits may be acceptable for large memory systems <ref type="bibr" target="#b65">[66]</ref> as page evictions are infrequent. In contrast, dirty bits require precise information and are updated upon an LLC writeback and the corresponding page walk.</p><p>Accelerating V2M translation: We use per-core Virtual Lookaside Buffers (VLBs) akin to traditional TLBs to accelerate V2M translation. Like VMA Tables, VLBs offer access control and fast translation from virtual to Midgard addresses at a VMA granularity. Because real-world programs use orders of magnitude fewer VMAs than pages, it suffices to build VLBs with tens of entries rather than TLBs with thousands of entries. VLBs are significantly smaller than TLBs, but they do rely on range comparisons to determine the VMA corresponding to a virtual address. As we discuss in Section IV, care must be taken to ensure that these comparisons do not unduly slow down the VLB access.</p><p>Accelerating M2P translation: Because the majority of memory references are satisfied from the cache hierarchy, M2P translations are far less frequent than V2M translations. We nevertheless propose two optimizations to accelerate M2P translation cost when necessary.</p><p>First and foremost, the contiguous layout of the pages used to realize the Midgard Page Table permits short-circuiting of the radix tree walk. A contiguous layout enables the identification of the desired required entry in each level based purely on the Midgard address. The backside logic that performs the M2P translation can calculate the Midgard address of the required entry using the target Midgard address of the data in tandem with the base address of the last-level contiguous region. Contiguous allocation permits walk short-circuiting similar to the way in which paging structure caches are looked up using just the data's target virtual address <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>. If this entry is present in the cache hierarchies, it can be looked up directly using the calculated Midgard address, thus skipping all the other levels in the Midgard Page Table <ref type="table">.</ref> If the requested entry is not present in the cache hierarchies, the backside Midgard Page Table traversal logic looks up the level above to find the Midgard Page Table's last-level entry's physical address and fetch it from memory. For each subsequent miss, this algorithm traverses the Midgard Page Table tree towards the root, as shown in Figure <ref type="figure" target="#fig_2">4</ref>. The physical address of the root node is used when none of the levels are present in the cache hierarchies.</p><p>We also explore a second optimization, a system-wide Midgard Lookaside Buffer (MLB), to cache frequently used entries from the Midgard Page Table with a mapping, access control information and access/dirty bits. MLBs are optional and useful really only for power/area-constrained settings where the capacity of the LLC is relatively small (i.e., &lt;32MB). MLBs are similar to traditional set-associative TLBs and are accessed with a Midgard address upon an LLC miss. An MLB miss leads to a Midgard Page  streaming accesses to pages. Much as in TLBs, MLB entries also maintain status bits to implement a replacement policy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Summary of the Two-Step Address Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Other Conceptual Benefits</head><p>So far in this section, we have focused on Midgard's ability to reduce the frequency of heavyweight virtual to physical address translations in traditional systems via a level of indirection. There are, however, a number of additional benefits that are possible because of Midgard.</p><p>Mitigation of shootdown complexity: VMAs are much coarsergrained than pages, allocated/deallocated less frequently than pages, and suffer far fewer permission changes than pages. For these reasons, switching the front side of the system to a VMA-based VLB from a page-based TLB means that expensive and bug-prone OS-initiated shootdowns become far less common in Midgard. Moreover, the fact that there is no need for dedicated translation hardware on the back side for M2P translation for most reasonably sized LLCs means that OS-initiated shootdowns can be entirely elided at that level. Even for conservative scenarios, where only a small LLC is permitted and a single system-wide MLB is required, we find shootdowns to be far less expensive versus the broadcast-based mechanisms that need to be implemented to maintain coherence across multiple TLBs and MMU cache hierarchies private to individual CPUs. The relative ease with which permissions can now be changed opens up optimization opportunities in data sharing and security outside of the scope of this paper.</p><p>Flexible page/frame allocations: Midgard allows independent allocation of V2M and M2P translations. Independent allocations at the two levels enable different allocation granularities at different levels. For example, virtual memory might be allocated at 2MB chunks, while physical memory is allocated in 4KB frames. With a larger V2M translation granularity, virtual and Midgard addresses share more bits thereby increasing the L1 set-index bits known prior to translation, ameliorating a limitation of modern VIPT (and our VIMT) cache hierarchies and allowing the L1 cache to scale in capacity <ref type="bibr" target="#b44">[45]</ref>. Implementations of guard pages <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b51">[52]</ref> can also be improved with Midgard. Logically united VMAs traditionally separated by a guard page can be merged as one in a Midgard system, and the guard page can be left as unmapped in the M2P translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTING MIDGARD</head><p>In this section, we discuss implementation details of percore VLBs and mechanisms to accelerate M2P translation. Without loss of generality, we assume a system with 64bit virtual addresses (mapping 16 exabytes), 52-bit physical addresses (mapping 4 petabytes), 64-bit Midgard addresses and that the OS allocates memory at a 4KB granularity. Figure <ref type="figure" target="#fig_4">5</ref> depicts the anatomy of a Midgard-based cachecoherent 4 × 4 multicore with various system components following the same color coding as Figure <ref type="figure" target="#fig_4">5</ref> to indicate which are affected by Midgard. The specific parameters for the system size and configuration are only selected for discussion purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Virtual Lookaside Buffers (VLBs)</head><p>VLBs benefit from abundant temporal and spatial locality in memory accesses. Real-world applications use few active VMAs requiring a few VLB entries to cache them. Nevertheless, unlike TLBs which require matching a page number, VLBs perform range lookups which are fundamentally slower in hardware. Because each memory access requires a V2M translation, VLB lookups must also match the core's memory access rate.</p><p>To understand the impact of the range lookup on VLB access time, we build and analyze VLB hardware in a 22nm CMOS library. Each VLB entry has a base and bound register which require range comparisons with the virtual address. The range comparison latency is largely determined by comparison bit-width and the number of VMA entries concurrently compared. For 64-bit virtual address spaces, the base and bound registers are 52 bits as the VMA capacity is a multiple of 4KB. A 16-entry VLB has an access time of 0.47ns, consuming the entire clock cycle of our 2GHz clock. We prefer leaving greater slack between VLB access time and clock cycle time so that we can accommodate optimizations like adding more ports to the VLB or supporting higher clock rates.</p><p>We therefore design a two-level VLB hierarchy, similar to recently proposed range TLBs <ref type="bibr" target="#b27">[28]</ref>. The L1 VLB is a traditional fixed-size page-based TLB while the L2 VLB is a fully associative VMA-based Range TLB, as shown in Figure <ref type="figure">6</ref>. As the L1 VLB requires equality comparison, it can be sized to meet the core's timing constraints. The L1 VLB filters most of the translation requests while maintaining the traditional translation datapath of the core. The L2 VLB performs a range comparison but only on L1 VLB misses and can therefore tolerate a higher (up to 9 cycles <ref type="bibr" target="#b16">[17]</ref>) access latency with more entries to capture all active VMAs.</p><p>Once VLB translation succeeds, the cache hierarchy is accessed using Midgard addresses. Recall that in our implementation, Midgard addresses are 12 bits wider than physical addresses. Therefore, a cache (or directory) in the Midgard address space integrates tags that are 12 bits longer as compared to a cache in the physical address space. Assuming 64KB L1 instruction and data caches, a 1MB LLC cache per tile, 64byte blocks and full-map directories (with a copy of the L1 tags), our 16-core Midgard example system maintains tags for ∼320K blocks. With 12 bits extra per tag, the system requires an additional 480KB of SRAM to support Midgard.</p><p>Finally, the dimensions and structure of the VMA Table determine the number of memory references for its traversal on a VLB miss. Because each VMA Table entry is 24 bytes, two 64-byte cache lines can store roughly five VMA entries which can accommodate a VMA Table as a balanced three-level B-Tree <ref type="bibr" target="#b27">[28]</ref> with 125 VMA mappings. The non-leaf entries in the B-Tree contain a Midgard pointer to their children instead of an offset value. A VMA Table walk starts from the root node, compares against the base and bound registers, and follows the child pointer on a match until arriving at the final leaf entry.  In response, the coherence fabric retrieves the most recently updated copy of the desired Midgard Page Table entry, which may be in any of the cache hierarchies. In other words, the coherence fabric satisfies M2P walks in the same way that it satisfies traditional page table walks from IOMMUs <ref type="bibr" target="#b41">[42]</ref>.</p><p>The latency of the M2P walk is determined by the level of the cache hierarchy that the coherence fabric finds the desired Midgard Page Since cache hierarchies operate on Midgard addresses, Midgard Page Table must also be mapped into the Midgard address space to be cacheable. We reserve a memory chunk within the Midgard address space for the Midgard Page Table <ref type="table">.</ref> To calculate the size of this memory chunk, consider that the last level of the page table can contain 2 52 pages and can thus occupy 2 55 bytes. Because the Midgard Page Table is organized as a radix tree with degree 512, its total size must therefore be no larger than 2 56 bytes. We reserve a memory chunk of 2 56 bytes in the Midgard address space for the Midgard Page Table, and use the Midgard Base Register to mark the start of this address chunk (e.g., Base L3 M A in Figure <ref type="figure">3b</ref>).</p><p>Recall that the contiguous layout of the Midgard Page Table permits short-circuited lookups of the radix tree as well as parallel lookups of each level of the radix tree. Short-circuited lookups reduce the latency of the Midgard Page Table walk and are a uniformly useful performance optimization. Parallel lookups of the Midgard Page Table, on the other hand, can potentially reduce walk latency (especially if LLC misses for lookups of deeper levels of the Midgard Page Table are frequent) but can also amplify LLC lookup traffic. We studied the potential utility of parallel lookups for multiple levels of the Midgard Page Table and found that the average page walk latency difference is small for the system configurations that we evaluate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Midgard Lookaside Buffer (MLB)</head><p>In the uncommon case where LLCs are relatively small, we can integrate a single central MLB shared among cores in the back-side. Centralized MLBs are better than per-core MLBs in many ways. Centralized MLBs offer the same utilization benefits versus private MLBs that shared TLBs enjoy versus private TLBs; by allocating hardware resources to match the needs of the individual cores rather than statically partitioning the resources into a fixed number of entries. Centralized MLBs eliminate replication of mappings that would exist in per-core MLBs. Centralized MLBs also simplify shootdown logic in the OS, by eliding the need for invalidation broadcasts across multiple MLBs.</p><p>The centralized MLB can be sliced to improve access latency and bandwidth, similar to LLCs. As modern memory controllers use page-interleaved policies, we can divide the MLB into slices and colocate them with the memory controllers as shown in Figure <ref type="figure" target="#fig_4">5</ref>. In this manner, colocating MLB slices with the memory controller can benefit the overall memory access latency as for an MLB hit, the local memory controller can be directly accessed to retrieve the required data from memory.</p><p>Finally, MLBs can be designed to concurrently cache mappings corresponding to different page sizes, similar to modern L2 TLBs <ref type="bibr" target="#b42">[43]</ref>. Traditional L2 TLBs tolerate longer access latencies and can therefore sequentially apply multiple hash functions, one for each supported page size, until the desired entry is found or a TLB miss is detected <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b42">[43]</ref> by masking the input address according to the page size and then comparing against the tag. Since the MLB resides at the bottom of the memory hierarchy and receives low traffic, it has even more relaxed latency constraints compared to L2 TLBs. A relaxed-latency MLB is therefore ripe for support of huge pages concurrently with traditional 4K pages if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. METHODOLOGY</head><p>We implement Midgard on top of QFlex <ref type="bibr" target="#b43">[44]</ref>, a family of full-system instrumentation tools built on top of QEMU. Table <ref type="table">I</ref> shows the detailed parameters used for evaluation. We model a server containing 16 ARM Cortex-A76 <ref type="bibr" target="#b63">[64]</ref> cores operating at 2GHz clock frequency, where each core has a 64KB L1 cache and 1MB LLC tile, along with an aggregate 256GB of memory. Each core has a 48-entry L1 TLB and a 1024-entry L2 TLB that can hold translations for 4KB or 2MB pages. For Midgard, we conservatively model an L1 VLB with the same Table <ref type="table">I</ref>: System parameters for simulation on QFlex <ref type="bibr" target="#b43">[44]</ref>.</p><p>capacity as the traditional L1 TLB along with a 16-entry L2 VLB. All the cores are arranged in a 4x4 mesh architecture with four memory controllers at the corners.</p><p>As Midgard directly relies on the cache hierarchy for address translation, its performance is susceptible to the cache hierarchy capacity and latency. We evaluate cache hierarchies ranging from MB-scale SRAM caches to GB-scale DRAM caches and use AMAT to estimate the overall address translation overhead in various systems.</p><p>To approximate the impact of latency across a wide range of cache hierarchy capacities, we assume three ranges of cache hierarchy configurations modeled based on AMD's Zen2 Rome systems <ref type="bibr" target="#b62">[63]</ref>: 1) a single chiplet system with an LLC scaling from 16MB to 64MB and latencies increasing linearly from 30 to 40 cycles, 2) a multi-chiplet system with an aggregate LLC capacity ranging from 64MB to 256MB for up to four chiplets with remote chiplets providing a 50-cycle remote LLC access latency backing up the 64MB local LLC, and 3) and a single chiplet system with a 64MB LLC backed by a DRAM cache <ref type="bibr" target="#b56">[57]</ref> using HBM with capacities varying from 512MB to 16GB with an 80-cycle access latency. Our baseline Midgard system directly relies on Midgard Page Table <ref type="table" target="#tab_3">walks</ref> for performing M2P translations. We also evaluate Midgard with optional architectural support for M2P translation to filter requests for Midgard Page Table walk for systems with conservative cache sizes.</p><p>We use Average Memory Access Time (AMAT) as a metric to compare the impact of address translation on memory access time. We use full-system trace-driven simulation models to extract miss rates for cache and TLB hierarchy components, assume constant (average) latency based on LLC configuration (as described above) at various hierarchy levels, and measure memory-level parallelism <ref type="bibr" target="#b11">[12]</ref> in benchmarks to account for latency overlap.</p><p>To evaluate the full potential of Midgard, we use graph processing workloads including the GAP benchmark suite <ref type="bibr" target="#b5">[6]</ref> and Graph500 <ref type="bibr" target="#b38">[39]</ref> with highly irregular access patterns and a high reliance on address translation performance.</p><p>The GAP benchmark suite contains six different graph algorithm benchmarks: Breadth-First Search (BFS), Betweenness Centrality (BC), PageRank (PR), Single-Source Shortest Path (SSSP), Connected Components (CC), and Triangle Counting (TC). We evaluate two graph types for each of these algorithms: uniform-random (Uni) and Kronecker (Kron). Graph500 is a single benchmark with behavior similar to BFS in the GAP suite. The Kronecker graph type uses the Graph500 specifications in all benchmarks. All graphs evaluated contain 128M vertices each for 16 cores <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>In this section, we first evaluate Midgard's opportunity for future-proofing virtual memory with minimal support for the VMA abstraction. We then present Midgard's performance sensitivity to cache hierarchy capacity with a comparison to both conventional TLB hierarchies and huge pages. We finally present an evaluation of architectural support to enhance Midgard's performance when the aggregate cache hierarchy capacity is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. VMA Usage Characterization</head><p>We begin by confirming that the number of unique VMAs needed for large-scale real-world workloads, which directly dictates the number of VMA entries required by the L2 VLB, is much lower than the number of unique pages. To evaluate how the VMA scales with the dataset size and the number of threads, we pick BFS and SSSP from the GAP benchmark suite as they exhibit the worst-case performance with page-based translation. Table <ref type="table" target="#tab_8">II</ref> depicts the change in the number of VMAs used by the benchmark as we vary the dataset size from 0.2GB to 2GB. Over this range, the VMA count only increases by one, possibly from the change in algorithm going from malloc to mmap for allocating large spaces. The VMA count plateaus when scaling the dataset from 2GB to the full size of 200GB (∼ 2 25 pages) because larger datasets use larger VMAs without affecting their count.</p><p>Table <ref type="table" target="#tab_8">II</ref> also shows the required number of VMAs while increasing the number of threads in our benchmarks using the full 200GB dataset. The table shows that each additional thread adds two VMAs comprising a private stack and an adjoining guard page. Because these VMAs are private per thread, their addition does not imply an increase in the working set of the number of L2 VLB entries for active threads.</p><p>Finally, Table <ref type="table" target="#tab_9">III</ref> depicts the required L2 VLB size for benchmarks. For each benchmark, the table presents the power-of-two VLB size needed to achieve a 99.5% hit rate. In these benchmarks, &gt;90% accesses are to four VMAs including the code, stack, heap and a memory-mapped VMA storing the graph dataset. TC is the only benchmark that achieves the required hit rate with four VLB entries. All other benchmarks require more than four entries but achieve the hit rate with 8, with BFS and Graph500 being the only benchmarks that require more than eight entries. These results corroborate prior findings <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b66">[67]</ref> showing that ∼10 entries are sufficient even for modern server workloads. We therefore conservatively over-provision the L2 VLB with 16 entries in our evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Address Translation Overhead</head><p>Besides supporting VMA translations directly in hardware, a key opportunity that Midgard exploits is that a wellprovisioned cache hierarchy filters the majority of the memory accesses, requiring M2P translation for only a minuscule fraction of the memory requests. Much like in-cache address translation, a baseline Midgard system uses table walks to perform the M2P translation for memory requests that are not filtered by the cache hierarchy. In contrast, a traditional TLB-based system typically requires provisioning more resources (e.g., TLB hierarchies, MMU caches) to extend address translation reach with an increased aggregate cache hierarchy capacity.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> compares the overall address translation overhead as a fraction of AMAT between Midgard and TLB-based systems. The figure plots the geometric mean of the address translation overhead across all benchmarks. In this figure, we vary the cache hierarchy configurations (as described in V) in steps to reflect aggregate capacity in recent products -such as Intel Kabylake <ref type="bibr" target="#b25">[26]</ref>, AMD Zen2 Rome <ref type="bibr" target="#b62">[63]</ref>, and Intel Knights Landing <ref type="bibr" target="#b56">[57]</ref>.</p><p>The figure shows that address translation overhead in traditional 4KB-page systems running graph workloads with large datasets even for minimally sized 16MB LLCs is quite high at around 17%. As shown in Table <ref type="table" target="#tab_9">III</ref>, the L2 TLB misses per thousand instructions (MPKI) in 4KB-page systems is overall quite high in our graph benchmarks (with the exception of BC and TC with Kron graphs). These miss rates are also much higher than those in desktop workloads <ref type="bibr" target="#b28">[29]</ref> or scaled down (e.g., 5GB) server workloads <ref type="bibr" target="#b29">[30]</ref>.</p><p>The figure shows that Midgard achieves only 5% higher overall address translation overhead as compared to traditional 4KB-page TLB-based systems for a minimally sized LLC while virtually eliminating the silicon provisioned for per-core 1K-entry L2 TLBs (i.e., ∼16KB SRAM), obviating the need for MMU caches and hardware support for M2P translation and page walks.</p><p>As the dataset sizes for server workloads increase, modern servers are now featuring increasingly larger aggregate cache hierarchy capacities <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b62">[63]</ref>. With an increase in aggregate cache capacity, the relative TLB reach in traditional systems decreases while the average time to fetch data decreases due to higher cache hit rates. Unsurprisingly, the figure shows that the address translation overhead for traditional 4KB-page TLB-based systems exhibit an overall increase, thereby justifying the continued increase in TLB-entry counts in modern cores. The figure also shows that our workloads exhibit secondary and tertiary working set capacities at 32MB and 512MB where the traditional 4KB-page system's address translation overhead increases because of limited TLB reach to 25% and 33% of AMAT respectively. In contrast, Midgard's address translation overhead drops dramatically at both secondary and tertiary working set transitions in the graph thanks to the corresponding fraction of memory requests filtered in the cache hierarchy. Table III also shows the amount of M2P traffic filtered by 32MB and 512MB LLCs for the two working sets. The table shows that 32MB already filters over 90% of the M2P traffic in the majority of benchmarks by serving data directly in the Midgard namespace in the cache hierarchy. With a 512MB LLC, all benchmarks have over 90% of traffic filtered with benchmarks using the Kron graph (virtually) eliminating all translation traffic due to enhanced locality. As a result, with Midgard the resulting address translation overhead (Figure <ref type="figure" target="#fig_6">7</ref>) drops to below 10% at 32MB, and below 2% at 512MB of LLC.</p><p>Next, we provide a comparison of average page table walk latency between a 4KB-page TLB-based system and Midgard. Because Midgard fetches the leaf page table entries from the caches during a page walk in the common case, on average it requires only 1.2 accesses per walk to an LLC tile which is (∼30 cycles) away (Table <ref type="table" target="#tab_9">III</ref>). In contrast, TLB-based systems require four lookups per walk. While these lookups are performed in the cache hierarchy, they typically miss in L1 requiring one or more LLC accesses. As such, Midgard achieves up to 40% reduction in the walk latency as compared to TLB-based systems. BC stands as the outlier with high locality in the four lookups in L1 resulting in a TLB-based average page walk latency being lower than Midgard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Huge Pages</head><p>To evaluate future-proofing virtual memory with Midgard, we also compare Midgard's performance against an optimistic lower bound for address translation overhead using  huge pages. Huge pages provide translation at a larger page granularity (e.g., 2MB or 1GB) and thereby enhance TLB reach and reduce the overall address translation overhead. Prior work <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b66">[67]</ref> indicates that creating and maintaining huge pages throughout program execution requires costly memory defragmentation and frequent TLB shootdowns <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Huge pages may also inadvertently cause a performance bottleneck -e.g., when migrating pages in a NUMA system <ref type="bibr" target="#b45">[46]</ref>. To evaluate a lower bound, we optimistically assume zero-cost memory defragmentation and TLB shootdown, thus allowing ideal 2MB pages for address translation that do not require migration. We also assume the same number of L1 and L2 2MB TLB entries per core as the 4KB-page system. Figure <ref type="figure" target="#fig_6">7</ref> also depicts a comparison of address translation overhead between Midgard and ideal 2MB-page TLB-based systems. Not surprisingly, a 2MB-page system dramatically outperforms both TLB-based 4KB-page and Midgard systems for a minimally sized 16MB LLC because of the 500x increase in TLB reach. Much like 4KB-page systems, the TLB-based huge page system also exhibits an increase in address translation overhead with an increase in aggregate LLC capacity with a near doubling in overall address translation overhead from 16MB to 32MB cache capacity.</p><p>In contrast to 4KB-page systems which exhibit a drastic drop in TLB reach from 256MB to 512MB caches with the In comparison to huge pages, Midgard's performance continuously improves with cache hierarchy capacity until address translation overhead is virtually eliminated. Midgard reaches within 2x of huge pages' performance with 32MB cache capacity, breaks even at 256MB which is the aggregate SRAMbased LLC capacity in AMD's Zen2 Rome <ref type="bibr" target="#b62">[63]</ref> products, and drops below 1% beyond 1GB of cache capacity. While Midgard is compatible with and can benefit from huge pages, Midgard does not require huge page support to provide adequate performance as traditional systems do. Overall, Midgard provides high-performance address translation for large memory servers without relying on larger page sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Architectural Support for M2P Translation</head><p>While we expect future server systems to continue integrating larger cache hierarchies, our memory-access intensive workloads exhibit a non-negligible degree of sensitivity to address translation overhead (i.e., &gt; 5%) with aggregate cache hierarchy capacities of less than 256MB. In this subsection, we evaluate architectural support for M2P translation using MLBs. While MLBs (like TLBs) complicate overall system design, the higher complexity may be justified by the improvements in address translation overhead for a given range of cache hierarchy capacity.</p><p>We first analyze the required aggregate MLB size (i.e., the total number of MLB entries across the four memory controllers) for the GAP benchmarks for a minimally sized LLC at 16MB. Figure <ref type="figure" target="#fig_7">8</ref> illustrates the MPKI (i.e., the number of M2P translations per kilo instruction requiring a page walk) as a function of log scale of MLB size. The figure shows that while the MLB size requirements across the benchmarks largely vary, there are approximately two windows of sizes that exhibit M2P translation working sets. The primary working set on average appears to be roughly around 64 aggregate MLB entries with many benchmarks exhibiting a step function in MPKI. The latter would provision only four MLB entries per thread indicating that the M2P translations are the results of spatial streams to 4KB page frames in memory. Beyond the first window, the second and final working set of M2P translations is around 128K MLB entries which is prohibitive, thereby suggesting that practical MLB designs would only require a few entries per memory controller.</p><p>Figure <ref type="figure">9</ref> illustrates the address translation overhead for cache hierarchies of upto 512MB while varying the number of aggregate MLB entries from 0 to 128 averaged over all the GAP benchmarks. Midgard in the figure refers to the baseline system without an MLB. The figure corroborates the results that on average 64 MLB entries is the proper sweet spot for 16MB caches. Comparing the figure with Figure <ref type="figure" target="#fig_6">7</ref>, we see that for a 16MB LLC, Midgard can break even in address translation overhead with traditional 4KB-page systems with only 32 overall MLB entries (i.e., 8 entry per memory controller). In contrast, practical provisioning for MLB will not help Midgard break even with an ideal huge page system for a minimally sized LLC. The figure also shows that with only 32 and 64 MLB entries, Midgard can virtually eliminate address translation overhead in systems with 256MB and 128MB aggregate LLC respectively. Moreover, with 64 MLB entries, Midgard outperforms huge pages for LLC capacity equal or greater than 32MB. Finally, for LLC capacity of 512MB or larger, there is very little benefit from architectural support for M2P translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>While there is a large body of work on virtual memory <ref type="bibr" target="#b8">[9]</ref>, we only list a few of them because of space constraints.</p><p>Virtual caches: Proposals for virtual caches date back to the '80s <ref type="bibr" target="#b15">[16]</ref>, with recent papers proposing virtual caches in the context of GPUs <ref type="bibr" target="#b67">[68]</ref>. Ceckleov et al. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> summarize a variety of tradeoffs in the context of virtual caches. Single Address Space Operating System <ref type="bibr" target="#b31">[32]</ref> eases virtual cache implementations but requires significant software modifications.</p><p>Intermediate address spaces: Wood et al. <ref type="bibr" target="#b64">[65]</ref> propose a global virtual address space used to address the caches. However, translations from virtual to global virtual address space are done using fixed-size program segments, which now have been replaced with pages. Zhang et al. <ref type="bibr" target="#b68">[69]</ref> propose an intermediate address space while requiring virtual addresses to be translated at a 256MB granularity. While this works well for GB-scale memory, it does not scale for TB-scale memory. Hajinazar et al. <ref type="bibr" target="#b17">[18]</ref> propose an intermediate address space containing fixed-size virtual blocks, which are then used by the applications. While these virtual blocks are similar to the VMAs in Midgard, using them requires the significant application and toolchain modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Huge pages and Ranges:</head><p>There is a large body of work targeting adoption and implementation of huge pages <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b59">[60]</ref> to reduce the address translation overheads. Midgard is compatible with most proposals on huge page integration and would benefit in M2P translation performance. Recent papers <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b66">[67]</ref> have also introduced the notion of ranges, which are contiguous data regions in the virtual address space mapped to contiguous regions in the physical address space. These optimizations are compatible with Midgard M2P translation as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Despite decades of research on building complex TLB and MMU cache hardware as well as hardware/OS support for huge pages, address translation has remained a vexing performance problem for high-performance systems. As computer systems designers integrate cache hierarchies with higher capacity, the cost of address translation has continued to surge.</p><p>This paper proposed, realized, and evaluated a proof-ofconcept design of Midgard, an intermediate namespace for all data in the coherence domain and cache hierarchy, in order to reduce address translation overheads and future-proof the VM abstraction. Midgard decouples address translation requirements into core-side access control at the granularity of VMAs, and memory-side translation at the granularity of pages for efficient capacity management. Midgard's decoupling enables lightweight core-side virtual to Midgard address translation, using leaner hardware support than traditional TLBs, and filtering of costlier Midgard to physical address translations to only situations where there are LLC misses. As process vendors increase LLC capacities to fit the primary, secondary, and tertiary working sets of modern workloads, Midgard to physical address translation becomes infrequent.</p><p>We used AMAT analysis to show that Midgard achieves only 5% higher address translation overhead as compared to traditional TLB hierarchies for 4KB pages when using a 16MB aggregate LLC. Midgard also breaks even with traditional TLB hierarchies for 2MB pages when using a 256MB aggregate LLC. For cache hierarchies with higher capacity, Midgard's address translation overhead drops to near zero as secondary and tertiary data working sets fit in the LLC, while traditional TLBs suffer even higher degrees of address translation overhead.</p><p>This paper is the first of several steps needed to demonstrate a fully working system with Midgard. We focused on a proofof-concept software-modeled prototype of key architectural components. Future work will address the wide spectrum of topics needed to realize Midgard in real systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Various arrangements of address spaces. The legend refers to various address spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Mapping from the virtual to the Midgard address space in units of VMAs and mapping from Migard address spaces to physical address spaces in units of pages. The cache hierarchy is placed in the Midgard address space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Logical flow of a memory reference from V2M translation through M2P translation. This diagram assumes the use of the optional MLB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4 summarizes the overall flow of Midgard for the case where all hardware and software features -even optional ones like the MLB -are included. The sequence of steps shows V2M translation, which begins with a VLB lookup and falls back on a lookup of the VMATable on a VLB miss. The VMA Table may in turn be absent from the cache hierarchy, in which case an M2P translation for the VMA Table is performed. Once that is satisfied (or in the event of a VLB hit or VMA Table hit in the cache hierarchy), the memory reference is replayed and data lookup proceeds. Only if lookup misses in all cache hierarchies does an M2P translation begin. At this point, the MLB can be consulted. In the event of an MLB miss, a walk commences for the Midgard Page Table. This walk can be optimized via short-circuited paging structure cache-style lookups of the LLC to accelerate the Midgard Page Table walk. We assume that traversals of the VMA Table and the Midgard Page Table are managed entirely in hardware without OS intervention, mirroring hardware page table walks in traditional systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The anatomy of a multicore system with Midgard. Dashed lines indicate that MLB is optional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Percent AMAT spent in address translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Sensitivity to MLB size for a 16MB LLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table is maintained in dedicated Midgard Page Table Base Registers at the memory controllers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table walk .</head><label>walk</label><figDesc>Because the LLC already absorbs most of the temporal locality, MLB lookups are expected to be primarily spatial in nature and fundamentally different from those of traditional TLBs, requiring typically only a few entries per thread for</figDesc><table><row><cell>Start</cell><cell></cell><cell></cell><cell>Cache lookup</cell><cell>Hit</cell><cell></cell><cell>End</cell></row><row><cell></cell><cell></cell><cell></cell><cell>for data</cell><cell></cell><cell>VMA</cell><cell>Data</cell></row><row><cell>lookup VLB</cell><cell cols="2">Hit</cell><cell>MLB Miss</cell><cell>Hit</cell><cell cols="2">from memory Get data/VMA</cell></row><row><cell cols="2">Miss</cell><cell></cell><cell>lookup</cell><cell></cell><cell></cell></row><row><cell>Walk the VMA Table</cell><cell></cell><cell></cell><cell>Walk the Miss</cell><cell></cell><cell cols="2">Get leaf PTE from memory</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Midgard Page Table</cell><cell></cell><cell>Hit</cell></row><row><cell>Cache lookup for VMA Table</cell><cell>Miss</cell><cell>Hit</cell><cell>Cache lookup for leaf PTE</cell><cell>Hit Miss</cell><cell cols="2">Cache lookup for leaf-1 PTE Recurse on miss</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table on a VLB miss. The VMA Table may in turn be absent from the cache hierarchy, in which case an M2P translation for the VMA Table is performed. Once that is satisfied (or in the event of a VLB hit or VMA Table hit in the cache hierarchy), the memory reference is replayed and data lookup proceeds. Only if lookup misses in all cache hierarchies does an M2P translation begin. At this point, the MLB can be consulted. In the event of an MLB miss, a walk commences for the Midgard Page Table. This walk can be optimized via short-circuited paging structure cache-style lookups of the LLC to accelerate the Midgard Page Table walk. We assume that traversals of the VMA Table and the Midgard Page Table are managed entirely in hardware without OS intervention, mirroring hardware page table walks in traditional systems.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc>The dimensions and structure of the Midgard Page Table determine the number of memory references for its traversal. We use a single traditional radix page table with a degree of 512 to store all the Midgard to physical address mappings at the granularity of pages. As our Midgard address space is 64 bits, we need a 6-level radix table. Traversing the Midgard Page Table therefore requires two sequential memory references beyond what is needed for four-level page tables in traditional VM. The contiguous layout of the Midgard Page Table enables short-circuited walks and that effectively hides the latency of the deeper Midgard Page Table. Midgard Page Table entries must be cacheable for fast lookup. On M2P translation, the back-side logic responsible for walking the Midgard Page Table generates a request to the cache hierarchy. LLC slices are closer to the back-side walker logic than L1 or intermediate cache levels. Therefore, memory references from the back-side walker are routed to the LLC.</figDesc><table><row><cell>Input</cell><cell>Miss Hit</cell><cell>Table</cell><cell>Walk</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 6: Two-level VLB design.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>B. Walking the Midgard Page</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table entry in. If, for example, the OS has recently accessed the Midgard Page Table entry, it may be in the L1 or intermediate levels of cache. But since Midgard Page Table walks are more frequent than OS changes to the Midgard Page Table entry, lookups for the Midgard Page Table entry usually hit in the LLC.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table II :</head><label>II</label><figDesc>VMA count against dataset size and thread count.</figDesc><table><row><cell></cell><cell cols="4">Dataset Size (GB)</cell><cell></cell><cell></cell><cell cols="2">Thread Count</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell><cell>24</cell><cell>32</cell></row><row><cell>BFS SSSP</cell><cell>51</cell><cell>51</cell><cell>52</cell><cell>52</cell><cell>52</cell><cell>60</cell><cell>68</cell><cell>76</cell><cell>84</cell><cell>108</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table III :</head><label>III</label><figDesc>Analysis of miss rate (MPKI) in traditional 4KB-page L2 TLBs, L2 VLB size for 99.5%+ hit rate, M2P traffic filtered, and page walk latency for traditional 4KB-page TLB-based and Midgard systems. Graph500 only uses the Kronecker graph type.</figDesc><table><row><cell></cell><cell cols="2">Traditional</cell><cell>Required</cell><cell cols="3">% Traffic filtered by LLC</cell><cell></cell><cell></cell><cell cols="2">Avg. page walk cycles</cell><cell></cell></row><row><cell>Benchmark</cell><cell cols="2">L2 TLB MPKI</cell><cell>L2 VLB</cell><cell>Uni</cell><cell></cell><cell cols="2">Kron</cell><cell>Uni</cell><cell></cell><cell>Kron</cell><cell></cell></row><row><cell></cell><cell>Uni</cell><cell>Kron</cell><cell>capacity</cell><cell>32MB</cell><cell>512MB</cell><cell>32MB</cell><cell>512MB</cell><cell>Traditional</cell><cell>Midgard</cell><cell>Traditional</cell><cell>Midgard</cell></row><row><cell>BFS</cell><cell>23</cell><cell>29</cell><cell>16</cell><cell>95</cell><cell>99</cell><cell>95</cell><cell>100</cell><cell>51</cell><cell>31</cell><cell>30</cell><cell>30</cell></row><row><cell>BC</cell><cell>&lt; 1</cell><cell>&lt; 1</cell><cell>8</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>20</cell><cell>35</cell><cell>20</cell><cell>35</cell></row><row><cell>PR</cell><cell>71</cell><cell>68</cell><cell>8</cell><cell>85</cell><cell>100</cell><cell>89</cell><cell>100</cell><cell>45</cell><cell>30</cell><cell>42</cell><cell>30</cell></row><row><cell></cell><cell>74</cell><cell>70</cell><cell>8</cell><cell>87</cell><cell>98</cell><cell>90</cell><cell>100</cell><cell>47</cell><cell>31</cell><cell>38</cell><cell>30</cell></row><row><cell>CC</cell><cell>23</cell><cell>18</cell><cell>8</cell><cell>98</cell><cell>100</cell><cell>97</cell><cell>100</cell><cell>39</cell><cell>34</cell><cell>44</cell><cell>31</cell></row><row><cell>TC</cell><cell>62</cell><cell>&lt; 1</cell><cell>4</cell><cell>80</cell><cell>92</cell><cell>100</cell><cell>100</cell><cell>48</cell><cell>30</cell><cell>48</cell><cell>30</cell></row><row><cell>Graph500</cell><cell>-</cell><cell>27</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>96</cell><cell>100</cell><cell>-</cell><cell>-</cell><cell>32</cell><cell>30</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Mark Silberstein, Mario Drumond, Arash Pourhabibi, Mark Sutherland, Ognjen Glamocanin, Yuanlong Li, Ahmet Yuzuguler, and Shanqing Lin for their feedback and support. This work was partially supported by FNS projects "Hardware/Software Co-Design for In-Memory Services" (200020B 188696) and "Memory-Centric Server Architecture for Datacenters" (200021 165749), and an IBM PhD Fellowship (612341).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing the TLB Shootdown Algorithm with Page Access Tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Amit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 USENIX Annual Technical Conference</title>
				<meeting>the 2017 USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t shoot down TLB shootdowns</title>
		<author>
			<persName><forename type="first">N</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 EuroSys Conference</title>
				<meeting>the 2020 EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translation caching: skip, don&apos;t walk (the page table)</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 37th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient virtual memory for big memory servers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 40th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Locality Exists in Graph Processing: Workload Characterization on an Ivy Bridge Server</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Symposium on Workload Characterization (IISWC)</title>
				<meeting>the 2015 IEEE International Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The GAP Benchmark Suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno>abs/1508.03619</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating two-dimensional page walks for virtualized systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Serebrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Spadini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIII)</title>
				<meeting>the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIII)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-reach memory management unit caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="383" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Architectural and Operating System Support for Virtual Memory, ser. Synthesis Lectures on Computer Architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtual-address caches. Part 1: problems and solutions in uniprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cekleov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="64" to="71" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Virtual-address caches.2. Multiprocessor issues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cekleov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="69" to="74" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microarchitecture Optimizations for Exploiting Memory-Level Parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 31st International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="76" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Better I/O through byte-addressable, persistent memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Condit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coetzee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM Symposium on Operating Systems Principles (SOSP</title>
				<meeting>the 22nd ACM Symposium on Operating Systems Principles (SOSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="133" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient Address Translation for Architectures with Multiple Page Sizes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXII</title>
				<meeting>the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXII</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="435" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Is SC + ILP=RC?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gniady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 26th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coherency for Multiprocessor Virtual Address Caches</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-I)</title>
				<meeting>the 1st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-I)</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Translation Leak-aside Buffer: Defeating Cache Side-channel Protections with TLB Attacks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giuffrida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th USENIX Security Symposium</title>
				<meeting>the 27th USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="955" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual Memory Framework</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hajinazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Appavoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 47th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1050" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Devirtualizing Memory in Heterogeneous Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXIII)</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXIII)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="637" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A study of virtual memory usage and implications for large memory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hornyack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gribble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Interactions of NVM/FLASH with Operating Systems and Workloads</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Guard Pages</title>
		<ptr target="https://patents.google.com/patent/US20080034179A1/en" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>IBM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Intel 5-level Paging and 5-level EPT</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://software.intel.com/sites/default/files/managed/2b/80/5-levelpagingwhitepaper.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">EC2 High Memory Update: New 18 TB and 24 TB Instances</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aws</forename></persName>
		</author>
		<ptr target="https://aws.amazon.com/blogs/aws/ec2-high-memory-update-new-18-tb-and-24-tb-instances/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Die-stacked DRAM caches for servers: hit ratio, latency, or bandwidth? have it all with footprint cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 40th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="404" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Intel Kabylake</title>
		<author>
			<persName><surname>Kabylake</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/kabylake" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enabling interposerbased disintegration of multi-core processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 48th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="546" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Redundant memory mappings for fast access to large memories</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 42nd International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="66" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Energy-efficient address translation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd IEEE Symposium on High-Performance Computer Architecture</title>
				<meeting>the 22nd IEEE Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="631" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance analysis of the memory management unit under scale-out workloads</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">S</forename><surname>Unsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE International Symposium on Workload Characterization (IISWC)</title>
				<meeting>the 2014 IEEE International Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sharing, Protection, and Compatibility for Reconfigurable Fabric with AmorphOS</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khawaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Landgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schkufza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Symposium on Operating System Design and Implementation (OSDI)</title>
				<meeting>the 13th Symposium on Operating System Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="107" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Architectural Support for Single Address Space Operating Systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Koldinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V)</title>
				<meeting>the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V)</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Do OS abstractions make sense on FPGAs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Korolija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Symposium on Operating System Design and Implementation (OSDI)</title>
				<meeting>the 14th Symposium on Operating System Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="991" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LATR: Lazy Translation Coherence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXIII)</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXIII)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="651" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coordinated and Efficient Huge Page Management with Ingens</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Symposium on Operating System Design and Implementation</title>
				<meeting>the 12th Symposium on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="705" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">COATCheck: Verifying Memory Ordering at the Hardware-OS Interface</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXI)</title>
				<meeting>the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXI)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="233" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ecotlb: Eventually consistent tlbs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prefetched Address Translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1023" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Introducing the graph 500</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Barrett</surname></persName>
		</author>
		<ptr target="http://www.richardmurphy.net/archive/cug-may2010.pdf" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Practical, Transparent Operating System Support for Superpages</title>
		<author>
			<persName><forename type="first">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Symposium on Operating System Design and Implementation (OSDI)</title>
				<meeting>the 5th Symposium on Operating System Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scaleout NUMA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIX)</title>
				<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIX)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Border control: sandboxing accelerators</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 48th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="470" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prediction-based superpage-friendly TLB designs</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Papadopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st IEEE Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>the 21st IEEE Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="210" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Qflex</title>
		<ptr target="https://qflex.epfl.ch" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>Parallel Systems Architecture Lab (PARSA) EPFL</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SEESAW: Using Superpages to Improve VIPT Caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parasar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 45th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Perforated Page: Supporting Fragmented Memory Allocation for Large Pages</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 47th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="913" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CoLT: Coalesced Large-Reach TLBs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 45th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="258" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large pages and lightweight memory management in virtualized environments: can you have it both ways?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 48th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Architectural support for address translation on GPUs: designing memory management units for CPU/GPUs with unified address spaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pichai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIX)</title>
				<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIX)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="743" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Near-Memory Address Translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Parallel Architecture and Compilation Techniques (PACT</title>
				<meeting>the 26th International Conference on Parallel Architecture and Compilation Techniques (PACT</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="303" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SPARTA: A divide and conquer approach to address translation for accelerators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A S</forename><surname>Kohroudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2001.07045" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2001">2001.07045. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Guard Pages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Plakosh</surname></persName>
		</author>
		<ptr target="https://us-cert.cisa.gov/bsi/articles/knowledge/coding-practices/guard-pages" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Memory Scaling is Dead, Long Live Memory Scaling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<ptr target="https://hps.ece.utexas.edu/yale75/qureshislides.pdf" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Concurrent Support of Multiple Page Sizes on a Skewed Associative TLB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="924" to="927" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Elastic Cuckoo Page Tables: Rethinking Virtual Memory Translation for Parallelism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kokolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXV)</title>
				<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1093" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Announcing the general availability of 6 and 12 TB VMs for SAP HANA instances on Google Cloud Platform</title>
		<author>
			<persName><forename type="first">Snehanshu</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/blog/products/sap-google-cloud/announcing-the-general-availability-of-6-and-12tb-vms-for-sap-hana-instances-on-gcp" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Knights Landing: Second-Generation Intel Xeon Phi Product</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sodani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gramunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chinthamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hutsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Increasing TLB Reach Using Superpages Backed by Shadow Memory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Symposium on Computer Architecture (ISCA</title>
				<meeting>the 25th International Symposium on Computer Architecture (ISCA</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Surpassing the TLB Performance of Superpages with Less Operating System Support</title>
		<author>
			<persName><forename type="first">M</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI</title>
				<meeting>the 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tradeoffs in Supporting Two Page Sizes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Symposium on Computer Architecture (ISCA</title>
				<meeting>the 19th International Symposium on Computer Architecture (ISCA</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fat Caches for Scale-Out Servers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mechanisms for store-wait-free multiprocessors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 34th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="266" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">AMD Zen2</title>
		<author>
			<persName><surname>Wikichip</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/amd/microarchitectures/zen2" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">ARM Cortex A76</title>
		<author>
			<persName><surname>Wikichip</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/armholdings/microarchitectures/cortex-a76" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An In-Cache Address Translation Mechanism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 13th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Supporting Reference and Dirty Bits in SPUR&apos;s Virtual Address Cache</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 16th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="122" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Translation ranger: operating system support for contiguity-aware TLBs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 46th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="698" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Filtering Translation Bandwidth with Virtual Caching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lowe-Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXIII)</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXIII)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="113" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Enigma: architectural and operating system support for reducing the impact of address translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Speight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajamony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Supercomputing</title>
				<meeting>the 24th ACM International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
