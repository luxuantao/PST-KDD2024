<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards multi-modal causability with Graph Neural Networks enabling information fusion for explainable AI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-27">27 January 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
							<email>andreas.holzinger@medunigraz.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Medical University Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Alberta Machine Intelligence Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernd</forename><surname>Malle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Medical University Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Saranti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Medical University Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bastian</forename><surname>Pfeifer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Medical University Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Medical University Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards multi-modal causability with Graph Neural Networks enabling information fusion for explainable AI</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-27">27 January 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">7A6423A72CE107AADC593600F629EB54</idno>
					<idno type="DOI">10.1016/j.inffus.2021.01.008</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Information fusion Explainable AI xAI Graph Neural Networks Multi-modal causability Knowledge graphs Counterfactuals</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is ''How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?''. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability -not to confuse with causality -is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human-AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Current medical AI is very successful in certain tasks due to the great advances in statistical machine learning. One of the most cited examples is the work of <ref type="bibr" target="#b0">Esteva et al. (2017)</ref>  <ref type="bibr" target="#b0">[1]</ref>, where classification of skin cancer via convolutional neural networks achieved a performance on par with human experts, demonstrating that AI is capable of classifying skin cancer with a level of competence comparable to dermatologists. Another success story is the work of De Fauw et al. (2018) <ref type="bibr" target="#b1">[2]</ref>: they achieved human expert performance on the classification of optical coherence tomography (OCT) scans to detect choroidal neovascularization (CNV) which is commonly known as age-related macular degeneration (AMD), the major cause of blindness. A very recent work of <ref type="bibr" target="#b2">Faust et al. (2019)</ref>  <ref type="bibr" target="#b2">[3]</ref>, on mapping brain tumour histomorphologies shows that deep learning provides a highly dynamic data-driven approach that can help to automate traditionally laborious and qualitatively difficult image-based analyses in pathology. Moreover, they showed that machine-engineered features correlate with salient human-derived morphological constructs. This is an important step in achieving an overlap between human and AI, helping in eliminating bias and improving the accountability for future AI assisted medicine. They also emphasized that the currently best performing methods, apart from requiring a lot of top-quality data, are highly opaque, so even narrow classification tasks lack interpretability as well as user-defined feature selection. This opaqueness (commonly called ''Black-Box'' behaviour) of statistical machine learning models shown in the best-practice examples mentioned above is inherent in model free stochastic approaches such as deep neural network machine learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The paradigms that underlie these problems fall into the growing field of explainable AI (xAI). Here, methods for the implementation of transparency and explainability of such Black-Box methods are developed, motivated often by legal issues <ref type="bibr" target="#b5">[6]</ref>. However, in the medical domain we are facing another complex challenge, which lies in the integration, fusion and mapping of various distributed and heterogeneous https://doi.org/10.1016/j.inffus.2021.01.008 Received 21 October 2020; Received in revised form 20 January 2021; Accepted 22 January 2021 data in arbitrarily high dimensional spaces in a multi-modal (MM) manner, i.e. we must always consider that diverse data and different features contribute to a result <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. A good example is cancer research <ref type="bibr" target="#b8">[9]</ref>, or radiomics where multi-faceted data from diverse sources contribute to a decision <ref type="bibr" target="#b9">[10]</ref>.</p><p>Therefore, Arrieta et al. (2020) <ref type="bibr" target="#b10">[11]</ref> emphasize, that both explainability and information fusion are important, most importantly as different users (laymen, physicians, computer scientists, . . . ) need different explanations. Via intra-modal feature extraction and MM embedding, a low-dimensional representation space comprising relevant data to assist the medical decision making process is necessary. We describe the challenge of constructing such a MM embedding space in Section 2.</p><p>In certain domains, especially in the medical field, there is a need for causability, introduced by Holzinger et al. (2019) <ref type="bibr" target="#b11">[12]</ref>. Causability is not a synonym for causality in the sense of Judea Pearl <ref type="bibr" target="#b12">[13]</ref>; the term causa-bil-ity was introduced in reference to usa-bil-ity. Whilst explainability (represented by the field of xAI) is about the technical implementation of transparency and traceability in AI approaches, causability is about measuring and ensuring the quality of explanations <ref type="bibr" target="#b13">[14]</ref>. That means the measurable extent to which an (xAI) explanation to a (human) user achieves a specified level of causal understanding, measured with effectiveness, efficiency and satisfaction in a specified context of use -similar as usability <ref type="bibr" target="#b14">[15]</ref>. To promote a better understanding, we summarize the definitions here:</p><p>• (A) Explainability ∶= technically highlights decision relevant parts of machine representations and machine models i.e., parts which contributed to model accuracy in training, or to a specific prediction for a particular observation. Here the xAI community has already developed a variety of successful methods. Explainability does not refer to a human model. • (B) Causality ∶= the relationship between cause and effect in the sense of Judea Pearl <ref type="bibr" target="#b12">[13]</ref>. • (C) Causability ∶= the measurable extent to which an explanation -resulting from (A) -to a human expert achieves a specified level of causal understanding. This can be measured e.g. with the System Causability Scale <ref type="bibr" target="#b13">[14]</ref>. Causability refers to a human model.</p><p>Understanding can be ensured when we can map explainability (the "technical explanation") with causability ("the human understanding"). Successful mapping between explainability and causability requires new human-AI interfaces which allow domain experts to interactively ask questions and counterfactual questions to gain deep insight into the underlying independent and disentangled explanatory factors of a result, adapted to the needs of the respective end user <ref type="bibr" target="#b15">[16]</ref>.</p><p>In an ideal world both human and AI statements would be identical and congruent with the ground truth, which is defined for both humans and AI equally <ref type="bibr" target="#b13">[14]</ref>. However, in the medical domain we are in the real world, thus we face two problems: (i) ground truth cannot always be well defined, especially when making a medical diagnosis; and (ii) human (scientific) models are often based on causality in the sense of Judea Pearl as ultimate aim for understanding the underlying explanatory mechanisms.</p><p>While correlation is accepted as a basis for decisions, it can only be an intermediate step, due to the importance of validity in medicine <ref type="bibr" target="#b16">[17]</ref>. Moreover, it is necessary (a) to build human trust, and (b) to build a kind of ''AI experience'' among the medical professionals, according to <ref type="bibr" target="#b17">(Cabitza, Campagner &amp; Balsano, 2020)</ref>  <ref type="bibr" target="#b17">[18]</ref>.</p><p>Currently there is a huge debate in the AI community about the avoidance of bias and how to ensure fairness in AI decisions <ref type="bibr" target="#b18">[19]</ref>. Bias is a core topic in causality, and causability is a possible measure. Validation of causal effects under determined causal structures is especially needed if and when such effects are estimated in limited settings. In the medical domain a good use case for such a limited setting are randomized controlled trials. Such trials permit to test for causal hypotheses, because a randomization-by-design is guaranteed, even with limited domain knowledge. A particular problem of generalizability has been described by <ref type="bibr" target="#b19">(Bareinboim &amp; Pearl, 2013)</ref>  <ref type="bibr" target="#b19">[20]</ref>, which is called transportability and can be seen as a ''data fusion framework'' for the external validation of intervention models and counterfactual queries. Transportability enables to transfer causal effects learned in experimental studies to a new setting, in which only observational studies can be conducted. Transportable models can be integrated into clinical guidelines to augment domain experts with ''action-savvy'' predictions, in pursuit of better precision medicine <ref type="bibr" target="#b20">[21]</ref>.</p><p>The field of xAI generally has huge potential to contribute towards a better understanding of diseases, which can furthermore lead to more accurate diagnoses, more rational disease prevention strategies, better treatment selection, and the development of novel therapies. Moreover, a better understanding of diseases can contribute to the long term goal of predictive preventive personalized participatory (P4) precision medicine which genuinely seeks to redefine the understanding of disease onset and progression, treatment response, and health outcomes through the most precise measurement of molecular, genetic, environmental, and behavioural individual factors that contribute to health and disease. In this case it is imperative that AI decisions are fully re-traceable across all modalities involved, giving the medical domain expert the power to (i) understand, (ii) confirm or (iii) overrule them (see Fig. <ref type="figure" target="#fig_1">1</ref>). Whatever future human-AI interfaces may look like, they must enable a medical expert to understand the causal pathways to produce meaningful counterfactuals <ref type="bibr" target="#b21">[22]</ref>. Here, the use of graphs and graph representation learning can be beneficial and therefore we describe some possible approaches in Section 3.</p><p>In the following sections we propose three core challenges and a series of experiments carried out in succession as part of an overall pipeline to establish a multi-modal, decentralized, explainable machine learning infrastructure for the medical domain. Each stage in the pipeline has a different focus and can theoretically work on its own, meaning experiments could be conducted in isolation. However, building upon and mutually extending their individual advancements, our integrated approach will yield its maximum benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Challenge I: Constructing a multi-modal (MM) embedding space</head><p>Learning on fused data from different sources and modalities can substantially outperform traditional methods on just one type of data structure. Jointly learning on input data of different modalities is a standard routine <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, with a constant stream of innovation in recent years <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>The fundamental challenge in fusing disparate modalities lies in bridging the semantic gap between them and handling potential disagreements thereof -this problem is identified within the literature as aligning local geometries of subjects across feature spaces <ref type="bibr" target="#b23">[24]</ref>. In earlier works multi-modality was understood to be limited to different image taking techniques (e.g. CT, MRI, PET, etc.) or different resolutions. Simple methods just concatenate feature vectors to fuse different domains, not considering varying distance or neighbourhood metrics across domain boundaries.</p><p>Wei et al. (2019) <ref type="bibr" target="#b26">[27]</ref> proposed a MM Graph Convolutional Network in order to consider the different modalities contained in videos -visual, acoustic, textual -by constructing a user-item interaction graph within each modality, thus learning different user preference functions which eventually fuse into a unified representation in a specifically designed combination layer. Dourado, Tabbone &amp; Torres (2019) <ref type="bibr" target="#b27">[28]</ref> deal with multi-source and MM features in the problem domain of information retrieval and rank aggregation, where they use graphs to encapsulate and correlate ranks as well as graph embeddings to reduce these graphs to vectors which are eventually fused into a response. Mai, Hu &amp; Xing (2020) <ref type="bibr" target="#b28">[29]</ref> observe a modality gap in cross-modal fusion techniques due to a failure to learn joint embedding spaces. They propose an encoder-decoder architecture translating  each source modality into a modality-invariant embedding space via adversarial training, including a Modality Attention Network since not all modalities contribute equally. In multi-similarity metric fusion for semi-supervised Label Propagation, <ref type="bibr" target="#b29">[30]</ref> propose to extend the Flexible Manifold Embedding technique to consider both label and feature spaces, constructing a label-based Correlation Graph to interact with other similarity graphs. Vivar et al. <ref type="bibr" target="#b25">[26]</ref> utilize MM Graph Attention Networks to deal with missing features, stemming from different sources of medical data.</p><p>In summary, it is interesting to note that although multi-modality is an important issue across the literature, and data/graph fusion is a method of choice for many, it seems that the concept is understood in different ways by researchers in different areas applying it to a varying range of objectives, from label propagation, to recommendations, to fusing data of different dimensionality, or compensating for missing features. Therefore, it is important to define clear goals, anticipated challenges and potential methods to overcome them, which we contribute to in this work.</p><p>It is important to initially capture each source domain's intrinsic ontology -e.g. relations, hierarchies, partitions in a graph, analogies, co-occurrence, and other forms of semantics within texts, or pathways on an *omics level. Concurrently, it is necessary to define cross-links -interactions and correspondences -between entities of different domains. For instance, a superpixel (which has perceptual meaning) in an image may correspond to an entry in a controlled vocabulary, or a mutation within a gene causes a different behaviour on the protein level. For a lack of pre-defined cross-domain semantics most of this work will have to be done either manually or by deriving connection rules from knowledge provided by human experts, which requires intensive inter-disciplinary and cross-domain effort. Sampling this ontology-enriched cross-domain graph produces positive/negative instances w.r.t. a potential decision boundary, which are subsequently fed into an embedding algorithm (Fig. <ref type="figure" target="#fig_1">1</ref>):</p><p>Two possible approaches to this end are joint embeddings <ref type="bibr" target="#b30">[31]</ref> and Graph Representation Learning (GRL) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Generally, embeddings are low-dimensional vector representations of entities which are usually learned from large corpora in an unsupervised fashion, i.e. by forcing an algorithm to approximate pair-wise distance in the embedding space to a given similarity function in the original, higher-dimensional input space. There is a wealth of literature on so-called neural embeddings, with the most prominent work presented by <ref type="bibr" target="#b33">Bengio et al. (2003)</ref>  <ref type="bibr" target="#b33">[34]</ref>.</p><p>In order to connect information from images, text &amp; *omics data we need to compute and learn representations for nodes/subgraphs in this low-dimensional space, considering node-level features as well as their structural surroundings (e.g. graph neighbourhoods). Within this embedding space, we can subsequently compare items in diverse ways (e.g. hierarchies, analogies, clustering, etc.) across different input modalities, whose intrinsic geometries have been aligned during the joint embedding procedure.</p><p>The most important aspect in this phase is generating positive/ negative samples within and across modalities as input features -for example, Word2Vec <ref type="bibr" target="#b34">[35]</ref>, as an unsupervised representation learning algorithm, assumes word co-occurrence within sentences as positive samples -this is highly domain specific and dependent on human assumptions, which constitutes a bias trap that must be carefully avoided through repeated experimentation with diverse sets of assumptions and automated as well as human control (objective performance measures and inter-expert validation combined).</p><p>Recent research has also successfully demonstrated a combination of different methods in a two-step process: (i) pre-processing intra-modal data using traditional assumptions about their respective domains, and (ii) feeding the resulting, normalized feature vectors into a graph representation learner (neighbourhood aggregation, GAN, etc.). It is a challenge to extend this approach to MM data sets. Others utilized traditional machine learning techniques (e.g. Random Forest) to build intra-modal similarity matrices, which are subsequently fused <ref type="bibr" target="#b24">[25]</ref>. However, this requires the full matrix to exist at computation time, which is unrealistic for larger graphs or distributed graphs. Depending on the approach, it might be necessary to utilize a pre-instantiated target dictionary of features curated by human experts (e.g. cell types visible in an image).</p><p>Frequent problems in machine learning (ML) and statistics concern missing data or data of different feature dimensionality. While the latter can be easily dealt with the application of dimensionality reduction techniques, there are no standard solutions for dealing with non-existent information. However, especially in the field of graph based ML, the structure of a network alone often carries sufficient information in order to obtain respectable results. Our own experiments on small, well-structured graphs showed that -depending on learning task and target attributes -randomly initialized representations can yield results on par with carefully designed and pre-processed feature vectors.</p><p>In this phase it is necessary to produce a corpus of MM feature representations, originating in e.g. histopathological images, patient case files (text), *omics data as well as medical knowledge bases, where a knowledge graph can be utilized as an initial connector. Such pretrained concept embeddings would constitute an advanced pendant to A. <ref type="bibr">Holzinger et al.</ref> word &amp; document embeddings, inlcuding the well-known GloVe <ref type="bibr" target="#b35">[36]</ref> or fasttext corpora. This could help institutions worldwide to elevate the utility of their already existing, yet unstructured and unconnected databases.</p><p>Beyond establishing broadly applicable embeddings, GRL can also be used in a supervised fashion by integrating a label-based term into the loss function. This would enable end-to-end learning on specific tasks, where one network architecture comprises all processing steps from handling raw inputs up to the final prediction. However, this comes with the downside of decreased generality in the learned representations (since those are now task-specific and thus not reusable across applications), unless informed data augmentation <ref type="bibr" target="#b36">[37]</ref> techniques are used. Moreover, embeddings learned as an intermediate step within a neural network will generally not correspond to concepts of human understanding and will therefore lack causability by nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Challenge II: Distributed GRL</head><p>Distributed learning has become a trending topic in recent years, either for reasons of limited central memory &amp; processing power, legal restrictions on data transmission, non-identically independend distributed data (non i.i.d.) over local sub-populations, or for the potential of hyper-scalable systems operating at lower costs. This is especially true for the medical domain, where locally generated data is sensitive, practically "stationary" (are not allowed to be transferred between institutions), and of huge volume (on the order of many Terabytes per day).</p><p>There are three main versions of distributed learning: (i) purely decentralized, where local models do not automatically contribute to each other above manual sampling of the models and update of hyperparameters; (ii) federated learning schemes <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, where a global model is constructed considering update signals of all local clients which are merged into a global model and distributed back to the clients; (iii) collaborative learning in various forms, where the goal is to exchange information about internal model formation among the parties involved in a peer-to-peer fashion, yet keep the local training data confidential (a variant could also train on de-centralized features supposedly modelling the same underlying instances <ref type="bibr" target="#b39">[40]</ref>). The great challenge for distributed learning of embeddings is to keep the representation space aligned across location boundaries -akin to the feature alignment problem in the MM setting.</p><p>Considering local pockets of data as natural clusters whose representative supernodes can be used as inputs to a more abstract graph embedding step. The literature contains several approaches to achieve such coarsened embeddings: while (i) simple aggregation of node features can already suffice <ref type="bibr" target="#b40">[41]</ref>, (ii) introducing virtual nodes and learning their embeddings together with the rest of the subgraph <ref type="bibr" target="#b41">[42]</ref> promises a very flexible clustering strategy, e.g. one could run a series of local predictions and record which nodes in the graph had the greatest influence on a desired outcome, then connect this set to a virtual node whose embedding can be expected to be particularly helpful in distributed predictions of the same kind. It is worth to mention that virtual nodes do not exist in the original graph and can therefore not be sampled, they are rather representatives of interesting clusters w.r.t. solving specific tasks; (iii) sampling strategies such as Random Walks (RW) or Anonymous RWs <ref type="bibr" target="#b42">[43]</ref>, where the local graph structure itself is translated to fixed-size vectors which are subsequently used as inputs to an embedder.</p><p>Lastly, Ying et al. (2018) <ref type="bibr" target="#b43">[44]</ref> have proposed an extension of their earlier GraphSAGE (Graph Sampling &amp; Aggregation) approach to take hierarchical feature levels into account; they invented graph coarsening DiffPool layers akin to pooling layers in traditional CNNs which learn a node assignment matrix, thus performing an embeddinglevel clustering step. Although this approach is innovative, it is unclear whether and to what extent it can handle distributed data.</p><p>Consequently, our proposed pipeline broadly consist of the following steps (and refer to Fig. <ref type="figure" target="#fig_2">2</ref>):</p><p>• Once a repertoire of GRL/embedding methods has been established, a decentralized scenario can be simulated. We understand distributed with the additional challenge that we have no throughput/latency guarantees concerning the connections between local subgraphs; in the extreme case we even need to consider common internet &amp; mobile connections. Thus, the principle challenge lies in propagating aggregated information per subgraph in such a way that GRL is still feasible from a performance &amp; accuracy standpoint. • Therefore it is necessary to extend existing neighbourhood aggregation techniques, selecting &amp; experimenting with different aggregation schemes, some of which are rather trivial (mean, max, sum) but nevertheless perform well on certain tasks according to literature; others can be complete neural networks in themselves, such as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b44">[45]</ref>, which were successfully used in GraphSAGE (Graph Sampling &amp; Aggregation). • Depending on its size, domain, or shape, experiments with different aggregators per local subgraph to obtain fitting representations are necessary. In practice, aggregation functions can have significant impact on results -it is therefore desirable to train a network so it is capable of establishing heuristics of when to use which aggregator, enabling the network to optimize its aggregation strategy autonomously. The heuristics can incorporate domain knowledge or be derived from user interpretation in a deep reinforcement learning setting <ref type="bibr" target="#b45">[46]</ref>, thus getting incorporated to define the reward function and maximize the accumulated reward of achieving this goal. The need for an explicit definition of predefined and rigid logic rules is thereby avoided, whereas the emergence of new strategies is enabled. • Regarding LSTMs, a complementary challenge lies in explaining their good performance on what are intuitively permutation invariant data, like the ordering of node neighbourhoods in a graph. We conjecture that many graph-related learning problems could be modelled under the aspect of information flow, where the sequence of nodes propagating signals is decisive. However, this assumption would need to be tested in various real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Challenge III: Explainable AI/interpretable ML</head><p>Graph Neural Networks (GNN) have been known for quite a while and are a well-established method to learn from graph structures <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. GNN can be used for encoding representations of data that do not have an ordered grid structure, contain different types of relationships and have obtained state of the art performance on different tasks including graph classification <ref type="bibr" target="#b48">[49]</ref>, node classification <ref type="bibr" target="#b49">[50]</ref> and link prediction <ref type="bibr" target="#b50">[51]</ref>.</p><p>There are different types of GNN architectures <ref type="bibr" target="#b51">[52]</ref> and different graph embeddings <ref type="bibr" target="#b52">[53]</ref>.</p><p>The need for dynamic GNN has led to newly invented architectures such as Pointer Graph Networks (PGN) <ref type="bibr" target="#b53">[54]</ref>, to enable the processing of adaptive graphs (also explained in detail in a recent survey <ref type="bibr" target="#b54">[55]</ref>).</p><p>The Open Graph Benchmark <ref type="bibr" target="#b55">[56]</ref> contains representative datasets for GNN benchmarking and is continuously updated. However, GNNs can be treated as black box models, lacking interpretability. This lead to an increase in development of interpretation techniques for all those different GNN architectures.</p><p>Duvenaud et al. (2015) <ref type="bibr" target="#b40">[41]</ref>, first introduced a convolutional neural network architecture for graphs which generalizes standard molecular feature extraction methods based on circular fingerprints, and demonstrated interpretability and predictive performance on fingerprint data.</p><p>Baldassarre et al. (2019) <ref type="bibr" target="#b56">[57]</ref> studied the explainability of GNN output using gradient-based and decomposition-based techniques on two tasks and implemented Sensitivity analysis (SA) <ref type="bibr" target="#b57">[58]</ref>, Guided backpropagation (GB) <ref type="bibr" target="#b58">[59]</ref>, and Layer wise relevance propagation (LRP) to explain node prediction (we briefly discuss LRP below).  <ref type="bibr" target="#b43">[44]</ref> to a decentralized setting where node clusters are generated locally &amp; their aggregated feature vectors propagated across the network in order to connect them in higher-order clusters based on some similarity metrics..</p><p>Recent work <ref type="bibr" target="#b59">[60]</ref> focuses on explaining comprehensive features such as node-and edge features as well as connecting patterns in a weighted graph for node classification. In addition, simulation on synthetic data was performed to compare results with human interpretation.</p><p>Prediction on graphs is usually influenced by complex combinations of nodes and edges between them; accurate prediction of node labels can only be achieved when they are considered together <ref type="bibr" target="#b60">[61]</ref>. Such a joint contribution cannot be modelled as simple linear combinations of individual contributions. Some research work on GNNs uses attention mechanisms for interpretability <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>The learned edge attention values indicate relevant graph structures, however, the values are the same across the nodes connected by that edge; this does not hold in many applications where the edge is the most important element for the label prediction of one particular node, but not the labels of other nodes. Furthermore, these approaches cannot explain predictions by considering both graph and node features, and are thus limited to specific GNN architectures. Motivated by these problems, Ying et al. (2019) <ref type="bibr" target="#b63">[64]</ref> proposed GNNExplainer, a model-agnostic approach that can provide interpretable explanations for predictions of any graph based model. The advantage of GNNExplainer is that the generated explanation is a rich subgraph (part of the entire graph on which GNN was trained), such that the subgraph maximizes mutual information with GNN prediction. In addition, this approach is capable of handling both single-and multi-instance explanations. In single instance, GNNExplainer explains predictions of one instance, such as a node, label or new link, whereas in multi-instance explanations it provides explanations that consistently explain a set of instances, such as nodes of a given class.</p><p>Another new approach for explanations has been introduced recently by Yuan et al. (2020) <ref type="bibr" target="#b64">[65]</ref>, called XGNN, for interpreting deep graph models at model level. XGNN introduced a technique of finding graph patterns that maximize a certain prediction through graph generation. It is formulated as a reinforcement learning (RL) problem and can generate graph patterns repetitively.</p><p>Certain graph rules are incorporated to make the generated graphs human-intelligible and valid. GraphLIME <ref type="bibr" target="#b65">[66]</ref> is a local interpretable model explanation framework that finds the most representative features as explanations in a non-linear manner. GraphLIME is a nonlinear graph variant of the Local Interpretable Model-Agnostic Explanation (LIME) method <ref type="bibr" target="#b66">[67]</ref>, which considers perturbation near the node being explained and applies a linear interpretable model. There exist some promising approaches which aim at capturing better model hierarchical relationships or causality mechanisms <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>.</p><p>With the increased adoption of graph convolutional neural networks (GCNNs) <ref type="bibr" target="#b32">[33]</ref>, explainability methods for GCNNs have been also introduced recently <ref type="bibr" target="#b69">[70]</ref>. Three prominent explainability methods of convolutional neural networks include: Contrastive Gradient-based (CG) saliency maps, Class Activation Mapping (CAM), and Excitation Backpropagation (EB) as well as their variants: Gradient-weighted CAM (Grad-CAM) and contrastive EB (c-EB) are extended to GCNNs. The explanations of each method are categorized into three metrics: fidelity, contrastivity and sparsity. The results from two application domains demonstrated that Grad-CAM is currently the most suitable among the studied methods for explanations on graphs of moderate size. Another work on local fidelity for explanation for GNNs is introduced in <ref type="bibr" target="#b70">[71]</ref>. A post-hoc framework known as TraP2 is proposed, which is based on local fidelity of any GNN model and generates high fidelity explanations. Furthermore, different gradient attribution analysis approaches for GC-NNs have been proposed <ref type="bibr" target="#b71">[72]</ref>, known as Node Attribution Method (NAM), which can get the model contribution from central node as well as its neighbouring nodes to the model output. In addition, Node Importance Visualization (NIV) visualizes the central node and its neighbour nodes based on the value of the contribution. Afterwards, perturbation analysis is utilized to verify the efficiency of the NAM based on citation network datasets. With the use of this method as well as visualization of node contribution in the decision making, more comprehensive contributions of each node are obtained. Schnake et al. (2020) <ref type="bibr" target="#b72">[73]</ref>, proposed the GNN-LRP approach which is derived from higher-order Taylor expansions based on Layer-wise Relevance Propagation (LRP). The LRP algorithm for pixel-based images explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself.</p><p>This method is originally based on two methodological principles: (i) the network propagation technique via max-pooling with rectified linear units by <ref type="bibr" target="#b73">Zeiler &amp; Fergus (2014)</ref>  <ref type="bibr" target="#b73">[74]</ref>, and (ii) Taylor decomposition <ref type="bibr" target="#b74">[75]</ref>. The overall idea is to identify patterns in the input A. <ref type="bibr">Holzinger et al.</ref> data of a network that are linked to a particular activation. As a first step, <ref type="bibr" target="#b75">Bach et al. (2015)</ref>  <ref type="bibr" target="#b75">[76]</ref> have used this for images as pixelwise decomposition and demonstrated how this decomposition can be used as an xAI method in combination with a pixel-wise relevance propagation algorithm. For this purpose they distinguished between (i) viewing the neural network as a function whilst disregarding the network topology, and (ii) message passing approaches (similar to learning representations by back-propagating errors <ref type="bibr" target="#b76">[77]</ref> and learning parameters of Conditional Random Fields <ref type="bibr" target="#b77">[78]</ref>). That way one can do a ''pixel-wise decomposition of a function:''</p><p>Let 𝑓 be a positive-valued function 𝑓 ∶ R 𝑑 → R + . The image can be decomposed as a set of pixel values 𝒙 = {𝑥 𝑝 } where 𝑝 denotes a particular pixel. The function 𝑓 (𝒙) quantifies the presence (or amount) of a certain type of object(s) in the image. This quantity can e.g. be a probability or the number of occurrences of the object. A function value 𝑓 (𝒙) = 0 indicates the absence of such object(s) in the image; a function value 𝑓 (𝒙) &gt; 0 expresses the presence of the object(s) with a certain probability.</p><p>The goal of this algorithm is that each pixel 𝑝 in the image is associated to a so-called relevance score 𝑅 𝑝 (𝒙). This relevance score indicates to what extent a certain pixel contributes to the classification result. The relevance of each pixel can eventually be stored in a heatmap <ref type="bibr" target="#b78">[79]</ref> denoted by 𝑹(𝒙) = {𝑅 𝑝 (𝒙)}, which is defined as the sum of the relevances in the pixel space corresponding to the total relevance detected by the model, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∀𝒙 ∶ 𝑓</head><formula xml:id="formula_0">(𝒙) = ∑ 𝑝 𝑅 𝑝 (𝒙)<label>(1)</label></formula><p>Montavon et al. (2017) <ref type="bibr" target="#b79">[80]</ref> observed that the application of deep Taylor decomposition to neural networks used for image classification yields rules, that are similar to those proposed by <ref type="bibr" target="#b75">Bach et al. (2015)</ref>  <ref type="bibr" target="#b75">[76]</ref> for pixel images. Consequently, they presented a heatmapping method for explaining the classification 𝑓 (𝒙) of a data point 𝒙, that is based on the Taylor expansion of the function 𝑓 at some wellchosen (the selection of which is difficult) root point x, where 𝑓 (x) = 0. As we know from standard math literature <ref type="bibr" target="#b74">[75]</ref> the first-order Taylor expansion of a function is given as</p><formula xml:id="formula_1">𝑓 (𝒙) = 𝑓 (x) + ( 𝜕𝑓 𝜕𝒙 | | |𝒙=x ) ⊤ ⋅ (𝒙 -x) + 𝜀 = 0 + ∑ 𝑝 𝜕𝑓 𝜕𝑥 𝑝 | | |𝒙=x ⋅ (𝑥 𝑝 -x𝑝 ) ⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟ 𝑅 𝑝 (𝒙) + 𝜀 (2)</formula><p>The sum ∑ 𝑝 is calculated over all pixels in the image, and {x 𝑝 } are the pixel values of the root point x. The added-up elements are the relevances 𝑅 𝑝 (𝒙). The 𝜀 denotes second-order and higher-order terms. Most of the terms in the higher-order expansion involve several pixels at the same time and are therefore difficult to redistribute, therefore <ref type="bibr" target="#b79">Montavon et al. (2017)</ref> proposed to consider only the first-order terms for the heatmapping. Consequently, the heatmap can be expressed as the element-wise Hadamard product 𝑥 ⊙ 𝑦 between the gradient of the function 𝜕𝑓 ∕𝜕𝒙 at the root point x and the difference between the image and the root point (𝒙 -x):</p><formula xml:id="formula_2">𝑹(𝒙) = 𝜕𝑓 𝜕𝒙 | | |𝒙=x ⊙ (𝒙 -x).<label>(3)</label></formula><p>For more details please directly refer to the work of <ref type="bibr" target="#b79">Montavon et al. (2017)</ref>. Images consist of grid-shaped inputs; non-grid-shaped inputs like graphs are processed by GNNs. The approach of Schnake et al.</p><p>(2020) is able to generate a decomposition of the GNN prediction as a collection of relevant walks. The higher-order Taylor expansions are computed using multiple backpropagation passes from the top layer to the first layer of the GNN. LRP applied in image processing neural networks only needs one backpropagation of relevance. Those backpropagations are not to be confused with the backpropagation training procedure; LRP is applied after the training is done and depends on its performance.</p><p>The graph is defined as an ordered pair 𝐺 = (, ) by its node set of objects  = {𝑣 1 , … , 𝑣 𝑛 } and edge (often called link) set of objects  ⊆ {(𝑣 𝑖 , 𝑣 𝑗 )|𝑣 𝑖 , 𝑣 𝑗 ∈ }. Each node 𝑣 can be represented by one or more features, which are in general tensors. The same applies for edges; if all nodes and all edges of the graph have the same type of features, then the graph is homogeneous.</p><p>As input, the GNN receives the structure of the graph, which is expressed by the adjacency matrix enhanced by self-connections 𝜦 and an initial state 𝑯 0 which corresponds to the initial representations of the node-and edge features. The GNN computes the following function (4):</p><formula xml:id="formula_3">𝑓 (𝜦; 𝑯 0 ) = 𝑔(𝑯 𝑡 (𝜦, 𝑯 𝑡-1 (𝜦, … 𝑯 1 (𝜦, 𝑯 𝟎 ))))<label>(4)</label></formula><p>where 𝑡 = 1 … 𝑇 the number of layers, and 𝑔 a readout function. The computation of the feature representations at each intermediate layer is based on the representation of the features of the previous layer, as expressed by Eqs. ( <ref type="formula" target="#formula_4">5</ref>) and ( <ref type="formula" target="#formula_6">6</ref>):</p><formula xml:id="formula_4">𝑍 𝑡 = 𝜦𝑯 𝑡-1 (<label>5</label></formula><formula xml:id="formula_5">)</formula><formula xml:id="formula_6">𝑯 𝑡 = (𝐶 𝑡 (𝑍 𝑡,𝐾 )) 𝐾<label>(6)</label></formula><p>The GNN-LRP method explains the prediction by attributing relevance to sequences of edges that connect nodes from the input to the output layer of the GNN. These paths now correspond to walks on the input graph . Thereby, the attribution of relevance of a node or edge is not made because it is important on its own, but also because of its connection to other relevant nodes or edges which they are connected to.</p><p>The prediction results from applying the transition function in Eq. ( <ref type="formula" target="#formula_3">4</ref>) iteratively from layer 0 to layer 𝑇 followed by the top-level readout function 𝑓 . This function receives as input the final state 𝑯 𝑇 which itself depends on the input graph through its representation 𝜦. Consequently, to extract walks in the input graph which are relevant for the GNN prediction, (Schnake et al. 2020) proposed to use the higherorder Taylor expansion of 𝑓 (𝜦). They considered a 𝑇 -order Taylor expansion and looked at terms of this expansion that match particular paths in the GNN, i.e. particular walks in the input graph. That means to let  = (..., 𝐽 , 𝐾, 𝐿...) be one such walk and view || as the number of edges in the walk . The ||th order terms of the Taylor expansion of 𝑓 (𝜦) at some root point Λ can then be expressed as follows:</p><formula xml:id="formula_7">𝑓 (𝜦) = ∞ ∑ 𝑘=0 ∑ ∈B 𝑘 1 𝛼  ! 𝜕 𝑘 𝑓 𝜕𝜆  1 ...𝜕𝜆  𝑘 | | | | Λ ⋅ 𝑘 ∏ 𝑖=1 (𝜆  𝑖 -λ 𝑖 )<label>(7)</label></formula><p>An appropriate root point Λ is difficult to find, but under the constraint that 𝐶 𝑡 and 𝑔 are piecewise linear and positively homogeneous, Λ = 𝑠𝜦 with 𝑠 → 0 is a mathematically sound choice. 𝛼  denotes the multiindex of a bag of edges , 𝜆  𝑖 refers to the element in the adjacency matrix 𝜦 corresponding to edge  𝑖 . It is shown by <ref type="bibr" target="#b72">[73]</ref>, that all terms where 𝑘 ≠ 𝑇 vanish, so that Eq. ( <ref type="formula" target="#formula_7">7</ref>) has the following form:</p><formula xml:id="formula_8">𝑓 (𝜦) = ∑ ∈B 𝑇 1 𝛼  ! 𝜕 𝑇 𝑓 𝜕𝜆  1 ...𝜕𝜆  𝑇 ⋅ 𝑇 ∏ 𝑖=1 𝜆  𝑖 (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>This reduced formulation still admits any graph  as input and many of the established GNN models: GCNN, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b48">[49]</ref>, and SchNet <ref type="bibr" target="#b80">[81]</ref> are contained in this formulation.</p><p>The relevance of the walk can be computed in two ways; the first corresponds to the ''Gradient × Input'' (GI) attribution, which is expressed by Eq. ( <ref type="formula" target="#formula_10">9</ref>):</p><formula xml:id="formula_10">𝑅  = 𝜕 𝜕 … ( 𝜕 𝜕𝜆 ⋆ 𝐽 𝐾 ( 𝜕 … 𝜕𝜆 ⋆ 𝐾𝐿 ⋅ 𝜆 ⋆ 𝐾𝐿 ) ⋅ 𝜆 ⋆ 𝐽 𝐾 ) ⋅ …<label>(9)</label></formula><p>The ''Hessian × Product'' propagation rule is more robust towards shattering gradient problems that occur in deeper neural networks: The LRP rule is different according to GNN architecture; for a GCNN in particular it is provided by the following Eq. ( <ref type="formula" target="#formula_12">11</ref>):</p><formula xml:id="formula_11">𝑅  = LRP ( LRP ( LRP ( … , 𝝀 ⋆ 𝐾𝐿 ) ⏟⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏟ 𝑹 𝐾𝐿… , 𝝀 ⋆ 𝐽 𝐾 ) ⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞ ⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞ ⏟ 𝑹 𝐽 𝐾𝐿… , … ) ,<label>(10)</label></formula><formula xml:id="formula_12">𝑅 𝑗𝐾𝐿… = ∑ 𝑘∈𝐾 𝜆 𝐽 𝐾 ℎ 𝑗 𝑤 ∧ 𝑗𝑘 ∑ 𝐽 ∑ 𝑗∈𝐽 𝜆 𝐽 𝐾 ℎ 𝑗 𝑤 ∧ 𝑗𝑘 𝑅 𝑘𝐿… (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>The learned weights 𝑤 𝑗𝑘 on the connection between neuron 𝑗 and neuron 𝑘 in the subsequent layers 𝐽 and 𝐾, are used for the backpropagation of relevance. The equation (⋅) ∧ = (⋅) + 𝛾𝜌(⋅), gives the user the opportunity to experiment with the hyperparameter 𝛾 and influence the intensity of the computed relevances. This is common to LRP derivation equations for other neural network architectures.</p><p>All terms of the expansion that are not bound to a walk  in  converge to zero, implying the conservation property ∑  𝑅  = 𝑓 (𝜦). Conservation is a commonly stated property that explanation techniques should satisfy and consists of a validation method for computational correctness. Despite the simplicity of Eq. ( <ref type="formula" target="#formula_10">9</ref>), computing this quantity directly, e.g. using automatic differentiation, can be extremely difficult; for further details on this approach please refer to the original work of Schnake et al. (2020) <ref type="bibr" target="#b72">[73]</ref>.</p><p>LRP is successfully applied in Graph Convolutional Networks (GCNN) applied in text sentiment analysis by graph classification <ref type="bibr" target="#b81">[82]</ref>, quantum chemistry, and image processing <ref type="bibr" target="#b72">[73]</ref>. The benefit of LRP in this case -although the implementation is quite different and tailored to a particular architecture -is that this method applies to graph classification, whereas most of the non-model agnostic methods explain GNNs that do node classification or link prediction. LRP reveals hidden dynamics over all layers, a property that other XAI methods for graphs <ref type="bibr" target="#b49">[50]</ref> do not exhibit. Furthermore, perturbation experiments on node flipping tasks show the monotonic degradation of classification performance, when removing the relevant elements one by one, sorted by decreasing relevance. This fact underlines that the highlighted features are important for the prediction in a proportional manner; this phenomenon is not evident in other XAI methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Towards an integrated vision</head><p>We envision an integrated medical ML pipeline starting at the input data level, building directly on image/signal taking processes. Via intramodal feature extraction and multi-modal embedding alignment, we arrive at a low-dimensional representation space comprising relevant data to assist the medical decision making process. Based on current stateof-the-art deep learning models we end up at classification/prediction results that can be presented interactively to medical professionals for inspection. The interactive procedure needs to be in place, facilitating re-integration of human feedback into the whole algorithmic loop. Although there is already a cornucopia of xAI methods to choose from, we find the expressiveness of counterfactuals particularly appealing for an interactive, interview-style exploration process due to their contrastive nature.</p><p>Initially, there is an urgent need to extend current state-of-the-art methods in graph explainability to the MM case, thereby establishing a baseline for evaluation of our own counterfactual-based approach. Thus, we need to describe, develop and evaluate a model to find the underlying explanatory factors already present in the low-level data, i.e. to answer questions of ''What is relevant?'' to change the graph structure correspondingly. Since automatic extraction will often fail, a human-in-the-algorithmic-loop <ref type="bibr" target="#b82">[83]</ref>, will be necessary here to enable the interactive learning setting. Due to the fact that humans are unable to directly orient themselves in high dimensional data sets, we need to design, develop and evaluate subspace visualization methods <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b84">85]</ref> to let the human expert interactively manipulate automatically generated samples, thereby iteratively assisting in forming causal and conceptual models.</p><p>In order to utilize the insights gained in this procedure, a crucial step is to formalize, develop &amp; test techniques to channel humanoriginated feedback back into the automated decision-making process, either via gradual model parameter updates or directly via efficient re-training of input feature representations.</p><p>Complex diseases such as cancer need to be studied on a systems level, because the interplay of highly diverse modalities (DNA mutations, Gene expression, Methylation, etc.) substantially contributes to disease progression <ref type="bibr" target="#b85">[86]</ref>. Here, graphs provide a natural way to efficiently model this phenomenon; however, semantic links between biological and disease-relevant features across modalities are largely unknown to this date.</p><p>We believe that learning these semantic links can again be facilitated by a human-in-the-loop as a regulator who is sometimes able to comprehend the context, and based on her/his conceptual understanding may judge the underlying ML decision paths <ref type="bibr" target="#b86">[87]</ref>. In Section 4 we have reviewed a range of XAI methods for the detection of walks within the input graph which are relevant for a prediction (e.g a prediction of disease relapse in biomedical applications). The degree of causability obtained from these paths, however, highly depends on the structure of the input graph defining the causal links. Our vision is the conversion of detected relevant paths to disease causing paths by incorporating human knowledge into the algorithmic loop.</p><p>Therefore, human-AI interactions should be based on the low-level (unfolded) input features in order to efficiently discover, reject and confirm causal links between biomedical modalities (using e.g disease ontology databases). Once these links are computed and the structure of the input graphs is updated accordingly, methods for explainable GNN (see Section 4) are consequently regularized towards decision paths with informed disease causing effects. The aforementioned procedure will certainly not converge within a single run, it requires multiple iterations with human domain knowledge in the loop promoting the model training process. Human interactions could be realized by ''whatif'' requests (counterfactuals) to the system resulting in a counterfactual graph (CG), where features are defined as nodes and the edges point to combinations of features, which we call counterfactual paths. Initially, the CG can be generated in a purely data-driven manner: Based on a test set comprising a sufficient number of samples, an algorithm will walk through the feature space swapping feature values between nearest neighbours of a different outcome class until the class of the instance itself changes. The nearest neighbour based sampling will result in adversarial examples of realistic patient profiles and thus are built upon plausibly counterfactuals. Furnishing such counterfactuals based roughly on the internals of a model does not suffice for explainability. The plausibility of the adversarial change is a must, i.e. the ''adversarial path'' leading to the label change should have a real chance of occurring in practice for the counterfactual to be realistic. In this regard, extensions of recent attempts at plausible counterfactuals for image classification <ref type="bibr" target="#b87">[88]</ref> should be extended to models for graph data.</p><p>The sampled feature path leading to the class change will be stored and forms a counterfactual decision path. Repeating this procedure results in a graph comprising multiple decision paths, which could be utilized as a communication channel back into the Black-Box model.</p><p>We suggest to transform the CG to a Decision Forest (DF) <ref type="bibr" target="#b88">[89]</ref> classifier comprising multiple trees derived from semantically enriched counterfactual subgraphs. The predictive power of this DF classifier could be compared with the classification outcomes of the Black-Box model. Decision Trees strongly supporting the outcome of the Black-Box model are directly linked to connected nodes within the counterfactual graph and thus may serve as the local explanatory factors of a decision. Moreover, it would be interesting to study whether the decision paths within the CG can be mapped back to the explained decision paths inferred by the methods in Section 4.</p><p>Recent work has shown how to efficiently reduce a DF to a single decision tree <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b90">91]</ref> from which counterfactuals can be easily observed by the leaf nodes, so it could be used as a model for global explanations (see Fig. <ref type="figure" target="#fig_3">3</ref>). In this approach the human-in-the-loop will have the opportunity to study this consensus decision tree, thereby adopting the modifications to the counterfactual graph. Exploring the effects on the decision trees caused by modifications of the counterfactual graph may facilitate the definition of symbolic rules in order to revise the internal structure of the input graph (see Fig. <ref type="figure" target="#fig_3">3</ref>). Possible modifications include adding or deleting semantic links between modalities, but also adjustments of their edge weights. A big advantage of our visionary approach is that the impact of regulation could be explored efficiently without the need for compute-intense re-training of the model after each modification. Accumulated modifications to the CG can be backpropagated as knowledge-based constraints and overall will regularize the training process of the Black-Box Model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we motivated the need for a novel, holistic approach to an automated medical decision pipeline building on state-of-the-art Machine Learning research, yet integrating the human-in-the-loop via an innovative, interactive &amp; exploration-based explainability technique called counterfactual graphs. We emphasized the need for multi-modality in every stage of this integrated approach, since medical decisions are mostly directed by various influence factors stemming from a multitude of underlying signals and knowledge bases. Towards this end, we outlined potential solutions to the challenge of aligning local geometries across different input feature spaces. Last but not least, we pointed out the necessity of computing a joint multi-modal representation space in a decentralized fashion, for the reasons of scalability &amp; performance as well as ever-evolving data protection regulations.</p><p>This effort is indented as a motivation for the international research community and a launchpad for further work in the fields of multi-modal embeddings, interactive explainability, counterfactuals, causability, as well as necessary foundations for effective future human-AI interfaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>A</head><label></label><figDesc>.Holzinger et al.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Graph fusion. Data points from four different input modalities -time-series, histopathological images, knowledge databases as well as patient histories in the form of unstructured text -are mapped into an interaction &amp; correspondence graph (ICG). Combined with their intra-modal geometry (network or similarity structure), we can generate positive/negative samples from this graph (akin to word-coocurrences in a text corpus) and embed them into low-dimensional, dense representations in a modality-aligned concept space.</figDesc><graphic coords="3,116.77,55.79,361.92,179.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.Learning hierarchies &amp; logical clusterings of nodes in a graph. This approach extends the original GraphSAGE (Graph Sampling &amp; Aggregation) as well as<ref type="bibr" target="#b43">[44]</ref> to a decentralized setting where node clusters are generated locally &amp; their aggregated feature vectors propagated across the network in order to connect them in higher-order clusters based on some similarity metrics..</figDesc><graphic coords="5,96.74,172.34,401.22,109.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Generating a counterfactual graph (CG) by sampling from a trained model. A human-in-the-loop interacts with and refines the CG which is subsequently transformed to a decision forest &amp; reduced to an easily-interpretable decision tree. Rules obtained from the decision tree are translated back to relevant paths in the original multi-modal fusion graph/embedding space.</figDesc><graphic coords="7,99.57,55.79,396.24,108.72" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful for the valuable comments of the anonymous reviewers. Parts of this work have received funding from the EU Project FeatureCloud. The FeatureCloud project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No. 826078. This publication reflects only the author's view and the European Commission is not responsible for any use that may be made of the information it contains. Parts of this work have been funded by the Austrian Science Fund (FWF), Project: P-32554 ''explainable Artificial Intelligence''.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of competing interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature21056</idno>
		<ptr target="http://dx.doi.org/10.1038/nature21056" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clinically applicable deep learning for diagnosis and referral in retinal disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Askham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Visentin</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-018-0107-6</idno>
		<ptr target="http://dx.doi.org/10.1038/s41591-018-0107-6" />
	</analytic>
	<monogr>
		<title level="j">Nature Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1342" to="1350" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intelligent feature engineering and ontological mapping of brain tumour histomorphologies by deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Ommeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Portante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al Qawahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diamandis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0068-6</idno>
		<ptr target="http://dx.doi.org/10.1038/s42256-019-0068-6" />
	</analytic>
	<monogr>
		<title level="j">Nature Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="316" to="321" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The limitations of opaque learning machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Possible Minds: 25 Ways of Looking at AI, Penguin</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Brockman</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The seven tools of causal inference, with reflections on machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="54" to="60" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The european legal framework for medical ai</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schneeberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stoeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-57321-8-12</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-030-57321-8-12" />
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge discovery and interactive data mining in bioinformatics -state-of-the-art, future challenges and research directions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jurisica</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-15-S6-I1</idno>
		<ptr target="http://dx.doi.org/10.1186/1471-2105-15-S6-I1" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">S6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why imaging data alone is not enough: Ai-based integration of imaging, omics, and clinical data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haibe-Kains</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jurisica</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00259-019-04382-9</idno>
		<ptr target="http://dx.doi.org/10.1007/s00259-019-04382-9" />
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2722" to="2730" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Jean-Quartier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jeanquartier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jurisica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12885-018-4302-0</idno>
		<idno>12885-018-4302-0</idno>
		<ptr target="http://dx.doi.org/10.1186/s" />
	</analytic>
	<monogr>
		<title level="m">silico cancer research towards 3r</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">408</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feasibility study of a multi-criteria decision-making based hierarchical model for multi-modality feature and multi-classifier fusion: Applications in medical prognosis prediction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2019.09.001</idno>
		<ptr target="http://dx.doi.org/10.1016/j.inffus.2019.09.001" />
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2019.12.012</idno>
		<ptr target="http://dx.doi.org/10.1016/j.inffus.2019.12.012" />
	</analytic>
	<monogr>
		<title level="m">Explainable Artificial Intelligence (xAI): Concepts, taxonomies, opportunities and challenges toward responsible AI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causability and explainability of artificial intelligence in medicine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.1312</idno>
		<ptr target="http://dx.doi.org/10.1002/widm.1312" />
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscipl. Rev.: Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning, and Inference</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measuring the quality of explanations: The system causability scale (scs). comparing human and machine explanations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13218-020-00636-z</idno>
		<ptr target="http://dx.doi.org/10.1007/s13218-020-00636-z" />
	</analytic>
	<monogr>
		<title level="m">Special Issue on Interactive Machine Learning</title>
		<editor>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="193" to="198" />
		</imprint>
		<respStmt>
			<orgName>TU Darmstadt</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Usability engineering methods for software developers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1145/1039539.1039541</idno>
		<ptr target="http://dx.doi.org/10.1145/1039539.1039541" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="74" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explainable ai and multi-modal causability in medicine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1515/icom-2020-0024</idno>
		<ptr target="http://dx.doi.org/10.1515/icom-2020-0024" />
	</analytic>
	<monogr>
		<title level="j">J. Interact. Media</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="179" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A giant with feet of clay: On the validity of the data that feed machine learning in medicine</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cabitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ciucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rasoini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Organizing for the Digital World</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="121" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bridging the last mile gap between ai implementation and operation:data awareness that matters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cabitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Balsano</surname></persName>
		</author>
		<idno type="DOI">10.21037/atm.2020.03.63</idno>
		<ptr target="http://dx.doi.org/10.21037/atm.2020.03.63" />
	</analytic>
	<monogr>
		<title level="j">Ann. Transl. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">501</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The long road to fairer algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-020-00274-3</idno>
		<ptr target="http://dx.doi.org/10.1038/d41586-020-00274-3" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">578</biblScope>
			<biblScope unit="page" from="34" to="36" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A general algorithm for deciding transportability of experimental results</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.7485</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal inference and counterfactual prediction in machine learning for actionable healthcare</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prosperi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sperrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Buchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-020-0197-y</idno>
		<ptr target="http://dx.doi.org/10.1038/s42256-020-0197-y" />
	</analytic>
	<monogr>
		<title level="j">Nature Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="369" to="375" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Varieties of counterfactual thinking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">What Might Have Been: The Social Psychology of Counterfactual Thinking</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Roese</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Olson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Taylor and Francis</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="375" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Similarity network fusion for aggregating data types on a genomic scale</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mezlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brudno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haibe-Kains</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<idno type="DOI">10.1038/nMeth.2810</idno>
		<ptr target="http://dx.doi.org/10.1038/nMeth.2810" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="340" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Propagation graph fusion for multi-modal medical content-based retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICARCV.2014.7064415</idno>
		<ptr target="http://dx.doi.org/10.1109/ICARCV.2014.7064415" />
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Control Automation Robotics &amp; Vision (ICARCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="849" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-modal classification of alzheimer&apos;s disease using nonlinear graph fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.10.009</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2016.10.009" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="181" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-modal graph fusion for inductive disease classification in incomplete datasets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burwinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zwergal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MMGCN: multi-modal graph convolution network for personalized recommendation of micro-video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3351034</idno>
		<ptr target="http://dx.doi.org/10.1145/3343031.3351034" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, ACM SIGMM</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Huet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Larson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Hung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Ooi</surname></persName>
		</editor>
		<meeting>the 27th ACM International Conference on Multimedia, ACM SIGMM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D S</forename><surname>Torres</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1912.10314" />
	</analytic>
	<monogr>
		<title level="j">Multimodal Prediction based on Graph Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i01.5347</idno>
		<ptr target="http://dx.doi.org/10.1609/aaai.v34i01.5347" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi similarity metric fusion in graphbased semi-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bahrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosaghzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
		<idno type="DOI">10.3390/computation7010015</idno>
		<ptr target="http://dx.doi.org/10.3390/computation7010015" />
	</analytic>
	<monogr>
		<title level="j">Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03856</idno>
		<title level="m">Starspace: Embed all the things</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res. (JMLR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2013, NIPS foundation</title>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Buttou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Biomedical image augmentation using augmentor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Bloice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz259</idno>
		<ptr target="http://dx.doi.org/10.1093/bioinformatics/btz259" />
	</analytic>
	<monogr>
		<title level="j">Oxford Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4522" to="4524" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The more the merrierfederated learning from local sphere recommendations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Giuliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66808-6-24</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-66808-6-24" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Extraction</title>
		<title level="s">Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10410</biblScope>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust and communicationefficient federated learning from non-iid data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2019.2944481</idno>
		<ptr target="http://dx.doi.org/10.1109/TNNLS.2019.2944481" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3400" to="3413" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Stochastic distributed optimization for machine learning from decentralized features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06415</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph classification via deep learning with virtual nodes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI Workshop on Learning in Graphs</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11921</idno>
		<title level="m">International Conference on Machine Learning (ICML 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Anonymous walk embeddings</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explainability in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heuillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Couthouis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2020.106685</idno>
		<ptr target="http://dx.doi.org/10.1016/j.knosys.2020.106685" />
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2005.1555942</idno>
		<ptr target="http://dx.doi.org/10.1109/IJCNN.2005.1555942" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>the 2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2005605</idno>
		<ptr target="http://dx.doi.org/10.1109/TNN.2008.2005605" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
		<ptr target="http://dx.doi.org/10.1109/TNNLS.2020.2978386" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: Problems, techniques, and applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1616" to="1637" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Pointer graph networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Overlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06380</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Foundations and modelling of dynamic networks using dynamic graph neural networks: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Skarding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gabrys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07496</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Explainability techniques for graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13686</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Internatioanal Coference on Learning Representations ICLR 2015</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saude</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00514</idno>
		<title level="m">Explain graph neural networks to understand weighted graph features in node classification</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Chem</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Graph neural networks including sparse interpretability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Bulusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Dry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hernandez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00119</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Gnnexplainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9244" to="9255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">XGNN: towards model-level explanations of graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403085</idno>
		<ptr target="http://dx.doi.org/10.1145/3394486.3403085" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;20</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</editor>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;20</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Graphlime: Local interpretable model explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06216</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Why should i trust you? explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning hierarchical categories in deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the Cognitive Science Society</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Knauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Pauen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebanz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Wachsmuth</surname></persName>
		</editor>
		<meeting>the 35th Annual Meeting of the Cognitive Science Society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1271" to="1276" />
		</imprint>
	</monogr>
	<note>COGSCI</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Neural network attributions: A causal perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manupriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02302</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Explainability methods for graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10772" to="10781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Perturb. more, Perturb more trap more: Understanding behaviors of graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09808</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Interpreting and understanding graph convolutional neural network using gradient-based attribution method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03768</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Montavon, xAI for graphs: Explaining graph neural network predictions by identifying relevant walks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schnake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lederer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03589</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1-53</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-10590-1-53" />
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<title level="s">Lecture Notes in Computer Science LNCS</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Advanced Mathematical Methods for Scientists and Engineers I: Asymptotic Methods and Perturbation Theory</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Orszag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0130140</idno>
		<ptr target="http://dx.doi.org/10.1371/journal.pone.0130140" />
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1038/323533a0</idno>
		<ptr target="http://dx.doi.org/10.1038/323533a0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic Graphical Models: Principles and Techniques</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Controlling explanatory heatmap resolution and semantics via decomposition depth</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2016.7532763</idno>
		<ptr target="http://dx.doi.org/10.1109/ICIP.2016.7532763" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2271" to="2275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep taylor decomposition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.11.008</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2016.11.008" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Schwarzenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harbecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10911</idno>
		<title level="m">Layerwise relevance visualization in convolutional text graph classifiers</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Interactive machine learning for health informatics: When do we need the human-in-the-loop?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40708-016-0042-6</idno>
		<ptr target="http://dx.doi.org/10.1007/s40708-016-0042-6" />
	</analytic>
	<monogr>
		<title level="j">Brain Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="131" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Integrated web visualizations for protein-protein interaction databases</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jeanquartier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jean-Quartier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0615-z</idno>
		<ptr target="http://dx.doi.org/10.1186/s12859-015-0615-z" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">195</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Visual analytics for concept exploration in subspaces of patient groups: Making sense of complex datasets with the doctor-in-theloop</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Majnaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40708-016-0043-5</idno>
		<ptr target="http://dx.doi.org/10.1007/s40708-016-0043-5" />
	</analytic>
	<monogr>
		<title level="j">Brain Inform</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="233" to="247" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Modules, networks and systems medicine for understanding disease and aiding diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gustafsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Nestor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baranzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brunak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Federoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Meehan</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13073-014-0082-6</idno>
		<ptr target="http://dx.doi.org/10.1186/s13073-014-0082-6" />
	</analytic>
	<monogr>
		<title level="j">Genome Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Interactive machine learning: experimental evidence for the human in the algorithmic loop</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kickmeier-Rust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Crişan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Palade</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-018-1361-5</idno>
		<ptr target="http://dx.doi.org/10.1007/s10489-018-1361-5" />
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2401" to="2414" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Plausible counterfactuals: Auditing deep learning classifiers with realistic adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barredo-Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><surname>Ser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11323</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2015.06.005</idno>
		<ptr target="http://dx.doi.org/10.1016/j.inffus.2015.06.005" />
	</analytic>
	<monogr>
		<title level="m">Decision forest: Twenty years of research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Random forest explainability using counterfactual sets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>De Diego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aceña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández-Isabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Moguerza</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2020.07.001</idno>
		<ptr target="http://dx.doi.org/10.1016/j.inffus.2020.07.001" />
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="196" to="207" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Explainable decision forest: Transforming a decision forest into an interpretable tree</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2020.03.013</idno>
		<ptr target="http://dx.doi.org/10.1016/j.inffus.2020.03.013" />
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="124" to="138" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
