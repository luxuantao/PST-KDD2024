<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-organizing Network for Optimum Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>I I I IO0</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-organizing Network for Optimum Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">02E84AA4A2801B5F440569177AB87807</idno>
					<note type="submission">Manuscript received June 29, 1989; revised October 10, 1989.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work introduces a new algorithm called the self-organizing neural network (SONN), and demonstrates its use in a system identification task. The algorithm constructs a network, chooses the node functions, and adjusts the weights. Here it is compared to the hack-propagation algorithm in the identification of the chaotic time series. The results show that SONN constructs a simpler, more accurate model, requiring less training data and epoches. The algorithm can also he applied as a classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION A. The System Identijication Problem</head><p>N VARIOUS engineering applications, it is important I to be able to estimate, interpolate, and extrapolate the behavior of an unknown system when only its input-output measurement pairs are available. Algorithms which produce an estimation of the system behavior based on these pairs fall under the category of system identification techniques. These techniques can be divided into two major groups: parameter estimation and jknctional estimation. These two groups are not mutually exclusive, but can be described in a continuum between the two extremes. The parameter estimation approach is relatively simple, if one can safely assume a known system function with only unknown parameters. This calls for a large body of a priori knowledge about the underlying process being observed, and so it is not very general. The second approach, functional estimation, deals with the estimation of the system function as well as its parameters, and it can be simplified if assumptions about the function can be made. The larger the number of assumptions, the more the parameter estimation process it resembles.</p><p>parameter based approach, and a functional estimator where the order of the system complexity is known (number of hidden units). The system identification problem can appear in a diverse number of ways, varying in the types of relationships the input-output pairs can have within themselves and with each other. If no relationship is ascribed to subsequent pairs in time, the system can be thought of as performing a static mapping over the input space. When a time structure is found in the input sequence with implications for the output sequence, the system is said to be acting on and producing time dependent sequences. Such systems can have a constant internal structure in time. This means that these systems would always produce the same output sequence for a given input sequence and internal state regardless of its previous history, and they would be called static or time independent. Some systems might also produce an output value based solely on past values of the input-output pairs. In these cases, the systems are said to be causal. This work describes a new supervised learning algorithm for selforganizing neuromorphic structures which minimizes the . number of assumptions about the underlying process. Although it does not necessarily have to be so, the examples presented here are of static, causal systems, operating on time varying signals.</p><p>Several different applications of identification techniques can be found in fields as diverse as computer vision [2], and system adaptive modeling <ref type="bibr" target="#b2">[3]</ref>. According to Zadeh [4], the system identification problems can be defined as: "the determination, on the basis of input and output, of a system with a specified class of systems, to which B. Svstem Identification Usin2 Neural Networks the system under test-is equivalent." The three key comv ponents to system identification are: 1) the class of systems, 2)-a class of signals, and 3) a criterion of equivalence. Astrom and Eykhoff [5] pointed out that the identification problem can be transformed into the optimization problem, if the criterion of equivalence is defined in terms of the error function. A general form to represent systems, both linear and nonlinear, is the Kolmogorov-Garbor polynomial [6] shown below:</p><p>Neurocomputing techniques can be applied to the system identification problem using adaptive algorithms for either parameter or functional estimation. When both input and output are observable and usable by the algorithm, the learning process is called supervised since the output signal works as a teacher signal. The generalized delta rule (GDR), the most popular form of this type of learning process, has been used for system identification successfully [l]. GDR falls somewhkre between a pure</p><formula xml:id="formula_0">y = a. + C aixi + C C aiixixj + - (1) I i j</formula><p>the coefficients of (1) by minimizing the mean square error between each desired output sample and the actual output. <ref type="bibr">Roy and Sherman [7]</ref> suggested the concept of "hypersurfaces" as an alternative way to think about the identification problem. In this case, the identification of a system, especially of a nonlinear case, is viewed as the construction of a n N + 1 dimension hypersurface; if the highest order of the polynomial in (1) is N . <ref type="bibr">Lapedes</ref> and Farber [ 11 also explored the hypersurface interpretation of the functional relationship in (1). Their work concentrated on the use of feedforward neural networks in which the node transfer function is the sigmoid, or the logistic function. The structure of the network is fixed, and the weights adjusted by the GDR algorithm. The elementary structure of the system is based on n nodes of the first connected to the nodes of the second layer, and the nodes of the second connected to the nodes of the third layer according to a particular connectivity pattern (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>This substructure forms a hypersurface in n + 1 dimension, which they call a "bump" <ref type="bibr">[l]</ref>. Combinations of such bumps are used to approximate the desired input-output relationship in (1). Here n is the dimensionality of the input, which in this case represents past samples of the input sequence x ( t ) , which should completely characterizey(t).</p><p>The approaches described above suffer from the rapid explosion of the possible combination of terms as the order of the polynomial increases. The number of samples needs to be much larger than the dimensionality of the hypersurface used, which for practical purposes can be difficult to achieve. They also require repeated presentation of the training data, or more preferably, infinite sequences.</p><p>To address some of the difficulties described above, a heuristic algorithm called the group method of data handling (GMDH) was developed [8]. This method constructs a feedforward network as it tries to estimate the system function. The node transfer function consists of a quadratic polynomial of two variables, and its parameters are obtained through regression. At each stage of the algorithm, nodes are created pairwisely connecting the output of one layer to form a new layer, starting with the input nodes. This process is exhaustively done for all possible input pairs in each layer, and the connections are always in feedforward, n-to-n -t 1 layer form. The power of this method comes from the use of simple elementary functions (low dimensionally producing low complexity, linear regression on the parameters, and low training data requirements), and the ability of the algorithm to discard unpromising nodes (elementary hypersurfaces which represent terms in (1)). This selection process is based on a performance criterion which evaluates how closely the new surface describes the output data in a least-meansquare sense. The GMDH method estimates the order and the complexity of the model (low U priori knowledge requirements), and generates suboptimal estimates at each node output. Thus the algorithm could be stopped at any point to obtain a model of the process. A heuristic stop- ping criterion picks the output of the last node before the increase of the fitting error is detected. This technique has drawbacks because of its heuristic nature. There were various improvements made to overcome the problems associated with these heuristics with some success</p><formula xml:id="formula_1">[9]-[l l].</formula><p>However, due to the constraints in the possible connectivity pattern in the nodes, the set of possible functions which can be expressed before the termination criteria is applied is at best limited. Thus, the models estimated by the GMDH are suboptimal by nature in expressivity, search power, richness of elementary functions, and suffering from gradient descent hill-climbing problems. This paper describes a supervised leaming algorithm for structure construction and adjustment. Here, systems which can be described by (1) are presented. The computation of the function for each node performs a choice from a set of possible functions previously assigned to the algorithm, and it is general enough to accept a wide range of both continuous and discrete functions. In this work, the set is taken from variants of the 2-input quadratic polynomial for simplicity, although there is no requirement making it so. This approach abandons the simplistic mean-square error for performance control in favor of a modified minimum description length (MDL) criterion <ref type="bibr" target="#b1">[12]</ref>, with provisions to measure the complexity of the model generated. The algorithm searches for the simplest model which generates the best estimate. The modified MDL, from hereon named the structure estimation criterion (SEC), is applied hierarchically in the selection of the optimal node transfer function from the function set, and then is used as an optimality criterion to guide the construction of the structure. The connectivity of the resulting structure is arbitrary, and under the correct conditions [13] the estimation of the structure is optimal in terms of the output error and low function complexity. This approach shares the same spirit of GMDH-type algorithms. However, the concept of parameter estimation from information theory, combined with a stochastic search algorithm-Simulated Annealing-was used to create a new tool for system identification.</p><p>This work is organized as follows: Section I1 presents the problem formulation and the self-organizing neural network (SONN) algorithm description; Section I11 describes the results of the application of SONN to a wellknown problem tested before using other neural network algorithms [l], [14]; Section IV compares this approach to the generalized delta rule applied to feedforward networks; and finally, Section V presents a discussion of the results and future directions for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">THE SELF-ORGANIZING NEURAL NETWORK ALGORITHM A. Self-organizing Structures</head><p>As mentioned before, the problem with system identification algorithms is the size of the possible system function space, which creates a need for the reliance of the algorithm on large amounts of a priori knowledge or strong assumptions about the process which might not hold true. It is necessary therefore to create algorithms which are not dependent on this type of knowledge, but rely mostly on the observed data behavior and could incorporate this knowledge when available. This leads to the adoption of self-organizing structures in the design of neural networks, since fixed structure algorithms present the same problems as parameter estimation algorithms, in which the assumed model is either an under-or overestimation of the true process. If the elementary functions of the nodes are permitted to be general enough to minimize the prior assumptions, and a self-organized rule is rich enough to capture the complexity of the underlying process, then the simplest model can be found.</p><p>The efficiency of the search for the correct model is directly proportional to the amount of knowledge available about the process. One would like to steer away from heavy dependence on this knowledge by allowing, for example, arbitrary connectivity and number of nodes, as is the case with SONN. On the other hand, if such a knowledge is available, it is of primary importance to be able to transparently incorporate such information into the algorithm. There have been very few attempts to design selforganizing structure algorithms due to the complexity of the search space, and only now are there a few tentatives using modifications of the GDR 1151. Algorithms with fixed structure depend on a good guess of the set of initial parameters and structure order, having their final performance dependent on the ability and knowledge of the designer.</p><p>The SONN algorithm performs a search on the model space by the construction of hypersurfaces. A network of nodes, each node representing a hypersurface, is organized to be an approximate model of the real system. SONN family of algorithms can be fully characterized by three major components, which can be modified to incorporate knowledge about the process: 1) a generating rule of the primitive node transfer functions, 2) an evaluation method which accesses the quality of the model, and 3) a structure search strategy. Below, the components of SONN are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Algorithm Structure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S:</head><p>1. The Generating Rule: Given a set of observations </p><formula xml:id="formula_2">, x,] is n. Every com- input variable X = [x,, x 2 , e * - ponent yl of Y forms a hypersurface yI = f i ( X ) in the space of n + 1. The problem is to findf(.) = [ f i , f i , -* -, f,], given the observations S.</formula><p>The approach taken here is to estimate the simplest model which best describesf ( . ) by generating an optimal function at each node. This can be viewed as the construction of a hypersurface based on the observed data. It can be described as follows. Select from the pool of variables A a subset of size p (initially A has all the input variables). From a set of predefined functions, construct a hypersurface of dimension p. This surface generates a dependent variable y which is added to A to generate a new pool of variables. If the optimality criteria are not reached by the dependent variable, repeat the process on the new pool of variables. More formally, given a set of observations S, for each dimension 1 of the output variable Y , create the hypersurface that best describes yl by a three step process. To explain the process, the following definitions are necessary. Let the terminal y j ( i = 1, N ) be defined as a sequence of N data points generated at the output of thejth hypersurface. Let yI -yn be the sequences constructed by each of the dimensions of the input variable X. Let Ak be the set of all the terminals present at state K. Let 7rj: A + yj be a projection function that chooses the jth element of Ak, and let the mapping $$:</p><formula xml:id="formula_3">Ak -+ { ni, * , n,} 1 I i, j I 1 Ak I consists o f p such n's.</formula><p>To perform the construction procedure: initialize k = 1, , yn } whose size corresponds to the dimension of X.</p><p>Step 1. From a set of prototype function F (described below), select a function h in the kth iteration. Construct the hypersurface hk ( $$ ( Ak ) ).</p><p>Step 2. If the global optimality criterion is reached by the construction of hk ( ${ ( A k ) ), then stop, otherwise continues to the third step.</p><p>Step 3:</p><formula xml:id="formula_4">Ak+i = Ak U ( y h k ( $ $ ( A k ) ) } , k = k 1, go to step 1.</formula><p>The above recursive procedure is the backbone of this family of self-organizing algorithms. Here, an instance of this family is presented by selecting the processes responsible for each step. The projection function performs a random equal probability selection of the terminals. The construction of the surface is subject to a stochastic search (Simulated Annealing) guided by an optimality criterion (Structure Estimation Criterion). For simplicity in this work, the set of prototype functions ( F ) is restricted to be 2-input quadratic surfaces or smaller, with only four possible types:</p><formula xml:id="formula_5">= {?I7 y29 * * ( 3 ) y = a0 + a l x l + U 2 X 2 TENOR10 A N D LEE: SELF-ORGANIZING NETWORK FOR OPTIMUM SUPERVISED LEARNING I03 y = uo + ~1 x 1 + ~2 x 2 + ~3 ~1 x 2 (4)</formula><p>( 5 ) y = a0 + alxl + a2x:</p><formula xml:id="formula_6">y = + ~1 x 1 + ~2 x 2 + ~3 ~1 x 2 + U ~X : + U ~X ; . (6)</formula><p>Type (3) indicates a linear relationship between the input and output variables, types (4) and ( 5 ) indicate a secondorder relationship, whereas type (6) is a complete quadratic polynomial in 2 variables. There is no restriction on the use of higher order polynomials, functions of more variables, or other functions such as the sigmoid (logistic) or transcendental functions. Each node describes a function h of p variables. The resulting model is a multilayered neural network whose topology is arbitrarily complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Evaluation of the Model Based on the MDL Criterion:</head><p>The selection rule ( T ) of the node transfer function was based on a modification of the minimal description length (MDL) information criterion. In [ 121, the principle of minimal description for statistical estimation was developed. This principle chooses the best model as the one that minimizes the total number of binary digits required to encode the observations. The reason for the choice of such a criterion is that, in general, the accuracy of the model can increase at the expense of simplicity in the number of parameters. The increase of the number of parameters might also be accompanied by the overfitting of the model. To overcome this problem, the MDL provides a tradeoff between the accuracy and the complexity of the model by including a structure estimation term for the final model. The final model (with the minimal MDL) is optimum in the sense of being a consistent estimate of the number of parameters while achieving the minimum error whert 8 is the maximum likelihood estimate of 0, log p (z 1 0) is the log likelihood of estimated density function of pz ( . ), k is the number of parameters in the model, and N is the number of observations. According to the explanation of Rissanen <ref type="bibr" target="#b1">[12]</ref>, the first term is the self-information of the model, which can be interpreted as the number of bits necessary to encode the observations. The second term can be also interpreted as the number of bits needed to encode the parameters of the model. Hence, the model which achieves the minimum of MDL is the most efficient model to encode the observations zl, z2, * , zN. As indicated in [ 171, the ML estimate is a special case of the MDL, as is also the maximum entropy principle [ 171, [ 181. A related estimator is the AIC derived by [ 191 which has a similar structure defined as:</p><formula xml:id="formula_7">AIC = -2 lOgp(x18) + 2 k. ( 8 )</formula><p>For the linear polynomial regression, as in the set F { (4)-(6) 1, the AIC reduces to N log ( S i ) + 2 k, where S i is the mean square error, and k is the number of parameters in the model <ref type="bibr" target="#b16">[20]</ref>. In spite of the success of the AIC estimator in various applications such as AR model estimation, and river environment identification, the estimator has been proven inconsistent <ref type="bibr" target="#b17">[21]</ref>. Using the same arguments, the MDL for linear polynomial models is:</p><formula xml:id="formula_8">MDL = 0.5 N log S i + 0.5 k log N (9)</formula><p>where k is the number of coefficients in the model selected.</p><p>In the SONN algorithm, the MDL criterion is modified to operate both recursively and hierarchically. First, the concept of the MDL is applied to each candidate prototype surface for a given node. Second, the acceptance of the node, based on Simulated Annealing, uses the MDL measure as the system energy. However, since the new node is generated from terminals which can be the output of other nodes, the original definition of the MDL is unable to compute the true number of system parameters of the final function. Recall that due to the arbitrary connectivity it is nontrivial to compute the number of parameters in the entire structure. In order to reflect the hierarchical nature of the model, a modified MDL called structure estimation criterion (SEC) is used in conjunction with an upper bound on the number of parameters in the system at each stage of the algorithm. The expression to compute the estimate? upper bound of the number of parameters in the system k is:</p><p>where r is equal to 2', 1 is the number of nodes between the input and the output (layers), and d is the number of different variables in the model. The proof of the upper bound is given in the Appendix. The upper bound of the number of parameters in the example of the Fig. <ref type="figure" target="#fig_4">2</ref> (the model whose output is node 7) can be estimated as follows:</p><formula xml:id="formula_9">22 + 4 I&amp; = (22 ) = (3 = 70. ( 1 1 )</formula><p>Another computationally efficient heuristic for the estimation of the number of parameters in the model is based on the fact that SONN creates a tree-like structure with multiple roots at the input terminals. Then k , in expression (12), can be estimated recursively by: + (number of parameters of the current node)</p><p>where iL and kR are the estimated number of parameters of the left and right parents of the current node, respectively. This heuristic estimator is neither a lower bound nor an upper bound of the true number of parameters in the model.  In the SONN algorithm, the search for the correct model structure is done via Simulated Annealing. Therefore, the algorithm at times can accept partial structures that look less than ideal. In the same way, it is able to discard partially constructed substructures in search of better results. The use of this algorithm implies that the node accepting rule ( R ) vanes at run time according to a cooling temperature ( T ) schedule. The SONN algorithm is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialize T, and SI Repeat Repeat S, = generate ( S , ) , Ifaccept (SEC-S', SEC-SI, T ) then SI = S,,</head><p>application of P .</p><p>application of R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>until the number of new nodes accepted is greater</head><p>Decrease the temperature T (cooling sequence) until the temperature Tis smaller than Tend -(terminal temperature for Simulated Annealing).</p><p>Each node output and the system input variables are called terminals. Terminals are viewed as potential dimensions from which a new hypersurface can be constructed. Every terminal represents the best tentative to approximate the system function with the available information, and is therefore treated equally. The rule P which produces new nodes can be defined as:</p><formula xml:id="formula_10">generate ( SI ):</formula><p>than 112.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Randomly select two terminals from A , , I = A, U ( Y h r ( 4 m , ) ) )</head><p>If the two selected terminals are part of a previously constructed from node, destroy the node and return to the previous state. For each prototype surface in the set F, jit the surface and calculate SEC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choose the sur$ace with the smallest SEC (best jitting and less complex). Construct the node using the prototype surface chosen</head><p>i f the SEC of the node is smaller than the SEC of pa rents.</p><p>The generation function can be further refined by keeping records of the combination of nodes being produced, and checking the new combination against the records. By such a checking process, we can avoid the useless nodes being regenerated during the search procedure. Such a scheme corresponds to a memory version of Simulated Annealing.</p><p>The rule R represents the test for acceptance of a new node. It searches the model space using Simulated Annealing, where the SEC is viewed as the state energy. The rule can be defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>accept(SEC-S,, SEC-SI, T):</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I f the new state SEC is smaller than the SEC of the current state, accept the node, else accept the node with probability:</head><formula xml:id="formula_11">p = exp ( -A SEC-State/Tcumnt).</formula><p>(</p><p>There have been different annealing sequences proposed in the literature <ref type="bibr" target="#b18">[22]</ref>, <ref type="bibr">[23]</ref>. We adopted the geometrical annealing sequence in the SONN algorithm: Tnw = a*To~,.</p><p>(14)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Structuring the State Search:</head><p>The SONN algorithm performs the organization of the model structure. Each node is generated competitively and cooperatively with one another. The competition occurs between front nodes competing for lower SEC values. The cooperation occurs between parent nodes and their children. The initial set of terminals contains only the input nodes. From this set, candidates are chosen randomly (here 2 are chosen per iteration due to the bivariate form of the functions chosen for F ) to construct a new terminal (new node for the list), which becomes the front node. This process of the combining of nodes starts at a high temperature T , and an initial state S (only input terminals). In the outer loop of the algorithm that defines SONN, the temperature is decreased after a certain number of new nodes are created. This slow cooling procedure is motivated by the analogy with the annealing process used in physical systems to minimize the system's potential energy <ref type="bibr" target="#b18">[22]</ref>. A final state S, is reached when the low temperature Tend is present. S, corresponds to the state of low energy. The energy of the state in terms of the SEC is defined to be the energy function of the Simulated Annealing algorithm.</p><p>The application of the P rule produces a state S, by a perturbation in the state S I . The transition probability be- to i occurs as node 5 is selected to be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>_ -</head><p>tween states is determined by the probability function (1 3) as part of the acceptance rule R . Fig. <ref type="figure" target="#fig_5">3</ref> shows the transition between states Sj and Si. The difference between the two states is the new node Nj, the connections from the previous structure into Nj, and the new terminal ( y j , the output of Nj ). Every terminal is an estimate of the output of the system; in other words, the structure relevant to the terminal is a suboptimal model of (1). The optimal model which represents the global minimum of the objective function will be embedded in final state S,. When the inputs of an existing front node are selected for a new node, the structure associated with that node is destroyed, reversing the state to its parent state. This is to assure that there is a nonzero probability of returning to a previously visited state, in accordance with the mechanism of Simulated Annealing. The definition of the system's SEC is consistent with the state generation mechanism. Given the state Si and the state Sj derived after the structure of Si, then:</p><p>1) State Sj has the same energy value of Si, if the new terminal generated has a higher SEC than the front nodes of Si. This is true since the structures in Si are subsumed in S'.</p><p>2) State Sj has a lower energy than Si, if the new terminal generated has a lower SEC than the front nodes of Si. In this case, there is a structure in Sj which better estimates the model and it is not present in Si.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">EXAMPLE-THE CHAOTIC TIME SERIES</head><p>Randomness is a prevalent characteristic in natural systems such as economical or physical systems. Traditionally, randomness has been viewed as a characteristic attributed to the inherent complexity in the large number of degrees of freedom present in the observed system. However, recently chaos has been proposed as a possible source of randomness in dynamical systems <ref type="bibr" target="#b20">[24]</ref>. A sys-tem is said to be chaotic if the evolutionary trajectory of the system is generated by a deterministic mechanism, but it is very sensitive to the system's initial condition. Examples of chaotic systems can be found in turbulent fluid flow analysis <ref type="bibr">[25]</ref>, and biological systems [26]. Since under certain conditions a chaotic system behaves randomly, the identification of such systems is difficult. Under those conditions, a model capable of identifying the underlying deterministic mechanism can greatly improved system performance, predictability, and control.</p><p>Lapedes and Farber [ 11 were the first to explore the use of neural networks for the identification of a choatic time series. A two hidden layer neural network with the sigmoid transfer function in the individual nodes was used to identify the choatic time series generated from the Mackey-Glass differential equations [26]. The two hidden layer neural network using the generalized delta rule (GDR) has been proven successful as a tool to identify such systems. This architecture, together with the GDR, is then referred to as the "nonlinear signal processing method. " Other neurocomputing methods have been applied to the same problem. For example, the work of Moody [ 141 used a variation of the CMAC algorithm proposed by Albus <ref type="bibr">[27]</ref>.</p><p>In the following results, the same chaotic time series was used. The SONN with the SEC, and its heuristic variant, were used to obtain the approximate model of the system. The result is compared with those obtained by using the nonlinear signal processing method [ 11. The advantages and disadvantages of both approaches are analyzed in the next section. (15) dt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">+ X"(t -7 )</head><p>By setting a = 0.2, b = 0.1, and 7 = 17, a chaotic time series with a strange attractor of fractal dimension about 3.5 will be produced <ref type="bibr" target="#b20">[24]</ref>.</p><p>If the points on the attractor of dimension A are related by: then the necessary condition <ref type="bibr" target="#b23">[28]</ref> for f ( . ) to be a smooth mapping is:</p><formula xml:id="formula_13">d A I d M I 2 d ~ + 1 (<label>17</label></formula><formula xml:id="formula_14">)</formula><p>where P is the prediction time, dA is the dimension of the attractor, and dM is the embedding dimension of the dynamic system of (15). Takens' theorem guarantees the existence of f (.), but it does not provide a constructive method. The nonlinear signal processing method utilizes the GDR in a four layer feedfonvard neural network to estimate f ( . ). To facilitate the comparison between the SONN and the GDR, we chose the embedding dimension to be 4. This also specifies the four data points should be chosen as state variables: x ( t ) , x ( t -7 ) , x ( t -27), x ( t -37). The prediction time P is chosen according to the need for long-term or short-term prediction. In this example, P was chosen to be 6. To compare the accuracy of prediction, the normalized root-mean-square error (rmse) is used as a performance index:</p><p>(18) rmse Standard Deviation Normalized rmse =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Using the SONN Algorithm for the Time Series Identijication</head><p>The SONN algorithm is based on the idea that all nodes generated try to solve the problem by best tuning themselves and the structure to the system's response. The "quality" of such estimation is given directly by the SEC, and the "quality" of the moves between states by the difference between state SEC measures. Therefore, each and every terminal (node output) produces of itself the best possible estimate for the system given the present structure. The user can select any suboptimal stopping criterion for the algorithm to obtain an estimate of the system, or he/she can allow the algorithm to converge to its global optimum. Since the search space is very large, it is sometimes undesirable to spend the computing time for the optimal estimate. Therefore, by considering the computation resource available, the designer can stop the algorithm by accepting a feasible but suboptimal solution.</p><p>1. SONN with the SEC: In this subsection, the SEC is used as the criterion in the choice of the model for the chaotic time series, using the estimator of (10). Recall that this estimator is the upper bound of the number of parameters in the model. The resultant model tends to be an underestimate of the number of parameters of the real system. Since the SONN is basically a stochastic search algorithm, to compare the result of the SONN with the nonlinear signal processing method, 10 runs of the SONN were initiated. The averaged statistics of the performance parameter were computed. In the example of the chaotic time series, the first 100 points of the series were used as training data. One example of a nonlinear model which has been obtained by SONN for this series is a two layer network with five nodes (Fig. <ref type="figure">4</ref>). There are 15 weights in the model, 4 connections. The output of the network overlapped with the output of the system and is shown in Fig. <ref type="figure">5</ref> . The performance index over the next 400 points ( IOlst-500th) used as testing data is equal to 0.137. The averaged number of regressions (the corresponding number of epoches' is 64.1 ) used to obtain the model is 320.5. The standard deviation of the number of regression is 29.4. The SONN using the SEC as an estimator did not produce satisfactory results. The SEC produces severe constraints on the number of parameters the model can have (underestimation), therefore drastically limiting the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SONN with the Heuristic SEC (SONN-h):</head><p>In the following examples, a modified heuristic version of the SEC is used to try to overcome the limitations described before. The estimator of the number of parameters is switched from the expression (10) to the estimator of <ref type="bibr" target="#b1">(12)</ref>. To compare the performance, the same averaging method used in last section is adopted. The first run was initiated. Two suboptimal models were obtained during the search procedure. The same state SEC'S were then used as the criterion to choose the corresponding suboptimal models in the next several runs. The averaged number of the regressions (i.e., the epoches) was calculated. a. node 19: First, the SONN is allowed to generate up to the 19th accepted node. In this example, the first 100 points of the time series was used for training, and samples 101 through 400 used for prediction testing. The total number of weights in the network is 27. The performance index average 0.077. The output of the network is overlapped in Fig. <ref type="figure" target="#fig_9">6</ref> with the original time series, and the averaged number of the regressions spent was 225.8 (i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="45.16">epoches).</head><p>For comparison purposes, a GDR network with the structure used in [ l ] is trained for 6500 epoches. The training data consisted of the first 500 points of the time series, and the testing data from the 501st sample to the 832nd. The total number of weights is 165, and the final performance index is equal to 0.165. Notice that the GDR algorithm was trained with the steepest descent imple- mentation which is more easily converted to a parallel form than the conjugate gradient of Lapedes and Farber. The total computational time of both algorithms was restricted to be of the same order of magnitude running on the same machine, to give both algorithms similar computational resources. Fig. <ref type="figure">7</ref> shows the original time series overlapped with the GDR network output. b. node 37: The second model chosen was formed by the 37th accepted node. Remember that simulated annealing requires a nonzero probability of returning to a previously visited state, and therefore, in this case, withdrawing part of the structure. The network was trained in a similar manner to the first example, since it was actually part of the same run. The final number of weights was 40, the performance index was 0.018, and the average number of the regressions spent was 1765.8 (i.e., 353.13 epoches). Fig. <ref type="figure">8</ref> shows the output of the network with the original time series. Fig. <ref type="figure">9</ref> shows the GDR with 11 500 epoches. Notice that in both cases, the GDR network requires 150 connections and 150 weights, as compared to 12 connections and 27 weights for the first example and 10 connections and 40 weights for the second example. Fig. <ref type="figure" target="#fig_0">10</ref> shows the final state of the algorithm; some of the connections were omitted for clarity.</p><p>Another interesting aspect of the final state is that not all, but only 3 of the inputs contribute to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison of the GDR and SONN Results</head><p>In Fig. <ref type="figure" target="#fig_0">11</ref>, the performance indexes versus sample points for the 2 experiments with GDR and SONN are shown. Table <ref type="table" target="#tab_1">I</ref> summarizes the comparison data, as well as Fig. <ref type="figure" target="#fig_4">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ADVANTAGES AND DISADVANTAGES</head><p>In this section, the advantages and disadvantages of SONN as compared with GDR are outlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Advantages Over GDR</head><p>The SONN algorithm requires considerably less samples to acquire an estimate of the system. In this exercise, GDR was given five times more samples than SONN. GDR assumes that the order of the model is known through the size of the hidden units; SONN estimates the complexity of the model at run time. It constructs the con-   The algorithm produces estimates of the system model at every new node, permitting the user to tradeoff the accuracy of the model for learning time. The final model is more accurate, and at the same time less complex, and is dependent on fewer parameters. The prediction error revealed to be almost constant over a wide range of samples. The arbitrary connectivity, with the two-input function used here, can be easily extended to accommodate functions of an arbitrary number of variables. Furthermore, the connectivity pattern can be restricted by incorporating knowledge about the problem, such as neurosensorial spatiotemporal mappings present on biologic hearing systems, or biological visual systems. SONN can be used as an investigation tool to hypothesize the connectivity pattern in such cases.</p><p>Different sets of transfer functions can be mixed in the same network, with the choice subject to a problem dependent criterion. This allows for another level of a priori knowledge incorporation. The functions can have temporal behavior, or can be simple linear discrminant functions. This work can easily be extended for the use of SONN as a classifier. Although not tested yet, the set of quadratic functions used here should work quite well for classification tasks, since they are special cases of the Mahalanobis distance.</p><p>The SONN algorithm can be used in cases where the connectivity pattern between the input and the output is unknown, such as sensori-neuronal pathways. The algorithm can be restricted to apply a set of functions known to be present in the problem, and have the choice of new terminals restricted by high level knowledge. This same idea can be used to integrate symbolic knowledge about the problem to restrict the search with the numeric knowledge being developed during the search. The SONN algorithm could reason about its experiments on the model, with the choice of the experiment being guided by a priori knowledge, as opposed to only using the outcome of the experiment. In this case the search can be viewed as a symbol-numeric planning strategy. The algorithm can then be used to produce a symbolic model of the underlying system.</p><p>Overall the algorithm has low computational demands as compared to the GDR, and reduces the learning problem at each node to a simple linear regression, therefore avoiding local minima problems. At every stage, a suboptimal partial structure is generated, allowing for variations on the algorithm stopping criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Disadvantages Over GDR</head><p>The SONN learning algorithm, based on a stochastic search, is nondeterministic, and therefore the learning time cannot be known a priori. For the same reason, successive runs of the algorithm do not generate the same partial structures, but they do generate the same optimal final structure if the algorithm is allowed to run to the end. Similarly, the connectivity cannot be known a priori for the partial structures, which demands flexibility of communication for parallel hardware architectures. Two other drawbacks arise in conjunction with the use of simulated annealing. First, this stochastic search algorithm is known not to be very efficient searching very large spaces, the consequence here is the fact that the probability of choosing any given node decreases geometrically with the iteration or with the state size (number of terminals), and so does the probability of finding the optimal model. For very complex models with large dimensionality, a larger </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I09</head><p>search time is required, proportional to the increase in the state size. Although in practice, the results of using a stochastic search were promising from a resultant model and computational time viewpoint, investigation into other possible search strategies are being carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this study, we proposed a new approach for the identification problem based on a flexible, self-organizing neural network (SONN) structure. The variable structure provides the opportunity to search and construct the optimal model based on input-output observations. The hierarchical version of the MDL, called the structure estimation criteria, was used to guide the tradeoff between the model complexity and the accuracy of the estimation. The SONN approach demonstrates potential usefulness as a tool for system identification through the example of modeling a chaotic time series.</p><p>An alternative interpretation of the competition between the nodes in different layers is the feature selection process in the pattern recognition literature <ref type="bibr" target="#b24">[29]</ref>. The quadratic node functions are specializations of the Mahalanobis distance, and can easily be changed to accommodate other discriminant functions. Thus it is not unreasonable to treat the final approximated model as a classifier. Polynomial classifiers have been successfully used before <ref type="bibr" target="#b25">[30]</ref>.</p><p>Future work on SONN algorithm includes the development of a better estimator for the SEC, the use of multiple sets of functions, the generalization to functions of an arbitrary number of variables (arbitrary node branching), the use of high level symbolic knowledge about the problem domain to aid the search, and a connection to a symbolic module to analyze the hypotheses, and generate model closed form solutions for symbolic manipulation. Possible future applications include: use as a static pattern classifier, extensions of the functions to operate on time varying patterns, knowledge based restriction of the connectivity pattern to search for neurobiologically plausible nerve connectivity, naive and experimental physics reasoning (modeling building and hypothesis testing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>The proof of expression (10) is the upper bound of the number of parameters in the layered system is followed from the induction. First, we prove that the nodes in layer 1 satisfy the expression. Let the node XI in layer 1 receive the inputs x l , x 2 from layer 0, then I = 1, d = 2. The maximum number calculated according to expression (10) will be 6 . Since the highest order polynomial in prototype functions ( F ) is second order, the number of parameters is 6. Hence, the expression holds for nodes in the 1st layer. Suppose the expression (10) holds for the two nodes, xk, Xk,2, with highest order k l , k2 separately, in the kth layer, then k l , k2 must be smaller than 2k. Observe that there are four possible combinations of the vari-ables according to ( F ) . The highest order terms of these formulas are of the following three types: .I.:</p><p>('41 1 a I x 1x2.</p><p>('43)</p><p>All of these three formulas of the combination of variables will produce the polynomial of a degree no more than 2 k + l , since d of the expression (10) is the union of the variables from the nodes in kth layer. By the expression derived in <ref type="bibr">[7]</ref>, all the possible combinations of terms appearing in the resultant polynomial can be computed according the expression (10). Since the actual order of t&gt;e polynomial d of node xk + I is no more than 2k + I . The k computed by expression (10) will be the upper bound for the number of parameters in the system. Q.E.D.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The network structure used in the nonlinear signal processing method, including solid and dashed lines. The "bump" surface is formed by the substructure (nodes linked by solid lines only.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>I</head><label></label><figDesc>IEEE TRANSACTIONS ON NEURAL NETWORKS. VOL. I. NO. I. MARCH 1990</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s = {(XI? Y I L (XZ, Y2L * * 9 (XN, YN)) generated by where f ( . ) can be represented by a Kolmogorov-Garbor polynomial, and the noise vi, Gaussian distributed with zero mean and variance U ' . The dimension of output variable Y = [ y , , y 2 , --, y,] is m, and the dimension of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[</head><label></label><figDesc>16]. Let's define zl, z2, -* , zN as a sequence of observations from the random variable 2, which is characterized by probability density function p z ( 0 ) . The dominant term of the MDL in [12] is: MDL = -lOgp(z18) + 0.5 k l o g N (7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>I04Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of a simple 7 node model used to compute the upper bound of the number of parameters of the system &amp;, in the configuration of hierarchically connected nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. State i with four nodes-3 basic nodes (node 1 , node 2, node 3 ) , 2 front nodes (node 3, node 4)-State_SEC = min(node 3_SEC, node 4-SEC) = min(4, 5 ) = 4 . State; with five nodes-3 basic nodes (node 1, node 2, node 3 ) , 2 front nodes(node 4, node 5)--State_SEC = min(node4-SEC. node 5-SEC) = min(3, 5 ) = 3 . State transition from i to; occurs as node 5 is generated and accepted. State transition from;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>I06 I I</head><label>I</label><figDesc>IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. I. NO. I . MARCH 1990</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>'Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Tenth suboptimal model of the Mackey-Glass chaotic series generated by SONN with exact SEC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Plot of the chaotic time series generated by the system (solid line)and by suboptimal 19 node model using SONN-h (SONN with heuristic SEC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 . 4 Fig. 8 .1 4 7Fig. 9 .</head><label>74849</label><figDesc>Fig.7. Plot of chaotic time series generated by system (solid line) and by the nonlinear signal processing method with 6500 epoches (dashed line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .Fig. 11 .Fig. 12 .</head><label>101112</label><figDesc>Fig. 10. Evolution of the generation of nodes by SONN-h during the search process of approximate model of the system. Not all links are shown for clarity. The figure contains the 37th node and shows the entire configuration of state at that point. Not all nodes will be used on the final model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>TENOR10 A N D LEE: SELF-ORGANIZING NETWORK FOR OPTIMUM SUPERVISED LEARNING</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EPOCHES</head><label>I</label><figDesc></figDesc><table><row><cell cols="5">COMPARISON BETWEEN THE FIVE EXPERIMENTS: AVERAGE NUMBER OF</cell></row><row><cell cols="5">, CONNECTIONS, A N D WEIGHTS, A N D NORMALIZED RMSE</cell></row><row><cell></cell><cell cols="3">FOR THE FINAL MODEL</cell><cell></cell></row><row><cell cols="4">Algorithm Epoches Connections Weights</cell><cell>P e r f o m c e</cell></row><row><cell>SONN</cell><cell>320.5</cell><cell>4</cell><cell>34</cell><cell>0.137</cell></row><row><cell>GDR</cell><cell>6500</cell><cell>150</cell><cell>150</cell><cell>0.165</cell></row><row><cell>SONN-h</cell><cell>225.8</cell><cell>12</cell><cell>27</cell><cell>0.077</cell></row><row><cell>GDR</cell><cell>11500</cell><cell>150</cell><cell>150</cell><cell>0.038</cell></row><row><cell>SONN-h</cell><cell>1765.8</cell><cell>10</cell><cell>40</cell><cell>0.018 -_</cell></row><row><cell cols="5">ing better results for a given amount of computation. The</cell></row><row><cell cols="5">SONN algorithm is not subject to the difficulties encoun-</cell></row><row><cell cols="5">tered with local minima, nor with the initial set of weights</cell></row><row><cell cols="5">given to the network. The learning process does not de-</cell></row><row><cell cols="5">pend on ad hoc error assignment mechanisms. It is based</cell></row><row><cell cols="5">on global rather than local performance measures.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors wish to thank the editor and the reviewers for the helpful comments and possible research directions, and for the suggestions in making this manuscript more readable.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nonlinear signal processing using neural networks; Prediction and system modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farber</surname></persName>
		</author>
		<idno>TR LA-UR-87-2662</idno>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic solution of inverse problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Marroquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, M.I.T</title>
		<imprint>
			<date type="published" when="1985-09">Sept. 1985</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Steams</surname></persName>
		</author>
		<title level="m">Adaptive Signal Processing</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From circuit to system theory</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IRE</title>
		<meeting>IRE</meeting>
		<imprint>
			<date type="published" when="1962">1962</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="856" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">System identification-A survey</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Astrom</surname></persName>
		</author>
		<author>
			<persName><surname>Eykhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="123" to="162" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A-universal nonlinear filter, predictor and simulator which optimizes itself by a learning process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garbor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Inst. Elec. Eng</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="422" to="438" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A learning identification algorithm and its application to an environmental system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ivakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Duffy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Franklin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="226" to="240" />
			<date type="published" when="1967-12">Dec. 1967. Oct. 1971. 1975</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Sysr.. Man, Cybern.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential GMDH algorithm and its application to river flow prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sawarogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst.. Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="473" to="479" />
			<date type="published" when="1976-07">July 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Heuristics free group method of data . . ~. handling algorithm of generating optimal partial polynomials with application to air pollution predication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kondo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochatic relaxation, Gibbs distribution, and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intellig</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
	<note>Automatica</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast learning in multiresolution hierarchies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems I</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparing biases for minimal network construction with back-propagation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Informarion Processing Systems I</title>
		<imprint>
			<date type="published" when="1978">1989. 1980. 1978</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="465" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Consistent order estimation of autoregression processes by shortest description of data,&quot; in Analysis and Opfimization of Stochasric System</title>
	</analytic>
	<monogr>
		<title level="s">110 IEEE TRANSACTIONS ON NEURAL NETWORKS</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
		</imprint>
	</monogr>
	<note>NO. I , MARCH 1990 [ 161 J. Rissanen</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A universal prior for integers and estimation by minimum description length</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Statist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="416" to="431" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum entropy as a special case of the minimum description length criterion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="847" to="849" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Auromat. Conrr</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="722" />
			<date type="published" when="1974-12">Dec. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic data structure search by the maximum likehood</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Biomed</title>
		<meeting>Comput. Biomed</meeting>
		<imprint/>
	</monogr>
	<note>Suppl., 5th Hawaii Int. Con5 Syst</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inconsistency of AIC rule</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kashyap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aufomaf. Contr</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="997" to="998" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983-05">May 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis of simulating annealing for optomization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24rh Con5 Decision and Contr</title>
		<meeting>24rh Con5 Decision and Contr</meeting>
		<imprint>
			<date>Dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting chaotic time series</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sidorowich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="845" to="848" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometry from a time series</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Packard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Crutchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Oscillation and chaos in physiological 1271</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. S. Albus, Brain, Behavior, and Robotics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting strange attractor in turbulence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Mathematics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Rand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Young</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page">366</biblScope>
			<date type="published" when="1981">1981</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>Berlin, West Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Pattern Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Note on contrast measure and polynomial classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L C</forename><surname>Sanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Hinkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1988-03">Mar. 1988</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How neural networks work</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TR LA</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-organizing neural network for the identification problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F M</forename><surname>Tenorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems I</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Byte Books</publisher>
			<date type="published" when="1972">1980. 1972. 1985. 1980. 1977. 1981</date>
			<biblScope unit="volume">712</biblScope>
			<biblScope unit="page" from="197" to="287" />
		</imprint>
	</monogr>
	<note>control system</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">he led a product design group as the Director of Research and Development at C.S. Components and Electronic Systems in Brazil, and from 1982 to 1985 he was a Research Fellow for the National Research Council of Brazil. While completing his doctoral studies, he taught graduate level courses in artificial intelligence at USC, UCLA, and Rockwell International in Los Angeles. Currently, he is an Assistant Professor at the School of Electrical Engineering, Purdue University, where his primary research interests are parallel and distributed systems, artificial intelligence, and neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Manoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tenorio</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">E E</forename><surname>Sc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">degree from Colorado State University in 1983, and the Ph</title>
		<imprint>
			<date type="published" when="1979">1979. 1980</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Telecommunication, Brazil</orgName>
		</respStmt>
	</monogr>
	<note>D. degree in computer engineering from the University of Southern California in 1987. He is the organizer of the interdisciplinary faculty group at Purdue called the Special Interest Group in Neurocomputing (SIGN) and heads the Parallel Distributed StNCtures Laboratory (PDSL) in the School of Electrical Engineering</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
