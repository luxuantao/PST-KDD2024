<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupling the Depth and Scope of Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
							<email>zengh@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
							<email>yxia@fb.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
							<email>ajiteshs@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
							<email>amalevich@fb.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
							<email>rajgopal.kannan.civ@mail.mil</email>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
							<email>prasanna@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
							<email>longjin@fb.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ren</forename><surname>Chen</surname></persName>
							<email>renchen@fb.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupling the Depth and Scope of Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art Graph Neural Networks (GNNs) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the scope (i.e., receptive field). Beyond just a few layers, two fundamental challenges emerge: 1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. We propose a design principle to decouple the depth and scope of GNNs -to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local neighborhood into informative representation rather than oversmoothing the global graph into "white noise". Theoretically, decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE) and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, our design achieves state-of-the-art accuracy with orders of magnitude reduction in computation and hardware cost.</p><p>• Expressivity challenge (i.e., oversmoothing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16]</ref>): iterative mixing of neighbor features collapses embedding vectors of different nodes into a fixed, low-dimensional subspace.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have now become the state-of-the-art models for graph mining <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b56">57]</ref>, facilitating applications such as social recommendation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b35">36]</ref>, knowledge understanding <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b57">58]</ref> and drug discovery <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b30">31]</ref>. With the numerous architectures proposed <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>, it still remains an open question how to effectively scale up GNNs with respect to both the model size and graph size. There are two fundamental obstacles when we increase the number of GNN layers:</p><p>• Scalability challenge (i.e., neighbor explosion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b53">54]</ref>): recursive expansion of multi-hop neighborhood results in exponentially growing receptive field size (and thus computation cost).</p><p>To address the expressivity challenge, most remedies focus on neural architecture exploration: <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b27">28]</ref> propose more expressive aggregation functions when propagating neighbor features. <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30]</ref> use residue-style design components to construct flexible and dynamic receptive fields. Among them, <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17]</ref> use skip-connection across multiple GNN layers, and <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30]</ref> encourage multi-hop message passing within each single layer. As for the scalability challenge, sampling methods have been explored to improve the training speed and efficiency. Importance based layer-wise sampling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b59">60]</ref> and subgraph-based sampling <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b53">54]</ref> alleviate neighbor explosion, while preserving training accuracy. Unfortunately, such sampling methods cannot be naturally generalized to inference without accuracy loss (see also <ref type="bibr">Section 4)</ref>.</p><p>The above lines of research have only guided us to partial solutions. Yet what is the root cause of both the expressivity and scalability challenges? Setting aside the design of GNN architectures or sampling schemes, we provide an alternative perspective by interpretting the data in a different way.</p><p>Two views on the graph. Given an input graph G with node set V, the most straightforward way to understand G is by viewing it as a single global graph. So any two nodes u and v belong to the same G, and if u and v lie in the same connected component, they will ultimately see each other in their own neighborhood no matter how far away u and v are. Alternative to the above global view, we can take a local view on G. For each node v, there is a latent G <ref type="bibr">[v]</ref> surrounding it which captures the characteristics of just v itself. The full G is observed (by the data collection process) as the union of all such G <ref type="bibr">[v]</ref> . Consequently, V [v] rather than V defines v's neighborhood: if u ∈ V [v] , v will never consider u as a neighbor. Our "decoupling" design is based on the local view.</p><p>Scope of GNNs. Both the expressivity and scalability challenges are closely related to the enlargement of the GNN's scope (i.e., receptive field). More importantly, how we define the scope is determined by how we view G. With the global view above, an L-layer GNN has the scope of the full L-hop neighborhood. With the local view, the GNN scope is simply V [v] regardless of the GNN depth. The two existing lines of research, one on architectural exploration and the other on sampling, both take the global view. <ref type="foot" target="#foot_0">1</ref> Consequently, the depth (i.e., number of layers) and scope of such GNNs are tightly coupled. Such coupling significantly limits the design space exploration of GNNs with various depths <ref type="bibr" target="#b51">[52]</ref>. Consider the example of ogbn-products, a medium-scale graph in Open Graph Benchmark <ref type="bibr" target="#b14">[15]</ref>. The average number of 4-hop neighbors is around 0.6M, corresponding to 25% of the full graph size. To generate representation of a single target node, a 4-layer coupled GNN needs to propagate features from the 0.6M neighbors. Such propagation can be inefficient or even harmful since most nodes in the huge neighborhood would be barely relevant to the target node.</p><p>Decoupling the GNN depth and scope. Taking the local view on G, we propose a general design principle to decouple the GNN depth and scope. To generate the representation of the target node v, we first extract from G a small subgraph</p><formula xml:id="formula_0">G [v] surrounding v. On top of G [v]</formula><p>, we apply a GNN whose number of layers and message passing functions can be flexibly chosen. "Decoupling" means we treat the scope extraction function and GNN depth as two independently tuned parameters -effectively we introduce a new dimension in the GNN design space. We intuitively illustrate the benefits of decoupling by an example GNN construction, where the scope is the L-hop neighborhood and depth is L (L &gt; L). When we use more layers (L ) than hops (L), each pair of subgraph nodes may exchange messages multiple times. The extra message passing helps the GNN better absorb and embed the information within scope, and thus leads to higher expressivity. We further justify the above intuition with multifaceted theoretical analysis. From the graph signal processing perspective, we prove that decoupled-GCN performs local-smoothing rather than oversmoothing, as long as the scopes of different target nodes are different. From the function approximation perspective, we construct a linear target function on neighbor features and show that decoupling the GraphSAGE model reduces the function approximation error. From the topological learning perspective, we apply deep GIN-style message passing to differentiate non-regular subgraphs of a regular graph. As a result, our model is more powerful than the 1-dimensional Weisfeiler-Lehman test <ref type="bibr" target="#b39">[40]</ref>.</p><p>Practical implementation: SHADOW-GNN. The decoupling principle leads to a practical implementation, SHADOW-GNN: Decoupled GNN on a shallow subgraph. In SHADOW-GNN, the scope is a shallow yet informative subgraph, only containing a fraction of the 2-or 3-hop neighbors of G (see <ref type="bibr">Section 5)</ref>. On the other hand, the model of SHADOW-GNN is deeper (e.g., L = 5).</p><p>To efficiently construct the shallow scope on commodity hardware, we propose various subgraph extraction functions. To better utilize the subgraph node embeddings after deep message passing, we propose neural architecture extensions such as pooling and ensemble. Empirically, our "decoupling" design improves both the accuracy and scalability. On seven benchmarks (including the largest ogbn-papers100M graph with 111M nodes) and across two graph learning tasks, SHADOW-GNNs achieve significant accuracy gains compared to the original models. Meanwhile, the computation and hardware costs are reduced by orders of magnitude. Our code is available at https://github.com/facebookresearch/shaDow_GNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let G (V, E, X) be an undirected graph, with node set V, edge set E ⊆ V × V and node feature matrix</p><formula xml:id="formula_1">X ∈ R |V|×d . Let N v denote the set of v's direct neighbors in G.</formula><p>The u th row of X corresponds to the length-d feature of node u. Let A be the adjacency matrix of G where</p><formula xml:id="formula_2">A u,v = 1 if edge (u, v) ∈ E and A u,v = 0 otherwise. Let D be the diagonal degree matrix of A. Denote A = D − 1 2 * A * D − 1 2 *</formula><p>as the adjacency matrix after symmetric normalization (" * " means augmented with self-edges), and</p><formula xml:id="formula_3">A = D −1 A (or D −1 * A * )</formula><p>as the one after random walk normalization. Let subscript "[u]" mark the quantities corresponding to a subgraph surrounding node u. For example, the subgraph itself is G</p><formula xml:id="formula_4">[u] . Subgraph matrices X [v] and A [v]</formula><p>have the same dimension as the original X and A. Yet, row vector</p><formula xml:id="formula_5">X [v] u = 0 for u ∈ V [v] . Element A [v] u,w = 0 if either u ∈ V [v] or w ∈ V [v] .</formula><p>For an L-layer GNN, let superscript "( )" denote the layer-quantities. Let d ( ) be the number of channels for layer ;</p><formula xml:id="formula_6">H ( −1) ∈ R |V|×d ( −1)</formula><p>and H ( ) ∈ R |V|×d ( ) be the input and output features. So H (0) = X and d (0) = d. Further, let σ be the activation and W ( ) be the learnable weight. For example, a GCN layer performs H ( ) = σ AH ( −1) W ( ) . A GraphSAGE layer performs</p><formula xml:id="formula_7">H ( ) = σ H ( −1) W ( ) 1 + AH ( −1) W ( ) 2 .</formula><p>Our analysis in Section 3 mostly focuses on the node classification task. Yet our design can be generalized to the link prediction task, as demonstrated by our experiments in Section 5. Definition 2.1.</p><formula xml:id="formula_8">(Depth of subgraph) Assume the subgraph G [v] is connected. The depth of G [v] is defined as max u∈V [v] d (u, v)</formula><p>, where d (u, v) denotes the shortest path distance from u to v.</p><p>The above definition enables us to make comparison such as "the GNN is deeper than the subgraph". For "decoupling the depth and scope", we refer to the model depth rather than the subgraph depth.</p><p>3 Decoupling the Depth and Scope of GNNs "Decoupling the depth and scope of GNNs" is a design principle to improve the expressivity and scalability of GNNs without modifying the layer architecture. We name a GNN after decoupling a SHADOW-GNN (see Section 3.6 for explanation of the name). Compared with a normal GNN, SHADOW-GNN contains an additional component: the subgraph extractor EXTRACT. To generate embedding of a target node v, SHADOW-GNN proceeds as follows: 1. We use EXTRACT (v, G) to return a connected G <ref type="bibr">[v]</ref> , where G [v] is a subgraph containing v, and the depth of</p><formula xml:id="formula_9">G [v] is L. 2. We build an L -layer GNN on G [v] by treating G [v]</formula><p>as the new full graph and by ignoring all nodes / edges not in G</p><formula xml:id="formula_10">[v] . So G [v]</formula><p>is the scope of SHADOW-GNN. The key point reflecting "decoupling" is that L &gt; L.</p><p>A normal GNN is closely related to a SHADOW-GNN. Under the normal setup, an L-layer GNN operates on the full G and propagates the influence from all the neighbors up to L hops away from v. Such a GNN is equivalent to a model where EXTRACT returns the full L-hop subgraph and L = L.</p><p>Using three popular GNN architectures as representatives, we demonstrate how SHADOW-GNN improves expressivity from three different angles. On SHADOW-GCN (Section 3.1), we come from the graph signal processing perspective. The GCN propagation can be interpreted as applying filtering on the node signals <ref type="bibr" target="#b45">[46]</ref>. Deep models correspond to high-pass filters. Applying such filtering on the local graph G [v] preserves much richer information than applying on the global G. On SHADOW-SAGE (Section 3.2), we view the GNN as a function approximator. We study how decoupling the GNN depth reduces the approximation error on a target function. On SHADOW-GIN (Section 3.3), we focus on learning topological information. We show that decoupling helps capture local graph structure which the powerful 1-dimensional Weisfeiler-Lehman test fails to capture.</p><p>We discuss the full architecture components of SHADOW-GNN in Sections 3.4 and 3.5, and present the practical implementation in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expressivity Analysis on SHADOW-GCN: Graph Signal Processing Perspective</head><p>GCNs <ref type="bibr" target="#b20">[21]</ref> suffer from "oversmoothing" <ref type="bibr" target="#b28">[29]</ref> -Each GCN layer smooths the features of the direct (i.e., 1-hop) neighbors, and many GCN layers smooths the features of the full graph. Eventually, such repeated smoothing process propagates to any target node just the averaged feature of all V. "Oversmoothing" thus incurs significant information loss by wiping out all local information. Formally, suppose the original features X reside in a high-dimensional space R |V|×d . Oversmoothing pushes X towards a low-dimensional subspace R |V|×d , where d &lt; d. Corresponding analysis comes from two perspectives: oversmoothing by a deep GCN, and oversmoothing by repeated GCNstyle propagation. The former considers the full neural network with non-linear activation, weight and bias. The later characterizes the aggregation matrix M = lim L→∞ A L X. It is shown that even with the vanilla architecture, a deep GCN with bias parameters does not oversmooth <ref type="bibr" target="#b15">[16]</ref>. In addition, various tricks <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33]</ref> can prevent oversmoothing from the neural network perspective. However, a deep GCN still suffers from accuracy drop, indicating that the GCN-style propagation (rather than other GCN components like activation and bias) may be the fundamental reason causing difficulty in learning. Therefore, we study the asymptotic behavior of the aggregation matrix M under the normal and SHADOW design. In other words, here in Section 3.1, we ignore the non-linear activation and bias parameters. Such setup is consistent with many existing literature such as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b58">59</ref>]. Proposition 3.1. ∞ number of feature propagation by SHADOW-GCN leads to</p><formula xml:id="formula_11">m [v] = e [v] v • e T [v] X [v]<label>(1)</label></formula><p>where e [v] is defined by e</p><formula xml:id="formula_12">[v] u = δ [v] (u) w∈V [v] δ [v] (w) ; δ [v] (u) returns the degree of u in G [v] plus 1.</formula><p>Oversmoothing by normal GCN propagation. With a large enough L, the full L-hop neighborhood becomes V (assuming connected G). So ∀ u, v, we have</p><formula xml:id="formula_13">G [u] = G [v] = G, implying e [u] = e [v]</formula><p>and X </p><formula xml:id="formula_14">[u] = X [v] = X. From Proposition 3.</formula><formula xml:id="formula_15">[v] = φ G (v) • m [v]</formula><p>where φ G is any non-zero function only depending on the structural property of v.</p><formula xml:id="formula_16">Let M = {m [v] | v ∈ V}. Given G, EXTRACT and some continuous probability distribution in R |V|×d to generate X, then m [v] = m [u] if V [u] = V [v] , almost surely. Corollary 3.2.1. Consider EXTRACT 1 , where ∀v ∈ V, V [v] ≤ n. Then |M| ≥ |V| n a.s. Corollary 3.2.2. Consider EXTRACT 2 , where ∀ u, v ∈ V, V [v] = V [u] . Then |M| = |V| a.s.</formula><p>Theorem 3.2 proves SHADOW-GCN does not oversmooth: 1. A normal GCN pushes the aggregation of same-degree nodes to the same point, while SHADOW-GCN with EXTRACT 2 ensures any two nodes (even with the same degree) have different aggregation. 2. A normal GCN wipes out all information in X after many times of aggregation, while SHADOW-GCN always preserves feature information. Particularly, with φ </p><formula xml:id="formula_17">G (v) = δ [v] (v) −1/2 ,</formula><formula xml:id="formula_18">( ) v = σ W ( ) 1 T h ( −1) v + W ( )<label>2</label></formula><formula xml:id="formula_19">T 1 |Nv| u∈Nv h ( −1) u .</formula><p>We can prove Point 1 by making an L -layer SHADOW-SAGE identical to an L-layer GraphSAGE with the following steps: 1. let EXTRACT return the full L-hop neighborhood, and 2. set W ( )</p><formula xml:id="formula_20">1 = I, W ( ) 2 = 0 for L + 1 ≤ ≤ L . For point 2, we consider a target function: τ X, G [v] = C • u∈V [v] δ [v] (u) • x u for some neighborhood G [v] , scaling constant C and δ [v] (u) as defined in Proposition 3.</formula><p>1. An expressive model should be able to learn well this simple linear function τ . GraphSAGE cannot learn τ accurately, while SHADOW-SAGE can. We first show the GraphSAGE case. Let the depth of G [v] be L. Firstly, we need GraphSAGE to perform message passing for exactly L times (where such a model can be implemented by, e.g., L layers or L layers with = I for all layers, and 2. either remove the non-linear activation or bypass ReLU by shifting X with bias. With known results in Markov chain convergence theorem <ref type="bibr" target="#b25">[26]</ref>, we derive the following theorem by analyzing the convergence of A L</p><formula xml:id="formula_21">W 2 = 0 for L − L layers). Otherwise, the extra L − L message passings will propagate influence from nodes v ∈ V [v] , violating the condition that τ is independent of v . Next, suppose GraphSAGE can learn a function ζ such that on some G [v] , we have ζ G [v] = τ G [v] . We construct another G [v]</formula><p>[v] when L → ∞. Theorem 3.3. SHADOW-SAGE can approximate τ with error decaying exponentially with depth.</p><p>We have the following conclusions from above: 1. SHADOW-SAGE is more expressive than Graph-SAGE. 2. appropriate EXTRACT function improves SHADOW-GNN expressivity, 3. There exists cases where it may be desirable to set the SHADOW-GNN depth much larger than the subgraph depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expressivity Analysis on SHADOW-GIN: Topological Learning Perspective</head><p>While GCN and GraphSAGE are popular architectures in practice, they are not the theoretically most discriminative ones. The work in <ref type="bibr" target="#b47">[48]</ref> establishes the relation in discriminativeness between GNNs and 1-dimensional Weisfeiler-Lehman test (i.e., 1-WL). And GIN <ref type="bibr" target="#b47">[48]</ref> is an example architecture achieving the same discriminativeness as 1-WL. We show that applying the decoupling principle can further improve the discriminativeness of such GNNs, making them more powerful than 1-WL.</p><p>Recall that 1-WL is a graph isomorphism test aiming at distinguishing graphs of different structures. A GNN as expressive as 1-WL thus well captures the topological property of the target node. While 1-WL is already very powerful, it may still fail in some cases. e.g., it cannot distinguish certain nonisomorphic, regular graphs. To understand why SHADOW-GNN works, we first need to understand why 1-WL fails. In a regular graph, all nodes have the same degree, and thus the "regular" property describes a global topological symmetry among nodes. Unfortunately, 1-WL (and the corresponding normal GNN) also operates globally on G. So intuitively, on two different regular graphs, there is no way for 1-WL (and the normal GNN) to assign different labels by breaking such symmetry.</p><p>On the other hand, SHADOW-GNN can break such symmetry by applying decoupling. In Section 1, we have discussed how SHADOW-GNN is built from the local perspective on the full graph. The key property benefiting SHADOW-GNN is that a subgraph of a regular graph may not be regular.</p><formula xml:id="formula_22">u v G u G 1 [u] G 1 [v]</formula><p>v Figure <ref type="figure">1</ref>: Example 3-regular graph and the 1-hop subgraphs of the target nodes Thus, SHADOW-GNN can distinguish nodes in a regular graph with the non-regular subgraphs as the scope. We illustrate the intuition with the example in Figure <ref type="figure">1</ref>. The graph G is 3-regular and we assume all nodes have identical features. Our goal is to discriminate nodes u and v since their neighborhood structures are different. No matter how many iterations 1-WL runs, or how many layers the normal GNN has, they cannot distinguish u and v. On the other hand, a SHADOW-GNN with 1-hop EXTRACT and at least 2 layers can discriminate u and v.</p><p>Theorem 3.4. Consider GNNs whose layer function is defined by</p><formula xml:id="formula_23">h ( ) v = f ( ) 1 h ( −1) v , u∈Nv f ( ) 2 h ( −1) v , h ( −1) u ,<label>(2)</label></formula><p>where f</p><formula xml:id="formula_24">( ) 1 and f ( )</formula><p>2 are the update and message functions of layer-, implemented as MLPs. Then, such SHADOW-GNN is more discriminative than the 1-dimensional Weisfeiler-Lehman test.</p><p>The theorem also implies that SHADOW-GIN is more discriminative than a normal GIN due to the correspondence between GIN and 1-WL. See Appendix A for the proof of all theorems in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Subgraph Extraction Algorithms</head><p>Our decoupling principle does not rely on specific subgraph extraction algorithms. Appropriate EXTRACT can be customized given the characteristic of G, and different EXTRACT leads to different implementation of our decoupling principle. In general, we summarize three approaches to design EXTRACT: 1. heuristic based, where we pick graph metrics that reflect neighbor importance and then design EXTRACT by such metrics; 2. model based, where we assume a generation process on G and set EXTRACT as the reverse process, and 3. learning based, where we integrate the design of EXTRACT as part of the GNN training. In the following, we present several examples on heuristic based EXTRACT, which we also empirically evaluate in Section 5. We leave detailed evaluation on the model based and learning based EXTRACT as future work. See also Appendix C for details.</p><p>Example heuristic based EXTRACT. The algorithm is derived from the selected graph metrics. For example, with the metric being shortest path distance, we design a L-hop extractor. i.e., EXTRACT returns the full set or randomly selected subset of the target node's L-hop neighbors in G. Picking the random walk landing probability as the metric, we can design a PPR-based extractor. i.e., we first run the Personalized PageRank (PPR) algorithm on G to derive the PPR score of other nodes relative to the target node. Then EXTRACT define V [v] by picking the top-K nodes with the highest PPR scores. The subgraph</p><formula xml:id="formula_25">G [v] is the node-induced subgraph 2 of G from V [v]</formula><p>. One can easily extend this approach by using other metrics such as Katz index <ref type="bibr" target="#b18">[19]</ref>, SimRank <ref type="bibr" target="#b17">[18]</ref> and feature similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Architecture</head><p>Subgraph pooling. For a normal GNN performing node classification, the multi-layer message passing follows a "tree structure". The nodes at level L of the tree correspond to the L-hop neighborhood. And the tree root outputs the final embedding of the target node. Thus, there is no way to apply subgraph pooling or READOUT on the final layer output, since the "pool" only contains a single vector. For a SHADOW-GNN, since we decouple the L th layer from the L-hop neighborhood, it is natural to let each layer (including the final layer) output embeddings for all subgraph nodes. This leads to the design to READOUT all the subgraph node embeddings as the target node embedding.</p><p>We can understand the pooling for SHADOW-GNN from another perspective. In a normal GNN, the target node at the final layer receives messages from all neighbors, but two neighbor nodes may not have a chance to exchange any message to each other (e.g., two nodes L-hop away from the target may be 2L-hop away from each other). In our design, a SHADOW-GNN can pass messages between any pair of neighbors when the model depth is large enough. Therefore, all the subgraph node embeddings at the final layer capture meaningful information of the neighborhood. In summary, the power of the decoupling principle lies in that it establishes the connection between the node-/ link-level task and the graph-level task. e.g., to classify a node is seen as to classify the subgraph surrounding the node. From the neural architecture perspective, we can apply any subgraph pooling / READOUT operation originally designed for graph classification (e.g., <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4]</ref>) to enhance the node classification / link prediction of SHADOW-GNN. In particular, in the vanilla SHADOW-GNN, we can implement a trivial READOUT as "discarding all neighbor embeddings", corresponding to performing center pooling. See Appendix D and F.3 for algorithm and experiments.</p><p>Subgraph ensemble. It may be challenging in practice to design a single EXTRACT capturing all meaningful characteristics of the neighborhood. We can use multiple EXTRACT to jointly define the receptive field, and then ensemble multiple SHADOW-GNN at the subgraph level. Consider R candidates {EXTRACT i }, each returning G i</p><p>[v] . To generate v's embedding, we first use R branches of L -layer GNN to obtain intermediate embeddings for each G i v , and then aggregate the R embeddings by some learnable function g. In practice, we design g as an attention based aggregation function (see Appendix D.2). Subgraph ensemble is useful both when {EXTRACT i } consists of different algorithms and when each EXTRACT i performs the same algorithm under different parameters.</p><p>CASE STUDY Consider PPR-based EXTRACT i with different threshold θ i on the neighbor PPR score. A SHADOW-GNN-ensemble can approximate PPRGo <ref type="bibr" target="#b4">[5]</ref>. PPRGo generates embedding as:</p><formula xml:id="formula_26">ξ v = u∈V [v] π u h v , where π u is u's PPR score and h v = MLP (x v ). We can partition V [v] = R i=1 V i [v] s.t. nodes in V i [v]</formula><p>have similar PPR scores denoted by π i , and</p><formula xml:id="formula_27">π i ≤ π i+1 . So ξ v = R i=1 ρ i u∈V i h u , where ρ i = π i − j&lt;i π j and V i = R k=i V k [v]</formula><p>. Now for each branch of SHADOW-GNN-ensemble, let parameter θ i = π i so that EXTRACT i returns V i . The GNN on V i can then learn u∈V i h u (e.g., by a simple "mean" READOUT). Finally, set the ensemble weight as ρ i . SHADOW-GNN-ensemble learns ξ v . As EXTRACT also preserves graph topology, our model can be more expressive than PPRGo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Practical Design: SHADOW-GNN</head><p>We now discuss the practical implementation of decoupled GNN -SHADOW-GNN. As the name suggests, in SHADOW-GNN, the scope is a shallow subgraph (i.e., with depth often set to 2 or 3).</p><p>In many realistic scenarios (e.g., citation networks, social networks, product recommendation graphs), a shallow neighborhood is both necessary and sufficient for the GNN to learn well. On "sufficiency", we consider the social network example: the friend of a friend of a friend may share little commonality with you, and close friends may be at most 2 hops away. Formally, by the γ-decaying theorem <ref type="bibr" target="#b54">[55]</ref>, a shallow neighborhood is sufficient to accurately estimate various graph metrics. On "necessity", since the neighborhood size may grow exponentially with hops, a deep neighborhood would be dominated by nodes irrelevant to the target. The corresponding GNN would first need to differentiate the many useless nodes from the very few useful ones, before it can extract meaningful features from the useful nodes. Finally, a shallow subgraph ensures scalability by avoiding "neighborhood explosion".</p><p>Remark on decoupling. So far we have defined a decoupled model as having the model depth L larger than the subgraph depth L. Strictly speaking, a decoupled model also admits L = L. For example, suppose in the full L-hop neighborhood, there are 70% nodes L hops away. Applying decoupling, the EXTRACT excludes most of the L-hop neighbors, and the resulting subgraph G [v] contains only 20% nodes L hops away. Then it is reasonable to consider an L-layer model on such a depth-L subgraph as also a decouple model. Compared with an L-layer model on the full L-hop neighborhood, an L-layer model on such a depth-L G <ref type="bibr">[v]</ref> propagates much less information from nodes L hops away. So the L message passings are indeed decoupled from the full L-hop neighborhood.</p><p>Remark on neighborhood. The "sufficiency" and "necessity" in shallow neighborhood are not universal. In many other applications, long-range dependencies can be critical, as studied in <ref type="bibr" target="#b1">[2]</ref>. In such cases, our practical implementation of SHADOW-GNN would incur accuracy loss. However, our decoupling principle in general may still be beneficial -"shallow subgraph" is a practical guideline rather than a theoretical requirement. We leave the study on such applications as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Deep GNNs.</p><p>To improve the GNN performance while increasing the model depth, various layer architectures have been proposed. AS-GCN <ref type="bibr" target="#b16">[17]</ref>, DeepGCN <ref type="bibr" target="#b26">[27]</ref>, JK-net <ref type="bibr" target="#b48">[49]</ref>, MixHop <ref type="bibr" target="#b0">[1]</ref>, Snowball <ref type="bibr" target="#b31">[32]</ref>, DAGNN <ref type="bibr" target="#b29">[30]</ref> and GCNII <ref type="bibr" target="#b32">[33]</ref> all include some variants of residue connection, either across multiple layers or within a single layer. In principle, such architectures can also benefit the feature propagation of a deep SHADOW-GNN, since their design does not rely on a specific neighborhood (e.g., L-hop). In addition to architectures, DropEdge <ref type="bibr" target="#b37">[38]</ref> and Bayesian-GDC <ref type="bibr" target="#b12">[13]</ref> propose regularization techniques by adapting dropout <ref type="bibr" target="#b40">[41]</ref> to graphs. Such techniques are only applied during training, and inference may still suffer from issues such as oversmoothing.</p><p>Sampling based methods. Neighbor or subgraph sampling techniques have been proposed to improve training efficiency. FastGCN <ref type="bibr" target="#b6">[7]</ref>, VR-GCN <ref type="bibr" target="#b5">[6]</ref>, AS-GCN <ref type="bibr" target="#b16">[17]</ref>, LADIES <ref type="bibr" target="#b59">[60]</ref> and MVS-GNN <ref type="bibr" target="#b8">[9]</ref> sample neighbor nodes per GNN layer. Cluster-GCN <ref type="bibr" target="#b7">[8]</ref> and GraphSAINT <ref type="bibr" target="#b53">[54]</ref> sample a subgraph as the training minibatch. While sampling also changes the receptive field, all the above methods are fundamentally different from ours. The training samplers aim at estimating the quantities related to the full graph (e.g., the aggregation of the full L-hop neighborhood), and so the inference model still operates on the full neighborhood to avoid accuracy loss. For SHADOW-GNN, since the decoupling principle is derived from a local view on G, our EXTRACT does not estimate any full neighborhood quantities. Consequently, the sampling based methods only improve the training efficiency, while SHADOW-GNN addresses the computation challenge for both training and inference.</p><p>Re-defining the neighborhood. Various works reconstruct the original graph and apply the GNN on the re-defined neighborhood. GDC <ref type="bibr" target="#b21">[22]</ref> views the reconstructed adjacency matrix as the diffusion matrix. SIGN <ref type="bibr" target="#b9">[10]</ref> applies reconstruction for customized graph operators. AM-GCN <ref type="bibr" target="#b44">[45]</ref> utilizes the reconstruction to separate feature and structure information. The above work re-define the neighborhood. However, they still have tightly coupled depth and scope. SHADOW-GNN can also work with the reconstructed graph G by simply applying EXTRACT on G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Setup. We evaluate SHADOW-GNN on seven graphs. Six of them are for the node classification task: Flickr <ref type="bibr" target="#b53">[54]</ref>, Reddit <ref type="bibr" target="#b10">[11]</ref>, Yelp <ref type="bibr" target="#b53">[54]</ref>, ogbn-arxiv, ogbn-products and ogbn-papers100M <ref type="bibr" target="#b14">[15]</ref>. The remaining is for the link prediction task: ogbl-collab <ref type="bibr" target="#b14">[15]</ref>. The sizes of the seven graphs range from 9K nodes (Flickr) to 110M nodes (ogbn-papers100M). Flickr, Reddit and Yelp are under the inductive setting. ogbn-arxiv, ogbn-products and ogbn-papers100M are transductive. Consistent with the original papers, for the graphs on node classification, we measure "F1-micro" score for Yelp and "accuracy" for the remaining five graphs. For the link prediction task, we use "Hits@50" as the metric. See Appendix E.1 for details.</p><p>We construct SHADOW with six backbone models: GCN <ref type="bibr" target="#b20">[21]</ref>, GraphSAGE <ref type="bibr" target="#b10">[11]</ref>, GAT <ref type="bibr" target="#b42">[43]</ref>, JK-Net <ref type="bibr" target="#b48">[49]</ref>, GIN <ref type="bibr" target="#b47">[48]</ref>, SGC <ref type="bibr" target="#b45">[46]</ref>. The first five are representatives of the state-of-the-art GNN architectures, jointly covering various message aggregation functions as well as the skip connection design. SGC simplifies normal GCN by moving all the neighbor propagation to the pre-processing step. Therefore, SGC is suitable for evaluating oversmoothing. The non-SHADOW models are trained with both full-batch and GraphSAINT-minibatch <ref type="bibr" target="#b53">[54]</ref>. Due to the massive size of the full L-hop neighborhood, we need to perform sampling when training normal GNNs in order to make the computation time tractable. GraphSAINT is suitable for our purpose since 1. it is the state-of-the-art minibatch method which achieves high accuracy, and 2. it supports various GNN architectures and scales well to large graphs. On the other hand, for SHADOW-GNN, both training and inference are always executed in minibatches. One advantage of SHADOW-GNN is that the decoupling enables straightforward minibatch construction: each target just independently extracts the small subgraph on its own. SHADOW-GNN neighborhood.</p><p>For both normal and SHADOW GNNs, Figure <ref type="figure">2</ref> shows on average how many neighbors are L hops away from the target. For a normal GNN, the size of the neighborhood grows rapidly with respect to L, and the nodes 4 hops away dominate the neighborhood. For SHADOW-GNN using the Table <ref type="table" target="#tab_2">1</ref> EXTRACT, most neighbors concentrate within 2 hops. A small number of nodes are 3 hops away. Almost no nodes are 4 or more hops away. Importantly, not only does the composition of the two kinds of neighborhood differ significantly, but also the size of SHADOW-GNN scope is much smaller (see also Table <ref type="table" target="#tab_2">1</ref>). Such small subgraphs are essential to high computation efficiency. Finally, we can ignore the very few distant neighbors (L ≥ 4), and regard the (effective) depth of SHADOW-GNN subgraph as L = 2 (or at most 3). Under such practical value of L, a model with L = 3 or 5 is indeed a SHADOW-GNN (see also the remark in Section 3.6).</p><p>Comparison with baselines. Table <ref type="table" target="#tab_2">1</ref> shows the performance comparison of SHADOW-GNN with the normal GNNs. All models on all datasets have uniform hidden dimension of 256. We define the metric "inference cost" as the average amount of computation to generate prediction for one test node. Inference cost is a measure of computation complexity (see Appendix B for the calculation) and is independent of hardware / implementation factors such as parallelization strategy, batch processing, etc. For SHADOW-GNN, we do not include the overhead on computing EXTRACT, since it is hard to calculate such cost analytically. Empirically, subgraph extraction is much cheaper than model computation (see Figure <ref type="figure">8</ref>, 9 for time evaluation on CPU and GPU). During training, we apply DropEdge <ref type="bibr" target="#b37">[38]</ref> to both the baseline and SHADOW models. DropEdge helps improve the baseline accuracy by alleviating oversmoothing, and benefits SHADOW-GNN due to its regularization effects. See Appendix F.2 for results on other architectures including GIN and JK-Net.</p><p>ACCURACY SHADOW-GNNs (with all neighborhood containing less than 200 nodes) achieve significantly higher accuracy in almost all cases. Since we use the identical backbone architectures, the accuracy gains validate our "decoupling" principle. For 3 layers, SHADOW-GNNs are at least as good as, and sometimes better than the baseline. This indicates that a shallow neighborhood contains sufficient information, and customizing the scope benefits GNN accuracy (from Figure <ref type="figure">2</ref>, a depth-3 G [v] differs significantly from the 3-hop neighborhood). Increasing to 5 layers, for normal GNNs, a deeper model sometimes results in accuracy degradation (even after DropEdge). For SHADOW-GNN, increasing the model depth is beneficial in many cases. Finally, the higher accuracy of the PPR EXTRACT compared with the 2-hop one demonstrates the importance of the extraction algorithm.</p><p>INFERENCE COST Inference cost of SHADOW-GNN is orders of magnitude lower than the baselines (a 5-layer SHADOW-GNN is still much cheaper than a 3-layer normal GNN). The high cost of the baselines is due to the "neighborhood explosion" with respect to more layers. SHADOW-GNN is efficient and scalable as the cost only grows linearly with the model depth. In addition, GraphSAINT only improves training efficiency because its inference operates on the full L-hop neighborhood. Scaling to 100 million nodes. We further scale SHADOW-GNN to ogbn-papers100M, one of the largest public dataset. Even through the full graph size is at least two orders of magnitude larger than the graphs in Table <ref type="table" target="#tab_2">1</ref>, the localized scope of SHADOW-GNN barely needs to increase. Since SHADOW-GNN performs minibatch computation, a low-end GPU with limited memory capacity can compute SHADOW-GNN on ogbn-papers100M efficiently. We show in Appendix F.1 that we can train and inference our model with as little as 4GB GPU memory consumption. This is infeasible using normal GNNs. Table <ref type="table" target="#tab_3">2</ref> summarizes our comparison with the top leaderboard methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b45">46]</ref>. We only include those methods that do not use node labels as the model input (i.e., the most standard setup). We achieve at least 3 orders of magnitude reduction in neighborhood size without sacrificing accuracy. For SIGN-XL and SGC, their neighborhood is too large to count the exact size. Also, their preprocessing consumes 5× more CPU memory than SHADOW-GNN (Appendix F.1). Extending to link-level task. We further show that SHADOW-GNN is general and can be extended to the link prediction task. There are two settings of ogbl-collab. We follow the one where validation edges cannot be used in training updates. This is the setting which most leaderboard methods follow. Table <ref type="table" target="#tab_4">3</ref> shows the comparison with the top GNN models under the same setting. SHADOW-SAGE outperforms the rank-1 model with significant margin.</p><p>Oversmoothing. To validate Theorem 3.2, we pick SGC as the backbone architecture. SGC with power L is equivalent to L-layer GCN without activation. Performance comparison between SGC and SHADOW-SGC thus reveals the effect of oversmoothing without introducing other factors due to optimizing deep neural networks (e.g., vanishing gradients). In Figure <ref type="figure">3</ref>, we vary the power of SGC and SHADOW-SGC from 1 to 40 (see Appendix E.5 for details). While SGC gradually collapses local information into global "white noise", accuracy of SHADOW-SGC does not degrade. This validates our theory that extracting local subgraphs prevents oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a design principle to decouple the depth and scope of GNNs. Applying such a principle on various GNN architectures simultaneously improves expressivity and computation scalability of the corresponding models. We have presented thorough theoretical analysis on expressivity from three different perspectives, and also rich design components (e.g., subgraph extraction functions, architecture extensions) to implement such design principle. Experiments show significant performance improvement over a wide range of graphs, GNN architectures and learning tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>by adding an extra edge e connecting two depth-L nodes in G [v] . Edge e changes the degree distribution δ [v] (•), and thus τ G [v] = τ G [v] . On the other hand, there is no way for GraphSAGE to propagate the influence of edge e to the target v, unless the model performs at least L + 1 message passings. So ζ G [v] = ζ G [v] regardless of the activation function and weight parameters. Therefore, ζ = τ . For SHADOW-SAGE, let EXTRACT return G [v] . Then the model can output ζ = A L [v] X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( d )</head><label>d</label><figDesc>Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>a normal GCN generates only one unique value of m for all v. By contrast, SHADOW-GNN generates |V| different values for any φ G function.3.2 Expressivity Analysis on SHADOW-SAGE: Function Approximation PerspectiveWe compare the expressivity by showing 1. SHADOW-SAGE can express all functions GraphSAGE can, and 2. SHADOW-SAGE can express some function GraphSAGE cannot. Recall, a GraphSAGE layer performs the following: h</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>We implement two EXTRACT described in Section 3.4: 1. "PPR", where we set the node budget K as {200, 400} for the largest ogbn-papers100M and K ≤ 200 for all other graphs; 2. "L-hop", where we set the depth as {1, 2}. As for the model depth, since L = 3 is the standard setting in the literature (see for example the benchmarking in OGB<ref type="bibr" target="#b14">[15]</ref>), we start from L = 3 and further evaluate a deeper model of L = 5. All accuracy are measured by five runs without fixing random seeds. Hyperparameter tuning and architecture configurations are in Appendix E.4. Comparison on test accuracy / F1-micro score and inference cost (tuned with DropEdge)</figDesc><table><row><cell>Ratio of nodes at different hop</cell><cell>0 0.2 0.4 0.6 0.8 1</cell><cell>Flickr</cell><cell cols="2">Reddit 4-layer GNN Yelp arxiv products</cell><cell>papers100M</cell><cell cols="4">L-layer SHADOW (PPR) Flickr Reddit Yelp arxiv products papers100M</cell><cell>= 1 = 2 = 3 = 4</cell><cell></cell><cell>S-SGC, F 0.4 0.6 0.8 Test accuracy 1 SGC, F</cell><cell>0</cell><cell>S-SGC, R Power on A or A [v] S-SGC, A 20 40 Reddit Arxiv Flickr SGC, R SGC, A</cell></row><row><cell cols="12">Figure 2: Neighbor composition of normal and SHADOW GNN</cell><cell>Figure 3: SGC oversmoothing</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>Layers</cell><cell cols="2">Flickr</cell><cell></cell><cell>Reddit</cell><cell></cell><cell></cell><cell>Yelp</cell><cell></cell><cell>ogbn-arxiv</cell><cell>ogbn-products</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell><cell>Cost</cell><cell>Accuracy</cell><cell>Cost</cell><cell cols="2">F1-micro</cell><cell>Cost</cell><cell>Accuracy</cell><cell>Cost</cell><cell>Accuracy</cell><cell>Cost</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell>3 5</cell><cell cols="9">0.5159±0.0017 2E0 0.9532±0.0003 6E1 0.4028±0.0019 2E1 0.7170±0.0026 1E1 0.7567±0.0018 5E0 0.5217±0.0016 2E2 0.9495±0.0012 1E3 OOM 1E3 0.7186±0.0017 1E3 OOM 9E2</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell>3</cell><cell cols="9">0.5155±0.0027 2E0 0.9523±0.0003 6E1 0.5110±0.0012 2E1 0.7093±0.0003 1E1 0.8003±0.0024 5E0</cell></row><row><cell cols="2">+ GraphSAINT-RW</cell><cell></cell><cell>5</cell><cell cols="9">0.5165±0.0026 2E2 0.9523±0.0012 1E3 0.5012±0.0021 1E3 0.7039±0.0020 1E3 0.7992±0.0021 9E2</cell></row><row><cell cols="2">SHADOW-GCN</cell><cell></cell><cell>3</cell><cell cols="2">0.5277±0.0017</cell><cell>(1)</cell><cell>0.9624±0.0002</cell><cell>(1)</cell><cell cols="2">0.5255±0.0012</cell><cell>(1)</cell><cell>0.7192±0.0025</cell><cell>(1)</cell><cell>0.7778±0.0030</cell><cell>(1)</cell></row><row><cell>+PPR</cell><cell></cell><cell></cell><cell>5</cell><cell cols="9">0.5284±0.0012 1E0 0.9614±0.0003 1E0 0.5272±0.0018 2E0 0.7207±0.0030 2E0 0.7844±0.0029 2E0</cell></row><row><cell cols="2">GraphSAGE</cell><cell></cell><cell>3 5</cell><cell cols="9">0.5140±0.0014 3E0 0.9653±0.0002 5E1 0.6178±0.0033 2E1 0.7192±0.0027 1E1 0.7858±0.0013 4E0 0.5154±0.0052 2E2 0.9626±0.0004 1E3 OOM 2E3 0.7193±0.0037 1E3 OOM 1E3</cell></row><row><cell cols="2">GraphSAGE</cell><cell></cell><cell>3</cell><cell cols="9">0.5176±0.0032 3E0 0.9671±0.0003 5E1 0.6453±0.0011 2E1 0.7107±0.0003 1E1 0.7923±0.0023 4E0</cell></row><row><cell cols="2">+ GraphSAINT-RW</cell><cell></cell><cell>5</cell><cell cols="9">0.5201±0.0032 2E2 0.9670±0.0010 1E3 0.6394±0.0003 2E3 0.7013±0.0021 1E3 0.7964±0.0022 1E3</cell></row><row><cell cols="2">SHADOW-SAGE</cell><cell></cell><cell>3</cell><cell cols="9">0.5288±0.0014 1E0 0.9660±0.0003 1E0 0.6493±0.0001 1E0 0.7163±0.0012 1E0 0.7993±0.0012 1E0</cell></row><row><cell>+ 2-hop</cell><cell></cell><cell></cell><cell>5</cell><cell cols="9">0.5338±0.0038 2E0 0.9661±0.0002 2E0 0.6503±0.0001 2E0 0.7183±0.0012 2E0 0.8014±0.0020 2E0</cell></row><row><cell cols="2">SHADOW-SAGE</cell><cell></cell><cell>3</cell><cell cols="2">0.5334±0.0028</cell><cell>(1)</cell><cell>0.9699±0.0002</cell><cell>(1)</cell><cell cols="2">0.6512±0.0002</cell><cell>(1)</cell><cell>0.7234±0.0032</cell><cell>(1)</cell><cell>0.7945±0.0021</cell><cell>(1)</cell></row><row><cell>+ PPR</cell><cell></cell><cell></cell><cell>5</cell><cell cols="9">0.5395±0.0025 2E0 0.9703±0.0003 2E0 0.6502±0.0001 2E0 0.7255±0.0013 2E0 0.8043±0.0026 2E0</cell></row><row><cell>GAT</cell><cell></cell><cell></cell><cell>3 5</cell><cell cols="3">0.5070±0.0032 2E1 0.5164±0.0033 2E2</cell><cell>OOM OOM</cell><cell>3E2 2E3</cell><cell cols="2">OOM OOM</cell><cell cols="2">2E2 0.7201±0.0011 1E2 2E3 OOM 3E3</cell><cell>OOM OOM</cell><cell>3E1 2E3</cell></row><row><cell>GAT</cell><cell></cell><cell></cell><cell>3</cell><cell cols="9">0.5225±0.0053 2E1 0.9671±0.0003 3E2 0.6459±0.0002 2E2 0.6977±0.0003 1E2 0.8027±0.0028 3E1</cell></row><row><cell cols="2">+ GraphSAINT-RW</cell><cell></cell><cell>5</cell><cell cols="9">0.5153±0.0034 2E2 0.9651±0.0024 2E3 0.6478±0.0012 2E3 0.6954±0.0013 3E3 0.7990±0.0072 2E3</cell></row><row><cell cols="2">SHADOW-GAT</cell><cell></cell><cell>3</cell><cell cols="2">0.5364±0.0026</cell><cell>(1)</cell><cell>0.9707±0.0004</cell><cell>(1)</cell><cell cols="2">0.6549±0.0002</cell><cell>(1)</cell><cell>0.7235±0.0011</cell><cell>(1)</cell><cell>0.8014±0.0012</cell><cell>(1)</cell></row><row><cell>+ PPR</cell><cell></cell><cell></cell><cell>5</cell><cell cols="9">0.5360±0.0013 2E0 0.9713±0.0004 2E0 0.6537±0.0004 2E0 0.7274±0.0027 2E0 0.8081±0.0034 2E0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Leaderboard comparison on papers100M</figDesc><table><row><cell>Method</cell><cell>Test accuracy</cell><cell>Val accuracy</cell><cell>Neigh size</cell></row><row><cell cols="3">GraphSAGE+incep 0.6706±0.0017 0.7032±0.0011</cell><cell>4E5</cell></row><row><cell>SIGN-XL</cell><cell cols="2">0.6606±0.0019 0.6984±0.0006</cell><cell>&gt; 4E5</cell></row><row><cell>SGC</cell><cell cols="2">0.6329±0.0019 0.6648±0.0020</cell><cell>&gt; 4E5</cell></row><row><cell>SHADOW-GAT 200</cell><cell cols="2">0.6681±0.0016 0.7019±0.0011</cell><cell>2E2</cell></row><row><cell>SHADOW-GAT 400</cell><cell cols="2">0.6708±0.0015 0.7067±0.0012</cell><cell>3E2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Leaderboard comparison on collab</figDesc><table><row><cell>Method</cell><cell>Test Hits@50</cell><cell>Val Hits@50</cell></row><row><cell>SEAL</cell><cell cols="2">0.5371±0.0047 0.6389±0.0049</cell></row><row><cell>DeeperGCN</cell><cell cols="2">0.5273±0.0047 0.6187±0.0045</cell></row><row><cell>LRGA+GCN</cell><cell cols="2">0.5221±0.0072 0.6088±0.0059</cell></row><row><cell>SHADOW-SAGE</cell><cell cols="2">0.5482±0.0046 0.6503±0.0054</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The sampling methods<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b8">9]</ref> aim at estimating the aggregation of the full L-hop neighborhood for a L-layer GNN model. Thus, even though the scope is sampled, the depth and scope are still coupled.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Unlike other PPR-based models<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5]</ref> which rewire the graph by treating top PPR nodes as direct neighbors, our PPR neighborhood preserves the original multi-hop topology by returning node-induced subgraph.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<title level="m">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Annual IEEE Symposium on Foundations of Computer Science (FOCS&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rózemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;19</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimal variance sampling with provable guarantees for fast training of graph neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1393" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">SIGN: Scalable inception graph neural networks</title>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Krishna Narayanan, and Xiaoning Qian. Bayesian graph neural networks with adaptive connection sampling</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Nick Duffield</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tackling oversmoothing for general Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simrank: a measure of structural-context similarity</title>
		<author>
			<persName><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sampling from large graphs</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06</title>
				<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph evolution: Densification and shrinking diameters</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2007-03">March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Markov chains and mixing times</title>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><surname>Peres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">107</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepGCNs: Can GCNs go as deep as CNNs?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">DeeperGCN: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deeper insights into Graph Convolutional Networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07606</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020-08">Aug 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Machine learning in chemoinformatics and drug discovery</title>
		<author>
			<persName><forename type="first">Yu-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><forename type="middle">E</forename><surname>Rensi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug discovery today</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1538" to="1546" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="10945" to="10955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph Neural Networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PinnerSage: Multi-modal user embedding framework for recommendations at Pinterest</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chantat</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020-08">Aug 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating node importance in knowledge graphs using graph neural networks</title>
		<author>
			<persName><forename type="first">Namyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="596" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DropEdge: Towards deep Graph Convolutional Networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><forename type="middle">M</forename><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Craig R Macnair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsey</forename><forename type="middle">A</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName><surname>Bloom-Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AM-GCN: Adaptive multichannel graph convolutional networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">GNNExplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17009" to="17021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rajgopal Kannan, and Viktor Prasanna. Accurate, efficient and scalable graph embedding</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Efficient probabilistic logic reasoning with graph neural networks</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">PairNorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Layerdependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
