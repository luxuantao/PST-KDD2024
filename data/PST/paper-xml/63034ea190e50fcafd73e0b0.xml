<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-shot Heterogeneous Graph Learning via Cross-domain Knowledge Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiannan</forename><surname>Zhang</surname></persName>
							<email>qiannan.zhang@kaust.edu.sa</email>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Wu</surname></persName>
							<email>xiaodong.wu@kaust.edu.sa</email>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<email>qiang.yang@kaust.edu.sa</email>
						</author>
						<author>
							<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
							<email>chuxuzhang@brandeis.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
							<email>xzhang33@nd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Brandeis University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-shot Heterogeneous Graph Learning via Cross-domain Knowledge Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539431</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>• Computing methodologies → Learning latent representations</term>
					<term>Neural networks Heterogeneous Graphs</term>
					<term>Few-shot Learning</term>
					<term>Knowledge Transfer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph few-shot learning seeks to alleviate the label scarcity problem resulting from the difficulties and high cost of data annotations in graph learning. However, the overwhelming solutions in graph few-shot learning focus on homogeneous graphs, ignoring the ubiquitous heterogeneous graphs (HGs), which represent real-world complex systems and domain knowledge with multi-typed nodes interconnected by multi-typed edges. To this end, we study the crossdomain few-shot learning problem over HGs and develop a novel model for Cross-domain Heterogeneous Graph Meta-learning (CrossHG-Meta). The general idea is to promote the HG node classification in the data-scarce target domain by transferring metaknowledge from a series of HGs in data-rich source domains. The key challenges are to 1) combat the heterogeneity in HGs to acquire the transferable meta-knowledge; 2) handle the domain shifts between the source HG and target HG; and 3) fast adapt to novel target tasks with few-shot annotated examples. Regarding the graph heterogeneity, CrossHG-Meta firstly builds a graph encoder to aggregate heterogeneous neighborhood information from multiple semantic contexts. Secondly, to tackle domain shifts, a cross-domain meta-learning strategy is proposed to include a domain critic, which is designed to explicitly lead cross-domain adaptation for metatasks in different domains and improve model generalizability. Last, to further alleviate data scarcity, CrossHG-Meta leverages unlabeled information in source domains with auxiliary self-supervised learning task to provide cross-domain contrastive regularization alongside the meta-optimization process to facilitate node embedding. Extensive experimental results on three multi-domain HG datasets demonstrate that the proposed model outperforms various state-of-the-art baselines for multiple few-shot node classification tasks under the cross-domain setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Powerful (semi-)supervised graph learning models have been developed to solve various real-world problems, which usually require a large amount of labeled data for model training. However, label scarcity is a common issue due to the expense and difficulties of data annotation in practice. To combat the performance degradation of (semi-)supervised graph learning methods in face of label scarcity, graph few-shot learning associates few-shot learning methods with graph neural networks in a variety of applications, such as node classification <ref type="bibr" target="#b13">[10,</ref><ref type="bibr" target="#b35">32,</ref><ref type="bibr" target="#b41">38,</ref><ref type="bibr" target="#b47">44]</ref>, link prediction <ref type="bibr" target="#b38">[35,</ref><ref type="bibr" target="#b44">41]</ref> and graph classification <ref type="bibr" target="#b11">[8]</ref>. Researchers endeavor to leverage meta-learning approaches such that the meta-knowledge is extracted from a distribution of related tasks as the inductive bias for the inference on novel classes provided with few-shot annotations. Assorted meta-knowledge to acquire includes optimization parameters <ref type="bibr" target="#b47">[44]</ref>, subgraph gradients <ref type="bibr" target="#b13">[10]</ref>, node informativeness <ref type="bibr" target="#b19">[16]</ref>, or auxiliary graph knowledge <ref type="bibr" target="#b41">[38]</ref>.</p><p>The recent overwhelming solutions developed for graph fewshot learning problem mostly focus on homogeneous graphs, and strive to predict novel classes in single (source) domain as illustrated on a HG in Fig. <ref type="figure" target="#fig_0">1 (a</ref>). However, real-world systems should be more comprehensively represented as heterogeneous graphs (HGs) <ref type="bibr" target="#b28">[25]</ref>, which empower graph expressiveness by preserving rich and hybrid semantic information, such as academic networks and social networks <ref type="bibr" target="#b32">[29]</ref>. Label scarcity problem exists in learning from HGs as well. Particularly, for HGs retaining specialized domain knowledge, it may require experts to annotate entities or relations at a large expense. Meta-knowledge from other domains may help the exploration in a HG under low-data scenario if there are enormous annotations available in related domains. Take academic networks as examples. One academic HG in Computer Science domain may be rich with labeled node entities and relations. However, another HG in Material Science domain may have a small population of annotated node entities and relations. Though the discipline terminology and categories in these two HGs are different, the principle of label inference could be implicitly shared between them. For instance, papers published in the same venue tend to have similar research topic labels. Authors who frequently publish in common venues tend to have similar research interest labels. These principles apply to academic HGs in different domains, because of the relevance of node types and node-node connectivity patterns generally shared across academic domains. The shared structural relevance can similarly take place in e-commerce activities as well, e.g., the association between products and brands. In light of this, cross-domain generalizability should be explored to enable the data-rich domains to help the target domain with mere labels.</p><p>We thus concentrate our study to make prediction on the target domain with novel categories while only a few examples per class are annotated. And the shareable knowledge from a number of source domains is expected to be transferred with no access of unlabeled target domain data during training, as shown in Fig. <ref type="figure" target="#fig_0">1  (c</ref>). This is a non-trivial task and has several key challenges. First, the graph heterogeneity should be considered when learning from hybrid semantic information in HGs. Second, the transferable metaknowledge should be extracted from the source domains without accessing the target domain, and then transferred to the target domain by combating domain shifts, as the target domain may have distinct node entities and relations from the source domains. The existing domain adaptation techniques proposed to alleviate the domain shift issue <ref type="bibr" target="#b39">[36]</ref> are not applicable, because they require access to the target domain data, as shown in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>. Last, the prediction on novel categories in the target domain is expected to be conducted with few-shot annotations. The model should thus possess the ability of rapid adaptation.</p><p>To tackle the challenges, we propose a novel model called Crossdomain Heterogeneous Graph Meta-learning (a.k.a. CrossHG-Meta). Specifically, CrossHG-Meta aggregates multiple fixed-size sets of semantic contexts (generated by meta-paths) to produce node embeddings. A cross-domain meta-learning methodology with awareness of domain shifts is proposed to execute meta-learning for the graph encoder. It imitates training/testing domain shift by splitting the source domains and includes a domain critic to align meta-tasks from distinct domains to improve cross-domain generalizability. To further alleviate data scarcity, CrossHG-Meta explores self-supervised signals in source domains to provide contrastive regularization to meta-optimization process. To our best knowledge, CrossHG-Meta is among the first works addressing cross-domain few-shot learning problem on heterogeneous graphs.</p><p>Our contributions in this work are summarized as follows:</p><p>(1) We study the problem of cross-domain few-shot learning on heterogeneous graphs, which is an important and non-trivial task due to the complexity and diversity of HG data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Heterogeneous Graph Representation Learning. By embedding nodes in HGs into low-dimensional representation vectors, heterogeneous graph representation learning facilitates various downstream tasks <ref type="bibr" target="#b22">[19]</ref>. Specifically, a line of existing approaches utilize meta-paths <ref type="bibr" target="#b30">[27]</ref>, which are compositions of intermediate relations, to extract hybrid semantics in HGs. For instance, meta-path2vec <ref type="bibr" target="#b6">[3]</ref> applies skip-gram model on meta-path based random walks to achieve unsupervised graph learning. By introducing graph neural networks, HAN <ref type="bibr" target="#b36">[33]</ref> learns node-level attention and semantic-level attention for hierarchical aggregation. HetGNN <ref type="bibr" target="#b43">[40]</ref> groups nodes by types in attributed graphs to fuse content and structural information. In this work, we are also motivated to efficiently learn node embeddings in HGs based on multiple meta-paths.</p><p>Graph Few-shot Learning. According to the way of extracting meta-knowledge, few-shot learning approaches are mainly categorized into two branches <ref type="bibr" target="#b42">[39]</ref>: 1) optimization-based methods taking few-shot learning as an optimization problem <ref type="bibr" target="#b8">[5,</ref><ref type="bibr" target="#b18">15,</ref><ref type="bibr" target="#b21">18]</ref>; and 2) metric-based methods learning a generalized metric space for distance-based inference <ref type="bibr" target="#b29">[26,</ref><ref type="bibr" target="#b41">38]</ref>. Graph few-shot learning combines few-shot learning with graph neural networks to alleviate label sparsity issue. For example, some approaches integrate graph neural networks <ref type="bibr" target="#b15">[12]</ref> with MAML <ref type="bibr" target="#b8">[5]</ref> to initialize the graph base learner <ref type="bibr" target="#b45">[42,</ref><ref type="bibr" target="#b47">44]</ref>, modulate network initialization with transferable graph signatures [1], or update network parameters by collecting meta-gradients from local subgraphs <ref type="bibr" target="#b13">[10]</ref>. Besides, some research work leverages node significance propagation to calculate weighted class prototypes for distance-based classifiers <ref type="bibr" target="#b5">[2]</ref> or combine selective prototypes with a gating mechanism <ref type="bibr" target="#b19">[16]</ref>.  <ref type="bibr" target="#b27">[24]</ref> or adversarial learning <ref type="bibr" target="#b26">[23]</ref>. For instance, DANE <ref type="bibr" target="#b46">[43]</ref> combines graph convolutional networks with adversarial learning to align distributions of embeddings. To transfer knowledge across HGs, MuSDAC <ref type="bibr" target="#b40">[37]</ref> utilizes multi-channel GCNs to project nodes into multiple spaces for pairwise alignment. However, these approaches work on graph knowledge transfer between domains associated with the same label space. The domain generalization approaches relax the requirement of target domain data for training <ref type="bibr" target="#b34">[31]</ref>. Recently, meta-learning strategies are employed to learn transferable knowledge with meta-training process on source domains <ref type="bibr" target="#b7">[4,</ref><ref type="bibr" target="#b16">13,</ref><ref type="bibr" target="#b17">14]</ref>. For instance, MLDG <ref type="bibr" target="#b16">[13]</ref> simulates train/test domain shifts with source domains during meta-optimization for classifying images in the target domain associated with the same label space. Feature-Critic <ref type="bibr" target="#b17">[14]</ref> learns domain-invariant feature extractor under an auxiliary loss for image classification, and a domain-specific classifier is trained over the learned extractor to infer novel classes in target domain.</p><p>Our mission is to tackle heterogeneous graph learning in a crossdomain few-shot setting. That is to leverage data-rich source domains to make inference on the novel categories in the target domain provided with only few-shot annotations. The above discussed related works have not or only partially addressed the key challenges of our problem, such as graph structure heterogeneity, domain shift and labeled data insufficiency. The existing models are either designed in a different setting from ours (e.g., requiring the same label space, requiring to access target domain data in training, or being designed for images), or usable by downgrading our setting (e.g., ignoring the heterogeneity, or giving up generalizability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>We first introduce the used concepts, and then present the crossdomain few-shot learning problem on heterogeneous graphs. Definition 1. Heterogeneous graph (HG) <ref type="bibr" target="#b28">[25]</ref>. A heterogeneous graph, denoted as G = (V; E; A; R; X), consists of a node set V and an edge set E. Respectively by the type mapping function 𝜑 (𝑣) : V → A and 𝛾 (𝑒) : E → R, where A and R denote the type sets of nodes and edges, each node 𝑣 ∈ V or edge 𝑒 ∈ E is associated with a specific type. Note that |A| + |R| &gt; 2 for HG. In addition, node 𝑣 is associated with content feature x 𝑣 ∈ X.</p><p>Take academic graph as an example. It may contain node types of Paper (P), Author (A) and Venue (V). Therefore, edges may represent different semantics, such as an edge between a paper and an author (PA) reflecting the author writes that paper, or one between a venue and a paper (PV) showing the venue publishes the paper. Definition 2. Meta-path <ref type="bibr" target="#b30">[27]</ref>. Given a HG G, a meta-path 𝜙 is defined as a composite path formed as A 1 Problem Formulation. We target on the few-shot node classification problem on HG by learning cross-domain transferable knowledge from HGs in multiple related domains. Hereby, we regard each domain 𝑖 as a collection of few-shot tasks 𝑇 𝑖 = {𝜏 1 , 𝜏 2 , . . . , 𝜏 𝑚 } to mimic the low-data scenario in testing tasks. In particular, provided with a number of source domains S = {𝑇 1 ,𝑇 2 , . . . ,𝑇 𝑛 } and the target domain T , our objective is to learn a model with good generalization ability to few-shot tasks in the target domain T after training with the tasks from the set of source domains S. Hereby, we build few-shot tasks for each domain normally as follows. Firstly, we consider data in domain 𝑇 𝑖 (i.e., nodes in HG G) as associated with the label space C 𝑖 with categorical values. Under the 𝑁 -way 𝐾-shot setting, few-shot task 𝜏 𝑗 can be sampled by randomly choosing 𝑁 different classes 𝐶 𝜏 𝑗 = {𝑐 1 , 𝑐 2 , . . . , 𝑐 𝑁 } from the domain label space C 𝑖 . Upon that, sampling of 𝐾 labeled nodes per class is conducted to form the support set of 𝜏 𝑗 as Ω 𝑠 = {Ω 𝑐 1 , Ω 𝑐 2 , . . . , Ω 𝑐 𝑁 }, where Ω 𝑐 𝑖 = {(𝑣 1 , 𝑐 𝑖 ), (𝑣 2 , 𝑐 𝑖 ), . . . , (𝑣 𝐾 , 𝑐 𝑖 )}. The query set </p><formula xml:id="formula_0">𝑟 1 − − → A 2 𝑟 2 − − → . . . 𝑟 𝑙 − → A 𝑙+1 (abbreviated</formula><formula xml:id="formula_1">Ω 𝑞 = {Ω ′ 𝑐 1 , Ω ′ 𝑐 2 , . . . , Ω ′ 𝑐 𝑁 } is created</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED CROSSHG-META</head><p>The proposed model is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. With a heterogeneous graph encoder to aggregate local neighborhood information from multiple semantic contexts, a cross-domain meta-learning strategy is proposed to include a domain critic which handles domain shifts between meta-tasks from different domains, such that the graph encoder is meta-trained with cross-domain generalizability for adapting to target domain well. Additionally, CrossHG-Meta introduces self-supervised information to provide cross-domain contrastive regularization upon meta-optimization. We next elaborate each module of CrossHG-Meta, whose pseudocode is in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Heterogeneous Information Aggregation</head><p>Given node 𝑣 and a meta-path set Φ = {𝜙 1 , 𝜙 2 , . . . , 𝜙 𝑀 } in a HG G, we sample a fixed number of nodes from each set of meta-path based neighbors N 𝜙 𝑖 𝑣 (𝜙 𝑖 ∈ Φ). Based on graph neural networks, the neighborhood information from multiple meta-path based contexts updates the embedding for node 𝑣 at layer 𝑘 as follows:</p><formula xml:id="formula_2">ℎ 𝑘+1 𝑣 = 𝜎          𝑊 𝑘 𝑀 𝑖=1 1 |N 𝜙 𝑖 𝑣 | ∑︁ 𝑢 ∈N 𝜙 𝑖 𝑣 𝑊 𝑘 𝜙 𝑖 ℎ 𝑘 𝑢 𝑊 𝑘 0 ℎ 𝑘 𝑣          ,<label>(1)</label></formula><p>where denotes concatenation operation, 𝑊 are learnable parameters, 𝜎 is the activation function, ℎ 𝑣 and ℎ 𝑢 represent embeddings for node 𝑣 and its sampled neighbor 𝑢. Taking initial node feature x 𝑣 , the aggregation strategy proceeds till final embedding z 𝑣 is generated (the last layer output). By sampling meta-path based neighbors, the aggregator 1) fuses heterogeneous information over hybrid semantics and 2) avoids overwhelming workload when dealing with many relations with meta-path based neighbor sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-domain Meta-learning</head><p>The heterogeneous graph encoder is expected to conduct fast adaptation on target domain with few-shot labeled data after being trained with source domain data. We propose the method of crossdomain meta-learning to fulfil this aim by exposing meta-optimization process to domain shifts. Yet considering the inaccessibility to target domain data during meta-training, our overall strategy is to imitate domain shifts by splitting source domains as virtual source and virtual target domains, as in previous works <ref type="bibr" target="#b16">[13,</ref><ref type="bibr" target="#b33">30]</ref>. Specifically, we randomly select one virtual source domain 𝑆 𝑣𝑠 and one virtual target domain 𝑆 𝑣𝑡 from the real source domains S at each iteration. Accordingly, a number of virtual source metatasks {𝜏 𝑣𝑠,1 , 𝜏 𝑣𝑠,2 , . . . , 𝜏 𝑣𝑠,𝑛 } and a same number of virtual target meta-tasks {𝜏 𝑣𝑡,1 , 𝜏 𝑣𝑡,2 , . . . , 𝜏 𝑣𝑡,𝑛 } are generated respectively from domain 𝑆 𝑣𝑠 and 𝑆 𝑣𝑡 . With randomness, we furthermore formulate 𝑛 dual meta-tasks, each consisting of {𝜏 𝑣𝑠,𝑖 , 𝜏 𝑣𝑡,𝑖 } by pairing a virtual source meta-task 𝜏 𝑣𝑠,𝑖 and a virtual target meta-task 𝜏 𝑣𝑡,𝑖 . The objective of cross-domain generalization requires that meta-parameters adapting well to the virtual source domain could also improve the virtual target domain performance. We will design the specialized procedure with dual meta-tasks to reach this goal next.</p><p>Let 𝜃 𝑔 be the parameters of the heterogeneous graph encoder, and 𝜃 𝑐 be the parameters of the few-shot classifier. Based on metaparameters {𝜃 𝑔 , 𝜃 𝑐 }, we first obtain the task-specific parameters for the virtual source meta-task 𝜏 𝑣𝑠,𝑖 after a few gradient steps guided by the cross-entropy loss L 𝑐 used for node classification. Take one step for simplicity, task-specific network parameters for virtual source meta-task 𝜏 𝑣𝑠,𝑖 are updated as follows:</p><formula xml:id="formula_3">(𝜃 ′ 𝑔 , 𝜃 ′ 𝑐 ) = (𝜃 𝑔 , 𝜃 𝑐 ) − 𝛼∇ 𝜃 𝑔 ,𝜃 𝑐 L 𝑠𝑢𝑝 𝑐 (𝜏 𝑣𝑠,𝑖 ; 𝜃 𝑔 , 𝜃 𝑐 ),<label>(2)</label></formula><p>where L 𝑠𝑢𝑝 𝑐 (𝜏 𝑣𝑠,𝑖 ; 𝜃 𝑔 , 𝜃 𝑐 ) is the loss evaluated on the support set of 𝜏 𝑣𝑠,𝑖 , and 𝛼 is the step size. Effectiveness of task-specific parameters is measured by loss L 𝑞𝑢𝑒 𝑐 (𝜏 𝑣𝑠,𝑖 ; 𝜃 ′ 𝑔 , 𝜃 ′ 𝑐 ) on the query set of 𝜏 𝑣𝑠,𝑖 . To ensure that the gradient directions taken for 𝜏 𝑣𝑠,𝑖 will lead to improvement on virtual target meta-task 𝜏 𝑣𝑡,𝑖 , a domain critic is proposed to produce a learnable domain transfer loss L 𝑑 , which explicitly depicts the discrepancy between dual meta-tasks. By minimizing L 𝑑 , the domain critic guides extra gradient update over task-specific parameters for combating the domain shifts between dual meta-tasks and enforcing cross-domain generalization. Design of the domain critic is discussed in the next subsection.</p><p>With the help of L 𝑑 , the procedure of cross-domain adaptation from virtual source meta-task to virtual target meta-task takes the following two steps: 1) adapting graph encoder under the guidance of L 𝑑 ; and 2) updating the classifier network based on classification loss evaluated on the support set of 𝜏 𝑣𝑡,𝑖 . During this procedure, we can modify task-specific parameters {𝜃 ′ 𝑔 , 𝜃 ′ 𝑐 } of virtual source metatask 𝜏 𝑣𝑠,𝑖 to obtain updated network parameters {𝜃 𝑡 𝑔 , 𝜃 𝑡 𝑐 } for virtual target meta-task 𝜏 𝑣𝑡,𝑖 with a few steps of the above cross-domain adaptation. For simplicity, we employ one-step adaptation:</p><formula xml:id="formula_4">𝜃 𝑡 𝑔 = 𝜃 ′ 𝑔 − 𝛽∇ 𝜃 ′ 𝑔 L 𝑑 (𝜏 𝑣𝑠,𝑖 , 𝜏 𝑣𝑡,𝑖 ; 𝜃 ′ 𝑔 , 𝜃 ′ 𝑐 ), 𝜃 𝑡 𝑐 = 𝜃 ′ 𝑐 − 𝛽∇ 𝜃 ′ 𝑐 L 𝑠𝑢𝑝 𝑐 (𝜏 𝑣𝑡,𝑖 ; 𝜃 𝑡 𝑔 , 𝜃 ′ 𝑐 ),<label>(3)</label></formula><p>where L 𝑑 denotes the learned domain transfer loss based on dual meta-tasks {𝜏 After obtaining classification losses on query sets of dual metatasks, we consider to minimize both of them for meta-optimization such that meta-parameters serve as good starting points for metatasks in distinct domains, thus considering domain shifts to improve domain generalizability. Therefore, for a batch of dual meta-tasks, we formulate the objective of meta-optimization as follows:</p><formula xml:id="formula_5">L 𝑚𝑒𝑡𝑎 = ∑︁ 𝜏 𝑣𝑠,𝑖 L 𝑞𝑢𝑒 𝑐 (𝜏 𝑣𝑠,𝑖 ; 𝜃 ′ 𝑔 𝑖 , 𝜃 ′ 𝑐 𝑖 ) + ∑︁ 𝜏 𝑣𝑡,𝑖 L 𝑞𝑢𝑒 𝑐 (𝜏 𝑣𝑡,𝑖 ; 𝜃 𝑡 𝑔 𝑖 , 𝜃 𝑡 𝑐 𝑖 ),<label>(4)</label></formula><p>where 𝜏 𝑣𝑠,𝑖 ∼ 𝑝 (𝜏 𝑣𝑠 ) (resp. 𝜏 𝑣𝑡,𝑖 ∼ 𝑝 (𝜏 𝑣𝑡 )) denotes the meta-task distribution for domain 𝑆 𝑣𝑠 (resp. 𝑆 𝑣𝑡 ). Consequently, the model is trained to learn proper initialization parameters which enable fast adaptation in the virtual source domain and meanwhile generalize well to the virtual target domain with the assistance of the intermediate cross-domain adaptation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Domain Critic for Calculating L 𝑑</head><p>In order to guide the cross-domain adaptation procedure between dual meta-tasks, we base the design of domain critic on the level of meta-tasks, such that each meta-task is characterized and the difference between dual meta-tasks is measured. Though metatasks from distinct domains might be reflected utilizing the network parameters 𝜃 ′ 𝑔 and 𝜃 𝑡 𝑔 , we avoid the overhead of parameter-level comparison and learn task representations with task examples.</p><p>Specifically, for the meta-tasks 𝜏 𝑣𝑠,𝑖 and 𝜏 𝑣𝑡,𝑖 sampled from the virtual source domain 𝑆 𝑣𝑠 and virtual target domain 𝑆 𝑣𝑡 , we obtain node embeddings of their support sets respectively, i.e., z 𝜏 𝑣𝑠,𝑖 and z 𝜏 𝑣𝑡,𝑖 . Here z 𝜏 𝑣𝑠,𝑖 is a 𝑝 × 𝐻 feature matrix, where 𝑝 = 𝑁 × 𝐾, and the matrix is the output from heterogeneous graph encoder based on task-specific parameters 𝜃 ′ 𝑔 by Eq. (2). Likewise, z 𝜏 𝑣𝑡,𝑖 is the feature matrix of the support set in the virtual target meta-task 𝜏 𝑣𝑡,𝑖 .</p><p>Then domain critic measures the difference between dual metatask representations based on node embeddings z 𝜏 𝑣𝑠,𝑖 and z 𝜏 𝑣𝑡,𝑖 . We define the domain transfer loss L 𝑑 as follows:</p><formula xml:id="formula_6">L 𝑑 =∥ Mean(MLP 𝜃 𝑑 (z 𝜏 𝑣𝑠,𝑖 )) − Mean(MLP 𝜃 𝑑 (z 𝜏 𝑣𝑡,𝑖 )) ∥ 2 ,<label>(5)</label></formula><p>where 𝜃 𝑑 is the parameters of the domain critic network implemented as a multi-layer perceptron. In order to obtain permutationinvariant task representations to the order of task examples, a mean-pooling operation is applied outside the perceptron. We apply Euclidean distance to the meta-task representations here. Basically, the domain critic provides learnable guidance to allow further cross-domain adaptation procedure, which actually incorporates extra gradient steps with awareness of domain shifts, and finally helps improve cross-domain generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cross-Domain Contrastive Regularization</head><p>To further compete scarce supervision in target domain, we propose to leverage graph heterogeneity with unlabeled data in source domains. As meta-path portrays the relevance of node types in HG, the relationship it represents could reveal semantic patterns and thus preserve cross-domain invariance among related domains. Thereby we build the auxiliary self-supervised learning task by taking advantage of contrastive learning <ref type="bibr" target="#b20">[17]</ref>, as shown to be adept at generating discriminative representations in low-data regime. However, current approaches for graph node-level contrastiveness are generally limited to comparing node entities within the same graph or relying on common node entities among graphs.</p><p>We propose to relax the limitation and extract cross-domain bi-directional relationships based on meta-paths upon the virtual domains. The underlying intuition is that, two nodes connected by one meta-path 𝜙 in the domain 𝑆 𝑣𝑠 (resp. 𝑆 𝑣𝑡 ) depict the semantic relevance between them. Meanwhile, nodes separated in domain 𝑆 𝑣𝑠 and 𝑆 𝑣𝑡 , though they may obey the node types of the meta-path 𝜙, probably have less closeness in terms of this semantic. Thus, it can be generally closer for the former node pairs than two arbitrary nodes separated in two domains upon most occasions. Through this contrastiveness, graph heterogeneity across HGs is explored.</p><p>Specifically, we randomly choose the anchor node 𝑣 𝑠,𝑖 in the virtual source domain 𝑆 𝑣𝑠 , and sample its meta-path 𝜙 based neighbors 𝑣 𝑠,𝑗 to form one positive pair (𝑣 𝑠,𝑖 , 𝑣 𝑠,𝑗 ). Meanwhile, another node 𝑣 𝑡,𝑘 from virtual target domain 𝑆 𝑣𝑡 is sampled to build the negative node pair (𝑣 𝑠,𝑖 , 𝑣 𝑡,𝑘 ), where the only constraint on node 𝑣 𝑡,𝑘 is to hold the same node type with node 𝑣 𝑠,𝑗 . Regarding the above discussion, an objective for a number of such node-pairs can be calculated with the triplet margin loss as follows:</p><formula xml:id="formula_7">L 𝜙 𝑆 𝑣𝑠 = ∑︁ 𝑖,𝑗,𝑘 max (0, 𝜖 − 𝑠 𝜙 𝑣 𝑠,𝑖 ,𝑣 𝑠,𝑗 + 𝑠 𝜙 𝑣 𝑠,𝑖 ,𝑣 𝑡,𝑘 ),<label>(6)</label></formula><p>where 𝜖 is the margin parameter, and 𝑠 𝜙 𝑢,𝑣 is the similarity score of node pair {𝑢, 𝑣 } regarding the meta-path 𝜙. It is derived as:</p><formula xml:id="formula_8">𝑠 𝜙 𝑢,𝑣 = 𝜎 (ℎ 𝜙 (z 𝑢 ; 𝜃 𝑎 ) 𝑇 ℎ 𝜙 (z 𝑣 ; 𝜃 𝑎 )),<label>(7)</label></formula><p>where ℎ 𝜙 (; 𝜃 𝑎 ) is a neural network (with parameter 𝜃 𝑎 ) for the self-supervised task under meta-path 𝜙, z 𝑢 and z 𝑣 are node embeddings of node 𝑢 and 𝑣, respectively. Likewise, we build the opposite objective based on anchor nodes in virtual target domain 𝑆 𝑣𝑡 and form positive node pair (𝑣 𝑡,𝑖 , 𝑣 𝑡,𝑗 ) under meta-path 𝜙 and negative node pair (𝑣 𝑡,𝑖 , 𝑣 𝑠,𝑘 ) with 𝑣 𝑠,𝑘 from virtual source domain 𝑆 𝑣𝑠 in the same way as above. For a few of such node pairs, the loss is:</p><formula xml:id="formula_9">L 𝜙 𝑆 𝑣𝑡 = ∑︁ 𝑖,𝑗,𝑘 max (0, 𝜖 − 𝑠 𝜙 𝑣 𝑡,𝑖 ,𝑣 𝑡,𝑗 + 𝑠 𝜙 𝑣 𝑡,𝑖 ,𝑣 𝑠,𝑘 ),<label>(8)</label></formula><p>where 𝜖 denotes the margin, and 𝑠 𝜙 𝑢,𝑣 is the similarity score. With possible extension to multiple meta-paths, the bi-directional contrastive objective is derived as:</p><formula xml:id="formula_10">L 𝑎𝑢𝑥 = 𝜙 L 𝜙 𝑆 𝑣𝑠 + L 𝜙 𝑆 𝑣𝑡 ,<label>(9)</label></formula><p>where the summation is over a number of meta-paths. The contrasive loss explores the relevance of node types by enforcing the learning on heterogeneous semantics existing in node types. The characteristics to some degree reflect the way of HG organization and help learn cross-domain knowledge. Associating the contrastive loss with the objective of the primary meta-learning tasks (i.e., node classification), the joint objective is as follows:</p><formula xml:id="formula_11">L 𝑗𝑜𝑖𝑛𝑡 = L 𝑚𝑒𝑡𝑎 + 𝜂L 𝑎𝑢𝑥 , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where 𝜂 is a trade-off parameter for weighting cross-domain contrastive regularization. Let Θ denote the set of all parameters including heterogeneous graph aggregator 𝜃 𝑔 , node label classifier 𝜃 𝑐 , domain critic module 𝜃 𝑑 and the contrastive regularization component 𝜃 𝑎 . The parameters are updated via gradient descent over the above joint objective as follows:</p><formula xml:id="formula_13">Θ = Θ − 𝜇∇ Θ L 𝑗𝑜𝑖𝑛𝑡 , (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>where 𝜇 is the learning rate. With the incorporation of cross-domain self-supervised information, the contrasive regularization assists the model to explore unlabeled data across multiple source domains and improves its capability of modeling HGs to promote node classification in the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting</head><p>5.1.1 Datasets. We evaluate CrossHG-Meta on AMiner, U.S. Patents and Amazon data under the cross-domain scenario, for which we adopt the leave-one-out setting as in previous works <ref type="bibr" target="#b17">[14,</ref><ref type="bibr" target="#b25">22]</ref>, where one domain is randomly chosen as the target domain with the remaining as source domains, except for a specific domain being the common source domain upon which meta-validation is conducted. We discuss the dataset details in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines</head><p>. We compare our model with baseline methods in five groups: 1) Graph embedding methods, for which we employ node2vec <ref type="bibr" target="#b10">[7]</ref> and metapath2vec (m2v) <ref type="bibr" target="#b6">[3]</ref>; 2) Graph neural networks including GCN <ref type="bibr" target="#b15">[12]</ref> and GraphSage <ref type="bibr" target="#b12">[9]</ref> for homogeneous graphs, and R-GCN <ref type="bibr" target="#b24">[21]</ref> and HAN <ref type="bibr" target="#b36">[33]</ref> for HGs; 3) Fewshot learning algorithms, i.e., MAML <ref type="bibr" target="#b8">[5]</ref> and ProtoNet <ref type="bibr" target="#b29">[26]</ref>; 4) Graph few-shot learning models including our created MAML-GCN and Proto-GCN, as well as MAML-Sage and Proto-Sage by replacing GCN with GraphSage; besides, we compare to GPN <ref type="bibr" target="#b19">[16]</ref>, G-Meta <ref type="bibr" target="#b13">[10]</ref>, Meta-SGC <ref type="bibr" target="#b47">[44]</ref> and AMM-GNN <ref type="bibr" target="#b35">[32]</ref>; 5) Domain generalization techniques incorporating Feature-Critic (FC) <ref type="bibr" target="#b17">[14]</ref> and MLDG <ref type="bibr" target="#b16">[13]</ref> which are associated with our heterogeneous graph encoder and modified to suit the problem setting. Details of baselines will be discussed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Reproducibility Details.</head><p>We follow the general workflow of few-shot learning under the leave-one-out setting, where the common source domain provides meta-evaluation set by random splitting the classes in it. Regarding the split randomness, we conduct 5 random runs for each 𝑁 -way 𝐾-shot problem, and report the average accuracy over the 5 runs. Considering the small number of support set in K = {1, 3, 5, 10}-shot tasks, we evaluate model performances based on 5000 repeated few-shot tasks for each experiment and present the average results. More experimental settings can be found in reproducibility supplement of Appendix D. Particularly, CrossHG-Meta achieves performance improvements for product classification tasks on Amazon data, while a majority of the baselines encounter difficulties under 10-way classification setting. It shows the robustness of our model to make better predictions against the baselines in a relatively large label space. The results demonstrate the effectiveness of CrossHG-Meta in dealing with domain shifts and achieving cross-domain generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Overall</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Ablation Study.</head><p>We analyze the impacts of different components in CrossHG-Meta, aiming for the following analysis: (RQ1) training with one single v.s. multiple source domains; (RQ2) controlling domain shifts or not; (RQ3) with v.s. without cross-domain contrastive regularization; (RQ4) impact of meta-path groups on model performances.</p><p>Accordingly, we conduct several ablation studies by creating three variants of CrossHG-Meta, including: (a) M-Single -which conducts general gradient-based meta-learning with single domain data (the common source domain), and we augment the variant by reducing cross-domain contrastive regularization to the metapath prediction task <ref type="bibr" target="#b14">[11]</ref>, which is applicable to single domain; (b) M-Multi -which learns from multiple source domains along with cross-domain contrastive regularization, but with no treatment for domain shifts; (c) M-NoAux -which handles domain shifts with the proposed cross-domain meta-learning framework but excludes the self-supervised tasks for contrastive regularization. Performances of the variants are shown in Fig. <ref type="figure" target="#fig_4">3</ref>, from which we find answers for:</p><p>• RQ1. M-Single shows inferior results compared to other variants, justifying that adopting multiple source domains can boost the model performances compared to single source domain.    • RQ2. M-Multi is outperformed by CrossHG-Meta. It indicates the treatment of domain shifts in the cross-domain meta-learning module improves cross-domain generalizability. • RQ3. M-NoAux performs inferiorly to CrossHG-Meta, for that cross-domain contrastive regularization helps battle low-data problem and transfer beneficial graph knowledge.</p><p>In addition, we evaluate the impact of different meta-path sets (RQ4) in heterogeneous graph aggregation of CrossHG-Meta. As the results reported in Table <ref type="table" target="#tab_9">5</ref> (AMiner data, paper classification), CrossHG-Meta achieves better performances using a set of two meta-paths than single meta-path in all the cases. Moreover, we experiment with a Random variant which aggregates over randomly sampled neighbors (taking HG as a homogeneous graph). It shows in some cases that performance of random sampling could outperform sampling based on single meta-path, for which we infer the cause to be the small population of available neighbors under certain meta-path. However, CrossHG-Meta with two meta-paths keeps the best performances, implying the necessity to leverage suitable meta-paths to capture semantic information in HG.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Case Study: Embedding Visualization. To qualitatively evaluate the model effectiveness, we employ t-SNE to visualize the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROCEDURE OF CROSSHG-META</head><p>The pseudocode for the proposed method is as follows.</p><p>Algorithm Calculate L 𝑑 using Eq. ( <ref type="formula" target="#formula_6">5</ref>)</p><p>9:</p><p>Obtain 𝜃 𝑡 𝑔 , 𝜃 𝑡 𝑐 using Eq. (3) 10:</p><p>end for 11:</p><p>Calculate meta-optimization loss using Eq. ( <ref type="formula" target="#formula_5">4</ref>)</p><p>12:</p><p>Calculate contrastive loss using Eq. ( <ref type="formula" target="#formula_10">9</ref>)</p><p>13:</p><p>Update Θ using Eq. ( <ref type="formula" target="#formula_13">11</ref>) 14: end while B BASELINES</p><p>(1) Graph embedding methods are evaluated with a logistic regression classifier fed with few-shot labeled data.</p><p>-Node2vec <ref type="bibr" target="#b10">[7]</ref> is a random-walk based unsupervised learning method for homogeneous graph. -Metapath2vec <ref type="bibr" target="#b6">[3]</ref> generates meta-path based random walks in HGs to conduct unsupervised learning. (2) Graph neural networks are tested following the workflow in <ref type="bibr" target="#b47">[44]</ref> for the scenario.</p><p>-GCN learns node embeddings here by taking HGs as homogeneous graphs, ignoring all node and edge types. -GraphSage <ref type="bibr" target="#b12">[9]</ref> learns embeddings inductively with neighborhood information, where we adopt mean pooling aggregation. -R-GCN <ref type="bibr" target="#b24">[21]</ref> deals with highly multi-relational graph by assigning a distinct weight matrix to each type of relation.</p><p>-HAN <ref type="bibr" target="#b36">[33]</ref> learns hierarchically the node-level and semanticlevel attention for heterogeneous information aggregation. (3) Few-shot learning algorithms here take examples as i.i.d.</p><p>data with no consideration of graph topology.</p><p>-MAML <ref type="bibr" target="#b8">[5]</ref> initializes network parameters with related tasks upon which a few fast updates achieve good generalizability. -ProtoNet <ref type="bibr" target="#b29">[26]</ref> learns a shared metric space, where novel tasks are inferred regarding distances to the class prototypes. (4) Graph few-shot learning models integrate graph neural networks with few-shot methods to conquer low-data regime.</p><p>-MAML-GCN <ref type="bibr" target="#b47">[44]</ref> employs GCN as the graph base leaner to be meta-learned by MAML. -MAML-Sage is created the way similarly as MAML-GCN to serve as another baseline for experimental comparison.</p><p>-Proto-GCN combines GCN with ProtoNet, which develops GCN to obtain the expressiveness in few-shot scenario. -Proto-Sage replaces GCN in Proto-GCN with GraphSage to test the performance of the graph neural network variant. -GPN <ref type="bibr" target="#b19">[16]</ref> builds dual networks for weighted metric learning by propagating node importance based on graph topology. -G-Meta <ref type="bibr" target="#b13">[10]</ref> samples local subgraph surrounding the target node to transfer subgraph-specific information. -Meta-SGC <ref type="bibr" target="#b47">[44]</ref> trains SGC <ref type="bibr" target="#b37">[34]</ref> with MAML, thus avoiding adjacency computation with pre-calculated node embeddings. -AMM-GNN <ref type="bibr" target="#b35">[32]</ref> extends Meta-SGC using attention mechanism to transform the input features for meta-tasks. (5) Domain generalization techniques gain domain generalizability by exposing the training procedure to domain shifts. -Feature-Critic <ref type="bibr" target="#b17">[14]</ref> meta-learns auxiliary loss to obtain a feature extractor for visual prediction. We associate it with the proposed heterogeneous graph encoder to suit the crossdomain few-shot learning problem setting for HGs. -MLDG <ref type="bibr" target="#b16">[13]</ref> simulates domain shifts within mini-batches for image classification on the same categories. We apply our graph encoder and few-shot classifier to it for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DATASETS</head><p>AMiner. This dataset is extracted from AMiner system data <ref type="bibr" target="#b31">[28]</ref>. We sample popular venues to form four domains according to the Google Scholar Metrics, such as System, AI, Mathematics and Interdisciplinary. The HG built on each domain consists of three node types (namely paper (P), author (A), and venue (V)) and three relation types (namely paper-paper citing (PP), author-paper writing (PA), paper-venue publishing (PV)). Papers are labeled based on the category of its venue, while authors are annotated by the majority label of their publications. We take System as the common source domain for the few-shot paper classification and author classification tasks under the settings of 2,3-way 1,3-shot. U.S. Patents. It is released by United States Patent and Trademark Office (USPTO)<ref type="foot" target="#foot_0">1</ref> . We extract patents issued in 1990-1999 in four application areas, i.e., Electricity, Chemistry, Materials/Metal and Computer Science. For each HG, patents (P) and assignees (A) are connected with possible patent-assignee relationship (PA) and citation relationship (PP). Patents are labeled by the assigned patent class code. We conduct patent classification tasks with Electricity as the common source domain under 2,3-way 5,10-shot settings. Amazon. We utilize five product domains from Amazon data <ref type="bibr" target="#b23">[20]</ref>, namely Office, Music, Outdoors , Arts and Toy, each containing node types of product (P) and brand (B). Product-brand (PB) edges showing the brand of the product may exist. Complementary relationship called "co-viewed" (PP) is leveraged <ref type="bibr" target="#b9">[6]</ref>. Low-level categories in Amazon taxonomy system are applied to annotate the products. The 5,10-way 3,5-shot product classification task will utilize Office for meta-evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTAL SETTING</head><p>We implement CrossHG-Meta with TensorFlow and run it on a NVIDIA Tesla V100 GPU. In CrossHG-Meta, 32 hidden units are utilized for neighborhood aggregation under each meta-path and for the node itself, followed by a non-linear transformation layer with 32 output units to fuse them. For domain critic, a two-layer MLP with the same hidden size is applied. We implement crossdomain contrastive regularization via a MLP with 16 hidden units. We use popular meta-paths for heterogeneous information aggregation. Specifically, meta-path set {PAP, PVP} and {APA, APVPA} are adopted for paper classification and author classification on AMiner data respectively. The set of {PAP, PP} is utilized on U.S. Patents data. Amazon data is learned under meta-path set {PBP, PP}.</p><p>For contrastive regularization, we employ one random meta-path for each dataset with a batch of 32 anchor nodes. The meta-path is PAP in paper classification task, APA for author classification task. Meta-path PAP and PBP are used on U.S. Patents and Amazon data. For node2vec and metapath2vec, we adopt the default parameters. Baselines associated with graph neural networks utilize the same hidden size as CrossHG-Meta for fair comparison. For HAN, we adopt the same meta-paths for information aggregation as our graph encoder. For GPN, G-Meta and Meta-SGC, we use their published implementations with the default settings. As for AMM-GNN with no implementation, we implement it and run experiments with the parameter setting in their paper. Implementation of MLDG and FC are adapted to be associated with our graph encoder and suitable for the problem setting. Besides, model learning rate is searched in {0.01, 0.001, 0.0005, 0.0001} for all the baselines. MAML-based baselines share the same fast gradient update rate and steps as CrossHG-Meta to make fair assessment. The model is trained with Adam optimizer and evaluated every 10 iterations with an early stopping strategy with a patience of 150. Hyperparameters including initial learning rate, fast gradient steps and domain adaptation update rate of CrossHG-Meta can be found in Table <ref type="table" target="#tab_12">7</ref>. Source code of CrossHG-Meta: https://github.com/aslandery/CrossHG-Meta.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of problem formulations. (a) Few-shot learning in one domain. Meta-knowledge is transferred from source classes to novel target classes. (b) Domain adaption with access to target domain. Meta-knowledge is transferred from source domain to target domain on the same classes. (c) Cross-domain few-shot learning (our setting). Meta-knowledge is transferred from source domains to novel classes in target domain, where inference is conducted with few-shot labeled data.</figDesc><graphic url="image-1.png" coords="2,53.80,83.69,240.24,199.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 2 )</head><label>2</label><figDesc>We propose a novel model called CrossHG-Meta to tackle graph structure heterogeneity, domain shift issue and labeled data insufficiency through joint optimization of heterogeneous graph aggregation, cross-domain meta-learning, and cross-domain contrastive learning.(3) Extensive experiments on three real-world HG datasets including academic network, patent network, and commercial network (i.e., AMiner, U.S. Patents, and Amazon) with multiple domains show the superior performance of CrossHG-Meta over the state-of-the-art approaches for multiple few-shot node classification tasks under the cross-domain setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The overall framework of CrossHG-Meta training procedure, employing both primary cross-domain meta-tasks for node classification (the right part of (a)) and auxiliary self-supervised learning tasks (the left part of (a)) that deploy positive and negative node pairs to produce contrastive regularization; (b) Heterogeneous graph aggregator (AGG) generates node embeddings by aggregating from multiple semantic contexts; (c) Domain critic measures domain transfer loss to guide the cross-domain adaptation procedure between meta-tasks.</figDesc><graphic url="image-2.png" coords="4,53.80,83.69,504.41,178.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Few-shot node classification accuracy (%) of CrossHG-Meta variants for multiple domains on AMiner (a,b), U.S. Patents (c) and Amazon (d) data.</figDesc><graphic url="image-6.png" coords="7,58.05,568.52,103.30,63.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Author node embedding visualization for 3-way author classification task for AI on AMiner data. Each point represents one author. Blue, orange and green colors show computer vision, NLP and web categories respectively.</figDesc><graphic url="image-10.png" coords="9,64.87,167.15,96.09,62.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of the trade-off weight 𝜂 for paper classification task for AI &amp; Mathematics domains on AMiner data. graph datasets, i.e., AMiner, U.S. Patents and Amazon, show the effectiveness of CrossHG-Meta for multiple few-shot node classification tasks under various cross-domain settings.</figDesc><graphic url="image-13.png" coords="9,62.69,307.17,100.90,68.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Despite the bulk of work, they study few-shot learning mostly on homogeneous graphs and generally assume the shared distribution for training and testing tasks in the same domain, making it unsuitable for cross-domain few-shot learning on HGs. Domain Adaptation and Generalization. Research works addressing domain shift issue caused by learning from different but relevant domains split into domain adaptation and domain generalization branches. Domain adaptation focuses on adaptation to the target domain by accessing the unlabeled data in the target domain during training. Some attempts apply domain adaptation to homogeneous graphs by extracting domain invariant features to reduce the distribution discrepancy via distance metric</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>as 𝐴 1 𝐴 2 . . . 𝐴 𝑙+1 ), where each transition 𝑟 𝑖 between the entity types 𝐴 𝑖 (𝐴 𝑖 ∈ A ) is an intermediate edge of type 𝑟 𝑖 (𝑟 𝑖 ∈ R).</figDesc><table><row><cell>Definition 3. Meta-path based neighbors. Given a HG G,</cell></row><row><cell>𝜙 𝑣 meta-path 𝜙 and node 𝑣 ∈ V, the meta-path based neighbors N</cell></row><row><cell>represent the nodes connected to node 𝑣 by meta-path 𝜙.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>𝑣𝑠,𝑖 , 𝜏 𝑣𝑡,𝑖 }, L is the loss on the support set of 𝜏 𝑣𝑡,𝑖 and 𝛽 denotes the update rate. The adapted network is evaluated on the query set of 𝜏 𝑣𝑡,𝑖 to obtain the classification loss L 𝑞𝑢𝑒 𝑐 (𝜏 𝑣𝑡,𝑖 ; 𝜃 𝑡 𝑔 , 𝜃 𝑡 𝑐 ), which reflects its efficacy on 𝜏 𝑣𝑡,𝑖 after the intermediate cross-domain adaptation steps.</figDesc><table /><note>𝑠𝑢𝑝 𝑐 (𝜏 𝑣𝑡,𝑖 ; 𝜃 𝑡 𝑔 , 𝜃 ′ 𝑐 )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>From these tables, we can find CrossHG-Meta achieves the best performances under all the settings. Additional observations are discussed as follows: Meta performs the best in all the cases. Compared to graph few-shot learning baselines without solving the domain shift issue, the proposed model extracts hybrid semantics and combats domain shifts during meta-optimization such that the learned graph encoder represents nodes better in the target domain. The model also outperforms domain generalization baselines, verifying that CrossHG-Meta transfers cross-domain metaknowledge well for the generalization to the novel target domain.</figDesc><table><row><cell>• Metapath2vec performs better than node2vec as it considers</cell></row><row><cell>heterogeneous graph information to learn node embeddings.</cell></row><row><cell>• GCN and GraphSage outperform network embedding methods</cell></row><row><cell>generally with the adoption of the end-to-end training strategy,</cell></row><row><cell>while HAN and R-GCN further show an improvement with the</cell></row><row><cell>consideration of graph heterogeneity.</cell></row><row><cell>• Traditional meta-learning baselines, MAML and ProtoNet, may</cell></row><row><cell>have inferior performances in some cases since the data is treated</cell></row><row><cell>as i.i.d. samples with no use of graph structure. By incorporating</cell></row><row><cell>topological information, graph few-shot learning models are in</cell></row><row><cell>general better than ProtoNet and MAML.</cell></row><row><cell>• FC and MLDG show competitive performances against graph</cell></row><row><cell>few-shot learning baselines in some cases, probably due to the</cell></row><row><cell>design for addressing domain shifts and the utilization of our</cell></row><row><cell>heterogeneous graph aggregator.</cell></row><row><cell>• CrossHG-</cell></row></table><note>Performance. Performances (in term of accuracy<ref type="bibr" target="#b47">[44]</ref>) of CrossHG-Meta and baseline models are reported in Table 1 (AMiner data, paper classification), Table 2 (AMiner data, author classification), Table 3 (U.S. Patents data) and Table 4 (Amazon data), where the best results are highlighted in bold and the best baseline scores are underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Paper classification accuracy (%) for AI, Mathematics &amp; Interdisciplinary on AMiner data.</figDesc><table><row><cell>Methods</cell><cell>2-way</cell><cell>AI</cell><cell cols="5">Mathematics 3-way 2-way 3-way 2-way Interdisciplinary 3-way</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1-shot</cell><cell></cell></row><row><cell>node2vec [7]</cell><cell>53.68</cell><cell></cell><cell>39.69</cell><cell>55.25</cell><cell>46.72</cell><cell>54.74</cell><cell>35.31</cell></row><row><cell>m2v [3]</cell><cell>65.22</cell><cell></cell><cell>59.27</cell><cell>74.49</cell><cell>67.02</cell><cell>71.33</cell><cell>60.99</cell></row><row><cell>GCN [12]</cell><cell>75.64</cell><cell></cell><cell>62.91</cell><cell>77.99</cell><cell>73.47</cell><cell>77.18</cell><cell>61.38</cell></row><row><cell>GraphSage [9]</cell><cell>76.89</cell><cell></cell><cell>61.60</cell><cell>75.02</cell><cell>75.64</cell><cell>74.69</cell><cell>60.14</cell></row><row><cell>HAN [33]</cell><cell>77.24</cell><cell></cell><cell>63.86</cell><cell>78.56</cell><cell>75.64</cell><cell>75.59</cell><cell>63.87</cell></row><row><cell>R-GCN [21]</cell><cell>76.91</cell><cell></cell><cell>63.59</cell><cell>79.45</cell><cell>75.94</cell><cell>77.54</cell><cell>65.42</cell></row><row><cell>ProtoNet [26]</cell><cell>74.63</cell><cell></cell><cell>64.55</cell><cell>85.70</cell><cell>79.13</cell><cell>78.71</cell><cell>65.71</cell></row><row><cell>MAML [5]</cell><cell>73.98</cell><cell></cell><cell>65.85</cell><cell>87.92</cell><cell>76.58</cell><cell>77.91</cell><cell>66.57</cell></row><row><cell>Proto-GCN</cell><cell>76.29</cell><cell></cell><cell>68.29</cell><cell>86.30</cell><cell>77.87</cell><cell>76.52</cell><cell>66.34</cell></row><row><cell>Proto-Sage</cell><cell>75.60</cell><cell></cell><cell>64.44</cell><cell>87.97</cell><cell>73.68</cell><cell>77.88</cell><cell>68.70</cell></row><row><cell>MAML-GCN</cell><cell>74.90</cell><cell></cell><cell>69.00</cell><cell>87.89</cell><cell>76.05</cell><cell>79.35</cell><cell>67.26</cell></row><row><cell>MAML-Sage</cell><cell>78.29</cell><cell></cell><cell>67.59</cell><cell>89.24</cell><cell>78.54</cell><cell>79.59</cell><cell>67.49</cell></row><row><cell>G-Meta [10]</cell><cell>76.50</cell><cell></cell><cell>63.27</cell><cell>86.75</cell><cell>75.60</cell><cell>78.43</cell><cell>65.27</cell></row><row><cell>GPN [16]</cell><cell>75.43</cell><cell></cell><cell>62.53</cell><cell>86.76</cell><cell>75.64</cell><cell>79.50</cell><cell>64.25</cell></row><row><cell>Meta-SGC [44]</cell><cell>77.41</cell><cell></cell><cell>61.85</cell><cell>87.57</cell><cell>77.86</cell><cell>75.69</cell><cell>67.31</cell></row><row><cell>AMM-GNN [32]</cell><cell>81.84</cell><cell></cell><cell>69.48</cell><cell>89.64</cell><cell>81.92</cell><cell>80.69</cell><cell>69.71</cell></row><row><cell>FC [14]</cell><cell>81.42</cell><cell></cell><cell>68.27</cell><cell>89.65</cell><cell>83.62</cell><cell>80.31</cell><cell>70.54</cell></row><row><cell>MLDG [13]</cell><cell>82.60</cell><cell></cell><cell>69.89</cell><cell>90.68</cell><cell>85.45</cell><cell>81.55</cell><cell>72.15</cell></row><row><cell>CrossHG-Meta</cell><cell>85.36</cell><cell></cell><cell>72.43</cell><cell>91.45</cell><cell>89.54</cell><cell>82.93</cell><cell>74.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3-shot</cell><cell></cell></row><row><cell>node2vec [7]</cell><cell>62.73</cell><cell></cell><cell>46.73</cell><cell>65.50</cell><cell>51.36</cell><cell>59.43</cell><cell>43.87</cell></row><row><cell>m2v [3]</cell><cell>79.96</cell><cell></cell><cell>71.72</cell><cell>86.35</cell><cell>74.57</cell><cell>82.80</cell><cell>68.80</cell></row><row><cell>GCN [12]</cell><cell>86.93</cell><cell></cell><cell>73.34</cell><cell>90.83</cell><cell>79.66</cell><cell>87.59</cell><cell>74.68</cell></row><row><cell>GraphSage [9]</cell><cell>85.26</cell><cell></cell><cell>73.01</cell><cell>88.25</cell><cell>80.39</cell><cell>86.09</cell><cell>72.96</cell></row><row><cell>HAN [33]</cell><cell>86.47</cell><cell></cell><cell>74.58</cell><cell>89.61</cell><cell>82.39</cell><cell>87.51</cell><cell>74.88</cell></row><row><cell>R-GCN [21]</cell><cell>86.78</cell><cell></cell><cell>75.64</cell><cell>90.16</cell><cell>83.57</cell><cell>88.26</cell><cell>75.91</cell></row><row><cell>ProtoNet [26]</cell><cell>83.81</cell><cell></cell><cell>70.37</cell><cell>91.07</cell><cell>87.41</cell><cell>87.89</cell><cell>75.01</cell></row><row><cell>MAML [5]</cell><cell>85.57</cell><cell></cell><cell>72.13</cell><cell>90.35</cell><cell>86.89</cell><cell>90.13</cell><cell>76.26</cell></row><row><cell>Proto-GCN</cell><cell>88.66</cell><cell></cell><cell>77.84</cell><cell>92.04</cell><cell>87.67</cell><cell>85.91</cell><cell>78.75</cell></row><row><cell>Proto-Sage</cell><cell>86.17</cell><cell></cell><cell>77.11</cell><cell>91.72</cell><cell>90.13</cell><cell>87.98</cell><cell>77.62</cell></row><row><cell>MAML-GCN</cell><cell>88.60</cell><cell></cell><cell>73.70</cell><cell>94.75</cell><cell>90.44</cell><cell>88.63</cell><cell>81.08</cell></row><row><cell>MAML-Sage</cell><cell>87.26</cell><cell></cell><cell>73.18</cell><cell>94.81</cell><cell>87.57</cell><cell>87.24</cell><cell>80.47</cell></row><row><cell>G-Meta [10]</cell><cell>82.37</cell><cell></cell><cell>72.53</cell><cell>92.65</cell><cell>87.56</cell><cell>87.04</cell><cell>78.83</cell></row><row><cell>GPN [16]</cell><cell>80.98</cell><cell></cell><cell>71.56</cell><cell>91.59</cell><cell>86.44</cell><cell>86.66</cell><cell>77.32</cell></row><row><cell>Meta-SGC [44]</cell><cell>83.50</cell><cell></cell><cell>71.57</cell><cell>93.45</cell><cell>87.68</cell><cell>85.86</cell><cell>79.43</cell></row><row><cell>AMM-GNN [32]</cell><cell>89.18</cell><cell></cell><cell>76.19</cell><cell>95.60</cell><cell>91.48</cell><cell>88.97</cell><cell>81.33</cell></row><row><cell>FC [14]</cell><cell>86.53</cell><cell></cell><cell>74.92</cell><cell>93.57</cell><cell>87.42</cell><cell>86.37</cell><cell>80.72</cell></row><row><cell>MLDG [13]</cell><cell>87.40</cell><cell></cell><cell>75.23</cell><cell>95.91</cell><cell>88.34</cell><cell>88.56</cell><cell>82.53</cell></row><row><cell>CrossHG-Meta</cell><cell>90.16</cell><cell></cell><cell>79.20</cell><cell>96.63</cell><cell>92.18</cell><cell>90.07</cell><cell>85.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Author classification accuracy (%) for AI, Mathematics &amp; Interdisciplinary on AMiner data.</figDesc><table><row><cell>Methods</cell><cell>2-way</cell><cell>AI</cell><cell cols="5">Mathematics 3-way 2-way 3-way 2-way Interdisciplinary 3-way</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1-shot</cell><cell></cell></row><row><cell>node2vec [7]</cell><cell>52.47</cell><cell></cell><cell>37.25</cell><cell>54.53</cell><cell>38.75</cell><cell>51.82</cell><cell>36.65</cell></row><row><cell>m2v [3]</cell><cell>63.12</cell><cell></cell><cell>50.45</cell><cell>69.84</cell><cell>61.67</cell><cell>61.35</cell><cell>50.29</cell></row><row><cell>GCN [12]</cell><cell>70.77</cell><cell></cell><cell>61.34</cell><cell>72.93</cell><cell>65.98</cell><cell>68.88</cell><cell>59.82</cell></row><row><cell>GraphSage [9]</cell><cell>69.59</cell><cell></cell><cell>60.72</cell><cell>71.82</cell><cell>66.01</cell><cell>69.52</cell><cell>55.04</cell></row><row><cell>HAN [33]</cell><cell>70.68</cell><cell></cell><cell>61.87</cell><cell>72.04</cell><cell>65.78</cell><cell>71.57</cell><cell>58.69</cell></row><row><cell>R-GCN [21]</cell><cell>71.02</cell><cell></cell><cell>62.31</cell><cell>72.26</cell><cell>67.54</cell><cell>72.16</cell><cell>59.78</cell></row><row><cell>ProtoNet [26]</cell><cell>58.07</cell><cell></cell><cell>49.63</cell><cell>73.91</cell><cell>67.00</cell><cell>64.65</cell><cell>50.07</cell></row><row><cell>MAML [5]</cell><cell>67.14</cell><cell></cell><cell>53.94</cell><cell>77.36</cell><cell>62.74</cell><cell>74.38</cell><cell>56.68</cell></row><row><cell>Proto-GCN</cell><cell>70.33</cell><cell></cell><cell>57.57</cell><cell>81.86</cell><cell>70.23</cell><cell>73.06</cell><cell>63.20</cell></row><row><cell>Proto-Sage</cell><cell>73.41</cell><cell></cell><cell>57.92</cell><cell>84.31</cell><cell>67.76</cell><cell>71.76</cell><cell>59.49</cell></row><row><cell>MAML-GCN</cell><cell>75.54</cell><cell></cell><cell>60.66</cell><cell>83.35</cell><cell>73.26</cell><cell>73.61</cell><cell>64.45</cell></row><row><cell>MAML-Sage</cell><cell>76.14</cell><cell></cell><cell>62.87</cell><cell>85.41</cell><cell>77.43</cell><cell>75.30</cell><cell>65.38</cell></row><row><cell>G-Meta [10]</cell><cell>70.98</cell><cell></cell><cell>55.37</cell><cell>83.14</cell><cell>74.68</cell><cell>74.33</cell><cell>61.38</cell></row><row><cell>GPN [16]</cell><cell>69.71</cell><cell></cell><cell>57.42</cell><cell>82.16</cell><cell>75.66</cell><cell>73.47</cell><cell>59.98</cell></row><row><cell>Meta-SGC [44]</cell><cell>71.41</cell><cell></cell><cell>55.46</cell><cell>84.24</cell><cell>75.96</cell><cell>74.12</cell><cell>62.98</cell></row><row><cell>AMM-GNN [32]</cell><cell>76.16</cell><cell></cell><cell>64.41</cell><cell>87.63</cell><cell>81.07</cell><cell>76.64</cell><cell>66.82</cell></row><row><cell>FC [14]</cell><cell>75.06</cell><cell></cell><cell>62.48</cell><cell>85.65</cell><cell>77.92</cell><cell>75.37</cell><cell>64.82</cell></row><row><cell>MLDG [13]</cell><cell>74.83</cell><cell></cell><cell>64.13</cell><cell>85.76</cell><cell>78.69</cell><cell>76.01</cell><cell>65.17</cell></row><row><cell>CrossHG-Meta</cell><cell>76.92</cell><cell></cell><cell>65.47</cell><cell>88.90</cell><cell>83.04</cell><cell>77.29</cell><cell>68.41</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3-shot</cell><cell></cell></row><row><cell>node2vec [7]</cell><cell>64.94</cell><cell></cell><cell>42.36</cell><cell>63.97</cell><cell>48.24</cell><cell>53.85</cell><cell>40.85</cell></row><row><cell>m2v [3]</cell><cell>75.84</cell><cell></cell><cell>61.57</cell><cell>82.48</cell><cell>73.25</cell><cell>74.58</cell><cell>59.49</cell></row><row><cell>GCN [12]</cell><cell>78.51</cell><cell></cell><cell>63.52</cell><cell>88.84</cell><cell>76.95</cell><cell>76.74</cell><cell>61.70</cell></row><row><cell>GraphSage [9]</cell><cell>80.05</cell><cell></cell><cell>65.78</cell><cell>87.18</cell><cell>78.75</cell><cell>75.34</cell><cell>61.28</cell></row><row><cell>HAN [33]</cell><cell>81.22</cell><cell></cell><cell>64.57</cell><cell>88.26</cell><cell>79.52</cell><cell>76.59</cell><cell>66.07</cell></row><row><cell>R-GCN [21]</cell><cell>79.53</cell><cell></cell><cell>64.12</cell><cell>88.12</cell><cell>78.95</cell><cell>78.75</cell><cell>69.51</cell></row><row><cell>ProtoNet [26]</cell><cell>69.14</cell><cell></cell><cell>58.15</cell><cell>79.07</cell><cell>75.76</cell><cell>73.75</cell><cell>64.14</cell></row><row><cell>MAML [5]</cell><cell>76.53</cell><cell></cell><cell>62.50</cell><cell>87.96</cell><cell>78.83</cell><cell>79.11</cell><cell>69.32</cell></row><row><cell>Proto-GCN</cell><cell>80.39</cell><cell></cell><cell>70.18</cell><cell>91.14</cell><cell>84.73</cell><cell>83.36</cell><cell>75.77</cell></row><row><cell>Proto-Sage</cell><cell>78.46</cell><cell></cell><cell>69.16</cell><cell>93.29</cell><cell>83.78</cell><cell>79.57</cell><cell>75.51</cell></row><row><cell>MAML-GCN</cell><cell>82.36</cell><cell></cell><cell>67.49</cell><cell>91.96</cell><cell>85.46</cell><cell>87.02</cell><cell>73.59</cell></row><row><cell>MAML-Sage</cell><cell>83.68</cell><cell></cell><cell>68.03</cell><cell>94.25</cell><cell>88.10</cell><cell>88.01</cell><cell>76.86</cell></row><row><cell>G-Meta [10]</cell><cell>78.42</cell><cell></cell><cell>63.55</cell><cell>91.64</cell><cell>83.59</cell><cell>84.23</cell><cell>73.06</cell></row><row><cell>GPN [16]</cell><cell>79.53</cell><cell></cell><cell>61.47</cell><cell>89.92</cell><cell>82.67</cell><cell>82.18</cell><cell>73.13</cell></row><row><cell>Meta-SGC [44]</cell><cell>81.78</cell><cell></cell><cell>60.32</cell><cell>93.48</cell><cell>85.52</cell><cell>85.56</cell><cell>70.58</cell></row><row><cell>AMM-GNN [32]</cell><cell>84.01</cell><cell></cell><cell>74.08</cell><cell>94.52</cell><cell>90.69</cell><cell>88.78</cell><cell>81.76</cell></row><row><cell>FC [14]</cell><cell>78.54</cell><cell></cell><cell>73.98</cell><cell>93.72</cell><cell>87.64</cell><cell>85.71</cell><cell>78.35</cell></row><row><cell>MLDG [13]</cell><cell>79.87</cell><cell></cell><cell>74.26</cell><cell>93.19</cell><cell>88.43</cell><cell>85.35</cell><cell>80.23</cell></row><row><cell>CrossHG-Meta</cell><cell>85.93</cell><cell></cell><cell>75.74</cell><cell>95.87</cell><cell>91.92</cell><cell>89.96</cell><cell>83.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Patent classification accuracy (%) for Chemistry, Material/Metal &amp; Computer Science on U.S. Patents data.</figDesc><table><row><cell>Methods</cell><cell cols="6">Chemistry 2-way 3-way 2-way Material/Metal 3-way 2-way Computer Science 3-way</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5-shot</cell><cell></cell><cell></cell></row><row><cell>node2vec [7]</cell><cell>51.01</cell><cell>34.83</cell><cell>54.97</cell><cell>44.66</cell><cell>51.53</cell><cell>33.89</cell></row><row><cell>m2v [3]</cell><cell>61.47</cell><cell>43.46</cell><cell>72.58</cell><cell>61.69</cell><cell>54.75</cell><cell>39.27</cell></row><row><cell>GCN [12]</cell><cell>56.06</cell><cell>45.69</cell><cell>79.84</cell><cell>68.19</cell><cell>56.59</cell><cell>38.57</cell></row><row><cell>GraphSage [9]</cell><cell>59.73</cell><cell>45.63</cell><cell>74.87</cell><cell>63.08</cell><cell>57.78</cell><cell>41.76</cell></row><row><cell>HAN [33]</cell><cell>60.37</cell><cell>46.82</cell><cell>81.07</cell><cell>69.26</cell><cell>56.87</cell><cell>42.03</cell></row><row><cell>R-GCN [21]</cell><cell>62.37</cell><cell>47.47</cell><cell>82.15</cell><cell>69.84</cell><cell>58.94</cell><cell>42.15</cell></row><row><cell>ProtoNet [26]</cell><cell>62.04</cell><cell>45.94</cell><cell>75.93</cell><cell>63.14</cell><cell>54.90</cell><cell>39.90</cell></row><row><cell>MAML [5]</cell><cell>64.25</cell><cell>50.43</cell><cell>83.01</cell><cell>72.63</cell><cell>57.79</cell><cell>41.96</cell></row><row><cell>Proto-GCN</cell><cell>64.60</cell><cell>51.53</cell><cell>82.89</cell><cell>71.02</cell><cell>57.33</cell><cell>42.21</cell></row><row><cell>Proto-Sage</cell><cell>63.16</cell><cell>47.25</cell><cell>80.63</cell><cell>68.52</cell><cell>57.14</cell><cell>41.44</cell></row><row><cell>MAML-GCN</cell><cell>65.24</cell><cell>50.18</cell><cell>83.12</cell><cell>73.16</cell><cell>59.06</cell><cell>43.11</cell></row><row><cell>MAML-Sage</cell><cell>65.31</cell><cell>49.69</cell><cell>84.38</cell><cell>72.37</cell><cell>57.42</cell><cell>42.81</cell></row><row><cell>G-Meta [10]</cell><cell>63.56</cell><cell>48.12</cell><cell>82.14</cell><cell>70.28</cell><cell>56.18</cell><cell>40.64</cell></row><row><cell>GPN [16]</cell><cell>64.25</cell><cell>48.98</cell><cell>83.02</cell><cell>72.32</cell><cell>59.03</cell><cell>41.68</cell></row><row><cell>Meta-SGC [44]</cell><cell>63.63</cell><cell>48.24</cell><cell>78.46</cell><cell>68.92</cell><cell>59.27</cell><cell>42.23</cell></row><row><cell>AMM-GNN [32]</cell><cell>61.04</cell><cell>46.31</cell><cell>82.84</cell><cell>71.42</cell><cell>59.32</cell><cell>43.34</cell></row><row><cell>FC [14]</cell><cell>64.57</cell><cell>49.21</cell><cell>83.48</cell><cell>71.65</cell><cell>57.24</cell><cell>41.95</cell></row><row><cell>MLDG [13]</cell><cell>66.34</cell><cell>48.35</cell><cell>83.62</cell><cell>70.74</cell><cell>59.88</cell><cell>42.61</cell></row><row><cell>CrossHG-Meta</cell><cell>67.03</cell><cell>54.19</cell><cell>85.13</cell><cell>77.23</cell><cell>61.08</cell><cell>45.92</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">10-shot</cell><cell></cell><cell></cell></row><row><cell>node2vec [7]</cell><cell>53.26</cell><cell>35.29</cell><cell>59.87</cell><cell>47.39</cell><cell>52.44</cell><cell>34.59</cell></row><row><cell>m2v [3]</cell><cell>62.51</cell><cell>49.43</cell><cell>78.12</cell><cell>67.05</cell><cell>55.70</cell><cell>40.94</cell></row><row><cell>GCN [12]</cell><cell>65.81</cell><cell>48.31</cell><cell>84.71</cell><cell>73.07</cell><cell>57.48</cell><cell>42.33</cell></row><row><cell>GraphSage [9]</cell><cell>64.38</cell><cell></cell><cell>80.70</cell><cell>74.57</cell><cell>59.83</cell><cell>43.51</cell></row><row><cell>HAN [33]</cell><cell>68.89</cell><cell>48.37</cell><cell>85.52</cell><cell>75.61</cell><cell>58.77</cell><cell>43.59</cell></row><row><cell>R-GCN [21]</cell><cell>69.42</cell><cell>49.57</cell><cell>85.69</cell><cell>75.18</cell><cell>59.97</cell><cell>42.39</cell></row><row><cell>ProtoNet [26]</cell><cell>62.92</cell><cell>51.28</cell><cell>79.59</cell><cell>71.08</cell><cell>56.18</cell><cell>41.78</cell></row><row><cell>MAML [5]</cell><cell>68.88</cell><cell>52.72</cell><cell>87.21</cell><cell>78.78</cell><cell>57.22</cell><cell>42.14</cell></row><row><cell>Proto-GCN</cell><cell>68.21</cell><cell>57.85</cell><cell>86.64</cell><cell>76.30</cell><cell>59.06</cell><cell>44.23</cell></row><row><cell>Proto-Sage</cell><cell>65.02</cell><cell>54.84</cell><cell>84.32</cell><cell>74.31</cell><cell>58.71</cell><cell>43.69</cell></row><row><cell>MAML-GCN</cell><cell>70.41</cell><cell>53.22</cell><cell>87.20</cell><cell>79.28</cell><cell>60.96</cell><cell>45.38</cell></row><row><cell>MAML-Sage</cell><cell>69.81</cell><cell>51.81</cell><cell>86.31</cell><cell>78.79</cell><cell>58.17</cell><cell>46.32</cell></row><row><cell>G-Meta [10]</cell><cell>65.97</cell><cell>49.16</cell><cell>84.35</cell><cell>74.94</cell><cell>58.64</cell><cell>43.18</cell></row><row><cell>GPN [16]</cell><cell>66.58</cell><cell>50.76</cell><cell>85.53</cell><cell>75.12</cell><cell>61.09</cell><cell>44.66</cell></row><row><cell>Meta-SGC [44]</cell><cell>64.29</cell><cell>48.65</cell><cell>82.36</cell><cell>73.60</cell><cell>60.14</cell><cell>43.13</cell></row><row><cell>AMM-GNN [32]</cell><cell>68.57</cell><cell>48.56</cell><cell>86.26</cell><cell>76.96</cell><cell>62.38</cell><cell>46.27</cell></row><row><cell>FC [14]</cell><cell>69.47</cell><cell>56.24</cell><cell>86.47</cell><cell>75.68</cell><cell>60.31</cell><cell>44.58</cell></row><row><cell>MLDG [13]</cell><cell>71.64</cell><cell>54.65</cell><cell>87.42</cell><cell>77.26</cell><cell>63.12</cell><cell>45.95</cell></row><row><cell>CrossHG-Meta</cell><cell>72.14</cell><cell>59.16</cell><cell>88.91</cell><cell>82.75</cell><cell>64.85</cell><cell>49.86</cell></row><row><cell cols="7">node embeddings (under 3-way-1-shot setting with author classifi-</cell></row><row><cell cols="7">cation task for AI on AMiner data) learned by CrossHG-Meta and</cell></row><row><cell cols="7">three baseline methods, including GCN, AMM-GNN and MLDG</cell></row><row><cell cols="7">which show relatively better peformances in this setting. Nodes</cell></row><row><cell cols="7">are colored based on their categories as shown in Fig. 4. We can</cell></row><row><cell cols="7">tell that MLDG learns the node embeddings with larger inter-class</cell></row><row><cell cols="7">dissimilarity than GCN and AMM-GNN, convincing the usefulness</cell></row><row><cell cols="7">of treating domain shifts for cross-domain knowledge transfer and</cell></row><row><cell cols="7">the heterogeneous graph encoder. Node embeddings obtained by</cell></row><row><cell cols="7">CrossHG-Meta have more distinct boundaries than those obtained</cell></row><row><cell cols="7">by the baselines, showing the capability of our model to learn ef-</cell></row><row><cell cols="6">fective node embeddings for cross-domain generalization.</cell><cell></cell></row><row><cell cols="7">5.2.4 Model Performance at Various Trade-off Weight. We explore</cell></row><row><cell cols="7">the trade-off weight (𝜂 in Eq. (10)) of the contrastive regularization</cell></row><row><cell cols="7">with paper classification tasks for AI and Mathematics on AMiner</cell></row><row><cell cols="7">data. The model performances with varied weight 𝜂 values are</cell></row><row><cell cols="7">reported in Fig. 5. According to this figure, CrossHG-Meta achieves</cell></row><row><cell cols="7">the best performance when the weight value is around 0.5 in most</cell></row><row><cell cols="7">cases. It indicates that a suitable trade-off weight is necessary for</cell></row><row><cell cols="7">obtaining the best model performance such that the model can</cell></row><row><cell cols="5">incorporate proper self-supervised information.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Product classification accuracy (%) for Music, Outdoors, Arts &amp; Toy on Amazon data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Paper classification accuracy (%) for AI, Mathematics &amp; Interdisciplinary on AMiner data with assorted meta-paths.</figDesc><table><row><cell>Meta-Path</cell><cell>2-way</cell><cell>AI</cell><cell>3-way</cell><cell cols="2">Mathematics 2-way 3-way</cell><cell cols="2">Interdisciplinary 2-way 3-way</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1-shot</cell><cell></cell></row><row><cell>Random</cell><cell>78.68</cell><cell></cell><cell>67.30</cell><cell>86.41</cell><cell>81.57</cell><cell>78.15</cell><cell>65.48</cell></row><row><cell>PAP</cell><cell>80.51</cell><cell></cell><cell>70.22</cell><cell>87.90</cell><cell>82.52</cell><cell>79.01</cell><cell>68.52</cell></row><row><cell>PVP</cell><cell>77.62</cell><cell></cell><cell>71.04</cell><cell>87.31</cell><cell>82.14</cell><cell>78.28</cell><cell>69.89</cell></row><row><cell>Both</cell><cell>85.36</cell><cell></cell><cell>72.43</cell><cell>91.45</cell><cell>89.54</cell><cell>82.93</cell><cell>74.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3-shot</cell><cell></cell></row><row><cell>Random</cell><cell>84.43</cell><cell></cell><cell>75.43</cell><cell>93.47</cell><cell>90.31</cell><cell>87.51</cell><cell>80.72</cell></row><row><cell>PAP</cell><cell>89.14</cell><cell></cell><cell>77.34</cell><cell>94.29</cell><cell>90.63</cell><cell>87.82</cell><cell>81.30</cell></row><row><cell>PVP</cell><cell>83.05</cell><cell></cell><cell>78.14</cell><cell>94.21</cell><cell>90.81</cell><cell>88.52</cell><cell>81.33</cell></row><row><cell>Both</cell><cell>90.16</cell><cell></cell><cell>79.20</cell><cell>96.63</cell><cell>92.18</cell><cell>90.07</cell><cell>85.34</cell></row><row><cell cols="4">6 CONCLUSIONS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">In this work, we tackle the problem of cross-domain few-shot</cell></row><row><cell cols="8">heterogeneous graph learning and develop a novel model named</cell></row><row><cell cols="8">CrossHG-Meta. CrossHG-Meta aggregates heterogeneous graph</cell></row><row><cell cols="8">information to learn node embeddings from hybrid semantics,</cell></row><row><cell cols="8">and proposes the novel method of cross-domain meta-learning,</cell></row><row><cell cols="8">including a domain critic to handle domain shifts for improved</cell></row><row><cell cols="8">cross-domain generalizability. Besides, CrossHG-Meta explores self-</cell></row><row><cell cols="8">supervised information to regularize the meta-optimization process.</cell></row><row><cell cols="8">Extensive experimental results on three real-world heterogeneous</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>1</head><label></label><figDesc>Procedure of training CrossHG-Meta Sample one virtual source domain 𝑆 𝑣𝑠 and one virtual target domain 𝑆 𝑣𝑡 from the source domains S Sample a batch of meta-tasks {𝜏 𝑣𝑠,1 , 𝜏 𝑣𝑠,2 , . . . , 𝜏 𝑣𝑠,𝑛 } from 𝑆 𝑣𝑠 and a same number of meta-tasks {𝜏 𝑣𝑡,1 , 𝜏 𝑣𝑡,2 , . . . , 𝜏 𝑣𝑡,𝑛 } from 𝑆 𝑣𝑡 following section 4.2</figDesc><table><row><cell cols="2">Input: G 𝑖 = (V; E; A; R; X): the HG for each domain 𝑖; S: the set</cell></row><row><cell cols="2">of source domains; 𝜙: meta-path for auxiliary self-supervised task</cell></row><row><cell cols="2">Output: learned model parameters Θ</cell></row><row><cell cols="2">1: Initialize Θ</cell></row><row><cell cols="2">2: while training do</cell></row><row><cell>3:</cell><cell></cell></row><row><cell>4:</cell><cell></cell></row><row><cell>5:</cell><cell>Sample a batch of node pairs under meta-path 𝜙 from 𝑆 𝑣𝑠</cell></row><row><cell></cell><cell>and 𝑆 𝑣𝑡 following section 4.4</cell></row><row><cell>6:</cell><cell>for 𝜏 𝑣𝑠,𝑖 and 𝜏 𝑣𝑡,𝑖 do</cell></row><row><cell>7:</cell><cell>Obtain 𝜃</cell></row></table><note>′𝑔 and 𝜃 ′ 𝑐 using Eq. (2) 8:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Dataset Statistics.</figDesc><table><row><cell>Datasets</cell><cell>Node Type #Nodes</cell><cell cols="2">Labeled Node Type #Domains #Classes</cell></row><row><cell>AMiner</cell><cell>Paper/Author/Venue 66,711 / 97,197 / 312</cell><cell>Paper/Author 33 / 33</cell><cell>4</cell></row><row><cell>U.S. Patents</cell><cell>Patent/Assignee 55,516 / 125</cell><cell>Patent 38</cell><cell>4</cell></row><row><cell>Amazon</cell><cell>Product/Brand 100,968 / 7,054</cell><cell>Product 498</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters of CrossHG-Meta.</figDesc><table><row><cell cols="2">Hyperparameters</cell><cell cols="3">AMiner U.S. Patents Amazon</cell></row><row><cell cols="2">Meta-Learning Rate 𝜇</cell><cell>0.0005</cell><cell>0.0001</cell><cell>0.01</cell></row><row><cell>Training Fast Steps</cell><cell>Update Rate 𝛼 Update Step</cell><cell>0.05 3</cell><cell>0.05 2</cell><cell>0.1 1</cell></row><row><cell>Testing Fast Steps</cell><cell>Update Rate 𝛼 Update Step</cell><cell>0.05 5</cell><cell>0.05 3</cell><cell>0.1 2</cell></row><row><cell cols="2">Adaptation Update Rate 𝛽</cell><cell>0.05</cell><cell>0.05</cell><cell>0.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://snap.stanford.edu/data/cit-Patents.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The research reported in this paper was supported by funding from King Abdullah University of Science and Technology (KAUST).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno>22.16 11.27 23.74 12.78 23.68 12.46 21.97 11.33 m2v [3] 25.92 12.23 25.76 13.72 27.42 14.45 25.61 12.95</idno>
		<title level="m">Music Outdoors Arts Toy 5-way 10-way 5-way 10-way 5-way 10-way 5-way 10-way 3-shot node2vec</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G-Meta</forename></persName>
		</author>
		<idno>10] 40.65 27.48 46.58 32.54 49.53 30.49 43.75 30.71</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Crosshg-Meta</surname></persName>
		</author>
		<idno>55.61 43.74 64.97 52.70 59.82 47.13 53.62 41.36 5-shot node2vec [7] 24.63 11.68 24.99 13.01 25.97 13.79 23.57 11.72 m2v [3] 27.84 13.86 26.78 14.52 27.81 15.48 26.25 13.21</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G-Meta</forename></persName>
		</author>
		<idno>10] 48.34 28.87 52.47 33.85 50.74 39.86 46.81 32.53</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Crosshg-Meta ;</forename><surname>Avishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09867</idno>
		<title level="m">Metagraph: Few shot link prediction via meta learning</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph prototypical networks for few-shot learning on attributed networks</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metanorm: Learning to normalize few-shot batches across domains</title>
		<author>
			<persName><forename type="first">Yingjun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning</title>
		<author>
			<persName><forename type="first">Tao-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-Shot Graph Learning for Molecular Property Prediction</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Graph Meta Learning via Local Subgraphs</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<title level="m">Self-supervised Learning on Graphs: Deep Insights and New Direction</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature-critic networks for heterogeneous domain generalization</title>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to propagate for graph meta-learning</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relative and absolute location embedding for few-shot node classification on graph</title>
		<author>
			<persName><forename type="first">Zemin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Are we really making much progress? Revisiting, benchmarking and refining heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inferring networks of substitutable and complementary products</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10745</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial deep network embedding for cross-network node classification</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Lai</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kup-Sze</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Network together: Node classification via cross-network deep network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Lai</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kup-Sze</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey of heterogeneous information network analysis</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<publisher>VLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aminer: Toward understanding big scholar data</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transfer learning to infer social ties across heterogeneous networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation</title>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalizing to Unseen Domains: A Survey on Domain Generalization</title>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph Few-shot Learning with Attribute Matching</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">One-Shot Relational Learning for Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain Adaptive Classification on Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Shuwen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Domain adaptive classification on heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Shuwen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph few-shot learning via knowledge transfer</title>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Few-Shot Learning on Graphs</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In KDD</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Few-shot knowledge graph completion</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">HG-Meta: Graph Meta-learning over Heterogeneous Graphs</title>
		<author>
			<persName><forename type="first">Qiannan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DANE: domain adaptive network embedding</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuwen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meta-gnn: On few-shot node classification in graph meta-learning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goce</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
