<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing clinical concept extraction with distributional semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-11-07">7 November 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Siddhartha</forename><surname>Jonnalagadda</surname></persName>
							<email>siddhartha@mayo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Health Sciences Research</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><surname>Cohen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Biomedical Informatics</orgName>
								<orgName type="institution">University of Texas Health Science Center</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Health Sciences Research</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Graciela</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Health Sciences Research</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing clinical concept extraction with distributional semantics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-11-07">7 November 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">29FD76E1FCBD60EF831233D7F7BE639A</idno>
					<idno type="DOI">10.1016/j.jbi.2011.10.007</idno>
					<note type="submission">Received 23 May 2011 Accepted 16 October 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>NLP Information extraction NER Distributional semantics Clinical informatics</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting concepts (such as drugs, symptoms, and diagnoses) from clinical narratives constitutes a basic enabling technology to unlock the knowledge within and support more advanced reasoning applications such as diagnosis explanation, disease progression modeling, and intelligent analysis of the effectiveness of treatment. The recent release of annotated training sets of de-identified clinical narratives has contributed to the development and refinement of concept extraction methods. However, as the annotation process is labor-intensive, training data are necessarily limited in the concepts and concept patterns covered, which impacts the performance of supervised machine learning applications trained with these data. This paper proposes an approach to minimize this limitation by combining supervised machine learning with empirical learning of semantic relatedness from the distribution of the relevant words in additional unannotated text.</p><p>The approach uses a sequential discriminative classifier (Conditional Random Fields) to extract the mentions of medical problems, treatments and tests from clinical narratives. It takes advantage of all Medline abstracts indexed as being of the publication type ''clinical trials'' to estimate the relatedness between words in the i2b2/VA training and testing corpora. In addition to the traditional features such as dictionary matching, pattern matching and part-of-speech tags, we also used as a feature words that appear in similar contexts to the word in question (that is, words that have a similar vector representation measured with the commonly used cosine metric, where vector representations are derived using methods of distributional semantics). To the best of our knowledge, this is the first effort exploring the use of distributional semantics, the semantics derived empirically from unannotated text often using vector space models, for a sequence classification task such as concept extraction. Therefore, we first experimented with different sliding window models and found the model with parameters that led to best performance in a preliminary sequence labeling task.</p><p>The evaluation of this approach, performed against the i2b2/VA concept extraction corpus, showed that incorporating features based on the distribution of words across a large unannotated corpus significantly aids concept extraction. Compared to a supervised-only approach as a baseline, the micro-averaged Fscore for exact match increased from 80.3% to 82.3% and the micro-averaged F-score based on inexact match increased from 89.7% to 91.3%. These improvements are highly significant according to the bootstrap resampling method and also considering the performance of other systems. Thus, distributional semantic features significantly improve the performance of concept extraction from clinical narratives by taking advantage of word distribution information obtained from unannotated data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Information contained in clinical records is of value for both clinical practice and research; however, text mining from clinical records, particularly from narrative-style fields (such as discharge summaries and progress reports), has proven to be an elusive target for clinical Natural Language Processing (NLP) <ref type="bibr" target="#b0">[1]</ref>, due in part to the lack of availability of annotated corpora specific to the task. Yet, the extraction of concepts (such as mentions of problems, treatments, and tests) from clinical narratives constitutes the basic enabling technology that will unlock the knowledge contained in them and drive more advanced reasoning applications such as diagnosis explanation, disease progression modeling, and intelligent analysis of the effectiveness of treatment.</p><p>A baseline approach to concept extraction typically relies on a dictionary or lexicon of the concepts to be extracted, using string comparison to identify concepts of interest. Clinical narratives contain drug names, anatomical nomenclature, other specialized names and phrases that are not standard in everyday English such as ''benign positional vertigo'', ''l shoulder inj'', ''po pain medications'', ''a c5-6 acdf'', ''st changes'', ''resp status'' and others. There is also a high incidence of abbreviation usage, and many of the abbreviations have a different meaning in other genres of English. Descriptive expressions (such as coil embolization of bleeding vessel, a large bloody bowel movement, a tagged RBC scan and R intracerebral hemorrhage drainage) are commonly used to refer to concepts rather than using canonical terms. The specialized knowledge required and the labor-intensive nature of the task make it difficult to create a lexicon that would include all such expressions, particularly given that their use is often non-standard and varies across institutions and medical specialties, or even from one department to another in the same hospital, rendering dictionary-based approaches less adaptable in this domain. While supervised machine learning approaches offer a promising alternative, a reliable system usually needs a large annotated corpus with as many relevant examples as possible. However, because of privacy concerns and the domain expertise needed to annotate them, large corpora of clinical text are not generally available for research purposes.</p><p>This work proposes the design of a semi-supervised machine learning approach for extracting clinical concepts. The idea of combining supervised and unsupervised learning is not without precedent, although prior approaches are not as scalable and portable as the one proposed here. An earlier system in the biomedical domain owned by IBM Watson Research Center <ref type="bibr" target="#b1">[2]</ref> used a large corpus of text (5 million Medline abstracts) to improve concept extraction, and achieved the best performance in the BioCreative II gene mention shared task. IBM's system used a computationally expensive machine learning algorithm called Alternating Structure Optimization (ASO). This limits the extent to which their methods can be utilized in settings where computational resources are more limited. For example, Liu and Ng <ref type="bibr" target="#b2">[3]</ref>, who applied ASO to Semantic Role Labeling, acknowledge that the extensive computing resources required by ASO limited their experiments. In this paper, we discuss a different approach to the use of unannotated data through construction of a vector-based similarity model using Random Indexing <ref type="bibr" target="#b3">[4]</ref>, which offers computational advantages over previous methods such as ASO. Our approach is also scalable to large unannotated corpora and performs within the limits of computational resources available to most researchers. Consequently, we anticipate the dissemination of our method will promote widespread use of unannotated data for the task of clinical concept extraction.</p><p>The following sections detail our approach, providing first a detailed background on the distributional semantics algorithms we used, followed by a discussion of our methods and results. The methods section elucidates how distributional semantic measures are adapted to concept extraction, what kind of distributional models are most effective, how their usefulness is estimated and how they have been integrated into an existing supervised machine learning system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>The problem of extracting the relevant concepts automatically from text is also known as ''Named Entity Recognition and Classification (NERC)'', ''Named Entity Recognition (NER)'', or simply, ''concept extraction''. It has been studied in Computer Science for almost two decades <ref type="bibr" target="#b4">[5]</ref>, with significant progress in the field. Early approaches to extracting concepts from text tended to use dictionary or rule-based approaches, while later generation systems tend to use variants of supervised machine learning. In this latter category, generative models (Naïve Bayes Classifier and Hidden Markov Models) and instance-based classifiers (Logistic Regression and Naïve Bayes Classifier) are reasoned to be less accurate for extracting concepts or named entities from text than sequencebased discriminative models such as Conditional Random Fields <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Thus, machine learning based methods vary on the specific technique or implementation details and the features used for it. Most of the contemporary high-performing tools use non-semantic features such as parts of speech, lemmata, regular expressions, prefixes and n-grams. The high computational cost associated with using deep syntactic and semantic features largely restricted the NERC systems to the orthographic, morphological and shallow syntactic features. Other advances in machine learning, such as active learning and feature selection, still need to be thoroughly explored for concept extraction.</p><p>The first application of concept extraction in clinical domain can be attributed to Hirschman et al. <ref type="bibr" target="#b7">[8]</ref> for converting a corpus of X-ray reports on patients with breast cancer into a structured database using a theory of sublanguage of grammars <ref type="bibr" target="#b8">[9]</ref>. Medical Language Extraction and Encoding System (MedLEE <ref type="bibr" target="#b9">[10]</ref>) automatically generates coded information from general clinical notes using a rule-based approach in addition to finding modifiers. As is the case with MedLEE, MetaMap <ref type="bibr" target="#b10">[11]</ref>, an initiative of NLM to map text to the UMLS Metathesarus using a large lexicon, has been widely adopted for research purposes. For example, a direct application of MetaMap for detecting medical problems <ref type="bibr" target="#b11">[12]</ref> shows that it has an F-score accuracy of 75% for that task. A recent open-source tool HITEX <ref type="bibr" target="#b12">[13]</ref> uses MetaMap to map concepts to UMLS strings. More recently, hospital systems are moving forward in developing open source clinical information extraction systems, such as cTAKES <ref type="bibr" target="#b13">[14]</ref>. Some other clinical extraction projects include <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. However, as pointed out in a comprehensive review of clinical concept extraction <ref type="bibr" target="#b0">[1]</ref>, clinical NLP is currently lagging behind because of insufficient exposure to clinical text and rarity of annotated corpora. For a systematic review on automatic clinical coding and concept classification systems, refer to <ref type="bibr" target="#b24">[25]</ref>.</p><p>The focal point of this paper is the use of distributional semantics for concept extraction, and how it can improve other machinelearning based approaches. We first introduce Random Indexing, a scalable way to assign meaningful vectors to words in the unannotated text, before we discuss different ways of empirically inferring the semantics of the words based on their distribution. This discussion covers the distinction between syntagmatic and paradigmatic relations, two different kinds of distributional relationships with different semantic implications. We then discuss methods to model different kinds of paradigmatic relationships.</p><p>Distributional semantics is based on the tenet that the semantics of a piece of text (discourse) can be inferred from the distribution of the elements across multiple contexts. Methods of distributional semantics derive measures of semantic relatedness between words and text passages from large bodies of unannotated natural language text (for a review, see <ref type="bibr">Cohen and Widdows, 2009 [26]</ref>). These measures of relatedness have been shown to correlate well with human estimates of relatedness; however, little is known about how best to apply these relations to support structured prediction tasks (such as parsing), or sequence labeling tasks such as concept extraction <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Geometric models of distributional semantics represent each word as a vector in high-dimensional space. The dimensions of this space each correspond to a context in the corpus, such as co-occurrence near another word or co-occurrence within a document. Consequently, the distribution of words is represented as a word-by-context matrix. However, since distributional semantic models constructed from millions of documents and/or millions of words would be unmanageable in size, distributional models approaching corpora of this magnitude tend to reduce dimensionality. Traditional dimensionality reduction techniques such as singular value decomposition (SVD) are computationally expensive (the commonly utilized algorithm for SVD is cubic in complexity <ref type="bibr" target="#b28">[29]</ref>). An exhaustive review of methods of distributional semantics is beyond the scope of this paper. We refer the interested reader to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref>. For the purposes of this paper we have attempted to provide overview of some of the key geometrically motivated methods, and the distinctions between them in Table <ref type="table" target="#tab_0">1</ref>. They start with the introduction of vector space models (row 1) and include adjustments to finer-grained context (rows 3, 4, 5, 10); the use of sophisticated dimensionality reduction techniques (rows 2, 8); and the introduction of generative probabilistic models (rows 6, 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Random Indexing</head><p>Recently, Random Indexing <ref type="bibr" target="#b3">[4]</ref> emerged as promising alternative to the use of computationally expensive SVD for the dimension reduction step in the generation of word-by-context vectors. Elemental vectors are assigned to each document (in word-document models) or word (in sliding window models); these are sparse, high-dimensional (on the order of 1000) vectors comprising of mostly zero elements with a small number (on the order of 10) of them set to either +1 or À1. These non-zero elements are determined at random, and because of the sparseness of the vectors, this results in a set of vectors that are highly likely to be orthogonal or close-to-orthogonal to one another (that is to say, they are likely to have few non-zero dimensions in common). Semantic vectors for each word are then generated as the normalized vector sum of the vectors representing the contexts in which they have occurred. For a detailed account of the Random Indexing method, we recommend Cohen et al.'s introduction <ref type="bibr" target="#b25">[26]</ref>.</p><p>Random Indexing and other similar methods are motivated by the Johnson-Lindenstrauss Lemma <ref type="bibr" target="#b42">[43]</ref> that states that the distance between points in a vector space will be approximately preserved if they are projected into a reduced-dimensional subspace of sufficient dimensionality. Random Indexing scales at a rate that is linear to the size of the data, and has the added advantage that it is not necessary to represent the word-document or word-word matrix in memory. Dimensionality reduction is taken care of ''onthe-fly'' as each new context is encountered.</p><p>The major advantages of Random Indexing over established methods employing SVD for dimension reduction are scalability and the capacity for incremental updates. For a relatively small corpus such as the i2b2/VA corpus, there are supervised dimensionality reduction techniques such as Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b43">[44]</ref> that use SVD computation and could complement Random Indexing for designing kernels or similarity matrices based on training set annotations. LDA, while having the limitation of not being applicable for reducing dimensionality in unannotated data, has been widely applied in NER before and it will not be surprising if kernels built using LDA perform better than the kernels built using Random Indexing, which is an unsupervised dimensionality reduction method and does not exploit the labels of the data. Random indexing is more suitable when applied to a huge unannotated corpus such as tens of thousands of clinical narratives or clinical abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Syntagamatic and paradigmatic relationships</head><p>Recent work (notably Sahlgren <ref type="bibr" target="#b44">[45]</ref>) in distributional semantics explored the differences between relations extracted depending on the type of context used to construct a model. Sahlgren and other authors <ref type="bibr" target="#b45">[46]</ref> distinguish between two types of relationships between words captured by distributional models, which they designate as ''syntagmatic'' and ''paradigmatic'' relations, terminology derived from the work of Swiss linguist Ferdinand de Saussure <ref type="bibr" target="#b46">[47]</ref>. If two words co-occur significantly in the same discourse, they are said to be in syntagmatic relationship. Examples include word pairs such as p53 and tumor, APOE and Alzheimer's, and poliomyelitis and legs. If two words could substitute for each other in a sentence (i.e. they occur in similar local contexts throughout the corpus), they are said to be in a paradigmatic relationship. Examples include different words of the same entity type or even synonymous, such as ''p53'' (gene) and ''gata1'' (gene), and ''poliomyelitis'' and ''polio'' (synonyms). Since words in a paradigmatic relationship generally do not occur close to each other in the same context, extracting such a relationship typically requires second order (across-document) analysis, while a 1st order (withindocument) analysis is sufficient to extract syntagmatic relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Different models of context</head><p>Distributional models derive representations for words such that words occurring in similar contexts across a set of documents in a corpus will have similar representations. However, the definition of what constitutes a context differs across models. For example, Latent Semantic Analysis (LSA) <ref type="bibr" target="#b32">[33]</ref>, perhaps the best-established distributional model, generally uses an entire document as a context. In LSA, a word-document matrix is built, and the semantic representation for words and documents is found through singular value decomposition (SVD). This type of context discovers syntagmatic relations by capturing similarity according to word collocation.</p><p>In contrast, other models such as the Hyperspace Analog to Language (HAL) <ref type="bibr" target="#b34">[35]</ref> use a sliding window that is moved through the text corpus as the context. Two words are said to be in vicinity of each other if and only if the number of words separating them is less than an integer parameter known as the sliding window radius. Hence when a small sliding window is used, this model captures paradigmatic relationships, as two words that can substitute for one another would be expected to occur within similar (if not identical) narrow sliding windows. Sahlgren argues that using a small sliding-window rather than an entire document as a context is better suited to extracting paradigmatic relations, and supports this argument with empirical results. For example, narrow sliding windows are shown to generate associations preferentially between words of the same part of speech, and between synonymous words, when compared to word-document based models or wider sliding windows <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Directional models</head><p>Directional models, such as HAL, use a sliding window to generate a word-word matrix, T, where T[i, j] is one if the word representing the jth column appears within the radius of the word representing the ith column and zero otherwise. Directional models take into account the direction in which one word occurs with respect to the other by having two columns for each word, with one column representing the number of occurrences to the left and the other column representing the number of occurrences to the right. Table <ref type="table" target="#tab_1">2</ref> shows a matrix computed for ''she was increasingly using her right side'' using a window radius of six words. The sliding window is moved through the entire corpus, such that words occurring in similar sliding windows will have similar (row) vector representations.</p><p>The elemental vectors for each word are created using Random Indexing. A random permutation function (p), a bijection that maps each element in a collection to a different element, is applied to each elemental vector to obtain a permutated vector. The original elemental vectors of each word occurring to the right in the sliding window are added to the permuted elemental vectors of each word occurring to the left in the sliding window to build the contextual vectors for each occurrence of a word. For example, the contextual vector for the word ''using'' in the sliding window is calculated as follows:</p><formula xml:id="formula_0">cðusingÞ ¼ eðherÞ þ eðrightÞ þ eðsideÞ þ pðeðsheÞÞ þ pðeðwasÞÞ þ pðeðincreasinglyÞÞ;</formula><p>where c(t) is the contextual vector for word t in that sliding window, e(t) is the elemental vector of word t and p(t) is the permuted vector of the vector t.</p><p>The word-word matrix such as the one in Table <ref type="table" target="#tab_1">2</ref> is used for this calculation. The semantic vector for a word is obtained by adding the contextual vectors obtained at each occurrence of that word and normalizing the sum. A one in the first seven columns represent that the word in the corresponding row is to the left of the word in the column in the sliding window. A one in the last seven columns represents that the word in the corresponding row is to the right of the word in the column in the sliding window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Positional permutation-based sliding window model</head><p>Sahlgren's permutation-based method <ref type="bibr" target="#b44">[45]</ref> goes a step further and encodes word-order, thus accounting for the sequential structure of language. Representing the word order may capture some of the grammatical role and hence the meaning of the word. This method is an alternative implementation to the convolution operations used by BEAGLE <ref type="bibr" target="#b48">[49]</ref> (based on Tony Plate's Holographic Reduced Representation (HRR) <ref type="bibr" target="#b49">[50]</ref>) to encode word-order information in word spaces. Sahlgren's method captures word information by permutation of vector coordinates which is a computationally convenient alternative to BEAGLE's convolution operation, although similar performance improvements have recently been achieved using HRR in the frequency domain <ref type="bibr" target="#b50">[51]</ref>.</p><p>To achieve this, Sahlgren et al. first randomly generate sparse high-dimensional elemental vectors for each word, and use permutation, specifically shuffling of coordinates (shifting of all of the non-zero values of a sparse elemental vector to the left or right according to the relative position of words) to replace the convolution operator. In this way, a different close-to-orthogonal elemental vector is generated for each word depending on its position within the sliding window. A semantic word vector for each word is then generated as the linear sum of the permuted elemental vectors for each word co-occurring with this word in a sliding window. An example is shown in Fig. <ref type="figure">1</ref> below. This permutation function is bijective (for every member in the range, there is exactly one element from domain mapped to it and vice versa), allowing for construction of order-based queries (such as a query to find words occurring one position to the right of the word ''president'').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>The purpose of our work is to apply the methods of distributional semantics we have discussed to enhance the performance of machine learning approaches to concept extraction. For concept extraction using supervised machine learning methods, the main challenges are the scarcity of annotated examples, the level of expertise required to generate these examples, and the fact that no such large corpus of clinical text can practically be shared without raising privacy concerns. Harris' sublanguage theory <ref type="bibr" target="#b51">[52]</ref> suggests that the constraints of language in a specialized domain such as that used in clinical narratives include domain semantics in addition to purely grammatical constraints, suggesting the applicability of a semantic grammar in which domain semantics are an integral component of syntax. In highly constrained genres of texts such as biomedical literature and clinical narratives, there are inherent inequalities of the likelihood of certain patterns of words occurring <ref type="bibr" target="#b52">[53]</ref>. As a corollary, high probability combinations convey implicit information. For example, sentences such as Patient X received drug Y are more common than sentences such as Doctor X received drug Y. Thus, a sentence pattern A received B could be decoded to assume that A might refer to patient and B might refer to treatment. Therefore, and this is the premise of our approach, it is </p><formula xml:id="formula_1">' She 0 1 1 1 1 1 1 0 0 0 0 0 0 0 Was 0 0 1 1 1 1 1 1 0 0 0 0 0 0 Increasingly 0 0 0 1 1 1 1 1 1 0 0 0 0 0 Using 0 0 0 0 1 1 1 1 1 1 0 0 0 0 Her 0 0 0 0 0 1 1 1 1 1 1 0 0 0 Right 0 0 0 0 0 0 1 1 1 1 1 1 0 0 Side 0 0 0 0 0 0 0 1 1 1 1 1 1 0</formula><p>likely that information derived from the distribution of words in a larger, unannotated corpus could be used to compensate for the limited vocabulary present in a smaller annotated corpus and allow more accurate concept recognition. We hypothesize that the concept extraction task can be seen as finding a class of words that could conceivably represent the word to be labeled without disturbing the surrounding syntactic structure. To evaluate this hypothesis, distributional semantic features are added in this work to commonly used features (Table <ref type="table" target="#tab_1">2</ref>) which were then used as a basis for supervised machine learning to extract medical problems and treatments from clinical narratives. Improvements in accuracy after adding distributional semantic features (using i2b2/VA NLP shared task corpus as gold standard) would validate the utility of these additional features. Building a sliding-window based model involves:</p><p>Constructing elemental word vectors of pre-determined dimension N and seed S, where N À 2 ⁄ S dimensions are zeroes, S dimensions are +1s, and S dimensions are À1s. To ensure that these elemental vectors have a high probability of being mutually orthogonal, or close-to-orthogonal, S ( N.</p><p>Computing the semantic vector representation of the word based on the words surrounding each occurrence of the word.</p><p>This process is performed using the open source Semantic Vectors package <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>, which supports the generation of basic, direction-aware and permutation-based sliding window models using Random Indexing for dimension reduction.</p><p>Medline abstracts indexed as clinical trials help in creating semantic vector spaces. Different distributional semantic algorithms with different parameters respectively are employed to create various models. The best model, determined by their ability to discern words from different categories, is used to generate a similarity matrix for the words or tokens present in the corpus. The CRF algorithm uses features such as lexicons and linguistic features from the corpus, in addition to the distributional semantic features. The CRF model created during the training phase is used to tag the input sentences with the concepts ''medical problem'', ''treatment'' and ''test''.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, features from a state of the art biomedical NER system, BANNER <ref type="bibr" target="#b55">[56]</ref>, were adapted to create the baseline. BANNER uses the Conditional Random Fields algorithm implemented in MALLET <ref type="bibr" target="#b56">[57]</ref>. However, any concept extraction system using a supervised machine learning algorithm can easily adopt the proposed set-up. We trained BANNER on i2b2/VA NLP training corpus using (a) sentence-level features (last two rows in Table <ref type="table" target="#tab_1">2</ref>); (b) a lexicon derived from the UMLS, DrugBank, Drugs@FDA and MedDRA knowledge resources; and (c) distributional semantic features based on a large unannotated corpus of Medline abstracts categorized as ''clinical trials'' (n = 447,000), considering those would be the abstracts with vocabulary closer to clinical narratives. Selecting the distributional semantic features required tuning the parameters and finding the better model among different distributional semantic models, based on performance on a related task that will be described in detail in sections to follow. We used the best model with parameters that led to best performance in the preliminary sequence labeling task to create a similarity matrix for adding distributional semantic features. After the systems are trained and tuned offline and a CRF (Conditional Random Fields) machine learning model was created, processing of a sentence was accomplished using the same set of features used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">i2b2/VA NLP shared task corpus</head><p>To the best of our knowledge, the i2b2/VA NLP shared task corpus is the only public corpus of deidentified clinical documents that has annotation for medical problems, treatments and tests. A complete detail of the corpus is made available by Uzuner et al. <ref type="bibr" target="#b57">[58]</ref>, especially in Table <ref type="table" target="#tab_0">1</ref>. The corpus consists of 394 documents for training and 877 documents for testing. All the documents are either discharge summaries from Partners Healthcare, Beth Israel Deaconess Medical Center and the University of Pittsburgh Medical Center or progress reports from the University of Pittsburgh Medical Center. Overall, there are 20,267 test named entities, 22,056 treatment named entities and 30,517 problem named entities. The overall Inter-Annotator agreement distributed uniformly across the three types of concepts is 85% for exact match and 91% for inexact match. The annotation guidelines used, the evaluation metrics that were suggested (followed in this paper) as well as three example documents along with the annotations are available at http://www.i2b2.org/NLP/Relations/Documentation.php.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Automatic word categorization test</head><p>The 447,000 Medline abstracts that are indexed as pertaining to ''clinical trials'' are used as unannotated data. We used Semantic Vectors software <ref type="bibr" target="#b53">[54]</ref> built on the top of Apache Lucene <ref type="bibr" target="#b58">[59]</ref> to find the vector model of each word in the abstracts that appears at least twice in the entire unannotated corpus and has not more than three non-alphabet characters. We are not using a stop word list. The different paradigmatic model algorithms considered were the positional model, the directional model and the positional + basic model. The positional + basic model is the combination of the positional and basic sliding window models where the semantic vector of a word is the normalized vector sum of the corresponding semantic vectors in the positional model and the basic model.</p><p>We can customize the following parameters of the distributional semantic model: (a) dimensions of the final vector space, (b) seed length, and (c) the sliding window radius. We used the following methodology for deciding among parameter values: Fig. <ref type="figure">1</ref>. Example of permuted vector computation. v1, v2, v3, v4, and v5 are respectively the randomly generated vectors for the words ''She'', ''underwent'', ''angiography'', ''on'' and ''5-9-92''. Each p(v,i) is a vector obtained by shuffling the vector by |i| positions to the right (for positive i) or left (for negative i). The final semantic vector for the word ''angiography'' is then generated by adding the vectors generated at each occurrence of the word in the corpus.</p><p>Experiment with dimensions of sizes 100, 500, 1000, 1500 and 2000 to choose the dimensions that resulted in better categorization with respect to semantic type. Experiment with seeds of sizes 5, 10, 15, and 20; chose the seed length that resulted in better categorization with respect to semantic type. Experiment with window radii 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10 separately for each paradigmatic model algorithm; and chose the window radius that resulted in better categorization with respect to semantic type.</p><p>In order to select parameters, we tested the distributional model on the related task of automatic word categorization test before annotating all the clinical concepts As our system has limited training examples, it will need to infer the semantic type of terms that have not been encountered in the training set, in order to determine whether or not they should be extracted. The purpose of including distributional information is to provide a basis for this inference. We would expect that the choice of parameters that leads to accurate classification with respect to semantic type would best support concept extraction. Consequently, we have chosen to explore the parameter space in the context of a related task so that we do not over fit the parameter selection for the specific task.</p><p>To model paradigmatic relationships, we choose the model that performed best in the experiments in the automatic word categorization test. This test is similar to part-of-speech test performed by Sahlgren <ref type="bibr" target="#b47">[48]</ref> to study the differences between different wordspace models. Sahlgren studied the extent to which different distributional models retrieved nearest neighboring words that shared a part of speech with a cue word. He used two evaluation metrics that he referred to as ''strict'' and ''lax''. Both of them are variants of the k-nearest neighbors algorithm -strict has k = 1 (i.e., the closest neighbor has the same part of speech) and lax has k = 10 (i.e., at least one of the ten closest neighbors has the same part of speech).</p><p>We divided all the UMLS single word phrases into four categories: problem (n = 122,264), treatment (n = 189,283), test (n = 19,149) and none (n = 476,424) according to their UMLS semantic type. According to the definitions from the 2010 i2b2/ VA task guidelines,<ref type="foot" target="#foot_0">1</ref> a word is a:</p><p>1. Medical problem if its UMLS semantic type is pathologic functions, disease or syndrome, mental or behavioral dysfunction, cell or molecular dysfunction, congenital abnormality, acquired abnormality, injury or poisoning, anatomic abnormality, neoplastic process, virus/bacterium, sign or symptom. 2. Medical treatment if its UMLS semantic type is therapeutic or preventive procedure, medical device, steroid, pharmacologic substance, biomedical or dental material, antibiotic, clinical drug, or drug delivery device. 3. Medical test if its semantic type is laboratory procedure or diagnostic procedure.</p><p>We performed the automatic word categorization test for each of the paradigmatic models discussed in the previous section. For each model, we found the 10 nearest neighbors for each word and removed the word itself from the nearest neighbor list. Each of the 4 categories had a weight equal to the sum of the cosines of the neighbors that belong to the category. We assigned the category with the highest score to the word. The evaluation compares this automatically determined word category to the actual word category. For example, thoracotomy has 6 ''treatment'' neighbors (sum of cosines = .79 + .86 + .82 + .82 + .79 + .81 = 4.89) and 3 '' test'' neighbors (sum of cosines = .82 + .80 + .82 = 2.44). Hence, the test classified (correctly) thoracotomy as treatment. Thus, we find whether the assignment is a true positive, false positive, false negative, or true negative with respect to each category. In each case, we recorded the individual recall and precision as well as micro-averaged recall, precision, F-score and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Distributional semantic features for clinical concept extraction</head><p>We now constructed the similarity matrix K over the words in the i2b2/VA corpus, where Kðw1; w2Þ ¼ cosine of the semantic vector representations of w1 and w2; if both the words w1 and w2 exist in the i2b2=VA corpus Zero; otherwise</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">&gt; &gt; &gt; &lt; &gt; &gt; &gt; :</head><p>The purpose of the similarity matrix is to build a thesaurus of words automatically. Each entry in the thesaurus consists of a token from the i2b2/VA NLP corpus and N most similar words (from the i2b2/VA corpus) based on the distributional semantics. Computing the similarity matrix (K) scales linearly in the number of dimensions of each vector and quadratic in the number of words in the i2b2/VA corpus. Computing the thesaurus (with a predetermined number of similar words from the similarity matrix) scales linearly with the number of words. Overall, our approach scales linearly with respect to the number of dimensions of each vector and quadratic in the number of word vectors. Using the pre-computed matrix instead of directly computing thesaurus online saves the time spent in computing the cosines (O(N) time complexity) during the construction of the thesaurus.</p><p>We used a 1st order Conditional Random Fields (CRF) algorithm, as implemented by MALLET <ref type="bibr" target="#b56">[57]</ref>. The time complexity of the CRF algorithm is O(L 2 Ã N Ã M Ã F), where L is the number of labels, N is the number sequences (sentences), M is the average length of the sequences, and F is the average number of the features. It is observed <ref type="bibr" target="#b55">[56]</ref> that the accuracy is almost the same for all label types such as -IO, IOB and IOBEW, where I stands for labeling a token to be Inside, O for Outside, B for Beginning, E for End and W for Within. We chose the IO notation for labeling to minimize time complexity. Thus, we used four labels -Iproblem, Itest, Itreatment and O since there are three annotated concepts. In addition to all the features used in BANNER <ref type="bibr" target="#b55">[56]</ref>, three additional feature types based on: (1) thesaurus, (2) vector representation of the token, and (3) dictionaries are added (see Table <ref type="table" target="#tab_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Automatic word categorization results</head><p>The evaluation metrics, as defined in Section 3.2, consist of micro-averaged precision, recall, F-score and accuracy. Both F-score and accuracy were measured while adjusting each parameter discussed in Section 3.2. The advantage of using F-score is that in certain concept extraction tasks, true positives are more important than true negatives. The advantage of using accuracy is that it is invariant to label switching, which means the value does not change when a label changes from positive to negative. Thus, the goal was to optimize both F-score and accuracy. Figs. <ref type="figure">3</ref><ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure">6</ref><ref type="figure">7</ref><ref type="figure">8</ref>show the results of the experiments. We used the positional vector model as baseline to determine the better number of dimensions and seed length to serve the parameters for the Random Indexing algorithm. These parameters are general to the application of Random Indexing for the different models in this task. Once these desirable parameters were found, we used them for evaluating different sliding window algorithms against each other.</p><p>The accuracy and F-score increased rapidly in the beginning as the number of dimensions increased. However, the accuracy and These are the different features used in the machine learning algorithm. Roughly, the features could be lexical (at the level of word), syntactic (at the level of grammar), pragmatic (at the level of context) and semantic (at the level of meaning).  Window radius in directional model vs. performance. The performance is the highest at the window radius of 6.</p><p>F-score beyond 1000 dimensions were only gradually increasing and could be considered almost constant (Fig. <ref type="figure">3</ref>). The accuracy and F-score were constant as the number of seeds was increased (Fig. <ref type="figure">4</ref>). Therefore, we used five seeds.</p><p>We then changed the window radius for the three models -the positional permutation vector model, the directional permutation vector model, and the positional + basic model. As expected, the performance varies differently in each case with increase in the window radius. The positional model had the highest accuracy and F-score at a window radius of 2 (Fig. <ref type="figure">5</ref>). This corroborates similar experiments by Sahlgren <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b47">48]</ref>. For directional model, the performance increased until we increased the window radius to 6 and it decreased after that (Fig. <ref type="figure">6</ref>). The positional + basic model performed better than the positional model, and the system achieves the best accuracy and F-score at the window radius of 4 (Fig. <ref type="figure">7</ref>). As the word categorization test can be considered as a test of our models' ability to find associations between words that serve the same semantic role (paradigmatic associations), we decided on the basis of these evaluations to obtain a better model for concept extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Concept extraction results</head><p>Two of the parameters involved in use of the model that lead to the best performance in the previous section are: (a) the number of most similar words to consider, and (b) the minimum similarity between the similar word and the original token. The experiments showed that the best number of most similar words to use is 20. With a threshold on the cosine similarity between the similar words and the original token, the performance decreased slightly. Consequently, the threshold was set to 0, i.e., we used all the 20 nearest words irrespective of their cosine value. Table <ref type="table" target="#tab_3">4</ref> compares the results using different models with these settings.</p><p>Table <ref type="table" target="#tab_4">5</ref> shows the results for a system without any distributional semantic features and that of the best system with distributional semantic features (direction-based, 2000 dimensions, 5 seeds and window radius = 6). Both were trained on the competition training corpus and tested on the testing corpus.</p><p>It is encouraging to see that addition of distributional semantic features increases the recall and precision in all cases. Bootstrap Resampling <ref type="bibr" target="#b59">[60]</ref> with 1000 repetitions on the test corpus showed the improvement after adding the distributional semantic features derived from clinical trial abstracts is highly significant (confidence = 100%). This metric, which involves calculating f-scores over different versions of corpora obtained by allowing repetitions over original corpora and averaging the f-scores, was also used in comparing state of the art systems that extract concepts in biomedical literature <ref type="bibr" target="#b60">[61]</ref> and general English <ref type="bibr" target="#b61">[62]</ref>. However, the improvement because of adding local distributional semantic based features after adding distributional semantic features from clinical trials was insignificant (confidence = 56.6%). Hence, we might also conclude that addition of distributional semantic features derived from large unannotated corpus is sufficient and features from smaller corpora need not supplement it. We also found that using each dimension of the vector representation of tokens as features does not significantly affect the accuracy of the system. This could be due to curse of dimensionality, which exponentially increases the size of the training set needed based on the number of features used. The distributional semantics model for Medline and the thesaurus of distributionally similar words for the words in the i2b2/VA corpus are available at http://diego.asu.edu/downloads/AZCCE/, as well as is the code to generate a thesaurus and integrate this with a CRF system.</p><p>Further, we asked clinical researchers at the Mayo Clinic, Rochester and the University of Texas Health Sciences Center at Houston to use 1.5 million clinical narratives each and report the evaluation results using the i2b2/VA shared task testing corpus. Both the centers reported significant increases in the accuracy of concept extraction (overall F-scores of 82.0% and 82.3% for exact  The highest scores are in bold. R exact is the micro-averaged recall for exact match. P exact is the micro-averaged precision for exact match. F exact is the micro-averaged f-score for exact match. R inexact is the micro-averaged recall for inexact match. P inexact is the micro-averaged precision for inexact match. F inexact is the micro-averaged f-score for inexact match. The direction-based model has the highest impact. match and 91.3% and 91.3% for inexact match respectively). An increase in F-score of 2% over the state of the art (80.3%) is considered very significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Through this work on concept extraction, we amalgamated two diverse research areas -distributional semantics and information extraction. Firstly, we evaluated different distributional semantic models based on their ability to predict the semantic types of words. The parameters we found to be better might be useful in future experiments by other researchers. A novel finding of this work is that semantic vector models are helpful in extracting named entities. The method proposed here using distributional semantics achieved a performance improvement of 2.0% in the task of extracting medical problems, tests, and treatments from clinical narratives when measured against a state of the art system. To be able to generalize the conclusion, we performed a similar experiment for biomedical literature <ref type="bibr" target="#b62">[63]</ref>. We used the settings found to yield better vector representation using UMLS (Section 3.2) to generate semantic vector representation for the words in the BioCreative II task <ref type="bibr" target="#b60">[61]</ref> corpus, one of the most commonly used corpus for benchmarking gene tagging. These semantic vectors are used to generate distributional semantic features which are integrated into BANNER <ref type="bibr" target="#b55">[56]</ref>, one of the best gene tagger <ref type="bibr" target="#b63">[64]</ref>. The F-score of gene tagging on the BioCreative training and testing set increased by 1.9% This suggests that the approach proposed in this work to use unlabeled text for concept extraction could be adapted to a different domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with other systems</head><p>Our evaluation showed that the accuracy of the system significantly improves after adding distributional semantic features. However, how much it contributes toward improving the state of the art determines the practical significance of the improvement. Thus, we compared the performance of the system to the top systems in the i2b2/VA 2010 concept extraction task in which 22 international teams have submitted multiple runs as shown in Fig. <ref type="figure">9</ref>. Our baseline system (without distributional semantic features) ranked 7th both in F-scores measured using exact match and inexact match. The best system uses a relatively complex variant of Hidden Markov Models algorithm. Systems with ranks 2-5 use proprietary (non-public) components. System 6 uses Conditional Random Fields algorithm (as does our baseline). After adding distributional semantic features as described in this paper, the F-score as measured by inexact match is better than systems 3-6 and equal to system 2. The F-score as measured by exact match is also better than systems 3-6. This suggests that adding distributional semantic features to a supervised machine learning system significantly improves the state of art.</p><p>In addition, this system is the only one among this list that adds a semi-supervised component on the top of an existing supervised system. Our semi-supervised component (distributional semantic features) can integrate with other machine learning algorithms such as Hidden Markov Models (used by NRC, the best system submission for i2b2/VA concept extraction task) and Naïve Bayes. This may lead to improved performance with these systems also.   <ref type="table" target="#tab_5">6</ref> and few more examples are available in the Supplementary material. This contributed to an increase in both precision and recall. We found this phenomenon for all the concepts aggregately and for each concept separately. We analyzed the annotations that are different in both versions. There are higher number of words that are not found in training set in the newly found true positives [360 vs. 283] and true negatives <ref type="bibr">[496 vs. 439</ref>]. This corroborates the hypothesis that the performance increases because of being able to give a semantic representation for the newly found words, and because of a more accurate semantic representation of the existing words. It was also found that approximately half of extra true positives found [1068 out of 2202] contained words that were predicted to belong to the class based on the weighted K-NN algorithm (refer to ''Automatic word categorization'' subsection in the methods section). This correlation is interesting, especially because the distributional semantic features are only a fraction of all the features used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Limitations</head><p>The improvement to concept extraction is higher if one uses clinical narratives from hospital records since that would also capture semantics of non-standard words used in the clinical narratives.</p><p>A limitation of most distributional models is that the association they encode is of a general nature. Consequently the nature of the relationship between words is not represented, and therefore these representations cannot support symbolic processes or logic. While not necessary for the current implementation, recent models that incorporate both symbolic and distributional information (for example <ref type="bibr" target="#b64">[65]</ref>) attempt to address limitation. Another limitation in using distributional semantics is that the meanings of words often differ with the passage of time and for specialized disciplines. This phenomenon is semantic change or semantic shift. For example, the word ''guy'' was originally used to mean ''grotesque person'' (as named Guy Fawkes hanged for an assassination attempt in 1605), and now is a common expression for a male person. The meaning of the word ''case'' changes with specialization <ref type="bibr" target="#b64">[65]</ref>. In the general domain, it means ''circumstance'', while in the medical domain, it means ''a patient'', and in the legal domain, ''a lawsuit''. The reliability of the empirically derived semantics needs to be analyzed in order to determine the extent to which domain specificity and temporal synchronization of the training corpus affect results. Further investigation of this effect is beyond the scope of the current paper (however, see for example <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Future work</head><p>Automatic coding of concepts in clinical narratives: Since systems for extracting concepts are still being explored, automatic coding is an emerging technology. In text mining pipeline, there is a further step after extracting concepts known as normalization. This step involves disambiguating polysemy and synonymy. Polysemy is the event of the same entity having different semantic meanings in two different contexts. For example, PPI could mean Inorganic Pyro Phosphate when the subject of discussion is about enzymes; it could mean Peptidyl prolyl cis-trans-isomerase when the discussion is about proteins; it could also mean protein-protein interaction in other situations. The first step in normalization will be to assign different senses to polysemous entities. Synonymy is the event of two different entities having the same semantic meanings in all contexts. For example, ''AD'' and ''SDAT'', and ''poliomyelitis'' and ''polio'' are synonyms. Automatic coding systems not only need concept extraction, but also require normalization using different schemas, such as the International Classification of Diseases (ICD) or Current Procedural Terminology (CPT), and use-case-specific, non-standardized schemas, such as the presence or absence of a given condition. The entities in the schemas could be pre-coordinated (coding as a single field) or post-coordinated (reassembled after storing individual components separately). Two problems associated with pre-coordination <ref type="bibr" target="#b67">[68]</ref> are: (a) updates to the cross index could be slow because of the knowledge resources needed and (b) computation ambiguity of the reuse of the concept. Given the ability of distributional semantics to detect words appearing in similar context and to predict the appropriateness of a concept to a context, it is possible that we could automatically address both these problems. Distributional semantics may also help in automatically mapping ''pre-coordinated'' terms with ''postcoordinated'' expressions and vice versa.</p><p>Secondary use of clinical narratives: Hospital systems could use the clinical narratives to create a conceptual thesaurus for the words in the narratives automatically. This thesaurus would be useful for annotating concepts in a machine learning system or a rule-based system. This thesaurus would be more useful for annotating the clinical narratives within the hospital system, since it is customized for that genre of text. However, as this distributional thesaurus is created without human intervention, the system could be conveniently transferred to a new institution given the availability of free-text records in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The approach we have used in this research is agnostic to the specific machine learning algorithm that is used to recognize entities in text once distributional semantic features have been generated. Other semi-supervised approaches such as ASO <ref type="bibr" target="#b1">[2]</ref> tend to be restricted to a specific framework and algorithm. This is disadvantageous considering most state-of-the-art concept extraction systems use supervised machine learning algorithms, such as CRF, and it is of interest to the scientific community to determine the upper limits of performance these algorithms can achieve. Our approach was to use NLP features generated through unsupervised means within a supervised machine learning system. While this approach renders all the advantages offered in other semi-supervised machine learning systems, it also permits maximum flexibility in the choice of the basic framework of the set-up.</p><p>In sum, the results indicate that distributional semantic features aid clinical concept extraction. In our current research, we are exploring the use of clinical records, rather than Medline records, as a source of unannotated data. We anticipate further increases in accuracy, since these corpora contain the language used by physicians in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall architecture of the system.</figDesc><graphic coords="6,78.02,67.91,426.31,218.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.</head><label></label><figDesc>Fig. Number of vs. performance. The accuracy and f-score of automatic word categorization do not change with the number of seeds used in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .Fig. 3 .</head><label>53</label><figDesc>Fig.5. Window radius in positional model vs. performance. The performance is the highest at the window radius of 2 corrobarating with results from similar experiments by Sahlgren<ref type="bibr" target="#b44">[45]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Window radius in positional + basic model vs. performance. The performance is the highest at the window radius of 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 . 5 . 2 .</head><label>952</label><figDesc>Fig. 9. Comparison of top medical concept extraction systems. NRC = National Research Council Canada. deBruijn et al.; VU = Vanderbilt Univ. Xu et al.; Erasmus = Erasmus Univ. Kors et al.; SCAI = SCAI. Hofmann et al.; Sydney = Univ. Sydney. Patrick et al.; George = Georgetown Univ. Liu et al.; ASUbefore = our baseline; ASUafter = Our baseline after adding distributional semantic features derived from UTHouston clinical data warehouse; TTI = TTI. Sasaki et al.; UTD = UTDallas. Roberts et al.; Emory = Emory Univ. Post et al. After adding distributional semantic features the rank of the system as per the 4th i2b2/VA shared task competition improves from 7th to 2nd.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Characteristics of various distributional semantic models.</figDesc><table><row><cell>Reference(s)</cell><cell>Name</cell><cell>Context</cell><cell>Semantic model</cell><cell>Dim. reduction/inference</cell></row><row><cell>1. Salton (1971) [31]</cell><cell>-</cell><cell>Document</cell><cell>tf-idf weighted term vectors</cell><cell>Vector clusters and centroid</cell></row><row><cell>2. Deerwester et al. (1990), Landauer and Dumais</cell><cell>LSA</cell><cell>Document</cell><cell>Word-by-document matrix</cell><cell>Truncated SVD</cell></row><row><cell>(1997) [32,33]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3. Schütze (1993) [34]</cell><cell>Word space</cell><cell>Sliding window</cell><cell>Word-by-ngram matrix</cell><cell>-</cell></row><row><cell>4. Lund and Burgess (1996) [35]</cell><cell>HAL</cell><cell>Sliding window</cell><cell>Word-by-word matrix</cell><cell>-</cell></row><row><cell>5. Lin (1998) [36]</cell><cell>-</cell><cell>Dependency triples</cell><cell>Similarity-weighted term vectors</cell><cell>Nearest neighbor words</cell></row><row><cell></cell><cell></cell><cell>(syntax)</cell><cell></cell><cell></cell></row><row><cell>6. Hofmann (2001) [37]</cell><cell>pLSA</cell><cell>Document</cell><cell>Probabilistic aspect model</cell><cell>Expectation-maximization</cell></row><row><cell>7. Turney (2001) [38]</cell><cell>-</cell><cell>Sliding window</cell><cell>PMI-weighted co-occurrence</cell><cell>-</cell></row><row><cell>8. Kanerva et al. (2000), Karlgren and Sahlgren</cell><cell>Random</cell><cell>Sliding window</cell><cell>Word-by-context matrix</cell><cell>Random indexing</cell></row><row><cell>(2001) [39,40]</cell><cell>Indexing</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p><p>9.</p><ref type="bibr" target="#b40">Blei et al. (2003)</ref> </p><ref type="bibr" target="#b40">[41]</ref> </p>LDA, topic models</p>Document collection Probabilistic model (generative) for topics and words</p>Gibbs sampling, variational inference 10. Pado and Lapata (2007)</p><ref type="bibr" target="#b41">[42]</ref> </p>-Dependency paths (syntax)</p>Word-by-''basis element'' matrix Any ''basis mapping function''</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Example for directional model matrix computation.</figDesc><table><row><cell>Focus word</cell><cell cols="5">Occurrence within six words following the focus word</cell><cell></cell><cell></cell><cell cols="5">Occurrence within six words preceding the focus word</cell><cell></cell></row><row><cell></cell><cell>She</cell><cell>Was</cell><cell>Increasingly</cell><cell>Using</cell><cell>Her</cell><cell>Right</cell><cell>Side</cell><cell>She'</cell><cell>Was'</cell><cell>Increasingly'</cell><cell>Using'</cell><cell>Her'</cell><cell>Right'</cell><cell>Side</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>List of features used.</figDesc><table><row><cell>Feature name</cell><cell>Type</cell><cell>Description</cell></row><row><cell>Dictionary</cell><cell>Semantic</cell><cell>UMLS, DrugBank, Drugs@FDA and MedDRA</cell></row><row><cell>Distributional</cell><cell>Semantic</cell><cell>Distributional thesaurus and dimensions of words (word embeddings)</cell></row><row><cell>Section</cell><cell>Pragmatic</cell><cell>Name of the section in which the sentence appears</cell></row><row><cell>Part-of-speech</cell><cell>Syntactic</cell><cell>Part of speech of the token in the sentence</cell></row><row><cell>Others</cell><cell>Lexical</cell><cell>Lower case token, lemma, prefixes, suffixes, n-grams, Matching patterns such as beginning with a capital, etc.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Comparison of concept extraction performance with different models.</figDesc><table><row><cell>#</cell><cell>Model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>The accuracy of baseline (subscript 1) vs. system with distributional semantic features (subscript 2). R = recall, P = precision, F = F-score. Using distributional semantic features improve the performance for all concept classes, whether exact match or inexact match, or whether matching the span or the class.</figDesc><table><row><cell>Type</cell><cell>R 1</cell><cell>R 2</cell><cell>P 1</cell><cell>P 2</cell><cell>F 1</cell><cell>F 2</cell></row><row><cell>Concept exact span</cell><cell>80.4</cell><cell>82.0</cell><cell>85.1</cell><cell>85.6</cell><cell>82.7</cell><cell>83.7</cell></row><row><cell>Class exact span</cell><cell>78.1</cell><cell>79.9</cell><cell>82.7</cell><cell>83.5</cell><cell>80.3</cell><cell>81.7</cell></row><row><cell>Problem exact span</cell><cell>80.0</cell><cell>81.9</cell><cell>83.6</cell><cell>84.7</cell><cell>81.8</cell><cell>83.2</cell></row><row><cell>Treatment exact span</cell><cell>80.2</cell><cell>81.6</cell><cell>85.8</cell><cell>86.1</cell><cell>82.9</cell><cell>83.8</cell></row><row><cell>Test exact span</cell><cell>81.3</cell><cell>82.4</cell><cell>86.6</cell><cell>86.5</cell><cell>83.4</cell><cell>84.4</cell></row><row><cell>Problem matching class</cell><cell>78.6</cell><cell>80.5</cell><cell>81.6</cell><cell>83.0</cell><cell>80.0</cell><cell>81.7</cell></row><row><cell>Treatment matching class</cell><cell>77.0</cell><cell>79.1</cell><cell>82.9</cell><cell>83.4</cell><cell>79.8</cell><cell>81.2</cell></row><row><cell>Test matching class</cell><cell>78.6</cell><cell>80.0</cell><cell>84.1</cell><cell>84.4</cell><cell>81.3</cell><cell>82.1</cell></row><row><cell>Concept inexact span</cell><cell>89.0</cell><cell>90.5</cell><cell>94.2</cell><cell>94.6</cell><cell>91.6</cell><cell>92.5</cell></row><row><cell>Class inexact span</cell><cell>88.9</cell><cell>90.3</cell><cell>90.8</cell><cell>91.6</cell><cell>89.7</cell><cell>90.9</cell></row><row><cell>Problem inexact span</cell><cell>89.7</cell><cell>91.5</cell><cell>93.8</cell><cell>94.7</cell><cell>91.7</cell><cell>93.1</cell></row><row><cell>Treatment inexact span</cell><cell>88.3</cell><cell>89.6</cell><cell>94.4</cell><cell>94.3</cell><cell>91.2</cell><cell>91.9</cell></row><row><cell>Test inexact span</cell><cell>88.8</cell><cell>90.1</cell><cell>94.7</cell><cell>94.8</cell><cell>91.6</cell><cell>92.4</cell></row><row><cell>Problem inexact span matching class</cell><cell>87.6</cell><cell>89.5</cell><cell>90.9</cell><cell>92.2</cell><cell>89.2</cell><cell>90.8</cell></row><row><cell>Treatment inexact span matching class</cell><cell>84.2</cell><cell>86.2</cell><cell>90.6</cell><cell>91.0</cell><cell>87.3</cell><cell>88.5</cell></row><row><cell>Test inexact span matching class</cell><cell>85.1</cell><cell>86.7</cell><cell>91.0</cell><cell>91.5</cell><cell>87.9</cell><cell>89.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Example outputs of the system.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.i2b2.org/NLP/Relations/assets/Concept%20Annotation%20Guideline. pdf.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank NLM for making us available the Medline abstracts to create semantic resources and for partially funding all authors for this work (Contract# HHSN276201000031C), Elmer V Bernstam's group at the School of Biomedical Informatics at University of Texas Houston (NCRR Grant 3UL1RR024148, NCRR 1RC1RR028254, NSF 0964613 and the Brown Foundation) and Mayo Clinic for testing the process using their respective clinical notes, the developers of BANNER, MAL-LET and Semantic Vectors for the software packages and the organizers of the i2b2/VA 2010 NLP challenge for sharing the corpus.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Sentence</head><p>Examples where true positives increased over baseline c = n n n''dry mucous membranesn n n'' 45:0 45:2||t = n n n''problemn n n'' The patient has had progressive failure to thrive and steady weight loss c = n n n''tamponaden n n'' 69:5 69:5||t = n n n''problemn n n'' Cardiology did not feel that tamponade at this time was a concern and that a tap need not be performed c = n n n''further ifexn n n'' 95:24 95:25||t = n n n''treatmentn n n''</p><p>The Ifex was held as it was felt that it could have precipitated the mental status changes and that she was to receive no further Ifex c = n n n''straight leg raise testn n n'' 100:0 100:3||t = n n n''testn n n'' The patient has had progressive failure to thrive and steady weight loss c = n n n''a rubn n n'' 76:4 76:5||t = n n n''treatmentn n n'' Cardiology felt she had a rub and her pulsus was still 10 c = n n n''that tamponaden n n'' 69:4 69:5||t = n n n''problemn n n''</p><p>Cardiology did not feel that tamponade at this time was a concern and that a tap need not be performed c = n n n''erythromycinn n n'' 89:11 89:11||t = n n n''treatmentn n n'' They therefore felt a rapid steroid taper was indicated and topical Erythromycin if desired by the patient These examples are from the annotated corpus that belongs to Partners Healthcare. We were allowed to share them publicly after removing the protected health information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Supplementary material</head><p>Supplementary data associated with this article can be found, in the online version, at doi:10.1016/j.jbi.2011.10.007.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extracting information from textual documents in the electronic health record: a review of recent research</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Meystre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kipper-Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Hurdle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yearbook Med Informat</title>
		<imprint>
			<biblScope unit="page" from="128" to="144" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BioCreative II gene mention tagging system at IBM Watson</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second biocreative challenge</title>
		<meeting>the second biocreative challenge</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning predictive structures for semantic role labeling of NomBank</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association of computational linguistics</title>
		<meeting>the 45th annual meeting of the association of computational linguistics<address><addrLine>Prague (Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random indexing of text samples for latent semantic analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kristofersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual conference of the cognitive science society</title>
		<meeting>the 22nd annual conference of the cognitive science society</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">1036</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Extracting company names from text</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Res</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Center</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Schenectady</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Discriminative models, not discriminative training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<idno>MSR-TR-2005-144</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A: an introduction to conditional random fields for relational learning. in introduction to statistical relational learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From text to structured information: automatic processing of medical reports</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national computer conference and exposition</title>
		<meeting>the national computer conference and exposition<address><addrLine>New York NY, (USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page">267</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sublanguage grammers in science information processing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Soc Inform Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="10" to="16" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards a comprehensive medical language processing system: methods and issues</title>
		<author>
			<persName><forename type="first">C</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluation of medical problem extraction from electronic clinical documents using MetaMap Transfer (MMTx)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meystre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stud Health Technol Informat</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">823</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extracting principal diagnosis, co-morbidity and smoking status for asthma research: evaluation of a natural language processing system</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">T</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goryachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazarus</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med Inform Decis Mak</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">UIMA-based clinical information extraction system</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kipper-Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Buntrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Chute</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>LREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparison of the Charlson comorbidities derived from medical language processing and administrative data</title>
		<author>
			<persName><forename type="first">J-H</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hripcsak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Symposium</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="160" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic identification of pneumonia related concepts on chest X-ray reports</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fiszman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="67" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Experience with a mixed semantic/syntactic parser</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Huff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Annu Symp Comput Appl Med Care</title>
		<imprint>
			<biblScope unit="page" from="284" to="288" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computerized extraction of coded findings from free-text radiologic reports. Work in progress</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Ranum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Frederick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="543" to="548" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automation of a problem list using natural language processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meystre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med Inform Decis Mak</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Natural language processing to extract medical problems from electronic clinical documents: performance evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meystre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Biomed Inform</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="589" to="599" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting structured information from free text pathology reports</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schadow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Annu Symp Proc</title>
		<imprint>
			<biblScope unit="page" from="584" to="588" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A statistical natural language processor for medical reports</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Soderland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc AMIA Symp</title>
		<imprint>
			<biblScope unit="page" from="970" to="974" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using implicit information to identify smoking status in smoke-blind medical discharge summaries</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wicentowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sydes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="31" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facilitating research in pathology using natural language processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Annu Symp Proc</title>
		<imprint>
			<biblScope unit="page">1057</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A systematic literature review of automated clinical coding and classification systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Stanfill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jenders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="646" to="651" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Empirical distributional semantics: methods and biomedical applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Biomed Informat</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="390" to="405" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Opérationnelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">61801</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A distributional semantics approach to simultaneous recognition of multiple classes of named entities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jonnalagadda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Linguist Intellig Text Process (CICLing)</title>
		<imprint>
			<biblScope unit="volume">6008</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Numerical linear algebra</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Trefethen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for industrial mathematics</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From frequency to meaning: vector space models of semantics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Artif Intellig Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic indexing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Soc Informat Sci</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Rev</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Word space</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems 5 (NIPS conference</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="895" to="902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hyperspace analog to language (HAL): a general model of semantic representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lang Cognit Process</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on computational linguistics -Montreal</title>
		<meeting>the 17th international conference on computational linguistics -Montreal<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining the web for synonyms: PMI-IR versus LSA on TOEFL</title>
		<author>
			<persName><forename type="first">P</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twelfth European conference on machine learning</title>
		<meeting>the twelfth European conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From words to understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found Real-world Intellig</title>
		<imprint>
			<biblScope unit="page" from="294" to="308" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Random indexing of text samples for latent semantic analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kristoferson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual conference of the cognitive science society</title>
		<meeting>the 22nd annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">1036</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Linguist</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extensions of Lipschitz mappings into a Hilbert space</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemp Math</title>
		<imprint>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Mullers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks for signal processing IX: proceedings of the 1999 IEEE signal processing society workshop</title>
		<meeting><address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note type="report_type">Cat. No. 98TH8468</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Permutations as a means to encode order in word space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual meeting of the cognitive science society</title>
		<meeting>the 30th annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The computation of word associations: comparing syntagmatic and paradigmatic approaches</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on computational linguistics</title>
		<meeting>the 19th international conference on computational linguistics</meeting>
		<imprint>
			<publisher>Association for computational linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Saussure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sechehaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riedlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Calvet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Mauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cours de linguistique générale. Payot</title>
		<imprint>
			<date type="published" when="1922">1922</date>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The word-space model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Representing word meaning and order information in a composite holographic lexicon</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djk</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Rev</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Plate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks, IEEE transactions on 2002</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="623" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic oscillations: encoding context and structure in complex valued holographic vectors</title>
		<author>
			<persName><forename type="first">De</forename><surname>Vine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI fall symposium series</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A theory of language and information: a mathematical approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford (England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Two biomedical sublanguages: a description based on the theories of Zellig Harris</title>
		<author>
			<persName><forename type="first">C</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rzhetsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Biomed Informat</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="222" to="235" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Semantic vectors package: new algorithms and public tools for distributional semantics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth IEEE international conference on semantic computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semantic vectors: a scalable open source package and online technology management application</title>
		<author>
			<persName><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ferraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth international conference on language resources and evaluation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">BANNER: an executable survey of advances in biomedical named entity recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific symposium in bioinformatics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">MALLET: a machine learning for language toolkit &lt;http</title>
		<ptr target="//mallet.cs.umass.edu&gt;" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text</title>
		<author>
			<persName><forename type="first">O</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Duvall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Med Inform Assoc</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Lucene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&lt;http</forename></persName>
		</author>
		<ptr target="//lucene.apache.org&gt;" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses: an introduction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">BioCreative 2 gene mention task</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second biocreative challenge workshop</title>
		<meeting>the second biocreative challenge workshop</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task</title>
		<author>
			<persName><forename type="first">Tjong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><forename type="middle">Ef</forename></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on natural language learning</title>
		<meeting>the seventh conference on natural language learning<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">An effective approach to biomedical information extraction with limited training data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jonnalagadda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Arizona State University</orgName>
		</respStmt>
	</monogr>
	<note>PhD Dissertation</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A realistic assessment of methods for extracting gene/protein interactions from free text</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kabiljo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Shepherd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformat</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">233</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Regularity in semantic change</title>
		<author>
			<persName><forename type="first">E</forename><surname>Traugott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Event detection in blogs using temporal random indexing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on events in emerging text types</title>
		<meeting>the workshop on events in emerging text types<address><addrLine>Stroudsburg, PA (USA)</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The trajectory of scientific discovery: concept cooccurrence and converging semantic distance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Schvaneveldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stud Health Technol Inform</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="661" to="665" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Clinical ontologies for discovery applications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic web</title>
		<editor>
			<persName><forename type="first">Cjo</forename><surname>Baker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K-H</forename><surname>Cheung</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="101" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
