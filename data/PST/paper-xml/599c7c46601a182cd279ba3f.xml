<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low Rank Approximation with Entrywise 1 -Norm Error *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
							<email>zhaos@utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodru</surname></persName>
							<email>dpwoodru@us.ibm.com</email>
						</author>
						<author>
							<persName><forename type="first">Peilin</forename><surname>Zhong</surname></persName>
							<email>peilin.zhong@columbia.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IBM Almaden Research Center San Jose</orgName>
								<address>
									<postCode>95120</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low Rank Approximation with Entrywise 1 -Norm Error *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C16DA56E5781B72566241DC8C71C63D9</idno>
					<idno type="DOI">10.1145/3055399.3055431</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Entry-wise 1 norm</term>
					<term>low rank approximation</term>
					<term>robust algorithms</term>
					<term>sketching</term>
					<term>numerical linear algebra</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the 1 -low rank approximation problem, where for a given n ×d matrix A and approximation factor α ≥ 1, the goal is to output a rank-k matrix A for which</p><p>where for an n × d matrix C, we let</p><p>version is available at https://arxiv.org/abs/1611.00898 † Work done while visiting IBM Research-Almaden, and supported in part by UTCS TAship (CS429 Fall 16 Computer Organization and Architecture and CS395T Fall 16 Sublinear Algorithms).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>j=1 |C i, j |. This error measure is known to be more robust than the Frobenius norm in the presence of outliers and is indicated in models where Gaussian assumptions on the noise may not apply. The problem was shown to be NP-hard by Gillis and Vavasis and a number of heuristics have been proposed. It was asked in multiple places if there are any approximation algorithms.</p><p>We give the rst provable approximation algorithms for 1 -low rank approximation, showing that it is possible to achieve approximation factor α = (log d ) • poly(k ) in nnz(A) + (n +d ) poly(k ) time, where nnz(A) denotes the number of non-zero entries of A. If k is constant, we further improve the approximation ratio to O (1) with a poly(nd )-time algorithm. Under the Exponential Time Hypothesis, we show there is no poly(nd )-time algorithm achieving a (1 + 1 log 1+γ (nd ) )-approximation, for γ &gt; 0 an arbitrarily small constant, even when k = 1.</p><p>We give a number of additional results for 1 -low rank approximation: nearly tight upper and lower bounds for column subset selection, CUR decompositions, extensions to low rank approximation with respect to p -norms for 1 ≤ p &lt; 2 and earthmover distance, low-communication distributed protocols and low-memory streaming algorithms, algorithms with limited randomness, and bicriteria algorithms. We also give a preliminary empirical evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Two well-studied problems in numerical linear algebra are regression and low rank approximation. In regression, one is given an n × d matrix A, and an n × 1 vector b, and one seeks an x ∈ R d which minimizes Ax -b under some norm. For example, for least squares regression one minimizes Axb 2 . In low rank approximation, one is given an n × d matrix A, and one seeks a rank-k matrix A which minimizes A -A under some norm. For example, in Frobenius norm low rank approximation, one minimizes A -A F = i, j (A i, j -A i, j ) 2 1/2 . Algorithms for regression are often used as subroutines for low rank approximation. Indeed, one of the main insights of <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b80">81]</ref> was to use results for generalized least squares regression for Frobenius norm low rank approximation. Algorithms for 1 -regression, in which one minimizes Axb 1 = i |(Ax ) i -b i |, were also used <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b82">83]</ref> to t a set of points to a hyperplane, which is a special case of entrywise 1 -low rank approximation, the more general problem being to nd a rank-k matrix A minimizing i, j |A i, j -A i, j |. Randomization and approximation were introduced to signicantly speed up algorithms for these problems, resulting in algorithms achieving relative error approximation with high probability. Such algorithms are based on sketching and sampling techniques; we refer to <ref type="bibr" target="#b90">[90]</ref> for a survey. For least squares regression, a sequence of work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b80">81]</ref> shows how to achieve algorithms running in nnz(A) + poly(d ) time. For Frobenius norm low rank approximation, using the advances for regression this resulted in nnz(A) + (n + d ) poly(k ) time algorithms. For 1 -regression, sketching and sampling-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b91">91]</ref> led to an nnz(A) + poly(d ) time algorithm.</p><p>Just like Frobenius norm low rank approximation is the analogue of least squares regression, entrywise 1 -low rank approximation is the analogue of 1 -regression. Despite this analogy, no non-trivial upper bounds with provable guarantees are known for 1 -low rank approximation. Unlike Frobenius norm low rank approximation, which can be solved exactly using the singular value decomposition, no such algorithm or closed-form solution is known for 1 -low rank approximation. Moreover, the problem was recently shown to be NP-hard <ref type="bibr" target="#b45">[46]</ref>. A major open question is whether there exist approximation algorithms, sketching-based or otherwise, for 1low rank approximation. Indeed, the question of obtaining better algorithms was posed in Section 6 of <ref type="bibr" target="#b45">[46]</ref>, in <ref type="bibr" target="#b37">[38]</ref>, and as the second part of open question 2 in <ref type="bibr" target="#b90">[90]</ref>, among other places. The earlier question of NP-hardness was posed in Section 1.4 of <ref type="bibr" target="#b51">[52]</ref>, for which the question of obtaining approximation algorithms is a natural followup. The goal of our work is to answer this question.</p><p>We now formally de ne the 1 -low rank approximation problem: we are given an n ×d matrix A and approximation factor α ≥ 1, and we would like, with large constant probability, to output a rank-k matrix A for which</p><formula xml:id="formula_0">A -A 1 ≤ α • min rank-k matrices A A -A 1 ,</formula><p>where for an n × d matrix C, we let C 1 = n i=1 d j=1 |C i, j |. This notion of low rank approximation has been proposed as a more robust alternative to Frobenius norm low rank approximation <ref type="bibr">[16-18, 55, 56, 58, 59, 63-65, 67, 74, 97]</ref>, and is sometimes referred to as 1 -matrix factorization or robust PCA. 1 -low rank approximation gives improved results over Frobenius norm low rank approximation since outliers are less exaggerated, as one does not square their contribution in the objective. The outlier values are often erroneous values that are far away from the nominal data, appear only a few times in the data matrix, and would not appear again under normal system operation. These works also argue 1 -low rank approximation can better handle missing data, is appropriate in noise models for which the noise is not Gaussian, e.g., it produces the maximum likelihood estimator for Laplacian noise <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b86">87]</ref>, and can be used in image processing to prevent image occlusion <ref type="bibr" target="#b95">[95]</ref>.</p><p>To see that 1 -low rank approximation and Frobenius norm low rank approximation can give very di erent results, consider the n ×n matrix A = n 0 0 B , where B is any (n -1) × (n -1) matrix with B F &lt; n. The best rank-1 approximation with Frobenius norm error is given by A = n • e 1 e 1 , where e 1 is the rst standard unit vector. Here A ignores all but the rst row and column of A, which may be undesirable in the case that this row and column represent an outlier. Note A -A 1 = B 1 . If, for example, B is the all 1s matrix, then A = [0, 0; 0, B] is a rank-1 approximation for which A -A 1 = n, and therefore this solution is a much better solution to the 1 -low rank approximation problem than n • e 1 e 1 , for which</p><formula xml:id="formula_1">A -n • e 1 e 1 1 = (n -1) 2 .</formula><p>Despite the advantages of 1 -low rank approximation, its main disadvantage is its computational intractability. It is not rotationally invariant and most tools for Frobenius low rank approximation do not apply. To the best of our knowledge, all previous works only provide heuristics. We provide hard instances for previous work in Section 7, showing these algorithms at best give a poly(nd )approximation (though even this is not shown in these works). We also mention why a related objective function, robust PCA <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b93">93,</ref><ref type="bibr" target="#b96">96]</ref>, does not give a provable approximation factor for 1low rank approximation. Using that for an n  <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b81">82]</ref> shows how to obtain an O (1)-approximation to this problem in nnz(A)</p><formula xml:id="formula_2">× d matrix C, C F ≤ C 1 ≤ √ nd C F ,</formula><formula xml:id="formula_3">+ (n + d ) poly(k ) + exp(k ) time, and using that C 1,2 ≤ C 1 ≤ √ d C 1,2 results in an O ( √ d )-approximation.</formula><p>There are also many variants of Frobenius norm low rank approximation for which nothing is known for 1 -low rank approximation, such as column subset selection and CUR decompositions, distributed and streaming algorithms, algorithms with limited randomness, and bicriteria algorithms. Other interesting questions include low rank approximation for related norms, such as p -low rank approximation in which one seeks a rank-k matrix A minimizing n i=1 d j=1 (A i, j -A i, j ) p . Note for 1 ≤ p &lt; 2 these are also more robust than the SVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Results</head><p>We give the rst e cient algorithms for 1 -low rank approximation with provable approximation guarantees. By symmetry of the problem, we can assume d ≤ n. We rst give an algorithm which runs in O (nnz(A)) + n • poly(k ) time and solves the 1 -low rank approximation problem with approximation factor (log d ) • poly(k ). This is an exponential improvement over the previous approximation factor of O ( √ d ), provided k is not too large, and is polynomial time for every k. Moreover, provided nnz(A) ≥ n • poly(k ), our time is optimal up to a constant factor as any relative error algorithm must spend nnz(A) time. We also give a hard instance for our algorithm ruling out log d k log k + k 1/2-γ approximation for arbitrarily small constant γ &gt; 0, and hard instances for a general class of algorithms based on linear sketches, ruling out k 1/2-γ approximation.</p><p>Via a di erent algorithm, we show how to achieve an O (k )approximation factor in poly(n)d O (k ) 2 O (k 2 ) time. This is useful for constant k, for which it gives an O (1)-approximation in poly(n) time, improving the O (log d )-approximation for constant k of our earlier algorithm. The approximation ratio of this algorithm, although O (1) for constant k, depends on k. We also show one can nd a rank-2k matrix A in poly(n) time for constant k for which</p><formula xml:id="formula_4">A -A 1 ≤ C min rank-k matrices A A -A 1 ,</formula><p>where C &gt; 1 is an absolute constant independent of k. We refer to this as a bicriteria algorithm. Finally, one can output a rank-k matrix A, instead of a rank-2k matrix A, in poly(n) time with the same absolute constant C approximation factor, under an additional assumption that the entries of A are integers in the range {-b, -b + 1, . . . , b} for an integer b ≤ poly(n). Unlike our previous algorithms, this very last algorithm has a bit complexity assumption, and runs in poly(b) time instead of poly(log(b)) time.</p><p>Under the Exponential Time Hypothesis (ETH), we show there is no poly(n)-time algorithm achieving a (1+</p><formula xml:id="formula_5">1 log 1+γ (n)</formula><p>)-approximation, for γ &gt; 0 an arbitrarily small constant, even when k = 1. The latter strengthens the NP-hardness result of <ref type="bibr" target="#b45">[46]</ref>.</p><p>We also give a number of results for variants of 1 -low rank approximation which are studied for Frobenius norm low rank approximation; prior to our work nothing was known about these problems.</p><p>Column Subset Selection and CUR Decomposition: In the column subset selection problem, one seeks a small subset C of columns of A for which there is a matrix X for which CX -A is small, under some norm. The matrix CX provides a low rank approximation to A which is often more interpretable, since it stores actual columns of A, preserves sparsity, etc. These have been extensively studied when the norm is the Frobenius or operator norm (see, e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref> and the references therein). We initiate the study of this problem with respect to the 1 -norm. We rst prove an existence result, namely, that there exist matrices A for which any subset C of poly(k ) columns satis es min X CX -A 1 ≥ k 1/2-γ • min rank-k matrices A A-A 1 , where γ &gt; 0 is an arbitrarily small constant. This result is in stark contrast to the Frobenius norm for which for every matrix there exist O ( k ϵ ) columns for which the approximation factor is 1 + ϵ. We also show that our bound is nearly optimal in this regime, by showing for every matrix there exists a subset of O (k log k ) columns providing an O ( k log k )approximation. One can nd such columns in poly(n)d O (k log k ) time by enumerating and evaluating the cost of each subset. Although this is exponential in k, we show it is possible to nd O (k log k ) columns providing an O ( k log k log d )-approximation in polynomial time for every k.</p><p>We extend these results to the CUR decomposition problem (see, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37]</ref>), in which one seeks a factorization CU R for which C is a subset of columns of A, R is a subset of rows of A, and CU R -A is as small as possible. In the case of Frobenius norm, one can choose O (k/ϵ ) columns and rows, have rank(U ) = k, have CU R -A F be at most (1 + ϵ ) times the optimal cost, and nd the factorization in nnz(A) log n + n • poly((log n)k/ϵ ) time <ref type="bibr" target="#b13">[14]</ref>. Using our column subset selection results, we give an nnz(A) + n • poly(k ) time algorithm choosing O (k log k ) columns and rows, for which rank(U ) = k, and for which CU R -A 1 is poly(k ) log d times the cost of any rank-k approximation to A.</p><p>p -Low Rank Approximation and EMD-Low Rank Approximation: We also give the rst algorithms with provable approximation guarantees for the p -low rank approximation problem, 1 ≤ p &lt; 2, in which we are given an n × d matrix A and approximation factor α ≥ 1, and would like, with large constant probability, to output a rank-k matrix A for which</p><formula xml:id="formula_6">A -A p p ≤ α • min rank-k matrices A A -A p p ,</formula><p>where for an n × d matrix C, C</p><formula xml:id="formula_7">p p = n i=1 d j=1 |C i, j | p .</formula><p>We obtain similar algorithms for this problem as for 1 -low rank approximation. For instance, we obtain an nnz(A) + n • poly(k ) time algorithm with approximation ratio (log d ) • poly(k ). We also provide the rst low rank approximation with respect to sum of earthmover distances (of the n rows of A and A) with a (log 2 d ) poly(k ) approximation factor. This low rank error measure was used, e.g., in <ref type="bibr" target="#b79">[80]</ref>. Sometimes such applications also require a non-negative factorization, which we do not provide. Distributed/Streaming Algorithms, and Algorithms with Limited Randomness: There is a growing body of work on low rank approximation in the distributed (see, e.g., <ref type="bibr">[3-5, 10, 15, 39, 53, 62, 75, 76, 86, 92]</ref>) and streaming models (see, e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b89">89]</ref>), though almost exclusively for the Frobenius norm. One distributed model is the arbitrary partition model <ref type="bibr" target="#b52">[53]</ref> in which there are s servers, each holding an n ×d matrix A i , and they would like to output a k × d matrix V for which min U UV -A is as small as possible (or, a centralized coordinator may want to output this). We give O (sdk ) + s poly(k )-communication algorithms achieving a poly(k, log(n))-approximation for 1 -low rank approximation in the arbitrary partition model, which is optimal for this approximation factor (see <ref type="bibr" target="#b14">[15]</ref> where lower bounds for Frobenius norm approximation with poly(n) multiplicative approximation were shown -such lower bounds also apply to 1 low rank approximation). We also consider the turnstile streaming model <ref type="bibr" target="#b69">[70]</ref> in which we receive positive or negative updates to its entries and wish to output a rank-k factorization at the end of the stream. We give an algorithm using O (dk ) + poly(k ) space to achieve a poly(k, log(n))-approximation, which is space-optimal for this approximation factor, up to the degree of the poly(k ) factor. To obtain these results, we show our algorithms can be implemented using O (dk ) random bits. We also give algorithms in these settings which can output both factors U and V .</p><p>We stress for all of our results, we do not make assumptions on A such as low coherence or condition number; our results hold for any n × d input matrix A.</p><p>We report a promising preliminary empirical evaluation of our algorithms in the full version of our paper <ref type="bibr" target="#b83">[84]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Technical Overview</head><p>Initial Algorithm and Optimizations: Let A * be a rank-k matrix for which</p><formula xml:id="formula_8">A -A * 1 = min rank-k matrices A A -A 1 . Let A * = U * V * be a factorization for which U * is n × k and V * is k × d.</formula><p>Suppose we somehow knew U * and consider the multi-response 1regression problem min</p><formula xml:id="formula_9">V U * V -A 1 = min V d i=1 U * V i -A i 1 ,</formula><p>where V i , A i denote the i-th columns of V and A, respectively. We could solve this with linear programming though this is not helpful for our argument here.</p><p>Instead, inspired by recent advances in sketching for linear algebra (see, e.g., <ref type="bibr" target="#b90">[90]</ref> for a survey), we could choose a random matrix S and solve min</p><formula xml:id="formula_10">V SU * V -SA 1 = min V d i=1 (SU * )V i -SA i 1 .</formula><p>If V is an approximate minimizer of the latter problem, we could hope V is an approximate minimizer of the former problem. If also S has a small number t of rows, then we could instead solve min V d i=1 (SU * )V i -SA i 2 , that is, minimize the sum of Euclidean norms rather than the sum of 1 -norms. Since</p><formula xml:id="formula_11">t -1/2 (SU * )V i - SA i 1 ≤ (SU * )V i -SA i 2 ≤ (SU * )V i -SA i 1 , we would obtain a √ t-approximation to the problem min V SU * V -SA 1 . A crucial observation is that the solution to min V d i=1 (SU * )V i -SA i 2 is</formula><p>given by V = (SU * ) † SA, which implies that V is in the row span of SA. If also S were oblivious to U * , then we could compute SA without ever knowing U * . Having a low-dimensional space containing a good solution in its span is our starting point.</p><p>For this to work, we need a distribution on oblivious matrices S with a small number of rows, for which an approximate minimizer V to min V SU * V -SA 1 is also an approximate minimizer to min V U * V -A 1 . It is unknown if there exists a distribution on S with this property. What is known is that if S has O (d log d ) rows, then the Lewis weights (see, e.g., <ref type="bibr" target="#b28">[29]</ref> and references therein) of the concatenated matrix [U * , A] give a distribution for which the optimal V for the latter problem is a (1+ϵ )-approximation to the former problem; see also earlier work on 1 -leverage scores <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref> which have poly(d ) rows and the same (1 + ϵ )-approximation guarantee. Such distributions are not helpful here as <ref type="bibr" target="#b0">(1)</ref>   <ref type="bibr" target="#b80">[81]</ref>, there is no convenient linear-algebraic analogue for the 1 -norm. We rst note that since regression is a minimization problem, to obtain an O (α )-approximation by solving the sketched version of the problem, it su ces that (1) for the optimal V * , we have</p><formula xml:id="formula_12">SU * V * -SA 1 ≤ O (α ) U * V * -A 1 , and (2) for all V , we have SU * V -SA 1 ≥ Ω(1) • U * V -A 1 .</formula><p>We show (1) holds for α = O (log d ) and any number of rows of S. Our analysis follows by truncating the Cauchy random variables</p><formula xml:id="formula_13">(SU * V * j -SA j ) i for i ∈ [O (k log k )] and j ∈ [d]</formula><p>, so that their expectation exists, and applying linearity of expectation across the d columns. This is inspired from an argument of Indyk <ref type="bibr" target="#b47">[48]</ref> for embedding a vector into a lower-dimensional vector while preserving its 1 -norm; for single-response regression this is the statement that SU * * -Sa 1 = Θ(1) U * -a 1 , implied by <ref type="bibr" target="#b47">[48]</ref>. However, for multi-response regression we have to work entirely with expectations, rather than the tail bounds in <ref type="bibr" target="#b47">[48]</ref>, since the Cauchy random variables (SU * V j -SA j ) i , while independent across i, are dependent across j. Moreover, our O (log d )-approximation factor is not an artifact of our analysis -we show in Section 6 that there is an n × d input matrix A for which with probability 1 -1/ poly(k ), there is no k-dimensional space in the span of SA achieving a log d t log t + k 1/2-γ -approximation, for S a Cauchy matrix with t rows, where γ &gt; 0 is an arbitrarily small constant. This shows (k log d ) Ω(1) -inapproximability. Thus, the fact that we achieve O (log d )-approximation instead of O (1) is fundamental for a matrix S of Cauchy random variables or any scaling of it.</p><p>While we cannot show (2), we instead show for all V , SU * V -</p><formula xml:id="formula_14">SA 1 ≥ U * V -A 1 /2 -O (log d ) U * V * -A 1 if S has O (k log k )</formula><p>rows. This su ces for regression, since the only matrices V for which the cost is much smaller in the sketch space are those providing an O (log d ) approximation in the original space. The guarantee follows from the triangle inequality:</p><formula xml:id="formula_15">SU * V -SA 1 ≥ SU * V - SU * V * 1 -SU * V * -SA 1 and</formula><p>the fact that S is known to not contract any vector in the column span of U * if S has O (k log k ) rows <ref type="bibr" target="#b82">[83]</ref>. Because of this, we have</p><formula xml:id="formula_16">SU * V -SU * V * 1 = Ω(1) U * V - U * V * 1 = Ω(1)( U * V -A 1 -U * V * -A 1 )</formula><p>, where we again use the triangle inequality. We also bound the additive term</p><formula xml:id="formula_17">SU * V * -SA 1 by O (log d ) U * V * -A 1 using (1) above.</formula><p>Given that SA contains a good rank-k approximation in its row span, our algorithm with a slightly worse poly(n) time and poly(k log(n))-approximation can be completely described here. Let S and T 1 be independent O (k log k ) × n matrices of i.i.d. Cauchy random variables, and let R and</p><formula xml:id="formula_18">T 2 be independent d × O (k log k ) matrices of i.i.d. Cauchy random variables. Let X = (T 1 AR) † ((T 1 AR)(T 1 AR) † (T 1 AT 2 )(SAT 2 )(SAT 2 ) † ) k (SAT 2 ) † , which is the rank-k matrix minimizing T 1 ARXSAT 2 -T 1 AT 2 F</formula><p>, where for a matrix C, C k is its best rank-k approximation in Frobenius norm. Output A = ARXSA as the solution to 1 -low rank approximation of A. We show with constant probability that A is a poly(k log(n))-approximation.</p><p>To improve the approximation factor, after computing SA, we 1 -project each of the rows of A onto SA using linear programming or fast algorithms for 1 -regression <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b67">68]</ref>, obtaining an n × d matrix B of rank O (k log k ). We then apply the algorithm in the previous paragraph with A replaced by B. This ultimately leads to a log d • poly(k )-approximation.</p><p>To improve the running time from poly(n) to nnz(A) +n •poly(k ), we show a similar analysis holds for the sparse Cauchy matrices of <ref type="bibr" target="#b67">[68]</ref>; see also the matrices in <ref type="bibr" target="#b91">[91]</ref>.</p><p>CUR Decompositions: To obtain a CUR decomposition, we rst nd a log d • poly(k )-approximate rank-k approximation A as above. Let B 1 be an n × k matrix whose columns span those of A, and consider the regression problem min V B 1 V -A 1 . Unlike the problem min V U * V -A 1 where U * was unknown, we know B 1 so can compute its Lewis weights e ciently, sample by them, and obtain a regression problem min V D 1 (B 1 V -A) 1 where D 1 is a sampling and rescaling matrix. Since</p><formula xml:id="formula_19">D 1 (B 1 V -A) 1 ≤ D 1 (B 1 V -B 1 V * ) 1 + D 1 (B 1 V * -A) 1 , where V * = argmin V B 1 V -A 1 , we can bound the rst term by O ( B 1 V -B 1 V * 1 ) using that D 1 is a subspace embedding if it has O (k log k ) rows, while the second term is O (1) B 1 V * -A 1 by a Markov bound. Note that B 1 V * -A 1 ≤ (log d ) • poly(k ) min rank-k matrices A A -A 1 .</formula><p>By switching to 2 as before, we see that V = (D 1 B 1 ) † D 1 A contains a (log d ) poly(k )-approximation in its span. Here D 1 A is an actual subset of rows of A, as required in a CUR decomposition. Moreover the subset size is O (k log k ). We can sample by the Lewis weights of V to obtain a subset C of O (k log k ) rescaled columns of A, together with a rank-k matrix U for which CU R -</p><formula xml:id="formula_20">A 1 ≤ (log d ) poly(k ) min rank-k matrices A A -A 1 .</formula><p>Algorithm for Small k: Our CUR decomposition shows how we might obtain an O (1)-approximation for constant k in poly(n) time. If we knew the Lewis weights of U * , an α-approximate solution to the problem min</p><formula xml:id="formula_21">V D 1 (U * V -A) 1 would be an O (α )- approximate solution to the problem min V U * V -A 1 ,</formula><p>where D 1 is a sampling and rescaling matrix of </p><formula xml:id="formula_22">O (k log k ) rows of A. Moreover, an O ( k log k )-approximate solution to min V D 1 (U * V -A) 1 is given by V = (D 1 U * ) † D 1 A, which implies the O (k log k ) rows of D 1 A contain an O ( k log k )-</formula><formula xml:id="formula_23">(k log k )-approximation of the form AD 2 W D 1 A, where W is an O (k log 2 k ) ×O (k log k ) matrix of rank k. By setting up the problem min rank-k W AD 2 W D 1 A-A 1 ,</formula><p>one can sample from Lewis weights on the left and right to reduce this to a problem independent of n and d, after which one can use polynomial optimization to solve it in exp(poly(k )) time. One of our guesses D 1 A will be correct, and for this guess we obtain an O (k )-approximation. For each guess we can compute its cost and take the best one found. This gives an O (1)-approximation for constant k, removing the O (log d )-factor from the approximation of our earlier algorithm.</p><p>Existential Results for Subset Selection: In our algorithm for small k, the rst step was to show there exist O (k log k ) rows of A which contain a rank-k space which is an O ( k log k )-approximation.</p><p>While for Frobenius norm one can nd O (k ) rows with an O (1)approximation in their span, one of our main negative results for 1 -low rank approximation is that this is impossible, showing that the best approximation one can obtain with poly(k ) rows is k 1/2-γ for an arbitrarily small constant γ &gt; 0. Our hard instance is an r × (r + k ) matrix A in which the rst k columns are i.i.d. Gaussian, and the remaining r columns are an identity matrix. Here, r can be twice the number of rows one is choosing. The optimal 1 -low rank approximation has cost at most r , obtained by choosing the rst k columns. Let R ∈ R r /2×k denote the rst k entries of the r /2 chosen rows, and let denote the rst k entries of an unchosen row. For r /2 &gt; k, there exist many solutions x ∈ R r /2 for which x R = . However, we can show the following tradeo :</p><formula xml:id="formula_24">whenever x R -1 &lt; √ k poly(log k ) , then x 1 &gt; √ k poly(log k ) .</formula><p>Then no matter which linear combination x of the rows of R one chooses to approximate by, either one incurs a √ k poly(log k ) cost on the rst k coordinates, or since A contains an identity matrix,</p><formula xml:id="formula_25">one incurs cost x 1 &gt; √ k</formula><p>poly(log k ) on the last r coordinates of x R. To show the tradeo , consider an x ∈ R r /2 . We decompose x = x 0 + j ≥1 x j , where x j agrees with x on coordinates which have absolute value in the range</p><formula xml:id="formula_26">1 √ k log c k • [2 -j , 2 -j+1 ],</formula><p>and is zero otherwise. Here, c &gt; 0 is a constant, and x 0 denotes the restriction of x to all coordinates of absolute value at least</p><formula xml:id="formula_27">1 √ k log c k . Then x 1 &lt; √ k log c k</formula><p>, as otherwise we are done. Hence, x 0 has small support. Thus, one can build a small net for all x 0 vectors by choosing the support, then placing a net on it. For x j for j &gt; 0, the support sizes are increasing so the net size needed for all x j vectors is larger. However, since x j has all coordinates of roughly the same magnitude on its support, its 2 -norm is decreasing in j. Since (x j ) R ∼ N (0, x j 2 2 I k ), this makes it much less likely that individual coordinates of (x j ) R can be large. Since this probability goes down rapidly, we can a ord to union bound over the larger net size. What we show is that for any sum of the form j ≥1 x j , at most k 10 of its coordinates are at least 1  log k in magnitude. For x R -1 to be at most </p><formula xml:id="formula_28">√ k log c k , for at least k 2 coordinates i, we must have |(x R -) i | &lt; 2 √ k log c k . With probability 1 -2 -Ω(k ) , | i | ≥</formula><formula xml:id="formula_29">-k 10 -k 3 = Ω(k ) coordinates i of x for which (1) |(x R -) i | &lt; 2 √ k log c k , (2) | j ≥1 x j i | &lt; 1 log k , and (3) | i | ≥ 1</formula><p>100 . On these i, (x 0 ) R i must be in an interval of width 1  log k at distance at least 1 100 from the origin. Since (x 0 ) R ∼ N (0, x 0 2 2 I k ), for any value of x 0 2 2 the probability this happens on Ω(k ) coordinates is at most 2 -Θ(k ) . Since the net size for x 0 is small, we can union bound over every sequence x 0 , x 1 , . . . , coming from our nets. Some care is needed to union bound over all possible subsets R of rows which can be chosen. We handle this by conditioning on a few events of A itself, which imply corresponding events for every subset of rows. These events are such that if R is the chosen set of half the rows, and S the remaining set of rows of A, then the event that a constant fraction of rows in S are close to the row span of R is 2 -Θ(kr ) , which is small enough to union bound over all choices of R.</p><p>Curiously, we also show there are some matrices A ∈ R n×d for which any 1 rank-k approximation in the entire row span of A cannot achieve better than a (2 -Θ(1/d ))-approximation.</p><p>Bicriteria Algorithm: Our algorithm for small k gives an O (1)approximation in poly(n) time for constant k, but the approximation factor depends on k. We show how one can nd a rank-2k matrix A for which A -A 1 ≤ C • OPT, where C is an absolute constant, and OPT = min rank-k matrices A A -A 1 . We rst nd a rank-k matrix B 1 for which A -B 1  1 ≤ p • OPT for a factor 1 ≤ p ≤ poly(n). We can use any of our algorithms above for this.</p><p>Next consider the problem min V ∈R k ×d U * V -(A -B 1 ) 1 , and let U * V * be a best 1 -low rank approximation to A -B 1 ; we later explain why we look at this problem. We can assume V * is an 1 well-conditioned basis <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>, since we can replace U * with U * R -1 and V * with RV * for any invertible linear transformation R.</p><p>For any vector x we then have</p><formula xml:id="formula_30">x 1 f ≤ x V * 1 ≤ e x 1</formula><p>, where 1 ≤ e, f ≤ poly(k ). This implies all entries of U * are at most 2f A -B 1 , as otherwise one could replace U * with 0 n×k and reduce the cost. Also, any entry of U * smaller than A-B 1 100enkp can be replaced with 0 as this incurs additive error OPT  100 . If we round the entries of U * to integer multiples of A-B 1 100enkp , then we only have O (enkp f ) possibilities for each entry of U * , and still obtain an O (1)-approximation. We refer to the rounded U * as U * , abusing notation.</p><p>Let D be a sampling and rescaling matrix with O (k log k ) nonzero diagonal entries, corresponding to sampling by the Lewis weights of U * . We do not know D, but handle this below. By the triangle inequality, for any V ,</p><formula xml:id="formula_31">D(U * V -(A -B 1 )) 1 = D(U * V -U * V * ) 1 ± D(U * V * -(A -B 1 )) 1 = Θ(1) U * V -U * V * 1 ± O (1) U * V * -(A -B 1 ) 1 ,</formula><p>where the Lewis weights give</p><formula xml:id="formula_32">D(U * V -U * V * ) 1 = Θ(1) U * V - U * V * 1 and a Markov bound gives D(U * V * -(A -B 1 )) 1 = O (1) U * V * -(A -B 1 ) 1 . Thus, minimizing DU * V -D (A -B 1 ) 1</formula><p>gives a xed constant factor approximation to the problem min</p><formula xml:id="formula_33">V ∈R k ×d U * V -(A -B 1 ) 1 .</formula><p>The non-zero diagonal entries of D can be assumed to be integers between 1 and n 2 .</p><p>We guess the entries of DU * and note for each entry there are only O (enkp f log(n 2 )) possibilities. One of our guesses corresponds to Lewis weight sampling by U * . We solve for V and by the guarantees of Lewis weights, the row span of this V provides an O (1)-approximation. We can nd the corresponding U via linear programming. As mentioned above, we do not know D, but can enumerate over all D and all possible DU * . The total time is n poly(k ) .</p><p>After nding U , which has k columns, we output the rank-2k space formed by the column span of [U , B 1 ]. By including the column span of B 1 , we ensure our original transformation of the problem min</p><formula xml:id="formula_34">V ∈R k ×d U * • V -A 1 to the problem min V ∈R k ×d U * • V - (A -B 1 )</formula><p>1 is valid, since we can rst use the column span of B 1 to replace A with A -B 1 . Replacing A with A -B 1 ultimately results in a rank-2k output. Had we used A instead of A -B 1 our output would have been rank k but would have additive error</p><formula xml:id="formula_35">A 1 poly(k /ϵ ) .</formula><p>If we assume the entries of A are in {-b, -b + 1, . . . , b}, then we can lower bound the cost U * V -A 1 , given that it is non-zero, by (ndb) -O (k ) (if it is zero then we output A) using Lemma 4.1 in <ref type="bibr" target="#b22">[23]</ref> and relating entrywise 1 -norm to Frobenius norm. We can go through the same arguments above with A -B replaced by A and our running time will now be (ndb) poly(k ) . For an O (k log k ) × d matrix S of i.i.d. Cauchy random variables, SA = S + (log d )S 1 e, where S 1 is the rst column of S. For a typical column of SA, all entries are at most poly(k ) log d in magnitude. Thus, in order to approximate the rst row of A, which is (log d )e, by x SA for an x ∈ R k log k , we need x 1 ≥ 1 poly(k ) . Also x S 1 = Ω( x 1 d log d ) with 1exp(-k log k ) probability, for d large enough, so by a net argument x 1 ≤ poly(k ) for all x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hard Instances for Cauchy</head><p>However, there are entries of SA that are very large, i.e., about one which is r = Θ(dk log k ) in magnitude, and in general about 2 i entries about r 2 -i in magnitude. These entries typically occur in columns C j of SA for which all other entries in the column are bounded by poly(k ) in magnitude. Thus, |x C j | ≈ r 2 -i for about 2 i columns j. For each such column, if r 2 -i log d, then we incur cost r 2 -i poly(k ) in approximating the rst row of A. In total the cost is When k is large this bound deteriorates, but we also show a k 1/2-γ lower bound for arbitrarily small constant γ &gt; 0. This bound applies to any oblivious sketching matrix. The idea is similar to our row subset selection lower bound. Let A be as in our row subset selection lower bound, consider SA, and write S = U ΣV in its full SVD. Then SA is in the row span of the top O (k log k ) rows of V A, since Σ only has O (k log k ) non-zero singular values. Since the rst k columns of A are rotationally invariant, V A has rst k columns i.i.d. Gaussian and remaining columns equal to V . Call the rst O (k log k ) rows of V A the matrix B. We now try to approximate a row of A by a vector in the row span of B. There are two issues that make this setting di erent from row subset selection: (1) B no longer contains an identity submatrix, and (2) the rows of B depend on the rows of A. We handle the rst issue by building nets for subsets of coordinates of x V rather than x as before; since x V 2 = x 2 similar arguments can be applied. We handle the second issue by observing that if the number of rows of B is considerably smaller than that of A, then the distribution of B had we replaced a random row of A with zeros would be statistically close to i.i.d. Gaussian. Hence, typical rows of A can be regarded as being independent of B.</p><p>Limited Independence, Distributed, and Streaming Algorithms: We show for an n × d matrix A, if we left-multiply by an O (k log k ) × n matrix S in which each row is an independent vector of O (d )-wise independent Cauchy random variables, SA contains a poly(k ) log d-approximation in its span. This allows players in a distributed model to share a common S by exchanging O (kd ) bits, independent of n. We use Lemma 2.2 of <ref type="bibr" target="#b50">[51]</ref> which shows for a reasonably smooth approximation f to an indicator function,</p><formula xml:id="formula_36">E[f (X )] = E[f (Y )] + O (ϵ ), where X = i a i X i , Y = i a i Y i , a ∈ R n is xed</formula><p>, X is a vector of i.i.d. Cauchy random variables, and Y is a vector of O (1/ϵ )-wise independent random variables.</p><p>To show the row span of SA contains a good rank-k approximation, we argue S 1 = Ω( 1 ) for a xed ∈ R n with 1exp(-k log k ) probability. We apply the above lemma with ϵ = Θ(1). We also need for an n × d matrix A with unit-1 columns, that SA 1 = O (kd ). We fool the expectation of a truncated Cauchy by taking a weighted sum of O (log(dk )) indicator functions and applying the above lemma with ϵ = Θ(1/d ). An issue is there are Θ(kd ) Cauchy random variables corresponding to the entries of SA, some of which can be as large as Θ(kd ), so to fool their expectation (after truncation) we need ϵ = Θ(1/(dk )), resulting in O (dk 2 ) seed length and ruining our optimal O (dk ) communication. We show we can instead pay a factor of k in our approximation and maintain O (dk )-wise independence. The distributed and streaming algorithms, given this, follow algorithms for Frobenius norm low rank approximation in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b52">53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardness Assuming Exponential Time Hypothesis:</head><p>By inspecting the proof of NP-hardness of <ref type="bibr" target="#b45">[46]</ref>, it at best gives a (1 + 1 n γ )-inapproximability for an arbitrarily small constant γ &gt; 0. We considerably strengthen this to (1</p><formula xml:id="formula_37">+ 1 log 1+γ n )-inapproximability</formula><p>by taking a modi ed version of the n × n hard instance of <ref type="bibr" target="#b45">[46]</ref> and planting it in a 2 o (n) × 2 o (n) matrix padded with tiny values. Under the ETH, the maximum cut problem that <ref type="bibr" target="#b45">[46]</ref> and that we rely on cannot be solved in 2 o (n) time, so our transformation is e cient. Although we use the maximum cut problem as in <ref type="bibr" target="#b45">[46]</ref> for our n × n hard instance, in order to achieve our inapproximability we need to use that under the ETH this problem is hard to approximate even if the input graph is sparse and even up to a constant factor; such additional conditions were not needed in <ref type="bibr" target="#b45">[46]</ref>. p -Low Rank Approximation and EMD-Low Rank Approximation: Our algorithms for entrywise p -Norm Error are similar to our algorithms for 1 . We use p-stable random variables in place of Cauchy random variables, and note that the p-th power of a p-stable random variable has similar tails to that of a Cauchy, so many of the same arguments apply. Our algorithm for EMD low rank approximation immediately follows by embedding EMD into 1 . 2n+2) where ϵ ∈ (0, .5), γ &gt; 0, and B is the n×n all 1s matrix. For this A we show the four heuristic algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59]</ref> cannot achieve an n min(γ ,0.5-ϵ ) approximation ratio when the rank parameter k = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counterexamples to Heuristics:</head><formula xml:id="formula_38">Let A = diag(n 2+γ , n 1.5+ϵ , B, B) ∈ R (2n+2)×(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NOTATION</head><p>Let N + denote the set of positive integers. For any</p><formula xml:id="formula_39">n ∈ N + , let [n] denote the set {1, 2, • • • , n}. For any p ∈ [1, 2]</formula><p>, the p -norm of a vector x ∈ R d is de ned as</p><formula xml:id="formula_40">x p = d i=1 |x i | p 1/p .</formula><p>For any p ∈ [1, 2), the p -norm of a matrix A ∈ R n×d is de ned as</p><formula xml:id="formula_41">A p = n i=1 d j=1 |A i j | p 1/p .</formula><p>Let A F denote the Frobenius norm of matrix A. Let nnz(A) denote the number of nonzero entries of A. Let det(A) denote the determinant of a square matrix A. Let A denote the transpose of A. Let A † denote the Moore-Penrose pseudoinverse of A. Let A -1 denote the inverse of a full rank square matrix. We use A j to denote the j th column of A, and A i to denote the i th row of A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES 3.1 Polynomial System Veri er</head><p>Renegar <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b78">79]</ref> and Basu et al . <ref type="bibr" target="#b7">[8]</ref> independently provided an algorithm for the decision problem for the existential theory of the reals, which is to decide the truth or falsity of a sentence</p><formula xml:id="formula_42">(x 1 , • • • , x )F ( f 1 , • • • , f m )</formula><p>where F is a quanti er-free Boolean formula with atoms of the form sign( f i ) = σ with σ ∈ {0, 1, -1}. Note that this problem is equivalent to deciding if a given semi-algebraic set is empty or not. Here we formally state that theorem. For a full discussion of algorithms in real algebraic geometry, we refer the reader to <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b6">[7]</ref>. T 3.1 (D P <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79]</ref>). Given a real polynomial system P (x 1 , x 2 , • • • , x ) having variables and m polynomial constraints</p><formula xml:id="formula_43">f i (x 1 , x 2 , • • • , x )∆ i 0, ∀i ∈ [m]</formula><p>, where ∆ i is any of the "standard relations": {&gt;, ≥, =, , ≤, &lt;}, let d denote the maximum degree of all the polynomial constraints and let H denote the maximum bitsize of the coe cients of all the polynomial constraints. Then in (md ) O ( ) poly(H ) time one can determine if there exists a solution to the polynomial system P.</p><p>Recently, this technique has been used to solve a number of low-rank approximation and matrix factorization problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b76">77]</ref>. Note that <ref type="bibr" target="#b48">[49]</ref> provides a tool which is able to determine the minimum nonzero value of the absolute value of a polynomial evaluated on a semi-algebraic set, provided the polynomial is never 0 on that set. That tool does not allow the polynomial system to be de ned by , &gt;, and &lt; relations. However, one can handle such relations by intersecting the set with a large enough ball (see, e.g., <ref type="bibr" target="#b76">[77]</ref>), and replacing an f (x ) 0 constraint with the constraint f (x ) -1 = 0 for a new variable with bounded value; one can further combine such not equal constraints together so as to not introduce too many variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cauchy and p-stable Transform</head><p>De nition 3.2 (Dense Cauchy transform). Let S = σ • C ∈ R m×n where σ is a scalar, and each entry of C ∈ R m×n is chosen independently from the standard Cauchy distribution. For any matrix A ∈ R n×d , SA can be computed in O (m • nnz(A)) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>De nition 3.3 (Sparse Cauchy transform). Let</head><formula xml:id="formula_44">Π = σ •SC ∈ R m×n ,</formula><p>where σ is a scalar, S ∈ R m×n has each column chosen independently and uniformly from the m standard basis vectors of R m , and C ∈ R n×n is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. For any matrix A ∈ R n×d , ΠA can be computed in O (nnz(A)) time.</p><p>De nition 3.4 (Dense p-stable transform). Let p ∈ (1, 2). Let S = σ • C ∈ R m×n where σ is a scalar, and each entry of C ∈ R m×n is chosen independently from the standard p-stable distribution. For any matrix A ∈ R n×d , SA can be computed in O (m nnz(A)) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>De nition 3.5 (Sparse</head><formula xml:id="formula_45">p-stable transform). Let p ∈ (1, 2). Let Π = σ • SC ∈ R m×n ,</formula><p>where σ is a scalar, S ∈ R m×n has each column chosen independently and uniformly from the m standard basis vectors of R m , and C ∈ R n×n is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution. For any matrix A ∈ R n×d , ΠA can be computed in O (nnz(A)) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lewis Weights</head><p>We follow the exposition of Lewis weights from <ref type="bibr" target="#b28">[29]</ref>.</p><p>De nition 3.6. For a matrix A, let a i denote i th row of A, and a i (= (A i ) ) is a column vector. The statistical leverage score of a row a i is</p><formula xml:id="formula_46">τ i (A) def = a i (A A) -1 a i = (A A) -1/2 a i 2 2 .</formula><p>For a matrix A and norm p, the p Lewis weights w are the unique weights such that for each row i we have</p><formula xml:id="formula_47">w i = τ i (W 1/2-1/p A).</formula><p>or equivalently</p><formula xml:id="formula_48">a i (A W 1-2/p A) -1 a i = w 2/p i . L 3.7 (L 2.4 [29] L 7 [28]). Given a matrix A ∈ R n×d , n ≥ d, for any constant C &gt; 0, 4 &gt; p ≥ 1,</formula><p>there is an algorithm which can compute C-approximate p Lewis weights for every row i of A in O ((nnz(A) + d ω log d ) log n) time, where ω &lt; 2.373 is the matrix multiplication exponent <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b87">88]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L</head><p>3.8 (T 7.1 <ref type="bibr" target="#b28">[29]</ref>). Given matrix A ∈ R n×d (n ≥ d) with p (4 &gt; p ≥ 1) Lewis weights w, for any set of sampling probabilities p i , i p i = N ,</p><formula xml:id="formula_49">p i ≥ f (d, p)w i , if S ∈ R N ×n</formula><p>has each row chosen independently as the i th standard basis vector, times 1/p 1/p i , with probability p i /N , then with probability at least 0.999,</p><formula xml:id="formula_50">∀x ∈ R d , 1 2 Ax 1 ≤ SAx 1 ≤ 2 Ax 1 Furthermore, if p = 1, N = O (d log d ). If 1 &lt; p &lt; 2, N = O (d log d log log d ). If 2 ≤ p &lt; 4, N = O (d p/2 log d ).</formula><p>3.4 Frobenius Norm and 2 Relaxation T 3.9 (G , T 2 <ref type="bibr" target="#b42">[43]</ref>). Given matrices</p><formula xml:id="formula_51">A ∈ R n×d , B ∈ R n×p , and C ∈ R q×d , let the SVD of B be B = U B Σ B V B and the SVD of C be C = U C Σ C V C . Then, B † (U B U B AV C C C ) k C † = arg min rank -k X ∈R p×q A -BXC F ,</formula><p>where</p><formula xml:id="formula_52">(U B U B AV C V C ) k ∈ R p×q is of rank at most k and denotes the best rank-k approximation to U B U B AV C V C ∈ R p×d in Frobenius norm. C 3.10 ( 2 p ). Let p ∈ [1, 2). For any A ∈ R n×d and b ∈ R n , de ne x * = arg min x ∈R d Ax -b p and x = arg min x ∈R d Ax -b 2 . Then, Ax * -b p ≤ Ax -b p ≤ n 1/p-1/2 • Ax * -b p . C 3.11 (F p ). Let p ∈ [1, 2</formula><p>) and for any matrix A ∈ R n×d , de ne</p><formula xml:id="formula_53">A * = arg min rank -k B ∈R n×d B -A p and A = arg min rank -k B ∈R n×d B -A F . Then A * -A p ≤ A -A p ≤ (nd ) 1/p-1/2 A * -A p .</formula><p>3.5 Converting Entry-wise 1 and p Objective Functions into Polynomials  </p><formula xml:id="formula_54">C 3.12 (C ). Given m polynomials f 1 (x ), f 2 (x ), • • • , f m (x ) where x ∈ R , solving the problem min x ∈R m i=1 | f i (x )|,</formula><formula xml:id="formula_55">min x ∈R ,σ ∈R m m i=1 σ i f i (x ) s.t. σ 2 i = 1, ∀i ∈ [m] f i (x )σ i ≥ 0, ∀i ∈ [m]. C 3.13. (Handling p ) Given m polynomials f 1 (x ), f 2 (x ), • • • , f m (x )</formula><formula xml:id="formula_56">min x ∈R ,σ ∈R m m i=1 i s.t. σ 2 i = 1, ∀i ∈ [m] f i (x )σ i ≥ 0, ∀i ∈ [m] (σ i f i (x )) a = b i , ∀i ∈ [m] i ≥ 0, ∀i ∈ [m].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Converting Entry-wise 1 Objective</head><p>Function into a Linear Program C 3.14. Given any matrix A ∈ R n×d and matrix B ∈ R k×d , the problem min U ∈R n×k U B -A 1 can be solved by solving the following linear program,</p><formula xml:id="formula_57">min U ∈R n×k ,x ∈R n×d n i=1 m j=1 x i, j U i B j -A i, j ≤ x i, j , ∀i ∈ [n], j ∈ [d] U i B j -A i, j ≥ -x i, j , ∀i ∈ [n], j ∈ [d] x i, j ≥ 0, ∀i ∈ [n], j ∈ [d],</formula><p>where the number of constraints is O (nd ) and the number of variables is O (nd ).</p><p>We note that that the time for solving such a linear program is poly(nd ) • L time, where L is the maximum bit complexity. For brevity we drop the dependence on L in the statements of the running times using this routine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">1 -LOW RANK APPROXIMATION</head><p>This section presents our main 1 -low rank approximation algorithms. Section 4.1 rst provides our three existence results. Section 4.2 then shows an input sparsity algorithm with approximation ratio poly(k ) log 2 d log n, and then improves the approximation ratio to poly(k ) log d by using our 1 -low rank approximation algorithm for a rank-r (where k ≤ r ≤ (n, d ) ) matrix as a black box (by setting r = poly(k )). Section 4.3 presents an algorithm with O (k ) approximation ratio, then another algorithm with approximation ratio as small as O (1) if one allows for outputting a rank-2k solution, and nally CUR decomposition algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Existence Results via Dense Cauchy</head><p>Transforms, Sparse Cauchy Transforms, Lewis Weights</p><p>The goal of this section is to present the existence results in Corollary 4.2. We rst provide several bicriteria algorithms in Theorem 4.1 which can be viewed as "warmups". Then, the proof of our bicriteria algorithm will actually imply our existence results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T 4.1 (B ).</head><p>Given matrix A ∈ R n×d , for any k ≥ 1, there exist bicriteria algorithms with running time T (speci ed below), which output two matrices U ∈ R n×m , V ∈ R m×d such that, with probability 9/10,  </p><formula xml:id="formula_58">UV -A 1 ≤ α min rank -k A k A k -A 1 . (I). Using a dense Cauchy transform, T = poly(n, d, k ), m = O (k log k ), α = O ( k log k log d ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(III). Sampling by Lewis weights,</head><formula xml:id="formula_59">T = (nd ) O (k ) , m = O (k log k ), α = O ( k log k ). C 4.2 (E C , C , L ). Given A ∈ R n×d , there exists a rank-k matrix A ∈ R n×d such that A ∈ rowspan(S A) ⊆ rowspan(A) and A -A 1 ≤ α • min rank -k A k A - A k 1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Input Sparsity Time Algorithms</head><p>We present two input sparsity time algorithms in this section. The rst algorithm has poly(k, log n, log d ) approximation ratio. The second algorithm has poly(k ) log d approximation ratio. The rst algorithm (Theorem 4.3) in this section is actually worse than the second algorithm (Theorem 4.5) described in this section, in terms of approximation ratio. But this algorithm will be much easier to extend to distributed and streaming settings (we refer the reader to the full version for further details). </p><formula xml:id="formula_60">U ∈ R n×k , V ∈ R k ×d such that UV -A 1 ≤ O (poly(k ) log n log 2 d ) min rank -k A k A k -A 1 ,</formula><p>holds with probability 9/10. Intuitively, our second algorithm has two stages. In the rst stage, we just want to nd a low rank matrix B which is a good approximation to A. Then, we can try to nd a rank-k approximation to B. Since now B is a low rank matrix, it is much easier to nd a rank-k approximation to B. The procedure L1L R A B(U B , V B , n, d, k, s) corresponds to Theorem 4.4. </p><formula xml:id="formula_61">U ∈ R n×k , V ∈ R k ×d such that UV -B 1 ≤ poly(r ) min rank -k B k B k -B 1 ,</formula><p>holds with probability 9/10.</p><p>Here, we state the input sparsity time algorithm with O (log d ) • poly(k ) approximation ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T 4.5 (I ).</head><p>Given matrix A ∈ R n×d , for any k ≥ 1, there exists an algorithm which takes nnz(A)</p><formula xml:id="formula_62">+ (n + d ) • poly(k ) time to output two matrices U ∈ R n×k , V ∈ R k ×d such that UV -A 1 ≤ poly(k ) log d min rank -k A k A k -A 1 ,</formula><p>holds with probability 9/10.   Choose sparse Cauchy matrices S ∈ R s×n and compute S • A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Better Approximation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Implicitly obtain B = U B V B by nding V B = SA ∈ R s×d and U B ∈ R n×s where ∀i ∈ [n], row vector (U B ) i gives an O (1) approximation to min x ∈R 1×s xSA -A i 1 .</p><p>13: Set Compute a sampling and rescaling matrix D ∈ R n×n ,T 1 ∈ R n×n corresponding to the Lewis weights of AR, and let them have m, t 1 nonzero entries on the diagonals, respectively.</p><formula xml:id="formula_63">U , V ←L1L R A B(U B , V B ,</formula><formula xml:id="formula_64">r ← O (k log k ), m ← t 1 ← O (r log r ), t 2 ← O (m log m).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>Compute a sampling and rescaling matrix T 2 ∈ R d ×d according to the Lewis weights of (DA) , and let it have t 2 nonzero entries on the diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>Solve min X,Y T 1 ARXY DAT 2 -T 1 AT 2 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>Take the best solution X , Y over all guesses of R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>23:</head><p>return ARX , Y DA. 24: end procedure veri er (see details in Section 3.1). At the end, we also provide a CUR decomposition algorithm for the 1 -norm. T 4.6. Given matrix A ∈ R n×d , there exists an algorithm that takes poly(n)</p><formula xml:id="formula_65">• d O (k ) • 2 O (k 2 ) time and outputs two matrices U ∈ R n×k , V ∈ R k ×d such that UV -A 1 ≤ O (k ) min rank -k A k A k -A 1 ,</formula><p>holds with probability 9/10.</p><p>We present an algorithm which is able to output a rank-2k solution and achieve an O (1)-approximation. This algorithm also leverages a technique called "guessing a sketch" from <ref type="bibr" target="#b76">[77]</ref>. </p><formula xml:id="formula_66">U B , V B ← min U ∈R n×k ,V ∈R k ×d UV -A F . 3:</formula><p>Set r ← O (k log k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Guess a diagonal matrix D ∈ R n×n with r nonzero entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Guess matrix DU ∈ R r ×k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Find V A by solving min V DUV -D(A -B) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Find U A by solving min U UV A -(A -B) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Take the best solution U A U B , V A V B over all guesses. 9:</p><formula xml:id="formula_67">return U A U B , V A V B .</formula><p>10: end procedure     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choose dense Cauchy matrices</head><formula xml:id="formula_68">S ∈ R s×n , R ∈ R d ×r , T 1 ∈ R t 1 ×n , T 2 ∈ R d ×t 2 . 21: Compute S • U B • V B , U B • V B • R and T 1 • U B • V B • T 2 . 22: Compute XY = arg min X,Y T 1 U B V B RXY SU B V B T 2 - T 1 U B V B T 2 F . 23: return U B V B RX , YSU B V B . 24: end procedure U ∈ R n×2k , V ∈ R 2k ×d , UV -A 1 min rank -k A k A k -A 1 ,</formula><p>holds with probability 9/10.</p><p>There is a long line of research on matrix CUR algorithms under the operator norm and Frobenius norm <ref type="bibr">[12-14, 32, 37, 39]</ref>. Here, we provide the rst CUR algorithms under Entry-wise 1 norm. </p><formula xml:id="formula_69">CU R) = k, c = O (k log k ), r = O (k log k ), and CU R -A 1 ≤ poly(k ) log d min rank -k A k A k -A 1 ,</formula><p>holds with probability 9/10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">HARDNESS</head><p>This section presents our hardness result under the Exponential Time Hypothesis ETH. We rst introduce the de nition of 3-SAT and ETH. For further details and background on the 3-SAT problem, we refer the reader to <ref type="bibr" target="#b0">[1]</ref>.</p><p>De nition 5.1 . Given an r variable and m clause conjunctive normal form CNF formula with the size of each clause at most 3, the goal is to decide whether there exists an assignment for the r Boolean variables to make the CNF formula satis ed.</p><p>We state the de nition of the Exponential Time Hypothesis: H 5.2 (E T H (ETH) <ref type="bibr" target="#b46">[47]</ref>). There is a δ &gt; 0 such that the 3-SAT problem de ned in De nition 5.1 cannot be solved in O (2 δ r ) running time.</p><p>Before we state our main lower bound, we introduce the de nition of MAX-CUT and known hardness results for MAX-CUT .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>De nition 5.3 (MAX-CUT decision problem)</head><p>. Given a positive integer c * and an unweighted graph G = (V , E) where V is the set of vertices of G and E is the set of edges of G, the goal is to determine whether there is a cut of G that has at least c * edges. T 5.4 (T 6.1 <ref type="bibr" target="#b41">[42]</ref>). There exist constants a, b ∈ (0, 1) and a &gt; b, such that, for a given MAX-CUT (see De nition 5.3) instance graph G = (E, V ) which is an n-vertex 5-regular graph, if there is an algorithm in time 2 o (n) which can distinguish the following two cases:</p><p>(1) At least one cut of the instance has at least a|E| edges, (2) All cuts of the instance have at most b |E| edges, then ETH (see Hypothesis 5.2) fails.</p><p>By combining the hard instance in <ref type="bibr" target="#b45">[46]</ref> with a padding argument, we obtain the following lower bound under the ETH: T 5.5. Unless ETH (see Hypothesis 5.2) fails, for arbitrarily small constant γ &gt; 0, there is no algorithm, which given a matrix 1) time.</p><formula xml:id="formula_70">A ∈ {-1, +1} n×d , can compute x ∈ R n , ∈ R d such that A -x 1 ≤ (1 + 1 log 1+γ nd ) min x ∈R n , ∈R d A -x 1 , in (nd ) O<label>(</label></formula><p>Finally, we reduce the rank-k case to the rank-1 case. The high level idea is that we construct a block diagonal matrix with k blocks where the rst block contains a rank-1 hard instance, and each of the remaining k -1 blocks is just a single large number. The approximate best rank-k solution needs to use k -1 dimensions to t these k -1 large numbers, and has one dimension left to solve the rank-1 hard instance. Thus, we can obtain a lower bound for general k ≥ 1 under ETH. T 5.6. For any constants c 1 &gt; 0, c 2 &gt; 0 and c 3 &gt; 0, and any constant c 4 ≥ 10(c 1 + c 2 + c 3 + 1), given any matrix A ∈ R n×n with absolute value of each entry bounded by n c 1 , we de ne a block diagonal matrix A ∈ R (n+k -1)×(n+k -1) as</p><formula xml:id="formula_71">A =             A 0 0 • • • 0 0 B 0 • • • 0 0 0 B • • • 0 • • • • • • • • • • • • • • • 0 0 0 • • • B             , where B = n c 4 . If A is an 1 -norm rank-k C-approximation solution to A, i.e., A -A 1 ≤ C • min rank -k A A -A 1 , where C ∈ [1, n c 3 ], then there must exist j * ∈ [n] such that min ∈R n A [1:n] j * -A 1 ≤ C • min u, ∈R n u -A 1 + 1/n c 2 ,</formula><p>i.e., the rst n coordinates of the column j * of A give an 1 -norm rank-1 C-approximation to A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">HARDNESS RESULTS FOR CAUCHY MATRICES, ROW SUBSET SELECTION, OSE</head><p>This section presents our inapproximability results. Theorem 6.1 presents inapproximability results if one uses random Cauchy matrices. Theorem 6.2 is then a warmup for our inapproximability result for row subset selection. Theorem 6.4 then shows inapproximability results for row subset selection. Theorem 6.6 shows general inapproximability results if one uses any linear oblivious subspace embedding (OSE). It is worth noting that all of our hard instances, except for Theorem 6.2, also give hard instances for bicriteria algorithms.</p><p>Our hard instance for the Cauchy embedding is a matrix A = B+I , where I denotes an identity matrix. The entries of the rst row of B are all equal to Θ(log d ), and the entries elsewhere are all 0. We can prove that it is expensive to t the rst row of A after applying a Cauchy embedding. In the following we show that even if we take the entire row span of A, there is no 2 -Θ(1/d ) rank-1 approximation in the row span of A, where A is a matrix obtained by concatenating an all 1s vector with an identity matrix. T 6.2. There exists a value k ≥ 1 and matrix A ∈ R (d -1)×d such that, there is no rank -k matrix B in the row span of A satisfying</p><formula xml:id="formula_72">A -B 1 &lt; 2(1 -Θ( 1 d )) min rank -k A A -A 1 .</formula><p>In the following, we prove that there is a hard input distribution for any xed sketching matrix. The lower bound on the approximation ratio is at least √ k. Although not immediately useful, we will later combine it with Yao's minimax principle <ref type="bibr" target="#b94">[94]</ref> in order to obtain lower bounds when the algorithm chooses a sketching matrix from a distribution. Figure <ref type="figure">1</ref>: Let A be (2n + 2) × (2n + 2) input matirx. (a) shows the performance of all the algorithms when the matrix dimensions are growing. The x-axis is n, and the -axis is A -A 1 where A is the rank-3 solution output by all the heuristic algorithms as well as ours. The 1 residual cost of all the other algorithms is growing much faster than ours, which is consistent with our theoretical results. (b) shows the running time (in seconds) of all the algorithms when the matrix dimension n is growing. The x-axis is n and the -axis is time (seconds). The running time of some of the algorithms is longer than 3 seconds. For most of the algorithms (including ours), the running time is always less than 3 seconds. where ε &gt; 0 is a constant which can be arbitrarily small.</p><p>We use the same hard instance in Theorem 6.3 to show hardness for row subset selection. Although one xed set of rows can be regarded as a xed sketching matrix, we need to (and show it is possible to) union bound over all subsets of rows to obtain hardness for row subset selection. where α ∈ (0, 0.5) is a constant which can be arbitrarily small.</p><p>We need the following de nition. De nition 6.5. Given a matrix A ∈ R n×d , a matrix S ∈ R r ×n , k ≥ 1 and γ ∈ (0, 1  2 ), we say that an algorithm M (A, S, k, γ ) which outputs a matrix B ∈ R n×r "succeeds", if</p><formula xml:id="formula_73">BSA -A 1 ≤ k γ • min rank -k A A -A 1 .</formula><p>To prove Theorem 6.6, we apply the hard instance shown in Theorem 6.3 together with Yao's minimax principle. T 6.6 (H ). Let Π denote a distribution over matrices S ∈ R r ×n . For any k ≥ 1, any constant γ ∈ (0, 1  2 ), arbitrary constants c 1 , c 2 &gt; 0 and min(n, d ) ≥ Ω(k c 2 ), if for all A ∈ R n×d , it holds that Pr S ∼Π [M (A, S, k, γ ) succeeds ] ≥ Ω(1/k c 1 ), then r must be at least Ω(k c 2 -2c 1 -2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS 7.1 Setup</head><p>We provide some details of our experimental setup. We obtained the R package of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59</ref>] from https://cran.r-project.org/web/ packages/pcaL1/index.html. We also implemented our algorithm and the r1-pca algorithm <ref type="bibr" target="#b33">[34]</ref> using the R language. The version of the R language is 3.0.2. We ran experiments on a machine with Intel X5550@2.67GHz CPU and 24G of memory. The operating system of that machine is Linux Ubuntu 14.04.5 LTS. All the experiments were done in single-threaded mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Counterexample for Previous Heuristics</head><p>For any ϵ ∈ (0, 0.5) and γ &gt; 0, we construct the input matrix A ∈ R (2n+2)×(2n+2) as follows</p><formula xml:id="formula_74">A =           n 2+γ 0 0 0 0 n 1.5+ϵ 0 0 0 0 B 0 0 0 0 B          </formula><p>, where B is n × n all 1s matrix. We want to nd a rank k = 3 solution for A. Then any of the four heuristic algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59</ref>] is not able to achieve better than an n min(γ ,0.5-ϵ ) approximation ratio. We present our main experimental results in Figure <ref type="figure">1</ref>. Both <ref type="bibr" target="#b55">[56]</ref> and <ref type="bibr" target="#b58">[59]</ref> have two di erent ways of initialization. In Figure <ref type="figure">1</ref> we use KK05r (resp. Kwak08r) to denote the way that uses a random vector as initialization, and use KK05s (resp. Kwak08s) to denote the way that uses the top singular vector as initialization. Figure <ref type="figure">1</ref>(a) shows the performance of all the algorithms and Figure <ref type="figure">1</ref>(b) presents the running time. The 1 residual cost of all the other algorithm grows much faster than in our algorithm. Most of the algorithms (including ours) are pretty e cient, i.e., the running time is always below 3 seconds. The running times of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b55">56]</ref> increase rapidly when the matrix dimension n grows.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Matrices and More General Sketches: We consider a d × d matrix A = I d + (log d )e 1 e, where e 1 = (1, 0, . . . , 0) and e = (1, 1, . . . , 1) and I d is the d × d identity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r log r poly(k ) = d log d poly(k ) , but the optimal cost is at most d, giving a log d poly(k ) lower bound. We optimize this to a log d k log 2 k lower bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For an n × d matrix A, for S a subset of [n] and T a subset of [d], we let A S denote the |S | ×d submatrix of A with rows indexed by S, while A T denotes the n × |T | submatrix of A with columns indexed by T , and A S T denote the |S | × |T | submatrix A with rows in S and columns in T . For any function f , we de ne O ( f ) to be f • log O (1) ( f ). In addition to O (•) notation, for two functions f , , we use the shorthand f (resp. ) to indicate that f ≤ C (resp. ≥) for an absolute constant C. We use f to mean c f ≤ ≤ C f for constants c, C. We use OPT to denote min rank -k A k A k -A 1 , unless otherwise speci ed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>is equivalent to solving another minimization problem with O (m) extra constraints and m extra variables,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where x ∈ R and p = a/b for positive integers a and b, solving the problem min x ∈R m i=1 | f i (x )| p , is equivalent to solving another minimization problem with O (m) extra constraints and O (m) extra variables,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>II). Using a sparse Cauchy transform, T = poly(n, d, k ),m = O (k 5 log 5 k ), α = O (k 4.5 log 4.5 k log d ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where S ∈ R m×n is a sketching matrix. If S (I). indicates the dense Cauchy transform, then α = O ( k log k log d ). (II). indicates the sparse Cauchy transform, then α = O (k 4.5 log 4.5 k log d ). (III). indicates sampling by Lewis weights, then α = O ( k log k ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>T 4 .</head><label>4</label><figDesc>3 (I ).Given matrix A ∈ R n×d , for any k ≥ 1, there exists an algorithm which takes O (nnz(A))+(n+d )•poly(k ) time and outputs two matrices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>T 4 . 4 (</head><label>44</label><figDesc>poly(k, r ) r B). Given a factorization of a rank-r matrix B = U B V B ∈ R n×d , where U B ∈ R n×r , V B ∈ R r ×d , for any 1 ≤ k ≤ r there exists an algorithm which takes (n + d ) • poly(k ) time to output two matrices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 : 4 : 5 : 6 :</head><label>3456</label><figDesc>Choose sparse Cauchy matricesS ∈ R s×n , R ∈ R d ×r , T 1 ∈ R t 1 ×n . Choose dense Cauchy matrices T 2 ∈ R d ×t 2 . Compute S • A, A • R and T 1 • A • T 2 . Compute XY = arg min X,Y T 1 ARXYSAT 2 -T 1 AT 2 F . Set s ← O (k 5 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>18 :</head><label>18</label><figDesc>Guess a diagonal matrix R ∈ R d ×d with only r 1s. R selects r columns of A ∈ R n×d .19:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>T 4 . 7 .</head><label>47</label><figDesc>Given matrix A ∈ R n×d , for any k ≥ 1, there exists an algorithm which takes (nd ) O (k 2 ) time to output two matrices Algorithm 2 Bicriteria O (1)-approximation Algorithm, CUR Decomposition Algorithm, poly(r , k )-approximation Algorithm for Rank-r matrix B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>13 :Let D 1 ∈</head><label>131</label><figDesc>R n×n be the sampling and rescaling diagonal matrix corresponding to the Lewis weights of B 1 = U B ∈ R n×k , and let D 1 have d 1 = O (k log k ) nonzero entries. 14: Let D 2 ∈ R d ×d be the sampling and rescaling diagonal matrix corresponding to the Lewis weights of B 2 = (D 1 B 1 ) † D 1 A ∈ R d ×k , and let D 2 have d 2 = O (k log k ) nonzero entries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>15 :C</head><label>15</label><figDesc>← AD 2 , U ← (B 2 D 2 ) † (D 1 B 1 ) † , and R ← D 1 A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>16 : 4 19 :</head><label>16419</label><figDesc>return C, U , R. 17: end procedure18: procedure L1L R A B(U B , V B , n, d, k, r ) Theorem 4.Set s ← O (r ), r ← O (r ), t 1 ← O (r ), t 2 ← O (r ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Given matrixA ∈ R n×d , for any k ≥ 1, there exists an algorithm which takes O (nnz(A)) + (n + d ) poly(k ) time to output three matrices C ∈ R n×c with columns from A, U ∈ R c×r , and R ∈ R r ×d with rows from A, such that rank(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>T 6 . 1 .</head><label>61</label><figDesc>Let k ≥ 1. There exist matrices A ∈ R d ×d such that for any o(log d ) ≥ t ≥ 1, for a random Cauchy matrix S ∈ R t ×d where each entry is sampled from an i.i.d. Cauchy distribution C (0, γ ), where γ is an arbitrary real number, with probability at least .99 we have minU ∈R d ×t U SA -A 1 ≥ Ω(log d/(t log t )) min rank -k A A -A 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>T 6 . 3 . 2 )</head><label>632</label><figDesc>For any k ≥ 1, and any constantsc 1 , c 2 which satisfy c 2 -2 &gt; c 1 &gt; 1, let r = Θ(k c 1 ), n = Θ(k c 2 ), and let A(k, n) denote a distribution over n × (k + n) matrices where each entry of the rst n ×k matrix is i.i.d. Gaussian N (0, 1) and the next n ×n matrix is an identity matrix. For any xed r × n matrix S and a random matrixA ∼ A(k, n), with probability at least 1-O (k 1+ c 1 -c 2 -2 -Θ(k ) , there is no matrix B ∈ R n×r such that BS A -A 1 ≤ O (k 0.5-ε ) min rank -k A A -A 1 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>For anyk ≥ 1, any constant c ≥ 1, let n = O (k c ),and let A(k, n) denote the same distribution stated in Theorem 6.3. For matrix A ∼ A(k, n), with positive probability, there is no matrix B ∈ R n×(n+k ) in the row span of any r = n/2 rows of A for whichA -B 1 ≤ O (k 0.5-α ) min rank -k A A -A 1 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, where for an n×d matrix C, C 1,2 = n i=1 C i 2 , where C i is the i-th row of C. A sequence of work</figDesc><table><row><cell>a</cell><cell>a Frobenius norm low rank approximation gives nd approximation for 1 -low rank approximation. Alternatively, √</cell></row><row><cell cols="2">one can use algorithms for low rank approximation with respect</cell></row><row><cell cols="2">to the sum of distances, i.e., to nd a rank-k matrix A minimizing</cell></row><row><cell></cell><cell>A-A 1,2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>they are not oblivious, and (2) the number O (d log d ) of rows gives an O ( d log d ) approximation factor, which is much larger than what we want.There are a few oblivious distributions S which are useful for single-response 1 -regression min U</figDesc><table /><note><p><p><p><p><p>* -a 1 for column vectors , a ∈ R k</p><ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b91">91]</ref></p>. In particular, if S is an O (k log k ) ×n matrix of i.i.d. Cauchy random variables, then the solution to min SU * -Sa 1 is an O (k log k )-approximation to min U * -a 1</p><ref type="bibr" target="#b82">[83]</ref></p>. The important property of Cauchy random variables is that if X and Y are independent Cauchy random variables, then αX + βY is distributed as a Cauchy random variable times |α | + |β |, for any scalars α, β ∈ R. The O (k log k ) approximation arises because all possible regression solutions are in the column span of [U * , a] which is (k + 1)dimensional, and the sketch S gives an approximation factor of O (k log k ) to preserve every vector norm in this subspace. If we instead had a multi-response regression problem min SU * V * -SA 1 the dimension of the column span of [U * , A] would be d +k, and this approach would give an O (d log d )-approximation. Unlike Frobenius norm multi-response regression min SU * V * -SA F , which can be bounded if S is a subspace embedding for U * and satis es an approximate matrix product theorem</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 100 on at least 2k 3 coordinates. From the previous paragraph, it follows there are at least k 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Set s ← r ← t 1 ← O (k 5 ), t 2 ← O (k ).</figDesc><table><row><cell cols="5">Algorithm 1 Input Sparsity Time Algorithm, poly(k ) log d-</cell></row><row><cell cols="5">approximation Algorithm, O (k )-approximation Algorithm</cell></row><row><cell>1: procedure L1L R</cell><cell>A</cell><cell>I</cell><cell>S</cell><cell>(A, n, d, k)</cell></row><row><cell>Theorem 4.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ratio Algorithms,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>and a CUR Decomposition Algorithm</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>n, d, k, s).</figDesc><table><row><cell></cell><cell>Theorem 4.4</cell><cell></cell><cell></cell></row><row><cell>14:</cell><cell>return U , V .</cell><cell></cell><cell></cell></row><row><cell cols="2">15: end procedure</cell><cell></cell><cell></cell></row><row><cell cols="2">16: procedure L1L R</cell><cell>A</cell><cell>K(A, n, d, k)</cell><cell>Theorem 4.6</cell></row><row><cell>17:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This section presents two algorithms running in exp(k ) time. Both of these algorithms also use a technique called a polynomial system</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Alexandr Andoni, Saugata Basu, Cho-Jui Hsieh, Daniel Hsu, Chi Jin, Fu Li, Ankur Moitra, Cameron Musco, Richard Peng, Eric Price, Govind Ramnarayan, James Renegar, and Cli ord Stein for useful discussions. The authors also thank Jiyan Yang, Yinlam Chow, Christopher Ré, and Michael Mahoney for sharing their code.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>‡ Supported in part by the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C-0323.</p><p>§ Supported in part by Simons Foundation, and NSF CCF-1617955.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Computational complexity: a modern approach</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing a nonnegative matrix factorization -provably</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Symposium on Theory of Computing Conference (STOC)</title>
		<meeting>the 44th Symposium on Theory of Computing Conference (STOC)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05-19">2012. May 19 -22, 2012</date>
			<biblScope unit="page" from="145" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Principal component analysis for distributed data sets with updating</title>
		<author>
			<persName><forename type="first">Zheng-Jian</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franklin</forename><forename type="middle">T</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Parallel Processing Technologies</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="471" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved Distributed Principal Component Analysis</title>
		<author>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vandana</forename><surname>Kanchanapally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3113" to="3121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Communication E cient Distributed Kernel Principal Component Analysis</title>
		<author>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Woodru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Computing approximate PSD factorizations</title>
		<author>
			<persName><forename type="first">Amitabh</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Dinitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/1602.07351</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Basu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1534</idno>
		<title level="m">Algorithms in real algebraic geometry: a survey</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the Combinatorial and Algebraic Complexity of Quanti er Elimination</title>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Françoise</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1002" to="1045" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Algorithms in real algebraic geometry</title>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Francoise</forename><surname>Roy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 20033</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed principal component analysis for wireless sensor networks</title>
		<author>
			<persName><forename type="first">Yann-Ael</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Raybaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Bontempi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Toward a Uni ed Theory of Sparse Dimensionality Reduction in Euclidean Space</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bourgain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Dirksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC)</title>
		<meeting>the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC)<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-14">2015. June 14-17, 2015</date>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Near Optimal Column-Based Matrix Reconstruction</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malik</forename><surname>Magdon-Ismail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 52nd Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting><address><addrLine>Palm Springs, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10-22">2011. October 22-25, 2011</date>
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An improved approximation algorithm for the column subset selection problem</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal CUR matrix decompositions</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.7910</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the 46th Annual ACM Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal principal component analysis in distributed and streaming models</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06729</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 48th Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="236" to="249" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The 1 -norm best-t hyperplane problem</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">H</forename><surname>Dulá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="51" to="55" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A pure 1 -norm principal component analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Paul</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">H</forename><surname>Dulá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">L</forename><surname>Boone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational statistics &amp; data analysis</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PCAl1: An implementation in R of three methods for 1 -norm principal component analysis</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sapan</forename><surname>Jot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Online preprint</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust Principal Component Analysis with Side Information</title>
		<author>
			<persName><forename type="first">Kai-Yang</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2291" to="2299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Subgradient and sampling algorithms for 1 regression</title>
		<author>
			<persName><surname>Kenneth L Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms (SODA)</title>
		<meeting>the sixteenth annual ACM-SIAM symposium on Discrete algorithms (SODA)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The fast cauchy transform and faster robust linear regression</title>
		<author>
			<persName><forename type="first">Petros</forename><surname>Kenneth L Clarkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malik</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Magdon-Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="466" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Numerical linear algebra in the streaming model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the 41st Annual ACM Symposium on Theory of Computing (STOC)<address><addrLine>Bethesda, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-05-31">2009. May 31 -June 2, 2009</date>
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Low rank approximation and regression in input sparsity time</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Theory of Computing Conference (STOC)</title>
		<meeting><address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-01">2013. June 1-4, 2013</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Input sparsity and hardness for robust subspace approximation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="310" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sketching for M-estimators: A uni ed approach to robust regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="921" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nearly Tight Oblivious Subspace Embeddings by Trace Inequalities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</title>
		<meeting>the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)<address><addrLine>Arlington, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-01-10">2016. January 10-12, 2016</date>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Uniform sampling for matrix approximation</title>
		<author>
			<persName><forename type="first">Yin Tat</forename><surname>Michael B Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science (ITCS)</title>
		<meeting>the 2015 Conference on Innovations in Theoretical Computer Science (ITCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">p Row Sampling by Lewis Weights</title>
		<author>
			<persName><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC)</title>
		<meeting>the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matrix multiplication via arithmetic progressions</title>
		<author>
			<persName><forename type="first">Don</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the nineteenth annual ACM symposium on Theory of computing (STOC)</title>
		<meeting>the nineteenth annual ACM symposium on Theory of computing (STOC)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sampling algorithms and coresets for p regression</title>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boulos</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2060" to="2078" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">E cient volume sampling for row/column subset selection</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Rademacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sampling-based dimension reduction for subspace approximation</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasturi</forename><forename type="middle">R</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the 39th Annual ACM Symposium on Theory of Computing (STOC)<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-11">2007. June 11-13, 2007</date>
			<biblScope unit="page" from="641" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">R1-PCA: rotational invariant 1 -norm principal component analysis for robust subspace factorization</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Subspace Sampling and Relative-Error Matrix Approximation: Column-Based Methods</title>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APPROX-RANDOM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="316" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Subspace Sampling and Relative-Error Matrix Approximation: Column-Row-Based Methods</title>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="304" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relative-Error CUR Matrix Decompositions</title>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Analysis Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="844" to="881" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Low-rank matrix approximation in terms of entrywise 1 norm</title>
		<author>
			<persName><surname>Stack Exchange</surname></persName>
		</author>
		<ptr target="http://math.stackexchange.com/questions/539652/low-rank-matrix-approximation-in-terms-of-entry-wise-l-1-norm" />
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distributed column subset selection on MapReduce</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Ahmed K Farahat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A uni ed framework for approximating and clustering data</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Langberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the 43rd ACM Symposium on Theory of Computing (STOC)<address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">2011. June 2011</date>
			<biblScope unit="page" from="569" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coresets and Sketches for High Dimensional Subspace Approximation Problems</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Monemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="630" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Fotakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lampis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vangelis</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paschos</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04391</idno>
		<title level="m">Subexponential Approximation Schemes for CSPs: From Dense to Almost Sparse</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalized rank-constrained matrix approximations</title>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatoli</forename><surname>Torokhti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="656" to="659" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust 1 principal component analysis and its Bayesian variational inference</title>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="555" to="572" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Relative errors for deterministic low-rank matrix approximations</title>
		<author>
			<persName><forename type="first">Mina</forename><surname>Ghashami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="707" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">On the Complexity of Robust PCA and 1 -norm Low-Rank Matrix Approximation</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Vavasis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.09236</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Which problems have strongly exponential complexity?</title>
		<author>
			<persName><forename type="first">Russell</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramamohan</forename><surname>Paturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Zane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="653" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stable distributions, pseudorandom generators, embeddings, and data stream computation</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="307" to="323" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the minimum of a polynomial function on a basic closed semialgebraic set and applications</title>
		<author>
			<persName><forename type="first">Gabriela</forename><surname>Jeronimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Perrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Tsigaridas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="241" to="255" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Practical global optimization for multiview geometry</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Manmohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the exact space complexity of sketching and streaming small norms</title>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Daniel M Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1161" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spectral Algorithms. Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="157" to="288" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Principal Component Analysis and Higher Correlations for Distributed Data</title>
		<author>
			<persName><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 27th Conference on Learning Theory</title>
		<meeting>The 27th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1040" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Single pass spectral sparsi cation in dynamic streams</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kapralov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin Tat</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="561" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Robust subspace computation using 1 norm</title>
		<author>
			<persName><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-CS-03-172</idno>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Robust 1 norm factorization in the presence of outliers and missing data by alternative convex programming</title>
		<author>
			<persName><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spectral Sparsi cation in the Semi-Streaming Setting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Theoretical Aspects of Computer Science (STACS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">E cient-Norm-Based Low-Rank Matrix Approximations for Large-Scale Problems Using Alternating Recti ed Gradient Method</title>
		<author>
			<persName><forename type="first">Eunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhwai</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="237" to="251" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Principal component analysis based on 1 -norm maximization</title>
		<author>
			<persName><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1672" to="1680" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Iterative Row Sampling</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10-29">2013. 2013, 26-29 October, 2013</date>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Simple and deterministic matrix sketching</title>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Consensus-based distributed principal component analysis in wireless sensor networks</title>
		<author>
			<persName><forename type="first">Pavle</forename><surname>Sergio V Macua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Belanovic</surname></persName>
		</author>
		<author>
			<persName><surname>Zazo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Advances in Wireless Communications (SPAWC)</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Some Options for 1 -subspace Signal Processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Panos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">N</forename><surname>Markopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">A</forename><surname>Karystinos</surname></persName>
		</author>
		<author>
			<persName><surname>Pados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Symposium on Wireless Communication Systems</title>
		<meeting><address><addrLine>Ilmenau, TU Ilmenau, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-27">2013. August 27-30, 2013</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>ISWCS 2013</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Optimal Algorithms for 1 -subspace Signal Processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Panos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">N</forename><surname>Markopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">A</forename><surname>Karystinos</surname></persName>
		</author>
		<author>
			<persName><surname>Pados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="5046" to="5058" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">E cient 1 -Norm Principal-Component Analysis via Bit Flipping</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Markopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chamadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pados</surname></persName>
		</author>
		<idno>arXiv:cs.DS/1610.01959</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Open problems in data streams and related topics</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IITK Workshop on Algorithms For Data Streams</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A Cyclic Weighted Median Method for 1 Low-Rank Matrix Factorization with Missing Entries</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Arti cial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression</title>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-fth annual ACM symposium on Theory of computing (STOC)</title>
		<meeting>the forty-fth annual ACM symposium on Theory of computing (STOC)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An Almost Optimal Algorithm for Computing Nonnegative Rank</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1454" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Data Streams: Algorithms and Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="117" to="236" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings</title>
		<author>
			<persName><forename type="first">Jelani</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyên</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Non-convex robust PCA</title>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Un Niranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1107" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Optimal mean robust principal component analysis</title>
		<author>
			<persName><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Iteratively Reweighted Least Squares Algorithms for 1 -Norm Principal Component Analysis</title>
		<author>
			<persName><forename type="first">Young</forename><surname>Woong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Klabjan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02997</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Elemental: A new framework for distributed memory dense matrix computations</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Poulson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Marker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><forename type="middle">R</forename><surname>Van De Geijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichols</forename><forename type="middle">A</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Principal component analysis for dimension reduction in massive distributed data sets</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Ostrouchov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagiza</forename><surname>Samatova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Weighted low rank approximations with provable guarantees</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Ilya Razenshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Symposium on the Theory of Computing (STOC)</title>
		<meeting>the 48th Annual Symposium on the Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="250" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">On the Computational Complexity and Geometry of the First-Order Theory of the Reals, Part I: Introduction. Preliminaries. The Geometry of Semi-Algebraic Sets. The Decision Problem for the Existential Theory of the Reals</title>
		<author>
			<persName><forename type="first">James</forename><surname>Renegar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Symb. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="255" to="300" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">On the Computational Complexity and Geometry of the First-Order Theory of the Reals, Part II: The General Decision Problem. Preliminaries for Quanti er Elimination</title>
		<author>
			<persName><forename type="first">James</forename><surname>Renegar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Symb. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="301" to="328" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with earth mover&apos;s distance metric</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1873" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Improved Approximation Algorithms for Large Matrices via Random Projections</title>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Sarlós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting><address><addrLine>Berkeley, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-10">2006. October 2006</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">E cient Subspace Approximation Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nariankadu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasturi</forename><forename type="middle">R</forename><surname>Shyamalkumar</surname></persName>
		</author>
		<author>
			<persName><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="44" to="63" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Subspace embeddings for the 1 -norm with applications</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-third annual ACM symposium on Theory of computing (STOC)</title>
		<meeting>the forty-third annual ACM symposium on Theory of computing (STOC)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="755" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Low Rank Approximation with Entrywise 1 -Norm Error</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00898</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC). ACM</title>
		<meeting>the 49th Annual Symposium on the Theory of Computing (STOC). ACM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Gaussian elimination is not optimal</title>
		<author>
			<persName><surname>Volker Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="354" to="356" />
			<date type="published" when="1969">1969. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A parallel divide and conquer algorithm for the symmetric eigenvalue problem on distributed memory architectures</title>
		<author>
			<persName><forename type="first">Françoise</forename><surname>Tisseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scienti c Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2223" to="2236" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Van Trees</surname></persName>
		</author>
		<title level="m">Detection, estimation, and modulation theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>1 ed.</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Multiplying matrices faster than</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Vassilevska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Williams</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><surname>Coppersmith-Winograd</surname></persName>
		</author>
		<title level="m">Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC)</title>
		<meeting>the forty-fourth annual ACM symposium on Theory of computing (STOC)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="887" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Low rank approximation lower bounds in row-update streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1781" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Sketching as a Tool for Numerical Linear Algebra</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="157" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Subspace Embeddings and p -Regression Using Exponential Random Variables</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Woodru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 26th Annual Conference on Learning Theory (COLT)</title>
		<meeting><address><addrLine>Princeton University, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-12">2013. June 12-14</date>
			<biblScope unit="page" from="546" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Distributed Low Rank Approximation of Implicit Functions of a Matrix</title>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Woodru</surname></persName>
		</author>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="847" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yigang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<idno>NIPS. 2080-2088</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Probabilistic computations: Toward a uni ed measure of complexity</title>
		<author>
			<persName><forename type="first">Chi-Chin</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="222" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">An e cient algorithm for 1 -norm principal component analysis</title>
		<author>
			<persName><forename type="first">Linbin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1377" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Analysis of Robust PCA via Local Incoherence</title>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1819" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Practical low-rank matrix approximation under robust 1 -norm</title>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
