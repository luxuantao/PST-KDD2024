<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Struck: Structured Output Tracking with Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sam</forename><surname>Hare</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Saffari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
						</author>
						<title level="a" type="main">Struck: Structured Output Tracking with Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FF4082C46F68356F2CCD515ACB3E4EF</idno>
					<idno type="DOI">10.1109/TPAMI.2015.2509974</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2509974, IEEE Transactions on Pattern Analysis and Machine Intelligence This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2509974, IEEE Transactions on Pattern Analysis and Machine Intelligence This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2509974, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>tracking-by-detection</term>
					<term>structured output SVMs</term>
					<term>budget maintenance</term>
					<term>GPU-based tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we avoid the need for an intermediate classification step. Our method uses a kernelised structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow our tracker to run at high frame rates, we (a) introduce a budgeting mechanism that prevents the unbounded growth in the number of support vectors that would otherwise occur during tracking, and (b) show how to implement tracking on the GPU. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased tracking performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual object tracking is one of the core problems of computer vision, with a wide range of applications that include human-computer interaction, surveillance and augmented reality. It also forms an essential part of higher-level vision tasks such as scene understanding and action recognition.</p><p>In some scenarios, the object to be tracked is known in advance, and it is possible to incorporate prior knowledge when designing the tracker; in others, however, the target is arbitrary and only specified at runtime, in which case the tracker must be able to model the object's appearance on the fly and adapt the model during tracking to account for object motion, lighting conditions and occlusion. Even when the object is known, having a flexible tracker that can adapt its model at runtime is attractive and, in real-world scenarios, often essential for successful tracking.</p><p>The tracking-by-detection approach <ref type="bibr" target="#b0">[1]</ref>, which treats the tracking problem as one of repeated detection over time, has become particularly popular recently, partly due to significant recent progress in object detection (with many of the ideas being directly transferrable to tracking) and partly due to the development of methods that allow the classifiers used by these approaches to be trained online, providing a natural mechanism for adaptive tracking <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>.</p><p>• (*) S. Hare, S. Golodetz and A. Saffari assert joint first authorship. • S. Hare was with Oxford Brookes University, and is now a co-founder of Obvious Engineering. • S. Golodetz, S.L. Hicks and P.H.S. Torr are with Oxford University.</p><p>• A. Saffari was with Sony Europe Ltd., and is now with Affectv. • V. Vineet was with Oxford University and is now with Stanford.</p><p>• M.-M. Cheng was with Oxford University and is now with Nankai.</p><p>Traditional approaches to adaptive tracking-by-detection have tended to train a binary classifier online to distinguish the target object from the background. This classifier is used to estimate the object's location in each new input frame by searching for the maximum classification score in a local region around the previous frame's estimate, typically using a sliding-window approach. After estimating the new object location, traditional approaches have tended to generate a set of binary-labelled training samples with which to update the classifier. Such approaches thus separate the learning of the tracker into two distinct parts: (i) the generation and labelling of samples, and (ii) the classifier update.</p><p>Whilst common, this separation is problematic. Firstly, it is unclear how to generate and label the samples in a principled manner (the usual approaches rely on predefined rules such as labelling a sample positive/negative based on its distance from the estimated object location). Secondly, the learning process does not explicitly couple the classifier's goal (predicting binary sample labels) to the tracker's goal (estimating object location), and so the maximum classifier confidence may not correspond to the best location estimate (Williams et al. <ref type="bibr" target="#b4">[5]</ref> raised a similar point). State-of-theart adaptive tracking-by-detection methods have mainly focused on improving tracking performance by increasing the robustness of the classifier to poorly-labelled samples resulting from this approach, e.g. using robust loss functions <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, semi-supervised learning <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, or multipleinstance learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this paper, we take a different approach and frame the overall tracking problem as one of structured output prediction, in which the task is to directly predict the change in object configuration between frames. We present a novel given the current estimated object location, traditional approaches (shown on the right-hand side) generate a set of samples and, depending on the type of learner, produce training labels. Our approach (left-hand side) avoids these steps and operates directly on the tracking output. and principled adaptive tracking-by-detection framework called Struck that integrates learning and tracking, avoiding the need for ad-hoc update strategies (see Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Many recent tracking-by-detection approaches, e.g. <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> have been inspired by the successes of boosting-based approaches in object detection. Indeed, elements of such methods, such as the Haar features used in the very successful Viola-Jones face detection approach <ref type="bibr" target="#b10">[11]</ref>, have become almost standard in tracking-by-detection. However, a significant amount of successful research in object detection itself has tended to use SVMs rather than boosting, since these generalise well, are robust to label noise and can represent objects flexibly using kernels <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. The approach we present here thus makes use of SVMs rather than boosting. Since we are interested in structured output prediction, we make use of the structured output SVM framework of Tsochantaridis et al. <ref type="bibr" target="#b14">[15]</ref>. In particular, we extend LaRank, the online structured output SVM learning method proposed by Bordes et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and adapt it to the task of adaptive object tracking.</p><p>Whilst structured output SVMs have been applied in computer vision before, most notably for object detection by Blaschko and Lampert <ref type="bibr" target="#b11">[12]</ref>, our work differs from such approaches in having no offline labelled data available for training (except the object's location in the first frame) and relying instead on online learning. This makes a significant difference, because online learning with kernels suffers from the so-called curse of kernelisation, whereby the number of support vectors in the SVM grows unboundedly as more and more training data is supplied. Since evaluation of a kernelised SVM is linear in the number of support vectors in its representation, this growth inevitably inhibits real-time evaluation of the SVM if steps are not taken to control the number of support vectors maintained. We therefore introduce a novel way of limiting the support vectors we maintain that builds on recent proposals for the online learning of classification SVMs on a fixed budget <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr">[19]</ref>. This approach allows us to achieve large gains in computational efficiency without significantly degrading the performance of our tracker.</p><p>An earlier version of this paper appeared in <ref type="bibr" target="#b19">[20]</ref>. We extend it here in the following ways:</p><p>1) We extend Struck to deal with scale ( §4.7.1).</p><p>2) We show how Struck can be implemented on the GPU so as to achieve high frame-rates ( §5 and supplementary material). 3) We evaluate Struck extensively on the recent benchmark of Wu et al. <ref type="bibr" target="#b20">[21]</ref> ( §6), performing experiments that quantify how tracking performance is affected by (i) multi-kernel learning, (ii) structured learning, (iii) parameter changes, and (iv) scale. In particular, we show that one of our multi-kernel variants of Struck can in many cases achieve comparable or even superior tracking performance to the state-of-the-art KCF tracker of Henriques et al. <ref type="bibr" target="#b21">[22]</ref>. 4) We include detailed derivations of our SVM formulation to make it easier for others to build on our approach (supplementary material). This paper is organised as follows: in §2, we briefly review related work; in §3, we provide an overview of traditional approaches to tracking-by-detection; in §4, we describe the Struck tracker; in §5, we describe our GPU implementation of Struck; in §6, we evaluate numerous variants of our CPU and GPU implementations of Struck on the Wu et al. <ref type="bibr" target="#b20">[21]</ref> benchmark and compare them to state-of-the-art trackers, and in §7 we conclude.</p><p>Code for our CPU implementation of Struck can be found at http://www.samhare.net/research. Our GPU implementation can be found at https://bitbucket.org/sgolodetz/ thunderstruck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Due to the importance of the tracking problem, a wide variety of different approaches have been proposed to solve it over the years. Whilst a comprehensive review of tracking techniques is beyond the scope of this paper, we direct the reader to <ref type="bibr" target="#b22">[23]</ref> for a survey, and also to <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> for some benchmarks that compare a significant number of trackers on large datasets. We focus here on a representative selection of recent trackers.</p><p>Dictionary-based trackers maintain dictionaries of object templates and aim to represent candidate object regions in a new frame using combinations of these templates. A popular idea is to try and represent the candidates sparsely using 1 -norm minimization <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>. Prediction from one frame to the next is often done using particle filtering <ref type="bibr" target="#b28">[29]</ref>, with a sensor model that assigns higher confidence to candidates that are more easily represented by the templates. For example, Xing et al. <ref type="bibr" target="#b27">[28]</ref> describe an approach that combines short-term, medium-term and longterm dictionaries to achieve a compromise between adaptivity (the short-term dictionary will adapt more quickly to new data) and robustness (the long-term dictionary will 0162-8828 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.  remember more of what the object originally looked like). Wang et al. <ref type="bibr" target="#b26">[27]</ref> describe another interesting dictionarybased approach that aims to learn templates that capture distinct aspects of the object.</p><p>Ensemble-based trackers combine the results of a set of individual 'weak' classifiers to form a strong classifier that can be used to predict an object's bounding box in a new frame. For example, Cao and Xue <ref type="bibr" target="#b29">[30]</ref> describe an adaptive random forest method that maintains a collection of candidate decision trees and picks half of them each frame to form an ensemble. (Other techniques that incorporate random forests can be found in <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>.) Bai et al. <ref type="bibr" target="#b32">[33]</ref> combine weak classifiers trained on 8×8 patches within the object's bounding box using weight vectors sampled from a Dirichlet distribution that is updated over time. Wang et al. <ref type="bibr" target="#b33">[34]</ref> show how to combine an ensemble of different trackers (including Struck) using a conditional particle filter approach to try and meld the best features of the trackers.</p><p>Segmentation-based trackers <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref> actually segment (possibly coarsely) the object being tracked in each frame, so as to try and avoid the problem of drift that occurs when trackers inadvertently incorporate parts of the background in their object representation. For example, Duffner and Garcia <ref type="bibr" target="#b34">[35]</ref> describe PixelTrack, an approach that co-trains a probabilistic segmentation model alongside a pixel-based Hough model so as to better handle non-rigid deformations of the tracked object between frames.</p><p>Circulant trackers are an interesting recent type of tracker that exploit the circulant structure of adjacent subwindows in an image to achieve extremely fast tracking. The original such tracker, CSK <ref type="bibr" target="#b37">[38]</ref>, works by evaluating a classifier trained using kernel regularised least squares (KRLS) quickly at all sub-windows around the estimated target location and maximising the response. Danelljan et al. <ref type="bibr" target="#b38">[39]</ref> build on this by introducing colour attributes to achieve superior performance on colour sequences. The current state-of-the-art approach is KCF <ref type="bibr" target="#b21">[22]</ref>, a kernelised correlation filter tracker that achieves its best results using a Gaussian kernel and histogram-of-oriented-gradients (HOG) features, and runs at 172 FPS.</p><p>Since <ref type="bibr" target="#b19">[20]</ref>, various approaches have used structured learning in a tracking context. For example, Yao et al. <ref type="bibr" target="#b39">[40]</ref> describe a part-based tracker based on online latent structured SVMs, which they solve in the primal by extending the online Pegasos solver to perform structured prediction. Shen et al. <ref type="bibr" target="#b40">[41]</ref> describe a generic boosting framework for combining weak structured learners, and show how it can be used to develop a visual tracker that achieves competitive results. Separately, structured SVMs have proved useful for solving data association problems in the context of multitarget tracking <ref type="bibr" target="#b41">[42]</ref>.</p><p>A number of trackers we survey do not fall into any of the above categories. For example, Pernici and Del Bimbo <ref type="bibr" target="#b42">[43]</ref> describe a tracker called ALIEN based on Nearest Neighbour classifiers that tracks using an oriented rather than axis-aligned bounding box, handles occlusions well and is designed for long-term tracking. Lu et al. <ref type="bibr" target="#b43">[44]</ref> describe an interesting approach based on And-Or graphs that achieves good tracking performance at the cost of some speed. Finally, Zhang and van der Maaten <ref type="bibr" target="#b44">[45]</ref> describe an appealing multi-object tracker based on structured SVMs that can be co-opted for single-object, part-based tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRACKING-BY-DETECTION</head><p>In this section, we provide an overview of traditional adaptive tracking-by-detection trackers, which attempt to learn a binary classifier to distinguish the target object from the background.</p><p>The typical structure of such a tracker is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Given a sequence of images (I 1 , I 2 , ...), in which I t is the image at time t ∈ T , the tracker produces a sequence (p 1 , p 2 , ...) of parameter vectors such that each p t ∈ P represents the estimated configuration of the target object in the corresponding image I t . In the simplest case, a parameter vector might contain only the position of a 2D bounding box of fixed size, but additional components can be added, e.g. to account for scale, rotation or shape. The initial parameter vector p 1 is usually given, in which case the tracker's goal is to estimate p t+1 from p t . To help with this, it maintains a classifier with a scoring function g : X → R that can be used to score feature vectors in a feature space X based on how well they correspond to the target object. The classifier is trained with binarylabelled examples of the form (x p t , ), in which x p t denotes the vector of features extracted from the patch of image I t denoted by p ∈ P, and label ∈ {+1, -1} specifies whether the example is a positive or negative one. The predicted label f (x) for a feature vector x can be computed as sign(g(x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning/Adaptation</head><p>Given an estimated object configuration p t in image I t , a traditional tracker will use a three-stage process to generate a set of training examples from I t and update the classifier. First, a sampler will generate a set of n different transformations {ȳ i } in a search space Y of P → P transformations around p t , and calculate the corresponding feature vectors {x ȳi(pt) t }. Then, a labeller will choose labels { i } for the samples to produce a set of labelled training examples {(x ȳi(pt) t , i )}. Finally, these examples will be used to update the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Propagation</head><p>In order to propagate the estimated object configuration from one image to the next, it is assumed that the estimate p t+1 for the object's configuration in image I t+1 can be determined by maximising g in a local region around p t . The tracker will generally iterate over the search space of transformations<ref type="foot" target="#foot_0">1</ref> Y, and pick the transformation y t ∈ Y that maximises the response of the classifier:</p><formula xml:id="formula_0">y t = argmax ȳ∈Y g x ȳ(pt) t+1 (1) It will then set p t+1 = y t (p t ).</formula><p>The form of Y depends on the type of motion being tracked. For most existing tracking-by-detection approaches, this is 2D translation, in which case one option is for it to be of the form {p → p + ∆p : ∆p &lt; r}, where r is a search radius. However, we will see in §4 that this approach can be straightforwardly extended to deal with other types of motion, e.g. scale can be incorporated by using parameter vectors of the form (x, y, s), where s ∈ R + denotes a factor that can be used to scale the bounding box around the object, and choosing Y appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Issues</head><p>This approach to tracking raises a number of issues. Firstly, the assumption made in (1) that maximising g provides an accurate estimate of object configuration is not explicitly incorporated into the learning algorithm, since the classifier is trained only with binary examples and has no information about transformations. Secondly, all the training examples are equally weighted, meaning that a negative example that overlaps significantly with the current object configuration is treated the same as one that overlaps very little. One implication of this is that slight inaccuracy during tracking can lead to poor labelling of examples, which is likely to reduce the accuracy of the classifier and lead to further tracking inaccuracy. Thirdly, the labeller is usually designed using heuristics, rather than being tightlycoupled to the classifier. Mistakes made by the labeller manifest themselves as label noise, and many state-ofthe-art approaches try to mitigate this problem by using robust loss functions <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, semi-supervised learning <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, or multiple-instance learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We argue that all of these techniques, though justified in increasing the robustness of the classifier to label noise, are not addressing the real problem, which stems from separating the labeller from the learner. Our algorithm does not depend on a labeller and tries to overcome all of these problems within a coherent framework by directly linking learning to tracking and avoiding an artificial binarisation step. Sample selection is fully controlled by the learner itself, and relationships between samples such as their relative similarity are taken into account during learning.</p><p>To conclude this section, we describe how a conventional labeller works, as this provides further insight into our algorithm. Traditional labellers use a transformation similarity function to determine the label of a sample. Such a function has the form s p : Y × Y → R and can be used to assign a similarity score to the samples denoted by a pair of transformations expressed relative to a reference configuration p. For example, the function defined by</p><formula xml:id="formula_1">s o p (y i , y j ) = y i (p) ∩ y j (p) y i (p) ∪ y j (p)<label>(2)</label></formula><p>measures the degree of overlap between the two bounding boxes y i (p) and y j (p). For 2D translation, an alternative function could be defined based on the difference between the translations. Let y 0 denote the identity (or null) transformation, i.e. y 0 (p) = p. Given a transformation similarity function s p , the labeller determines the label of the sample generated by transformation y by applying a labelling function = L(s p (y 0 , y)). Most commonly, this can be expressed as</p><formula xml:id="formula_2">L(s p (y 0 , y)) =    +1 if s p (y 0 , y) ≥ θ u -1 if s p (y 0 , y) &lt; θ l 0 otherwise,<label>(3)</label></formula><p>in which θ u and θ l are upper and lower thresholds, respectively. Binary classifiers generally ignore the unlabelled examples (those with a label of 0) <ref type="bibr" target="#b1">[2]</ref>, whilst classifiers based on semi-supervised learning use them in their update phase <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. In approaches based on multiple-instance learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>, the labeller groups all of the positive examples into a bag and assigns a positive label to the bag instead. Most, if not all, variants of adaptive tracking-bydetection algorithms use a labeller which can be expressed in this fashion. However, it is not clear how the labelling parameters (e.g. the thresholds θ u and θ l ) should be estimated in an online learning framework. Additionally, such heuristic approaches are often prone to noise and it is not clear why such a function is in fact suitable for tracking.</p><p>In the next section, we derived our structured output algorithm that fundamentally addresses these issues and can be thought of as generalising these heuristic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE STRUCK TRACKER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Our Struck tracker broadly follows the general structure of a tracking-by-detection approach outlined in the previous section, but with a couple of key differences.</p><p>The first difference is that instead of learning a binary classifier, we learn a prediction function f : T → Y that directly estimates the object transformation between consecutive images. That is, f (t) is the required transformation from p t to p t+1 . We learn f in a structured output SVM framework <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and make use of a scoring function g : T × Y → R that can be used for prediction via</p><formula xml:id="formula_3">f (t) = argmax y∈Y g(t, y).<label>(4)</label></formula><p>The output space of f is now the search space Y, rather than {+1, -1}. Note the similarity between ( <ref type="formula" target="#formula_3">4</ref>) and ( <ref type="formula" target="#formula_21">1</ref>): we are still performing an argmax in order to predict the object transformation, but the scoring function g now has direct access to the transformation y, allowing it to be incorporated into the learning algorithm.</p><p>The second difference is that we introduce a budgeting mechanism at the end of the adaptation stage to limit the number of support vectors we maintain. As we will see shortly, our approach uses a kernelised SVM, which must explicitly maintain a set of support vectors (as opposed to a linear SVM, for which it is sufficient to maintain only a weight vector). Since kernelised SVM evaluation is linear in the size of the support vector set, it is crucial that we limit this size in order to achieve efficient evaluation and make our tracker run in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Primal SVM Formulation</head><p>To formulate our SVM, we first restrict g to be a linear function g(t, y) = w, Φ(t, y) , in which w is the SVM's weight vector and Φ is a function (known as the joint kernel map) that is used to generalise the SVM to non-linear kernels in a way that will be described later. When using a linear kernel, Φ(t, y) = x y(pt) t+1 , and g(t, y) = w, x y(pt) t+1 . We can learn g in a large-margin framework from a set of examples {(t 1 , y 1 ), ..., (t n , y n )} by solving the quadratic program</p><formula xml:id="formula_4">min w 1 2 w 2 + C n i=1 ξ i s.t. ∀i : ξ i ≥ 0 ∀i, ∀y = y i : w, δΦ i (y) ≥ ∆(y i , y) -ξ i ,<label>(5)</label></formula><p>where δΦ i (y) = Φ(t i , y i ) -Φ(t i , y) and we set C = 100.</p><p>Intuitively, an example (t i , y i ) specifies that the 'correct' transformation of the object from p ti to p ti+1 is y i . The optimisation aims to ensure that the value of g(t i , y i ) for the i th training example is greater than g(t i , y) for y = y i , by a margin that depends on a (symmetric) loss function ∆. This loss funcion should satisfy ∆(y, ȳ) = 0 iff y = ȳ, and increase as y and ȳ become more dissimilar. The loss function plays an important role in our approach, as it allows us to address the issue raised previously of all samples being treated equally. This can be achieved by making use of the transformation similarity function introduced in §3.3. For example, as suggested by Blaschko and Lampert <ref type="bibr" target="#b11">[12]</ref>, we can base our loss function on bounding box overlap according to</p><formula xml:id="formula_5">∆(y, ȳ) = 1 -s o p (y, ȳ),<label>(6)</label></formula><p>in which s o p is the overlap function ( <ref type="formula" target="#formula_1">2</ref>). The attentive reader will observe that the formulation above involves a large (indeed, potentially infinite) number of constraints, and it is by no means clear at this stage how the SVM can be solved in a computationally-feasible way. In practice, as we will see in §4.5, we address this issue by making use of the LaRank solver of Bordes et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which optimises the SVM using a sequence of minimal steps rather than trying to solve it monolithically. It also bears mentioning that at an implementation level, we do not actually consider an infinite set of constraints: in practice, our search space Y of possible transformations is finite, and we only retain constraints relating to images in the tracking sequence that are providing one or more current support vectors to the SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dual SVM Formulation</head><p>Using standard Lagrangian duality techniques <ref type="bibr" target="#b45">[46]</ref>, (5) can be converted into its equivalent dual form</p><formula xml:id="formula_6">max α i,y =y i ∆(y, yi)α y i - 1 2 i,y =y i j,ȳ =y j α y i α ȳ j δΦi(y), δΦj(ȳ) s.t.</formula><p>∀i, ∀y = yi : α y i ≥ 0 ∀i :</p><formula xml:id="formula_7">y =y i α y i ≤ C</formula><p>(7) and the scoring function expressed as</p><formula xml:id="formula_8">g(t, y) = i,ȳ =yi α ȳ i δΦ i (ȳ), Φ(t, y) .<label>(8)</label></formula><p>A derivation for this can be found in the supplementary material. As in the case of classification SVMs, a benefit of this dual representation is that because the joint kernel map Φ only ever occurs inside scalar products, it can be defined implicitly in terms of an appropriate joint kernel function k(t, y, t, ȳ) = Φ(t, y), Φ( t, ȳ) . (This is the so-called kernel trick, which allows us to define non-linear kernels k for which we do not have an explicit representation for Φ.)</p><p>The kernel functions we use during tracking are discussed in Section 4.7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Reparameterising the Dual SVM</head><p>By reparameterising (7) <ref type="bibr" target="#b15">[16]</ref> according to</p><formula xml:id="formula_9">β y i =    ȳ =yi α ȳ i if y = y i -α y i otherwise,<label>(9)</label></formula><p>the dual can be considerably simplified to</p><formula xml:id="formula_10">max β - i,y ∆(y, y i )β y i - 1 2 i,y,j,ȳ β y i β ȳ j k(t i , y, t j , ȳ) s.t.</formula><p>∀i, ∀y :</p><formula xml:id="formula_11">β y i ≤ δ(y, y i )C ∀i : y β y i = 0,<label>(10)</label></formula><p>where δ(y, ȳ) = 1 if y = ȳ and 0 otherwise. (A detailed derivation can be found in the supplementary material.) This also simplifies the scoring function to</p><formula xml:id="formula_12">g(t, y) = i,ȳ β ȳ i k(t i , ȳ, t, y).<label>(11)</label></formula><p>In this form, we refer to those pairs (t i , ȳ) for which β ȳ i = 0 as support vectors and those t i included in at least one support vector as support patterns (note that a support pattern corresponds to one of the images in the tracking sequence, and support vectors correspond to samples taken from those images at particular locations). Note that for a given support pattern t i , only the support vector (t i , y i ) will have β yi i &gt; 0, while any other support vectors (t i , ȳ), ȳ = y i , will have β ȳ i &lt; 0. We refer to these as positive and negative support vectors respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Learning/Adaptation</head><p>To update the SVM, we perform online optimisation of (10) using the LaRank approach of Bordes et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. LaRank is an SVM solver based on the sequential minimal optimisation (SMO) approach introduced by Platt <ref type="bibr" target="#b46">[47]</ref>. The key idea is that a large quadratic program can be decomposed into a series of small sub-programs (each involving only two Lagrange multipliers) that can be solved analytically.</p><p>LaRank adopts such an approach, allowing us to solve our SVM by performing a sequence of SMO steps (see Algorithm 1), each of which monotonically improves <ref type="bibr" target="#b9">(10)</ref> with respect to a pair of coefficients β y+ m and β ym . Due to the constraint y β y m = 0, these coefficients must be modified by opposite amounts,</p><formula xml:id="formula_13">β y+ m ← β y+ m + λ, β y- m ← β y- m -λ.</formula><p>The value of λ is chosen so as to maximally increase the value of <ref type="bibr" target="#b9">(10)</ref>: see the supplementary material for a derivation.</p><p>The key to LaRank is in how it chooses the coefficients to be optimised by each SMO step. For a given m, y + and y -are chosen to define the feasible search direction with the highest gradient, where the gradient of <ref type="bibr" target="#b9">(10)</ref> with respect to a single coefficient β y i is given by </p><formula xml:id="formula_14">∇ y i = -∆(y, y i ) - j,ȳ β ȳ j Φ(t i , y), Φ(t j , ȳ) = -∆(y, y i ) -g(t i , y). (<label>12</label></formula><formula xml:id="formula_15">) Algorithm 1 SMOSTEP Require: m, y + , y -, S, β, ∇, C 1: k (++) = k(t m , y + , t m , y + ) 2: k (--) = k(t m , y -, t m , y -) 3: k (+-) = k(t m , y + , t m , y -) 4: λ u = ∇ y + m -∇ y - m k (++) +k (--) -2k (+-)<label>5</label></formula><formula xml:id="formula_16">k (+) = k(t i , y, t m , y + ) 12: k (-) = k(t i , y, t m , y -) 13: ∇ y i ← ∇ y i + λ k (-) -k (+) 14: end for</formula><p>Three different update steps are considered, which map very naturally onto a tracking framework:</p><formula xml:id="formula_17">• PROCESSNEW Processes a new example (t m , y m ).</formula><p>Since all the β y m s are initially 0, and only β ym m may be &gt; 0, y + = y m . We choose y -= argmin y∈Y ∇ y m . During tracking, this corresponds to adding the true label y m as a positive support vector and searching for the most important sample to become a negative support vector according to the current state of the learner, taking into account the loss function. Note, however, that this step does not necessarily add new support vectors, since the SMO step may not need to adjust the β y m s away from 0. • PROCESSOLD Processes a random existing support pattern t m . We choose y + = argmax y∈Y ∇ y m , but since a feasible search direction requires β y+ m &lt; δ(y + , y m )C (since we need to able to increase β y+ m without violating the relevant constraint), this maximisation will only involve existing support vectors (since if y + = y m then β y+ m &lt; 0, and if y + = y m then β y+ m must be &gt; 0 because t m is a support pattern). As for PROCESSNEW, y -= argmin y∈Y ∇ y m . During tracking, this corresponds to revisiting a frame for which we have retained some support vectors and potentially adding another sample as a negative support vector, as well as adjusting the associated coefficients. Again, this new sample is chosen to take into account the current learner state and loss function.</p><p>• OPTIMIZE Processes a random existing support pattern t m , but only modifies coefficients of existing support vectors. We choose y + as for PROCESSOLD, and set y -= argmin y∈Ym ∇ y m , where Y m = {y ∈ Y | β y m = 0}. PROCESSNEW and PROCESSOLD steps are both able to add new support vectors, which gives the learner the ability to perform sample selection during tracking and discover important negative samples. This selection involves searching over Y to minimise ∇ y m , which may be a relatively expensive operation. In practice, we found that for the 2D translation case, it was sufficient to sample from Y on a polar grid, rather than considering every pixel offset. The OPTIMIZE case only considers existing support vectors, so is a much less expensive operation.</p><p>As suggested by Bordes et al. <ref type="bibr" target="#b16">[17]</ref>, we schedule these update steps as follows. A REPROCESS step is defined as a single PROCESSOLD step followed by n O OPTIMIZE steps. Given a new training example (t i , y i ), we call a single PROCESSNEW step followed by n R REPROCESS steps. In practice, we typically set n O = n R = 10, as recommended in <ref type="bibr" target="#b16">[17]</ref>, but the effects of changing them can be seen in the experiments we perform in the supplementary material.</p><p>During tracking, we maintain a set of support vectors S ⊂ T × Y. For each (t i , y) ∈ S, we store the coefficient β y i and gradient ∇ y i , which are both incrementally updated during an SMO step (see the supplementary material for the derivation of the gradient update). If the SMO step results in a β y i becoming 0, the corresponding support vector is removed from S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Incorporating a Budget</head><p>The kernelised SVM we have described thus far suffers from the so-called curse of kernelisation (the number of support vectors it maintains can grow unboundedly over time). This is problematic for two reasons. Firstly, storage costs increase linearly with the number of support vectors. Secondly, evaluating the SVM is linear in the size of the support vector set: indeed, we can see from <ref type="bibr" target="#b10">(11)</ref> that evaluating g(t, y) involves evaluating scalar products (or kernel functions) between (t, y) and each support vector. Moreover, since <ref type="bibr" target="#b11">(12)</ref> involves evaluating g, both the PRO-CESSNEW and PROCESSOLD update steps will become more expensive as the number of support vectors increases. This issue is particularly important in the case of tracking, as in principle we could be presented with an infinite number of training examples. It is thus crucial to limit the number of support vectors maintained in order to achieve efficient tracking using a reasonable amount of memory.</p><p>Recently, a number of approaches have been proposed for online learning of classification SVMs on a fixed budget <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr">[19]</ref>, meaning the number of support vectors cannot exceed a specified limit. If the limit has already been reached and a new support vector needs to be added, these approaches identify a suitable support vector to remove and adjust the coefficients of the remaining support vectors as necessary to compensate for the removal.</p><p>We build on these approaches to devise a budgeting strategy for our scenario. Like Wang et al.</p><p>[19], we choose to remove the support vector that results in the smallest change to the weight vector w, as measured by ∆w 2 . However, as with the SMO step used during optimisation, we must also ensure that the y β y i = 0 constraints remain satisfied: this means that when we remove a support vector, we must modify the coefficient of one of the other support vectors for the same support pattern in order to compensate for the change. In practice, there is only one positive support vector for each support pattern, so we restrict our choice of which support vector to remove to negative support vectors, and modify the coefficient of the end for 13: end for 14: return p t+1 , S t+1 positive support vector for that pattern to compensate for the removal. A special case occurs when there are only two support vectors being maintained for a pattern (i.e. the positive one and a single negative one): in that case, after removing the negative support vector, we also remove the positive support vector (whose coefficient is now 0) and the corresponding pattern. In general, removing the negative support vector (t r , y) results in the following change to w:</p><formula xml:id="formula_18">∆w = -β y r Φ(t r , y) + β y r Φ(t r , y r ).<label>(13)</label></formula><p>However, since we only have an explicit expression for Φ if we are using a linear kernel, we are not in general able to calculate ∆w explicitly. Instead, we use ∆w 2 , which can be calculated in terms of the joint kernel map k:</p><formula xml:id="formula_19">∆w 2 = (β y r ) 2 {k(t r , y, t r , y) + k(t r , y r , t r , y r ) -2k(t r , y, t r , y r )}<label>(14)</label></formula><p>Each time the budget is exceeded, we remove the negative support vector resulting in the minimum ∆w 2 . We show in the experimental section that this does not impact significantly on tracking performance, even with modest budget sizes. We note in passing that because we do not directly consider positive support vectors for removal during budgeting, the only way in which a negative support vector can be removed is if it causes the minimum change in w. Thus, the y β y i = 0 constraints do not cause us to discard potentially useful information about the scene background. However, they can force us to maintain less useful positive support vectors to balance the useful negatives, which can in theory take up valuable space in our support vector budget. We did not observe this to be a problem in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Practical Considerations</head><p>The overall tracking loop for Struck is shown in Algorithm 2. Having learnt/adapted the SVM as just discussed, we can use it to propagate the estimated object configuration from one image in the tracking sequence to the next as described in §4.1. However, the formulation discussed thus far has been entirely general, and we have not yet committed ourselves to specific choices for our parameter vectors, search spaces, kernel functions and image features. At an implementation level, these are obviously crucial, so we discuss them now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Parameter Vectors and Search Spaces</head><p>We consider two different types of parameter vector, one designed to handle only 2D translation, and the other designed to handle both 2D translation and scale. For 2D translation, we use parameter vectors of the form (x, y) ∈ N 2 , denoting the top-left corner of a fixed-size, rectangular bounding box around the target object (the size is obtained from the initial bounding box in the first image). For our search space, we consider all integer-valued offsets within a finite search radius r (e.g. r = 30):</p><formula xml:id="formula_20">Y = {p → p + ∆p : ∆p &lt; r}<label>(15)</label></formula><p>For 2D translation and scale, we use parameter vectors of the form (x, y, s) ∈ N 2 × R + , in which the scale factor s denotes the amount by which to scale the initial fixed-size bounding box relative to its centre. For our search space, we consider the same offsets as before, but also relative scaling factors that scale from one image to the next. For example, we might choose possible scaling factors of {0.95, 0.96, ..., 1.05}, allowing the bounding box to become at most 5% smaller or larger between consecutive frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Kernel Functions</head><p>We consider three different types of kernel:</p><p>• The linear kernel sets Φ(t, y) = x y(pt) t+1 , whence k(t, y, t, ȳ) = x y(pt) t+1 , x ȳ(pt) t+1 .</p><p>• The Gaussian kernel defines k(t, y, t, ȳ) to be exp(-σ x</p><formula xml:id="formula_21">y(pt) t+1 -x ȳ(pt)<label>t+1</label></formula><p>2 ), where σ is the standard deviation of the Gaussian. (We used σ = 0.2 in our experiments.) • The intersection kernel defines k(t, y, t, ȳ) to be</p><formula xml:id="formula_22">1 D D j=1 min(x y(pt) t+1 [j], x ȳ(pt) t+1 [j]</formula><p>), where D is the feature vector size. Of these, the linear kernel is especially computationally attractive because it uses an explicit representation for Φ, whence the SVM can be evaluated efficiently on a set of feature vectors by first calculating its weight vector w, and then computing a dot product of w with each of the feature vectors. Specifically, we can derive:</p><formula xml:id="formula_23">g(t, y) = i,ȳ β ȳ i x ȳ(pt i ) ti+1 , x y(pt) t+1 = i,ȳ β ȳ i j x ȳ(pt i ) ti+1 [j] × x y(pt) t+1 [j] = j x y(pt) t+1 [j]   i,ȳ β ȳ i x ȳ(pt i ) ti+1   w [j] = w, x y(pt) t+1<label>(16)</label></formula><p>However, a disadvantage of this kernel is that it can cope poorly with data that is not linearly separable in feature space, so it is not always a good choice when trying to learn a classifier that incorporates different views of the same object. To address this problem, non-linear kernels such as the Gaussian or intersection kernels map feature vectors to a high-dimensional space in which linear separability may be easier to achieve. The downside is that we no longer have an explicit representation for Φ and must evaluate the SVM in terms of its support vectors <ref type="bibr" target="#b10">(11)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.3">Image Features</head><p>We consider three different types of image feature:</p><p>• Raw features, obtained by scaling an image patch to 16×16 pixels, normalising the greyscale value of each pixel into the range [0, 1] and converting the result into a 256D feature vector by reading the normalised values in raster order.  L=1 L 2 = 480D feature vector. Some kernel/feature combinations tend to work better than others. In particular, our raw features worked best with a linear kernel, our Haar features worked best with a Gaussian kernel and our histogram features worked best with an intersection kernel. Moreover, the number of features used mediates a trade-off between tracking performance and speed: in general, it is possible to achieve better tracking performance using larger feature vectors in exchange for a decrease in the speed of the tracker. In our experiments, we found that our Gaussian/Haar combination achieved a reasonable compromise between tracking performance and speed; however, we obtained better performance using the multi-kernel approach described in the following section (which uses larger feature vectors), and better speed by combining a linear kernel (which is a natural fit to the GPU architecture) with raw features (which are cheap to compute). To quantify the difference that larger feature vectors can make, our supplementary material contains an additional experiment in which we investigate the effects of using more Haar features than described here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.4">Multiple-Kernel Learning (MKL)</head><p>In addition to experimenting with individual kernels, we investigated combining kernels to implement a basic form of multiple-kernel learning (MKL). The idea is to average the results of a number of kernels k (i) on potentially different sets of features: </p><formula xml:id="formula_24">k(t, y, t, ȳ) = 1 N k N k i=1 k (i) (t,</formula><formula xml:id="formula_25">+1 -1 +1 -1 +1 -2 +1 +1 -2 +1 +1 -1 -1 +1 -4 +1<label>9</label></formula><p>Fig. <ref type="figure">3</ref>: The different types of Haar feature used by Struck. The numbers in the boxes are the (unnormalised) weights used when calculating the features. Note that no feature requires more than four boxes, which makes for efficient evaluation on the GPU (see §5.4).</p><p>It has been shown <ref type="bibr" target="#b47">[48]</ref> that in terms of performance, full MKL (in which the relative weighting of the different kernels is learned from training data) does not greatly improve upon this simple approach.</p><p>We found experimentally (see §6.3) that MKL was able to yield substantial improvements in tracking performance, enough for our mklHGHI tracking variant (which combines a Gaussian kernel with an intersection one on Haar and histogram features) to outperform the state-of-the-art KCF tracker <ref type="bibr" target="#b21">[22]</ref> on the Wu et al. <ref type="bibr" target="#b20">[21]</ref> benchmark. However, since the larger feature vectors involved have a significant impact on the speed of our tracker, simpler variants may be preferable for tasks for which a reasonable compromise between performance and speed must be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THUNDERSTRUCK</head><p>To investigate the speed potential of an optimised implementation of Struck, we implemented a CUDA version of it called ThunderStruck. In this section, we briefly describe some of the key features of this implementation (additional details can be found in the supplementary material). Its speed is compared to the original CPU version in §6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SVM Representation</head><p>A Struck SVM maintains two separate sets of data: the features computed for patches associated with the support patterns (some of the previously-seen frames), and a record of which patches are in the current set of support vectors, together with their corresponding β coefficients and gradient values. Since GPU code is easier to optimise when using fixed-size arrays and it is possible to use a finite budget of support vectors without degrading tracking performance, we chose to use a fixed-size representation for our SVM data in ThunderStruck (see Figure <ref type="figure" target="#fig_3">4</ref>). We use a single large GPU-based array to store all of the features for every patch within every support pattern. We use a smaller array of indices to specify the current support vectors: each element of this array either refers to a patch or is -1 to indicate the absence of a support vector. The corresponding β coefficients and gradient values are stored in the similarly-sized arrays betas and gradients, such that  the β value for support vector k is stored in betas[k] and the gradient value is stored in gradients <ref type="bibr">[k]</ref>. Since access to the three support vector arrays is required on both the CPU and GPU, we store mirrored copies of them in both places to minimise costly memory transfers over the CPU-GPU bus (note that the storage cost involved is low due to the small size of the arrays). Addition and removal of support vectors can be implemented via a simple ID allocator that maintains a set of used IDs and a set of free ones. The IDs index into the svIndices, betas and gradients arrays. To add a support vector, we allocate a free ID and use the corresponding elements in the arrays; to remove one, we deallocate the ID, causing it to be returned to the free set, and set the corresponding element of svIndices to -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SVM Evaluation</head><p>We focus on the key part of SVM evaluation using the linear kernel here, and defer both how to calculate the SVM's weight vector w and how to implement other kernels to the supplementary material. For the linear kernel, we first calculate w and then compute a straightforward dot product of w with each of the samples. Each thread block in our implementation evaluates the SVM for a single sample (see Figure <ref type="figure">5</ref>). Individual threads multiply corresponding elements of the SVM's weight vector and the sample's feature vector and store the results in shared memory (accessing shared memory is hundreds of times faster than accessing global memory). The result of the dot product is then computed in shared memory using a reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Budget Maintenance</head><p>As a result of the fixed-size SVM representation we use, the way in which we maintain our support vector budget for ThunderStruck has to differ slightly from that described in §4.6. In particular, instead of removing a support vector when the budget is exceeded, we now remove a support vector at the point at which we need to add a new one have no available space in the arrays. We found this to be an equivalent scheme that made little difference to the results; however, it would nevertheless be possible to implement a version of the original scheme for fixed-size arrays by adding additional space at the ends of the arrays and maintaining the budget after adding a support vector. To evaluate an SVM with a linear kernel efficiently using CUDA, we use a thread block for each sample and compute a dot product between the sample's features and the SVM's weight vector. Each dot product is computed using a pointwise multiplication followed by a reduction in shared memory. The coloured boxes indicate where the data is stored (cyan = global memory, green = shared memory).</p><p>The coloured arrows distinguish between thread blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Feature Calculation</head><p>Prior to calculating features, we transfer the current frame and any derived images that are best computed on the CPU (e.g. an integral image, for computing Haar features) across to the GPU as CUDA textures. To calculate raw features, we use a variant of the approach in the original Struck that is a better fit for the GPU architecture. Rather than scaling a patch to 16 × 16 pixels and then densely sampling the result, we instead sample from a 16×16 uniform grid placed over the unscaled patch: this allows us to avoid resizing patches on the GPU, whilst producing equivalent results.</p><p>We compute each raw feature on a separate CUDA thread and calculate the features for all patches in parallel.</p><p>To calculate Haar features, we observe that each Haar feature we use can be calculated as the weighted combination of at most four box sums over the pixels of the current frame (see Figure <ref type="figure">3</ref>). The sum of each box can be calculated efficiently using the integral image for the frame <ref type="bibr" target="#b10">[11]</ref>. We can thus compute all of the Haar features without needing to branch on feature type on the GPU by making each CUDA thread calculate a single feature and assigning zero weights to boxes that are unnecessary for features of particular types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Multi-Threading on the CPU</head><p>Whilst it does not represent an improvement to the speed of the tracker itself, it is worth observing that we were able to obtain a further improvement in the speed of the ThunderStruck system as a whole by running the tracker on one CPU thread whilst rendering the output on another. This allows the tracker to process the next frame whilst the current one is still being rendered. This improvement could clearly also have been made to the CPU version of Struck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We evaluate our CPU and GPU implementations of Struck on the full 50-video benchmark of Wu et al. <ref type="bibr" target="#b20">[21]</ref>. In the main paper, we perform experiments at a single scale, first using only individual kernels ( §6.2), and then combining multiple kernels ( §6.3) as described in §4.7.4. We also perform experiments showing the difference structured learning makes in improving tracking performance relative to a baseline classification SVM ( §6.4), and evaluate our implementations for speed ( §6.5). Further experiments investigating multi-scale tracking and the effects of changing various parameters of our approach can be found in the supplementary material, together with a study comparing our approach to the state-of-the-art KCF tracker <ref type="bibr" target="#b21">[22]</ref> and more details on the effects of structured learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmark</head><p>The Wu dataset consists of numerous test videos from the recent literature. In their paper, the authors performed a large-scale evaluation that compared 29 trackers (including the CPU version of Struck, SCM <ref type="bibr" target="#b48">[49]</ref>, TLD <ref type="bibr" target="#b49">[50]</ref> and ASLA <ref type="bibr" target="#b50">[51]</ref>). Their key ideas were (a) to perturb the initialisation of the trackers in both time and space to improve the robustness of the evaluation, and (b) to evaluate the trackers on sequences that had been annotated to highlight tracking challenges, e.g. fast motion, occlusion, or changes in scale or illumination. Trackers were evaluated using a variety of different tests, in each case using location error (the percentage of frames whose predicted bounding boxes were within a fixed pixel threshold of the ground truth bounding boxes) as a precision measure, and overlap (the percentage of frames whose predicted bounding boxes overlapped the ground truth bounding boxes by more than a fixed threshold) as a success measure. These measures were calculated for a range of thresholds in each case. The trackers were ranked in terms of precision using a fixed location error threshold of 20 pixels, and in terms of success using the area under the curve (AuC) approach.</p><p>Three different types of test were performed to compare the trackers' performance: (a) one-pass evaluation (OPE), which initialises the trackers with the ground truth bounding box from the first frame of each sequence, (b) temporal robustness evaluation (TRE), which initialises the trackers at another starting frame in the sequence, and (c) spatial robustness evaluation (SRE), which shifts or scales the ground truth bounding box in the first frame. Overall performance tests of all three types were performed for each tracker on all of the available sequences; further tests were also performed to compare the trackers' performance when restricted to specific types of sequence, e.g. those with a significant amount of fast motion or occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Single-scale, single-kernel tracking</head><p>For our initial experiments, we tested single-scale (i.e. 2D translation only), single-kernel CPU and CUDA variants of Struck on the entire dataset using the TRE and SRE tests and various different feature/kernel/budget combinations.  [21] benchmark using various feature/kernel/budget combinations. We used search radii of 30 pixels for propagation and 60 pixels for learning, and set n R and n O , the numbers of reprocessing and optimisation steps used for LaRank, to 10.</p><p>For these experiments, we searched for the target object in each new frame using a fixed search radius of 30 pixels around the previous object location. For learning the SVM, we used a larger search radius of 60 to help ensure stability, and sampled potential transformations from a polar grid with 5 radial and 16 angular divisions, giving 81 potential transformations in total (including the identity transformation).</p><p>Table <ref type="table" target="#tab_5">1</ref> shows the results obtained by our two implementations for various combinations of parameters, along with published results from the best-performing methods in the original Wu benchmark, and the results we obtained for the state-of-the-art KCF tracker <ref type="bibr" target="#b21">[22]</ref>. It can be seen from these results that Struck fkbHG100, which uses Haar features, a Gaussian kernel and a budget of support vectors, outperforms methods like ASLA, SCM and TLD on all but the TRE success tests, in some cases by a considerable margin. The corresponding CUDA variant of our tracker (ThunderStruck fkbHG100) achieves slightly lower tracking performance than this due to differences in the way in which the budget maintenance step is implemented (see but compensates for this by a significant increase in speed (see §6.5). The Struck fkbHG25 results demonstrate that it is possible to achieve reasonable tracking performance using a relatively small number of support vectors.</p><p>Although Struck fkbHG100 achieves excellent results in comparison to trackers such as ASLA, SCM and TLD, the KCF tracker substantially outperforms it. We believe a key advantage KCF has is its relative computational efficiency in comparison to Struck, which gives it the freedom both to use better features than Struck fkbHG100 (it uses HOG rather than Haar features) and to sample densely around the object without encountering speed or memory issues. Of these, we believe that the features make the more significant difference: when we tried densely sampling around the object, we found that it made little difference to the overall results. By contrast, as our experiments in the next section demonstrate, when we used more effective features as part of a multi-kernel learning (MKL) approach, we achieved significant improvements in tracking performance, allowing the mklHGHI variant of our tracker to outperform KCF on all but the TRE success tests of the Wu benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Single-scale, multi-kernel tracking</head><p>In order to examine the effects of the multi-kernel learning (MKL) approach we articulated in §4.7.4, we compared a number of multi-kernel variants of our Struck tracker (using various combinations of features and kernels) with the corresponding single-kernel variants by running them on the entire Wu benchmark. The results are shown in Table <ref type="table" target="#tab_7">2</ref>. For all variants, we set the unrelated parameters to default values to ensure a fair comparison (specifically, we used a support vector budget of 100, a learning radius of 60 pixels and a propagation radius of 30 pixels, and set n R and n O , respectively the numbers of reprocessing and optimisation steps used for LaRank, to 10).</p><p>Our results demonstrate that by combining the right features and kernels, it is possible to achieve significant improvements in tracking performance. In particular, the mklHGHI variant of our tracker, which combines a Gaussian kernel with an intersection kernel on Haar and histogram features, achieves state-of-the-art results on the Wu benchmark, outperforming even KCF. This improvement in tracking performance is bought at the expense of using larger feature vectors, which leads to a roughly eight-fold decrease in the speed of the tracker when compared to Struck fkbHG100 (see §6.5). However, given the increases in speed obtained by ThunderStruck, we believe that a careful implementation of Struck mklHGHI on the GPU could run at a decent frame rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effects of structured learning</head><p>To investigate the effects of structured learning on the performance of our tracker, we compared the results of Struck fkbHG100 against a baseline classification SVM with the same parameters (Haar features, Gaussian kernel, budget of 100) on the Wu benchmark. To achieve this, we modified our tracking framework to train the learner using binary rather than structured examples. In each frame, a single positive example was generated from the estimated object , SCM <ref type="bibr" target="#b48">[49]</ref> and ASLA <ref type="bibr" target="#b50">[51]</ref>. Videos of these results can be found at https://goo.gl/cJ1Dg7.   The results of the baseline classifier are shown in Table 1. In comparison with those for Struck fkbHG100, they demonstrate that for the Wu benchmark as a whole, our structured learning framework achieves meaningful improvements over a traditional classification-based approach. Indeed, we show in the supplementary material that our structured learner improves upon the baseline for each individual attribute-based subset of the benchmark. However, the improvements achieved did vary with the attributes of the sequences tested. In particular, for sequences exhibiting fast motion or in which some part of the target leaves the view, we found that the structured learner did not significantly improve upon the baseline. We investigate this further in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Timings</head><p>To investigate the speed improvements yielded by using the GPU, and to quantify the effects on tracking speed of changing various parameters of our trackers, we timed a number of variants of both Struck and ThunderStruck on the entire Wu benchmark (see Table <ref type="table" target="#tab_9">3</ref>). The machine we used had a 12-core Intel i7-4960X CPU, running at 3.6 GHz, and an NVIDIA GeForce GTX Titan Black GPU, and we ran our experiments under Ubuntu 14.04. In all cases, we suppressed rendering and text output so as to time only the speed of the actual tracking process.</p><p>Our results demonstrate that in a head-to-head comparison between similar variants of Struck and ThunderStruck, our GPU variants achieve significantly higher average frame rates. In particular, our default fkbHG100 variant of ThunderStruck (Haar features, a Gaussian kernel with σ = 0.2 and a budget of 100 support vectors) runs at an average of 93.9 FPS, whilst our ro5 5 variant, which is  similar to fkbHG100 but uses fewer LaRank optimisation steps, is even faster, running at an average of 125.1 FPS.</p><p>Even faster tracking can be obtained using ThunderStruck fkbRL100 (raw features, a linear kernel and a budget of 100 support vectors), but with a sizeable decrease in tracking performance (see Table <ref type="table" target="#tab_5">1</ref>). By contrast, our multi-kernel mklHGHI variant of Struck achieves high performance values (see §6.3), but is comparatively slow. Our current multi-kernel implementation is CPU-only, and given the speed improvements demonstrated by ThunderStruck, we believe it could be speeded up significantly using the GPU; however, the larger feature vectors and more complicated kernels used by mklHGHI mean that it will always be slower than simpler variants.</p><p>The effect that incorporating scale has on speed can be seen by comparing ThunderStruck sHG95 105 1 (a multiscale variant that uses 11 possible scaling factors) with ThunderStruck fkbHG100. Although the multi-scale variant is nearly five times slower, owing to the much greater number of object configurations it must consider during the propagation step, it remains real-time at nearly 20 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we have presented Struck, an adaptive tracking-by-detection framework based on structured output prediction. From a learning point of view, we take advantage of the well-studied large margin theory of SVMs, which brings benefits in terms of generalisation and robustness to noise (both in the input and output spaces). Unlike prior methods based on classification, our algorithm does not rely on a heuristic intermediate step for producing labelled binary samples with which to update the classifier, which is often a source of error during tracking. Our approach uses an online structured output SVM learning framework, making it easy to incorporate image features and kernels. To prevent unbounded growth in the number of support vectors, and allow real-time performance, we introduced a budget maintenance mechanism for online structured output SVMs. We have experimentally shown the benefits of structured output prediction by comparing our approach with a baseline classification SVM.</p><p>In comparison with the state-of-the-art KCF tracker, the conventional single-kernel variants of Struck achieve lower tracking performance. However, by incorporating larger feature vectors into a multi-kernel tracking approach, we have shown that we are able to outperform KCF and achieve state-of-the-art results on the popular Wu et al. benchmark.</p><p>From a speed perspective, we have shown how Struck can be implemented on the GPU using CUDA. This improves upon the already real-time performance of the original unoptimised CPU implementation of Struck to achieve high frame-rates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: Different adaptive tracking-by-detection paradigms: given the current estimated object location, traditional approaches (shown on the right-hand side) generate a set of samples and, depending on the type of learner, produce training labels. Our approach (left-hand side) avoids these steps and operates directly on the tracking output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The typical structure of a tracking-by-detection tracker (see §3). The dashed box is specific to Struck and indicates the place in which we add a budget maintenance step to the pipeline (see §4).</figDesc><graphic coords="3,92.78,185.64,61.11,50.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>: λ = max(0, min(λ u , Cδ(y + , y m ) -Update gradients 10: for (t i , y) ∈ S do 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The representation of the SVM in ThunderStruck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig.5: To evaluate an SVM with a linear kernel efficiently using CUDA, we use a thread block for each sample and compute a dot product between the sample's features and the SVM's weight vector. Each dot product is computed using a pointwise multiplication followed by a reduction in shared memory. The coloured boxes indicate where the data is stored (cyan = global memory, green = shared memory). The coloured arrows distinguish between thread blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: Example frames from benchmark sequences, comparing the results of Struck (variant mklHGHI) with KCF<ref type="bibr" target="#b21">[22]</ref>, SCM<ref type="bibr" target="#b48">[49]</ref> and ASLA<ref type="bibr" target="#b50">[51]</ref>. Videos of these results can be found at https://goo.gl/cJ1Dg7.</figDesc><graphic coords="12,36.16,239.66,72.13,54.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>configuration, and negative examples were generated by sampling from Y and keeping examples that overlapped the estimated object configuration by less than 0.5 (i.e. θ u = 1 and θ l = 0.5 in the labelling function described in §3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 Struck tracking loop. Require: f , t, p t , S t 1: Propagate the estimated object configuration 2: y t = f (t) 3: p t+1 = y t (p t )</figDesc><table><row><cell cols="2">4: Update the SVM</cell></row><row><cell cols="2">5: PROCESSNEW(t, y t )</cell></row><row><cell cols="2">6: MAINTAINBUDGET()</cell></row><row><cell cols="2">7: for i = 1 to n R do</cell></row><row><cell>8:</cell><cell>PROCESSOLD()</cell></row><row><cell>9:</cell><cell>MAINTAINBUDGET()</cell></row><row><cell>10:</cell><cell>for j = 1 to n O do</cell></row><row><cell>11:</cell><cell>OPTIMIZE()</cell></row><row><cell>12:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>•</head><label></label><figDesc>Haar features, inspired by the features used in the Viola-Jones face detector<ref type="bibr" target="#b10">[11]</ref>, and calculated as a weighted combination of box sums over the pixels of the current image. By default, we use 6 different types of Haar feature (see Figure3), arranged at 2 scales on a 4 × 4 grid, resulting in a 192D feature vector with each feature normalised to the range [-1, 1].</figDesc><table /><note><p>• Histogram features, obtained by concatenating 16-bin intensity histograms from a spatial pyramid of 4 levels. At each level L, the patch is divided into L × L cells, resulting in a 16</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2509974, IEEE Transactions on Pattern Analysis and Machine Intelligence</figDesc><table /><note><p>y, t, ȳ) 0162-8828 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. ?, NO. ?, ? 2015</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 1 :</head><label>1</label><figDesc>The tracking performance of single-scale, single-kernel Struck and ThunderStruck variants on the Wu et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 2 :</head><label>2</label><figDesc>Comparing the tracking performance of some single-kernel variants of Struck with variants that use multiple kernel learning (MKL). For all variants, we use a learning search radius r L of 60 pixels, a propagation search radius r P of 30 pixels and a support vector budget of 100, and set n R and n O , respectively the numbers of reprocessing and optimisation steps used for LaRank, to 10. We show the results of the KCF tracker for comparison purposes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 3 :</head><label>3</label><figDesc>Comparing the average speed (in frames per second) of a number of variants of Struck and Thunder-Struck over the entire Wu benchmark. For details of the parameters used and the tracking performance obtained for each variant, see the corresponding experiments sections.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In practice, it is often profitable to use different search spaces for learning and propagation, but we do not distinguish between the two search spaces here to simplify the presentation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. ?, NO. ?, ? 2015</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>Financial support was provided by ERC grant ERC-2012-AdG 321162-HELIOS. Stuart Golodetz was funded via a Royal Society Brian Mercer Award for Innovation awarded to Stephen L. Hicks. Philip H. S. Torr is in receipt of a Royal Society Wolfson Research Merit Award and acknowledges support from the Leverhulme Trust and EPSRC.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1064" to="1072" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Real-Time Tracking via On-line Boosting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust Object Tracking with Online Multiple Instance Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online Multi-Class LPBoost</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Sparse Probabilistic Learning Algorithm for Real-Time Tracking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On Robustness of On-line Boosting -A Competitive Study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCVW</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the design of robust classifiers for computer vision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-Supervised On-Line Boosting for Robust Tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust multiview boosting with priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online Semi-Supervised Multiple-Instance Boosting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust Real-Time Face Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to Localize Objects with Structured Output Regression</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiple kernels for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part-Based Models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large Margin Methods for Structured and Interdependent Output Variables</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Solving multiclass support vector machines with LaRank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence Labelling SVMs Trained in One Pass</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online Classification on a Budget</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Holloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
		<title level="m">Multi-Class Pegasos on a Budget,&quot; in ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Struck: Structured Output Tracking with Kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online Object Tracking: A Benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High-Speed Tracking with Kernelized Correlation Filters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual Tracking: An Experimental Survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finding the Best from the Second Bests -Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Visual Object Tracking VOT2014 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimum Error Bounded Efficient 1 Tracker with Occlusion Detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online Robust Non-negative Dictionary Learning for Visual Tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust Object Tracking with Online Multi-lifespan Dictionary Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Arulampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Clapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSP</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="174" to="188" />
			<date type="published" when="2002-02">February 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking via Adaptive Forest</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICICIP</title>
		<imprint>
			<biblScope unit="page" from="30" to="34" />
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online Random Forests</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV-OLCV</title>
		<meeting>ICCV-OLCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PROST: Parallel Robust Online Simple Tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Randomized Ensemble Tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2040" to="2047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ensemble-Based Tracking: Aggregating Crowdsourced Structured Time Series Data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PixelTrack: a fast adaptive algorithm for tracking non-rigid objects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2480" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking as Repeated Figure/Ground Segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Superpixel Tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1323" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting the Circulant Structure of Tracking-by-detection with Kernels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="702" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive Color Attributes for Real-Time Visual Tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Partbased Visual Tracking with Online Latent Structural Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2363" to="2370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.3283v3</idno>
		<title level="m">StructBoost: Boosting Methods for Predicting Structured Output Variables</title>
		<imprint>
			<date type="published" when="2014-02">February 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Online Multi-Target Tracking by Large Margin Structured Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feyereisl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="98" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Object Tracking by Oversampling Local Features</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pernici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online Object Tracking, Learning and Parsing with And-Or Graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Preserving Structure in Model-Free Tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="756" to="769" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fast Training of Support Vector Machines Using Sequential Minimal Optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On Feature Combination for Multiclass Object Classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust Object Tracking via Sparsity-based Collaborative Model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">P-N Learning: Bootstrapping Binary Classifiers by Structural Constraints</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual Tracking via Adaptive Structural Local Sparse Appearance Model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
