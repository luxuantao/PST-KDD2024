<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Keywords-Guided Abstractive Sentence Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
							<email>lihaoran24@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
							<email>junnan.zhu@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<email>jjzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<email>cqzong@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<email>xiaodong.he@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Keywords-Guided Abstractive Sentence Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Guided Summarization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of generating a summary for a given sentence. Existing researches on abstractive sentence summarization ignore that keywords in the input sentence provide significant clues for valuable content, and humans tend to write summaries covering these keywords. In this paper, we propose an abstractive sentence summarization method by applying guidance signals of keywords to both the encoder and the decoder in the sequence-to-sequence model. A multi-task learning framework is adopted to jointly learn to extract keywords and generate a summary for the input sentence. We apply keywords-guided selective encoding strategies to filter source information by investigating the interactions between the input sentence and the keywords. We extend pointer-generator network by a dual-attention and a dual-copy mechanism, which can integrate the semantics of the input sentence and the keywords, and copy words from both the input sentence and the keywords. We demonstrate that multi-task learning and keywords-oriented guidance facilitate sentence summarization task, achieving better performance than the competitive models on the English Gigaword sentence summarization dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Sentence summarization is a task that creates a condensed version of a long sentence 1 . Different from extractive methods <ref type="bibr" target="#b2">(Cheng and Lapata 2016;</ref><ref type="bibr">Jadhav and Rajan 2018;</ref><ref type="bibr" target="#b5">Dong et al. 2018;</ref><ref type="bibr" target="#b22">Zhang et al. 2018)</ref>, which select a subset of text units in the original text to form the summary, abstractive methods (Rush, <ref type="bibr" target="#b15">Chopra, and Weston 2015;</ref><ref type="bibr">Takase et al. 2016;</ref><ref type="bibr" target="#b1">Chen et al. 2016;</ref><ref type="bibr" target="#b17">See, Liu, and Manning 2017;</ref><ref type="bibr" target="#b19">Tan, Wan, and Xiao 2017;</ref><ref type="bibr" target="#b23">Zhou et al. 2017;</ref><ref type="bibr" target="#b14">Narayan, Cohen, and Lapata 2018;</ref><ref type="bibr" target="#b9">Lebanoff, Song, and Liu 2018;</ref><ref type="bibr" target="#b24">Zhu et al. 2019</ref>) can generate novel words not present in the input. Compared with extractive methods, abstractive summarization is much closer to the way human make a summary, while it is more challenging. Intuitively, some important words (a.k.a. keywords) in the original sentence pro-Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>1 For sentence compression task <ref type="bibr" target="#b4">(Clarke 2008)</ref>, the word of the output must be in the input, while the vocabulary for sentence summarization is not constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation:</head><p>Input sentence: France and Germany called on world leaders Monday to take rapid action to press for the closure of Ukraine 's Chernobyl nuclear plant , site of the world 's worst ever nuclear disaster . Reference summary: World leaders urged to back Chernobyl closure plan .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution:</head><p>Step1. Extracting keywords: world leaders closure Chernobyl Step2. Generating summary guided by the keywords: World leaders called for action on Chernobyl closure .</p><p>Figure <ref type="figure">1</ref>: The overlapping keywords (marked in red) between the input sentence and the reference summary cover the main ideas of the input sentence. Our motivation is to generate summary guided by the keywords extracted from the input sentence. vide significant clues for the main points about the sentence. Humans tend to write summaries containing these keywords and then perform necessary modifications to ensure the fluency and grammatically correctness. Thus, we believe that it will be easier for a machine to generate a summary with the help of keywords. Existing work has not explored the effectiveness of keywords for sentence summarization task, and our work focuses on it.</p><p>Given a pair of input sentence and reference summary, we can roughly take the overlapping words (except for stopwords) as the keywords. As shown in Figure <ref type="figure">1</ref>, the overlapping words cover the gist of the input. For example, the keywords "closure" and "Chernobyl" can guide us to focus on the part "closure of Ukraine 's Chernobyl nuclear plant" of the original sentence, which is highly related to "Chernobyl closure plan" in the summary. This phenomenon is common for sentence summarization. In the Gigaword sentence summarization dataset (Rush, <ref type="bibr" target="#b15">Chopra, and Weston 2015)</ref>, over half of the words in the summary are presented in the source sentence.</p><p>Existing researches show that keywords are beneficial for extractive summarization <ref type="bibr" target="#b16">(Saggion and Lapalme 2002;</ref><ref type="bibr" target="#b22">Zhang, Zincir-Heywood, and Milios 2004;</ref><ref type="bibr" target="#b21">Wang and Cardie 2013)</ref>. They point out that keywords compose the main body of the sentence, which are regarded as the indicators for important sentence selection. On the other hand, keywords are proved useful for decoding process in abstractive document-level summarization task <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b6">Gehrmann, Deng, and Rush 2018)</ref>. We argue that keywords can point out valuable content in the input sentence, which can guide the summarizer to capture the gist of the input in the process of both encoding and decoding.</p><p>A prerequisite for our model is the keywords of the input sentence. For training, we can directly use the overlapping words between the input sentence and the reference summary as the ground-truth keywords, while the groundtruth keywords are not available for testing. Consequently, a keyword extractor is required. Sentence summarization and keyword extraction both aim to mine the primary ideas of the input text but with different forms of output. Sentence summarization aims to express the main meanings of the input with a complete sentence, while keyword extraction is to select the important words from the input. Thus, these two tasks both require the capacity of the encoder to recognize the crucial text fragments in the source sentence. Based on this, we adopt a multi-task learning framework to model sentence summarization and keyword extraction jointly, which is expected to be beneficial for both tasks. Then we explore the effectiveness of the keywords for sentence summarization task through following three strategies. We first apply keywords-guided selective encoding strategies to filter source information, and then we dynamically integrate the semantics of the input sentence and the keywords to build context representation via dual-attention. Furthermore, we extend the copy mechanism to a dual-copy mode that can copy words from both the input sentence and the keywords. The framework of our model is shown in Figure <ref type="figure">2</ref>.</p><p>Our main contributions are as follows:</p><p>• We propose an abstractive sentence summarization method guided by the keywords in the original sentence. • Our encoder builds an optimized latent representation by keywords-guided selective encoding. • Our decoder dynamically combines the information of the original sentence and the keywords via dual-attention, and we propose a dual-copy mechanism which facilitates copying words from both the input sentence and the keywords. • We achieve significantly better performance than the competitive methods on the English Gigaword dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Abstractive Text Summarization</head><p>Seq2seq model is the dominating framework for abstractive text summarization. Rush, <ref type="bibr" target="#b15">Chopra, and Weston (2015)</ref> are the first to apply the seq2seq model to abstractive sentence summarization.  </p><formula xml:id="formula_0">0 h r 1 h r 2 h r 3 h r n x 2 x 3 x n h r' 2 h r' 3 h r' n x 1 h r' 1 c r t ... ... ... 1 1 h k 1 h k 2 h k 3 h k m k 2 k 3 k m h k' 2 h k' 3 h k' m k 1 h k' 1 c k t .</formula><formula xml:id="formula_1">(x 1 , x 2 , • • • , x n ) and the ground-truth keywords (k 1 , k 2 , • • • , k m )</formula><p>into the first-level hidden states h r i and h k i . A jointly trained keyword extractor takes h r i as the input to predict whether the input word is a keyword or not. Co-selective encoding layer builds the second-level hidden states h r i and h k i . Then the summary is generated via dualattention and dual-copy for both the original sentence and the keyword sequence. During testing, the ground-truth keywords are replaced by the keywords predicted by our trained keyword extractor. <ref type="bibr" target="#b8">Gulcehre et al. (2016)</ref> introduce a copy mechanism into seq2seq learning. <ref type="bibr" target="#b17">See, Liu, and Manning (2017)</ref> incorporate the pointer-generator model with the coverage mechanism. <ref type="bibr" target="#b23">Zhou et al. (2017)</ref> employ a selective encoding mechanism to filter secondary information. <ref type="bibr" target="#b19">Tan, Wan, and Xiao (2017)</ref> propose a graph-based attention to tackle document-level summarization. <ref type="bibr" target="#b1">Cao et al. (2018)</ref> and <ref type="bibr" target="#b11">Li et al. (2018b)</ref> solve the problem of fake facts in a summary.</p><p>tive document-level summarization task. <ref type="bibr" target="#b10">Li et al. (2018a)</ref> use the keywords to calculate attention distribution and copy probability. <ref type="bibr" target="#b6">Gehrmann, Deng, and Rush (2018)</ref> propose a content selector to restrict the summarization model to copy phrases from the source document. Above-mentioned work focuses on enhancing the decoder. We believe that keywords can eliminate the redundant information in the source, and thus, we propose keywords-guided selective mechanisms to improve the source encoding representations. Besides, our decoder can dynamically combine the information of the input sentence and the keywords to generate summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Proposed Model Overview</head><p>The input of sentence summarization task is a long sentence, and the output is a condensed summary. Our hypothesis for this task is that the keywords can provide essential clues for the gist of the input sentence. The standard seq2seq model takes into account of all source words, and attention mechanism may work as a soft keyword extractor. We propose a novel pointer-generator-based abstractive sentence summarization method incorporating a keyword extractor, which can explicitly point out the valuable content of the input sentence.</p><p>A prerequisite for our model is the keywords for the input sentence. For training, we take the words appearing both in the input sentence and the reference summary as the groundtruth keywords. To acquire the keywords for testing, we train a keyword extractor by multi-task learning (MTL) with summarization model. The encoder read the input sentence into a latent representation sequence, and a softmax layer over latent representation predicts whether a word is a keyword or not. Our summarization model consists of three major parts: an encoder with selective encoding strategy, a generator combining context information of the original sentence and the keywords via dual-attention, and a pointer copying words from both the original sentence and the keywords via dual-copy.</p><p>Here are the main steps of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Step 1. Extracting overlapping words between the input and the reference as the ground-truth keywords.</p><p>• Step 2. Multi-task learning: generating summary using the input sentence and the ground-truth keywords; training the keyword extractor.</p><p>• Step 3. Generating keywords using the trained keywords extractor for the input sentence in the training set and then fine-tuning the sentence summarizer using the original sentence and the predicted keywords.</p><p>• Step 4. During testing, first generating keywords using the trained keywords extractor for the input sentence and then producing the summary using the input sentence and the predicted keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth Keyword Generation</head><p>Standard Gigaword dataset for sentence summarization task does not provide the keywords for the input sentence. To train our keyword extractor and keywords-guided summarizer, we roughly regard the overlapping words (stop-words are excluded) between the input sentence and the reference summary as the ground-truth keywords (sentence-summary pair in the Gigaword dataset is bound to have overlapping non-stop-words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Text Encoder</head><p>The BiLSTM encodes input text forwardly and backwardly to generate two sequences of the hidden states:</p><formula xml:id="formula_2">( − → h 1 , • • • , − → h n ) and ( ← − h 1 , • • • , ← − h n ), respectively, where: − → h i = LSTM(E[x i ], − → h i−1 ) (1) ← − h i = LSTM(E[x i ], ← − h i+1 ) (2) E[x i ]</formula><p>is the embedding for word x i . The final hidden representation h i is the concatenation of the forward and backward vectors:</p><formula xml:id="formula_3">h i = [ − → h i ; ← − h i ].</formula><p>Sentence summarization task and keyword extraction task are very similar in the sense that both aim to select important information contained in the input sentence. The output of sentence summarization is a complete sentence, while the output for keyword extraction is a set of words. It requires different modules to generate corresponding output for various tasks, while for the encoders, we believe they can benefit from sharing parameters to promote the capacity of capturing the gist of the input text. To this end, we use a shared text encoder to generate hidden state sequences for both the original sentence and the keyword sequence following Equation 1 and 2. Hereafter, h r i and h k i denote the hidden representations for the input sentence and the keywords, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keyword Extraction</head><p>For keyword extraction task, the output layer is a softmax classifier over the hidden representation h r i for each word in the sentence. The classifier predicts one of the following two labels: '1' for the keywords, and '0' for the non-keywords.</p><p>We also try BiLSTM-CRF model (Huang, Xu, and Yu 2015) which extends the BiLSTM model with a CRF layer allowing the model to use sequence-level tag information for sequence prediction, but our experimental results show that BiLSTM-CRF model leads to a similar performance (0.2% higher accuracy) with around 20% more trainable parameters and doubled training time compared with BiLSTMsoftmax model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keywords-Guided Selective Encoding</head><p>We are inspired by the work of <ref type="bibr" target="#b23">Zhou et al. (2017)</ref> on a new paradigm to encode sentences. Specifically, they propose a selective encoding mechanism to model the selection process for sentence summarization. A selective gate is applied to h r i to construct a second-level tailored representation h r i . The selective gate in their model is expected to maintain the important information from an encoded sentence by exploring the semantic relationship between every word in the input sentence and the whole sentence. This kind of selective encoding strategy can be referred to as self-selective encoding because the selective signal comes from the original sentence itself. Compared with the original sentence, keywords contain more condensed semantics, and thus, the selective signal could be more powerful. Unlike self-selective, we leverage information of the keywords to construct the selective gate. We propose two keywords-guided selective encoding strategies: keywords-selective which builds h r i for the sentence guided by the keywords and co-selective encoding which builds h r i and h k i for the input sentence and the keywords, respectively. More details are as follows.</p><p>Self-Selective Encoding A self-selective gate vector for each h r i is computed as follows:</p><formula xml:id="formula_4">self Gate s i = σ(W r h r i + U r a r )<label>(3)</label></formula><p>where σ denotes the sigmoid function, and</p><formula xml:id="formula_5">a r = [ − → h r n ; ← − h r 1 ]</formula><p>is the sentence representation. Then, h r i is computed as follows:</p><formula xml:id="formula_6">h r i = h r i self Gate s i (4)</formula><p>where is element-wise multiplication.</p><p>Keywords-Selective Encoding Keywords-selective gate uses keywords to guide the encoding of the original sentence as follows:</p><formula xml:id="formula_7">keyGate r i = σ(W k h r i + U k a k )<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">a k = [ − → h k n ; ← − h k 1 ]</formula><p>is the keyword sequence representation. Then, h r i is computed as follows:</p><formula xml:id="formula_9">h r i = h r i keyGate r i (6)</formula><p>Co-Selective Encoding Beyond selecting encoding for the sentence, we argue that each keyword contributes differently to the summarization task, and thus, we propose a co-selective encoding to select information for both the sentence and the keywords jointly.</p><formula xml:id="formula_10">coGate r i = σ(W p h r i + U p a k ) (7) coGate k i = σ(W q h k i + U q a r )<label>(8)</label></formula><p>Then, h r i and h k i are computed as follows:</p><formula xml:id="formula_11">h r i = h r i coGate r i (9) h k i = h k i coGate k i (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual-Attention Generator</head><p>Pointer-generator network is a seq2seq model with copy mechanism, which predicts words based on probability distributions from the generator and the pointer <ref type="bibr" target="#b20">(Vinyals, Fortunato, and Jaitly 2015)</ref>. The generator applies a dual-attention mechanism to generate the context vector based on attention over both the source sentence and the extracted keywords.</p><p>We use an LSTM with attention as the decoder to produce the output summary. At each timestep t, LSTM reads the previous decoder state s t−1 , predicted output y t−1 , and context vector c t−1 to compute the current decoder state s t as follows:</p><formula xml:id="formula_12">s t = LSTM(s t−1 , y t−1 , c t )<label>(11)</label></formula><p>The decoder's hidden state s 0 is initialized as follows:</p><formula xml:id="formula_13">s 0 = tanh(W h [ − → h r n ; ← − h r 1 ])<label>(12)</label></formula><p>The input for our model includes the original sentence and the keywords, and thus we first construct the context vector for the sentence and the keywords with the attention mechanism. Then we obtain the final context vector c t .</p><p>We compute the sentence context vector c r t as follows:</p><formula xml:id="formula_14">c r t = N i=1 α r t,i h r i (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where each vector is weighted by the attention weight α r t,i , as calculated in Equations 14 and 15 as follows:</p><formula xml:id="formula_16">e r t,i = v T a tanh(U a s t−1 + W a h r i ) (14) α r t,i = exp(e r t,i ) N j=1 exp(e r t,j )<label>(15)</label></formula><p>Similarly, the keyword attention α k t,i and keyword context vector c k t can be calculated using h k i and s t−1 . Next, we apply three approaches to fuse c r t and c k t into the final context vector c t , including concatenation fusion, gated fusion, and hierarchical fusion.</p><p>Concatenation Fusion This fusion method simply concatenates two context vectors:</p><formula xml:id="formula_17">c t = [c r t ; c k t ]<label>(16)</label></formula><p>Gated Fusion We first compute a fusion gate vector using two context vectors and then combine context vectors by the gate:</p><formula xml:id="formula_18">g t = σ(W g c r t + U g c k t ) (17) c t = g t c r t + (1 − g t ) c k t (<label>18</label></formula><formula xml:id="formula_19">)</formula><p>Hierarchical Fusion Gated fusion combines the two context vectors with the gate related to the interaction between them. Intuitively, the fusion process should reflect the relative informativeness of the sentence and the keywords toward the decoder state s t−1 . We adopt a hierarchical attention mechanism which aims to establish a target-oriented bond between the original sentence and the keywords as follows:</p><formula xml:id="formula_20">β r t = σ(U β s t−1 + W β c r t )<label>(19)</label></formula><formula xml:id="formula_21">β k t = σ(U β s t−1 + W β c k t ) (20) c t = β r t c r t + β k t c k t (<label>21</label></formula><formula xml:id="formula_22">)</formula><p>where β r t and β k t are two scalars. Then, we can calculate probability distribution P gen over all words in the target vocabulary is calculated as follows:</p><formula xml:id="formula_23">P gen (w) = softmax(W h s t + V h c t + b h ) (22)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual-Copy Pointer</head><p>A general pointer copies a word w from the source via pointing:</p><formula xml:id="formula_24">P copy (w) = i:xi=w α t,i<label>(23)</label></formula><p>We propose a dual-copy pointer that copies a word w from both the source sequence and keyword sequence as follows:</p><formula xml:id="formula_25">P copys (w) = i:xi=w α r t,i (24) P copyk (w) = i:ki=w α k t,i (25) (26)</formula><p>The final distribution is a weighted sum of the generation distribution and dual-copy distribution:</p><formula xml:id="formula_26">λ t = sigmoid(w T a c t + u T g s t + v T g y t−1 + b g )<label>(27</label></formula><p>)</p><formula xml:id="formula_27">P (w) = λ t P gen (w) + 1 2 (1 − λ t )(P copys (w) + P copyk (w))<label>(28)</label></formula><p>The loss function L t for each timestep t is the negative log-likelihood of the ground-truth target word y t :</p><formula xml:id="formula_28">L t = −logP (y t )<label>(29)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning</head><p>In our MTL setup, summary generation task is much more complicated than keyword extraction, leading to different learning difficulties and convergence rates. Therefore, summary generation is regarded as the central task and keyword extraction as the auxiliary task. We optimize the two tasks alternatively during training until convergence. Let γ be the number of mini-batches of training for keyword extracting after 100 mini-batches of training for sentence summarization <ref type="bibr" target="#b15">(Pasunuru, Guo, and Bansal 2017)</ref>. We adopt γ = 10 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning with Generated Keywords</head><p>Our model is trained with the ground-truth keywords, instead of using the results of our keyword detector, while the ground-truth keywords cannot be obtained during testing. Using ground-truth keywords causes it to converge faster, but the mismatching between the processes of training and testing may exhibit instability. To solve this problem, we propose to fine-tune our model with the predicted keywords generated by our keyword detector as the input after the training process converges using ground-truth keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Dataset</head><p>We conduct experiments on the English Gigaword dataset, which has about 3.8 million training sentence-summary pairs. We use 8, 000 pairs as the validation set and 2, 000 pairs as the test set, provided by <ref type="bibr" target="#b23">Zhou et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We set the size of word embedding and LSTM hidden state to 300 and 512, respectively. Adam optimizer is applied with the learning rate of 0.0005, momentum parameters β 1 = 0.9 and β 1 = 0.999, and = 10 −8 . We use dropout <ref type="bibr" target="#b18">(Srivastava et al. 2014)</ref>   <ref type="bibr" target="#b12">(Lin 2004</ref>) F1 score on the validation set for every 2,000 batches, and we halve the learning rate if model performance drops. We use the halved learning rate for finetuning. At test time, we use beam search with a beam size of 10 to generate the summary. We report ROUGE F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparative Methods</head><p>ABS. Rush, <ref type="bibr" target="#b15">Chopra, and Weston (2015)</ref> use an attentive CNN encoder and neural network language model decoder to summarize a sentence. SEASS. <ref type="bibr" target="#b23">Zhou et al. (2017)</ref> present a selective encoding model to control the information flow from the encoder to the decoder. PG. <ref type="bibr" target="#b17">See, Liu, and Manning (2017)</ref> introduce a hybrid pointer-generator model that can copy words from the source sentence via pointing. KIGN. <ref type="bibr" target="#b10">Li et al. (2018a)</ref> adopt key information to guide the summarization generation. They use the key information representation as the extra input to calculate the attention distribution and the copy probability in the pointer mechanism. Bottom-up. Gehrmann, Deng, and Rush (2018) employ a content selection model to identify important phrases in the input document. Our models. We first take a seq2seq (S2S) model <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref> as the baseline model with different input: the original sentence only (S2S-Sentence) , the keywords only (S2S-Keywords), a new sentence merging the original sentence and keywords (S2S-Sentence&amp;Keywords). We compare different selective-based models, including Self-Selective, Keywords-Selective, and Co-Selective. We also report the results for three approaches to fuse the context of the original sentence and keywords, including Concatenation fusion, Gated fusion, and Hierarchical fusion. Finally, we conduct experiment with PG and our proposed dual-copy pointer-generator (DualPG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Table <ref type="table" target="#tab_3">1</ref> shows that our proposed models perform better than the models without keyword guidance. SEASS brings in an excellent promotion for sentence summarization task with the help of implicit emphases on the keywords, and our best model achieves a better performance with explicit use of the keywords. KIGN and Bottom-up are keywords-based document summarization models that mainly focus on improving the decoder, and we apply them to sentence summarization task. Similar to S2S-Keywords, Bottom-up also blocks out the words detected as non-keywords with high probabilities, which also leads to unsatisfactory results. KIGN is proposed to use the keywords selected from the input document using unsupervised TextRank algorithm <ref type="bibr" target="#b13">(Mihalcea and Tarau 2004)</ref> based on words co-occurrence relation, which may not be appropriate for sentence-level keyword extraction (Table <ref type="table">4</ref> shows that keyword extraction performance for TextRank is quite poor compared with our model). Thus, we further implement KIGN with keywords extracted by our model, and the results are: 46.84% for ROUGE-1, 24.33% for ROUGE-2, and 43.98% for ROUGE-L, which are better than those of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis and Discussion</head><p>To get better insights into our model, we conduct further analysis on (1) upper bound performance, (2) multi-task learning, (3) fine-tuning, and (4) selective encoding mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upper Bound Performance</head><p>We explore the upper bound performance for our keywordguided sentence summarization model. We do this by directly using the ground-truth keywords for both training and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTL v.s. Two-Stage Learning</head><p>MTL involves sharing parameters between related tasks, whereby each task can benefit from extra information on other tasks in the training process. We compare the performance for MTL and two-stage learning (TSL, learning two tasks independently). Figure <ref type="figure">3</ref> shows the results for sentence summarization, and Table <ref type="table">4</ref> shows the results for keyword extraction. Our models with MTL achieve better performances than those with TSL, proving that summarization and keyword extraction can benefit from each other. In addition, we also train our summarizer by TSL directly with the predicted keywords as input (without pre-training with the ground-truth keywords), and it drops about 1% ROUGE-2 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Fine-tuning</head><p>In this section, we evaluate the Co-Selective summarization models with and without fine-tuning. The results in Figure <ref type="figure">4</ref> show that fine-tuning steadily enhances the performance, improving absolute ROUGE-2 score about 1% for Hierarchical fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual Evaluation</head><p>We employ six graduate students to evaluate readability and informativeness of summaries generated by different methods with a score from 1 (worst) to 5 (best). Each participant is provided 100 sentences randomly sampled from the test set. As shown in Table <ref type="table" target="#tab_6">5</ref>, our method with keyword guidance achieves higher scores than other methods except for the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective Encoding Visualization</head><p>Following <ref type="bibr" target="#b23">Zhou et al. (2017)</ref>, we visualize the selective gate values with salience heat maps shown in Table <ref type="table" target="#tab_5">3</ref>. For our model with co-selective gate, the important words are selected correctly by the aid of keywords, while the output of the model with self-selective gate is mismatched with the reference summary because of inaccurate selective values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper addresses the sentence summarization task, namely, how to transform a sentence into a short-length summary. Our proposed model can take advantage of keywords achieving better performance than the competitive methods. We adopt a multi-task learning framework to extract keywords and generate summaries jointly. We design keywordsguided selective encoding strategies to select important information during encoding. We adopt a dual-attention structure to dynamically integrate context information of the input sentence and the keywords. We propose a dual-copy mechanism to copy the words from the input sentence and the keywords. Experimental results on standard dataset verify the effectiveness of keywords for sentence summarization task. Oracle testing with ground-truth keywords leads to absolute 20% ROUGE-2 score improvement over baseline, indicating a promising future direction based on keyword extraction for sentence summarization task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Results of Co-Selective models with MTL and two-stage learning (TSL) for summarization task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Among our models with different selective mechanisms, the experimental results are improved steadily as more keyword guidance signals added into the models, from Self-selective to Co-Selective. The models with Hierarchical fusion exhibit ad-Main results (%). Concat, Gated, and Hier denote Concatenation, Gated, and Hierarchical Fusion, respectively. Our Co-Selective+Hier+DualPG model performs significantly better than other baselines by the 95% confidence interval in the ROUGE script.</figDesc><table><row><cell>Method</cell><cell></cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>ABS</cell><cell></cell><cell cols="3">37.41 15.87 34.70</cell></row><row><cell>SEASS</cell><cell></cell><cell cols="3">46.86 24.58 43.53</cell></row><row><cell>PG</cell><cell></cell><cell cols="3">46.97 24.63 43.66</cell></row><row><cell>KIGN</cell><cell></cell><cell cols="3">46.18 23.93 43.44</cell></row><row><cell>Bottom-up</cell><cell></cell><cell cols="3">45.80 23.61 42.54</cell></row><row><cell>S2S-Sentence</cell><cell></cell><cell cols="3">45.09 23.58 42.37</cell></row><row><cell cols="2">S2S-Keywords</cell><cell cols="3">44.85 20.54 41.61</cell></row><row><cell cols="2">S2S-Sentence&amp;Keywords</cell><cell cols="3">46.14 24.07 43.30</cell></row><row><cell>Self-</cell><cell>Concat</cell><cell cols="3">46.23 24.13 43.26</cell></row><row><cell>Selective</cell><cell>Gated</cell><cell cols="3">46.44 24.31 43.44</cell></row><row><cell></cell><cell>Hier</cell><cell cols="3">46.51 24.32 43.45</cell></row><row><cell cols="2">Keywords-Concat</cell><cell cols="3">46.32 24.11 43.38</cell></row><row><cell>Selective</cell><cell>Gated</cell><cell cols="3">46.70 24.48 43.59</cell></row><row><cell></cell><cell>Hier</cell><cell cols="3">46.72 24.50 43.81</cell></row><row><cell>Co-</cell><cell>Concat</cell><cell cols="3">46.53 24.15 43.24</cell></row><row><cell>Selective</cell><cell>Gated</cell><cell cols="3">46.71 24.53 43.63</cell></row><row><cell></cell><cell>Hier</cell><cell cols="3">46.80 24.75 43.83</cell></row><row><cell>Co-</cell><cell>Concat+PG</cell><cell cols="3">46.68 24.33 43.31</cell></row><row><cell>Selective</cell><cell>Gated+PG</cell><cell cols="3">46.91 24.61 43.71</cell></row><row><cell></cell><cell>Hier+PG</cell><cell cols="3">46.93 24.83 43.92</cell></row><row><cell>Co-</cell><cell cols="4">Concat+DualPG 47.05 24.39 43.77</cell></row><row><cell>Selective</cell><cell>Gated+DualPG</cell><cell cols="3">47.13 24.87 44.34</cell></row><row><cell></cell><cell>Hier+DualPG</cell><cell cols="3">47.14 25.06 44.39</cell></row><row><cell cols="5">1.48% ROUGE-2 score, and 2.02% ROUGE-L score. S2S-</cell></row><row><cell cols="5">Keywords with only the keywords as the input degrades</cell></row><row><cell cols="5">the performance, showing that missing information from the</cell></row><row><cell cols="2">keywords is also necessary.</cell><cell></cell><cell></cell></row></table><note>vantages over those with other fusions. The model with Co-Selective encoding, Hierarchical fusion decoding, and DualCopy obtains the highest ROUGE score, which outperforms S2S-Sentence absolute 2.05% ROUGE-1 score,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Upper bound performances with the ground-truth keywords for both training and testing.</figDesc><table><row><cell>Method</cell><cell></cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell cols="2">S2S-Sentence (baseline)</cell><cell cols="3">45.09 23.58 42.37</cell></row><row><cell cols="2">S2S-Keywords</cell><cell cols="3">65.00 37.29 58.96</cell></row><row><cell cols="2">S2S-Sentence&amp;Keywords</cell><cell cols="3">66.35 41.34 61.63</cell></row><row><cell>KIGN</cell><cell></cell><cell cols="3">66.00 41.19 60.66</cell></row><row><cell>Bottom-up</cell><cell></cell><cell cols="3">65.11 38.12 59.39</cell></row><row><cell>Co-</cell><cell cols="4">Concat+DualPG 66.13 41.55 60.47</cell></row><row><cell cols="2">Selective Gated+DualPG</cell><cell cols="3">67.35 42.82 61.85</cell></row><row><cell></cell><cell>Hier+DualPG</cell><cell cols="3">67.61 42.94 62.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Heat maps for our model with co-selective and self-selective encoding.Self-selective gate for input sentence about #,### people gathered saturday in a central prague park to support the legalization of marijuana Summary generated by self-selective model about #,### people gathered in central prague</figDesc><table><row><cell cols="3">Co-selective gate for input sentence</cell><cell>about #,### people gathered saturday in a central prague</cell></row><row><cell></cell><cell></cell><cell></cell><cell>park to support the legalization of marijuana</cell></row><row><cell cols="2">Co-selective gate for keywords</cell><cell></cell><cell>people prague support legalization marijuana</cell></row><row><cell cols="3">Summary generated by co-selective model</cell><cell>people gather to support legalization of marijuana</cell></row><row><cell>Reference summary</cell><cell></cell><cell></cell><cell>thousand gather to support legalization of marijuana</cell></row><row><cell cols="4">Table 4: Comparison of MTL and two-stage learning for</cell></row><row><cell cols="4">keyword extraction. Accuracy is for all the words, and F1</cell></row><row><cell>score is for the keywords.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Accuracy F1</cell></row><row><cell>TextRank</cell><cell></cell><cell>73.25</cell><cell>31.45</cell></row><row><cell>Two-Stage Learning</cell><cell></cell><cell>84.37</cell><cell>59.35</cell></row><row><cell cols="3">MTL with Co-selective Concat 85.34</cell><cell>60.11</cell></row><row><cell>Encoding</cell><cell>Gate</cell><cell>85.75</cell><cell>60.24</cell></row><row><cell></cell><cell>Hier</cell><cell>85.81</cell><cell>60.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Manual evaluation.</figDesc><table><row><cell>Method</cell><cell cols="2">Readability Informativeness</cell></row><row><cell>S2S-Sentence</cell><cell>3.02</cell><cell>3.16</cell></row><row><cell>Co-Select+Hier</cell><cell>3.36</cell><cell>3.47</cell></row><row><cell>Co-Select+Hier+DualPG</cell><cell>3.61</cell><cell>3.79</cell></row><row><cell>Reference</cell><cell>4.17</cell><cell>4.54</cell></row><row><cell cols="3">testing. In this way, the keyword extraction error is elimi-</cell></row><row><cell cols="3">nated. The results are shown in Table 2. With this oracle set-</cell></row><row><cell cols="3">ting, ROUGE score improvements are more than 20% over</cell></row><row><cell cols="3">seq2seq model. Although the improvement is impressive, it</cell></row><row><cell cols="3">is based on the premise that we know which words should</cell></row><row><cell cols="3">be included in the summary in advance. Actually, choosing</cell></row><row><cell cols="3">a golden set of keywords may be difficult even for humans.</cell></row><row><cell cols="3">We believe this experiment holds out a promising prospect</cell></row><row><cell cols="3">for further development of sentence summarization task.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research work descried in this paper has been supported by the National Key Research and Development Program of China under Grant No. 2017YFC0820700.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A neural probabilistic language model</title>
				<imprint>
			<date type="published" when="2003">2015. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
		</imprint>
	</monogr>
	<note>Proceedings of ICLR. Bengio,</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Faithful to the original: Fact aware neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016">2018. 2016</date>
			<biblScope unit="page" from="2754" to="2760" />
		</imprint>
	</monogr>
	<note>Proceedings of AAAI</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Global inference for sentence compression : an integer linear programming approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Banditsum: Extractive summarization as a contextual bandit</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3739" to="3748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extractive summarization with SWAP-NET: Sentences and words from alternating pointer networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Jadhav</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Rajan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2016. 2015. 2018</date>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adapting the neural encoder-decoder framework from single to multi-document summarization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guiding generation for abstractive text summarization based on key information guide network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensure the correctness of the summary: Incorporate entailment knowledge into abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="1430" to="1441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
	<note>Proceedings of CoNLL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013">2018. 2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
	<note>Proceedings of ICML</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards improving abstractive summarization via entailment generation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
				<meeting>the Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2015">2017. 2015</date>
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating indicativeinformative summaries with SumUM</title>
		<author>
			<persName><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="526" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2016</date>
		</imprint>
	</monogr>
	<note>Neural headline generation on abstract meaning representation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graph-based attentional neural model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<title level="m">Pointer networks. In NeurIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03382</idno>
	</analytic>
	<monogr>
		<title level="m">Efficient summarization with read-again and copy mechanism</title>
				<imprint>
			<date type="published" when="2007">2007. 2013. 2016</date>
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural latent extractive document summarization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Zincir-Heywood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World wide web site summarization</title>
				<imprint>
			<date type="published" when="2004">2018. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="39" to="53" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NCLS: Neural cross-lingual summarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3045" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
