<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bag of contour fragments for robust shape classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-01-03">3 January 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Engineering</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<addrLine>1037 Luoyu Road</addrLine>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei Province</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Feng</surname></persName>
							<email>fengbin@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Engineering</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<addrLine>1037 Luoyu Road</addrLine>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei Province</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Engineering</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<addrLine>1037 Luoyu Road</addrLine>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei Province</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Engineering</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<addrLine>1037 Luoyu Road</addrLine>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei Province</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Latecki</surname></persName>
							<email>latecki@temple.edu</email>
							<affiliation key="aff1">
								<orgName type="department">CIS Department</orgName>
								<orgName type="institution">Temple University</orgName>
								<address>
									<addrLine>1805 N. Broad St</addrLine>
									<postCode>19122</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bag of contour fragments for robust shape classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-01-03">3 January 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">9802B8F04AC90C5B4FC6FAF838F2B63E</idno>
					<idno type="DOI">10.1016/j.patcog.2013.12.008</idno>
					<note type="submission">Received 22 March 2013 Received in revised form 11 December 2013 Accepted 22 December 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Shape classification Shape representation Bag of contour fragments</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Shape representation is a fundamental problem in computer vision. Current approaches to shape representation mainly focus on designing low-level shape descriptors which are robust to rotation, scaling and deformation of shapes. In this paper, we focus on mid-level modeling of shape representation. We develop a new shape representation called Bag of Contour Fragments (BCF) inspired by classical Bag of Words (BoW) model. In BCF, a shape is decomposed into contour fragments each of which is then individually described using a shape descriptor, e.g., the Shape Context descriptor, and encoded into a shape code. Finally, a compact shape representation is built by pooling shape codes in the shape. Shape classification with BCF only requires an efficient linear SVM classifier. In our experiments, we fully study the characteristics of BCF, show that BCF achieves the state-of-the-art performance on several wellknown shape benchmarks, and can be applied to real image classification problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Shape is an intrinsic feature for image understanding, which is stable to illumination and variations in object color and texture. Because of these advantages, shape is widely considered for object recognition. In particular, with the recent advance in contour detection proposed by Arbelaez et al. in <ref type="bibr" target="#b0">[1]</ref>, shape based object recognition in natural image is becoming more practical and attracts more attention in computer vision community. Main challenges in shape based object recognition include deformation, occlusion and viewpoint variation of objects. Various shape descriptors have been proposed to address these challenges, e.g., <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Shape based object recognition is usually considered as a classification problem. Given a set of training shapes and category label of each training shape, we need to determine which category a testing shape belongs to. Traditional shape classification methods are usually based on matching shape descriptors from two different shapes: for every training shape, we find correspondences between its shape descriptors and the shape descriptors in the testing shape using matching algorithms, such as Hungarian algorithm, dynamic programming algorithm; then we compute matching costs according to the matching results; finally, we rank training shapes based on the matching costs and classify the testing shape using the nearest neighbor (NN) classifier. This exemplar-based shape classification strategy has been widely used, for example, in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref>. However, it has its own limitations.</p><p>With few training samples, it is difficult to capture the large intraclass variation using these algorithms. For large training samples, it is extremely time consuming to perform shape matching oneby-one.</p><p>Different from exemplar-based shape matching, in this paper, we propose a compact shape representation and handle the large intra-class variation by discriminative learning. Inspired by the huge progress in image classification and representation with Bag-of-Words (BoW) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, we decompose shape into contour fragments and quantize the contour fragments into shape codes. The contour fragments under different scales contain both local and global shape information which can be encoded utilizing coding strategies for local descriptors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Then, a statistical histogram of shape codes is used to represent each shape and similarity of shapes can be directly computed from these histograms. Matching shapes based on this new shape representation does not explicitly give correspondences between contour fragments. But using a classifier for shape classification is much more efficient than using the typical matching algorithms such as Hungarian, thin plate spline (TPS), dynamic programming, dynamic time warping, and so on. In fact, BoW model is a natural solution for finding correspondences between two sets of features and can be used efficiently for recognition tasks. However, it has seldom been successfully applied to shape analysis, since the popular image descriptors such as SIFT <ref type="bibr" target="#b10">[11]</ref> and LBP <ref type="bibr" target="#b11">[12]</ref> are mainly designed for describing the local texture/appearance variations. These image features are not good at capturing the intrinsic structure in shape. Toward this end, we directly work on shape contour by decomposing it into contour fragments. We name our method Bag of Contour Fragments (BCF), which can not only provide a compact and informative representation, but also achieve the state-of-the-art classification performance on several popular shape benchmarks.</p><p>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/pr Pipeline of building shape representation in BCF is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The outer contour of each shape is decomposed into salient contour fragments using a well-known contour decomposition method named discrete contour evolution (DCE) <ref type="bibr" target="#b12">[13]</ref>. Each contour fragment is then described by collecting the shape context features <ref type="bibr" target="#b1">[2]</ref> on its reference points, and encoded into shape codes. Finally, the shape codes are pooled into a compact image representation with spatial pyramid. We utilize the current advances in image classification, such as local-constrained linear coding (LLC) <ref type="bibr" target="#b8">[9]</ref> for feature coding and spatial pyramid matching (SPM) <ref type="bibr" target="#b13">[14]</ref> in our BCF shape classification framework. Both LLC and SPM are seldom used in shape analysis. LLC utilizes the locality constraints and encodes each descriptor with its local-coordinate system in a codebook. In practice, it first performs k-nearest-neighbor search to find localcoordinates for feature to be encoded, and then solves a constrained least square fitting problem on the local-coordinates. The state-ofthe-art performance on PASCAL VOC <ref type="bibr" target="#b14">[15]</ref> image classification has shown effectiveness of LLC. SPM is a simple and computationally efficient extension of the orderless BoW model for image representation. It works by partitioning the image into increasingly fine subregions and computing histograms of local features found inside each sub-region. Histograms of different sub-regions are concatenated as final image representation. SPM can capture the spatial information in contour fragments which are useful for shape recognition. BCF naturally utilizes LLC and SPM to improve the accuracy of shape classification.</p><p>One of the major difficulties involved in shape classification for many shape-matching based algorithms is to directly match two shapes with large deformation since shapes are only partially similar to each other. BCF can easily solve this problem caused by large shape deformation, and is good at classifying shapes with partial similarity. As each shape contour is divided into contour fragments in BCF, the contour fragments contain partial shape information. After coding, a discriminative classifier such as SVM or Adaboost can be used to select the representative and informative contour parts for each shape category. Fig. <ref type="figure" target="#fig_4">4</ref> shows some contour fragments selected by linear SVM in four shape categories in our experiments. We can see that even though contour fragments are parts of the shapes, they are very informative for recognizing shape category. Thus, BCF is able to deal with partial occlusion in shape, especially, in the edge map extracted from real image. Besides, we find that BCF is also robust to noisy contour in our experiments.</p><p>In summary, the proposed BCF has several good properties:</p><p>1. It provides a very compact shape representation which is a single vector rather than a set of feature vectors used in many other methods.</p><p>2. It precisely preserves information of individual shape contour via LLC and spatial layout of contour fragments in one shape via SPM. 3. For shape classification it avoids pairwise matching between local shape descriptors and significantly reduces the time cost. 4. It is robust to the shapes with occlusions or parts missing, and can be easily applied to real image classification.</p><p>The rest of the paper is organized as follows. We review the related works in Section 2. Then, we introduce the details of our shape representation with BCF in Section 3, including extracting, encoding and pooling contour fragments, and so on. We evaluate the proposed method on several popular shape benchmarks, illustrate good properties of BCF in applications, and demonstrate its effectiveness in shape classification in various datasets in Section 4. Finally, we conclude this paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Here, we briefly review the recent progress in shape classification. Sun and Super <ref type="bibr" target="#b15">[16]</ref> proposed a shape classification framework for recognizing contour shapes using class contour segments as input features with Bayesian classifier. Bai et al. <ref type="bibr" target="#b5">[6]</ref> adopted contour segments and skeleton paths as the input features for shape classification with a Gaussian mixture model. Daliri and Torre <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> transformed the contour points into a symbol representation, and then used the edit distance between pair of strings is used for classification with a kernel support vector machine. Wang et al. <ref type="bibr" target="#b18">[19]</ref> proposed a tree-union <ref type="bibr" target="#b19">[20]</ref> representation as the prototype for each shape category, and performed shape classification is determined by the shape similarity between a test shape and each prototype. Edem and Tari <ref type="bibr" target="#b20">[21]</ref> also used a skeletal tree model to represent the prototype of each category, and then used the edit distance between a given shape and each prototype is used as the input feature for a linear SVM. Thus, each prototype in <ref type="bibr" target="#b20">[21]</ref> can be considered as a shape codebook. Shape classification by skeleton matching has been studied by <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>.</p><p>Various shape descriptors have been proposed for shape matching and recognition. There are some region-based methods, such as Zernike moments <ref type="bibr" target="#b25">[26]</ref> and generic Fourier descriptor <ref type="bibr" target="#b26">[27]</ref>. Other methods based on contour include curvature scale space (CSS) <ref type="bibr" target="#b3">[4]</ref>, multi-scale convexity concavity (MCC) <ref type="bibr" target="#b27">[28]</ref>, triangle area representation (TAR) <ref type="bibr" target="#b4">[5]</ref>, hierarchical procrustes matching (HPM) <ref type="bibr" target="#b28">[29]</ref>, shape-tree <ref type="bibr" target="#b29">[30]</ref>, contour flexibility <ref type="bibr" target="#b30">[31]</ref>, shape context (SC) <ref type="bibr" target="#b1">[2]</ref>, inner-distance shape context (IDSC) <ref type="bibr" target="#b2">[3]</ref> and so on. In this paper, we only use shape context to describe contour fragments in BCF. Generally speaking, most of these shape descriptors can be adopted as low-level representation in BCF, since each contour fragment can be considered as a shape. We use discrete contour evolution (DCE) <ref type="bibr" target="#b12">[13]</ref> for decomposing shape into contour fragments. Other recent shape evolution methods, e.g., <ref type="bibr" target="#b31">[32]</ref>, can also be adopted in BCF.</p><p>Our BCF approach can be considered as a two-layer feature learning framework on shape contours. In the first layer, contour fragment features are encoded into shape codes using localconstrained linear coding (LLC), which is first proposed in <ref type="bibr" target="#b8">[9]</ref> for encoding SIFT <ref type="bibr" target="#b10">[11]</ref> features in real images. Other feature coding methods include fisher kernel (FK) <ref type="bibr" target="#b9">[10]</ref> and kernel codebook encoding (KCB) <ref type="bibr" target="#b32">[33]</ref>; we choose LLC for its high efficiency. In the second layer, we use spatial pyramid matching (SPM) for pooling the shape codes. Theory of SPM is given in <ref type="bibr" target="#b33">[34]</ref>. SPM is first proposed by Lazebnik et al. for image classification in <ref type="bibr" target="#b13">[14]</ref>. Recently deep learning is very popular for feature learning and obtains good results on large-scale image classification <ref type="bibr" target="#b34">[35]</ref>. Different from BCF, deep learning approaches have more layers; in <ref type="bibr" target="#b35">[36]</ref>, for example, a shape model based on deep Boltzmann machine called shape Boltzmann machine (SBM) is proposed; SBM directly works on raw shape pixels and learns probability distributions over object shapes, which is good at shape completion task but hardly works on the challenging Mpeg-7 shape dataset <ref type="bibr" target="#b36">[37]</ref> for shape classification. However, BCF learns shape representation over shape contours, which is more robust to deformation of shape and more suitable for shape recognition. Textures feature and BoW model are directly used for shape classification in <ref type="bibr" target="#b37">[38]</ref>. BCF utilizes contour fragment as shape feature, which is obviously superior to <ref type="bibr" target="#b37">[38]</ref>.</p><p>The strategy of partitioning shape into contour parts for shape recognition has been adopted by <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b5">6]</ref>. Unlike these previous works where contour parts are put in an orderless set as shape representation, BCF explores the spatial layout of contour parts and builds a compact shape representation via feature coding and pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bag of contour fragments</head><p>In this section, given a shape S, we show how to build BCF shape representation fðSÞ for S and use fðSÞ for shape classification step by step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contour fragments</head><p>Contour fragments have been validated as powerful shape features in several previous approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>, since they contain both local and global shape information. We adopt contour fragments as basic shape features for learning a shape codebook and building our shape representation. An object boundary can be decomposed into contour fragments in different ways, such as dense sampling and sampling based on curvature like in <ref type="bibr" target="#b15">[16]</ref>.</p><p>Here, we use a more robust technique named discrete contour evolution (DCE) <ref type="bibr" target="#b12">[13]</ref> for partitioning the whole object contour into meaningful contour fragments. Let SðtÞ ¼ ðxðtÞ; yðtÞÞ be the outer contour of a shape S parameterized by t A ½0; 1. We first apply the DCE to obtain a simplified polygon on S with vertices denoted as</p><formula xml:id="formula_0">u ! ¼ ðu 1 ; …; u T Þ;</formula><p>where T denotes number of vertices, which is not previously known but can be automatically computed given a threshold parameter τ. u ! includes critical points on S. Given an object contour S, its contour fragments set is denoted by CðSÞ, which are the segments between every pair of critical points ðu i ; u j Þ. Let c ij denote the contour fragment between u i and u j , we have</p><formula xml:id="formula_1">CðSÞ ¼ fc ij ¼ ðu i ; u j Þ; i a j; i; j A ½1; …; Tg:<label>ð1Þ</label></formula><p>Note that u i and u j do not have to be adjacent to each other. Also</p><formula xml:id="formula_2">S ¼ c ij [ c ji ;<label>ð2Þ</label></formula><p>since one represents a fragment and the other is its counterpart. CðSÞ contains very rich information in shape S, since contour fragments between all pairs of critical points are extracted. All the contour fragments extracted from a shape contain multi-scale information, which can be summarized as short-range, middle-range and longrange information as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. Therefore, contour fragments are totally different from local descriptors (SIFT, HOG, or LBP, etc.) for image classification, since the local descriptors only contain information of local patches in image. In the rest of this section, we will show how we describe contour fragments and how we select informative contour fragments for shape recognition.</p><p>For each contour fragment c ij , we describe it using shape context x ij A R dÂ1 where d is the dimension of the feature vector of c ij . As illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>, x ij is computed as follows: we sample 5 reference points on c ij from u i to u j equidistantly, and then compute 5 shape context histograms based on the reference points individually. Shape context descriptor for c ij is a concatenation of the 5 shape context histograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoding of contour fragments</head><p>Encoding contour fragment features x ij is to map feature vectors of contour fragments into a new space B spanned by a shape codebook B; in this new space, contour fragments are represented by shape codes w ij .</p><p>Many codebook learning methods have been proposed for image representation, including unsupervised methods <ref type="bibr" target="#b8">[9]</ref> and supervised method <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, and so on. In this paper, we choose kmeans <ref type="bibr" target="#b41">[42]</ref> as the codebook learning since it is a simple yet stable one. A set of training shape features is randomly selected from all the contour fragment features. We then run k-means algorithm on the selected shape features for clustering. The clustering centers are used as shape codebook B ¼ ½b 1 ; …; b M A R dÂM , where each column is a clustering center. So the obtained M clustering centers can be approximately considered as M prototypes for describing the whole shape space.</p><p>To compute shape codes x ij , a traditional way is to do vector quantization (VQ) like in <ref type="bibr" target="#b13">[14]</ref>. VQ only assigns a shape feature x ij to its nearest neighbor in shape codebook B; it is fast but its quantization error is large. Local-constraint linear coding (LLC) is  a very good choice for feature coding proposed in <ref type="bibr" target="#b8">[9]</ref>, as it is both fast and effective. The LLC method is inspired by the theory of local linear embedding (LLE) <ref type="bibr" target="#b42">[43]</ref>. To represent x ij in the space B spanned by shape codebook B, LLC uses k nearest neighbors in B as local bases for x ij to form a local coordinate system. The k nearest neighbors of x ij are denoted as B π ij A R dÂk where π ij is a set containing the indexes of the k nearest neighbors in B, denoted as</p><formula xml:id="formula_3">π ij ¼ fπ 1 ij ⋯π k ij g. B π ij is a matrix consisting of the π 1 ij ⋯π k ij th columns of B.</formula><p>Following the assumption in LLE, we expect that x ij and its nearest neighbors lie on or close to a local linear patch of the manifold. The local geometry of x ij and B π ij can be characterized by linear coefficients obtained through reconstructing x ij from B π ij . The coefficients w π ij A R kÂ1 can be obtained by solving the following minimization problem: min</p><formula xml:id="formula_4">wπ ij J x ij ÀB π ij w π ij J 2 s:t: 1 T w π ij ¼ 1;<label>ð3Þ</label></formula><p>where weight vector w π ij summarizes the contributions of local bases to x ij 0 s reconstruction, which is required to be summed to 1. The minimization problem in ( <ref type="formula" target="#formula_4">3</ref>) is a small-scale least square problem, and its time complexity is Oðk 2 Þ. In our experiments, we always set the value of k as 5. We denote code of x ij as w ij A R dÂ1 :</p><p>values of the π<ref type="foot" target="#foot_1">1</ref> ij ⋯π k ij th entries of w ij are equal to w π ij and the rest of entries in w ij are set to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Max-pooling with spatial pyramid</head><p>In this subsection, we build a compact shape representation based on statistics of shape codes w ij . In addition, we utilize spatial pyramid matching (SPM) <ref type="bibr" target="#b13">[14]</ref> method to add spatial layout information of contour fragments into our shape representation.</p><p>The process of building shape representation is given as follows: First, we divide shape into different regions. Specifically, shape is divided into 1 Â 1, 2 Â 2 and 4 Â 4 regions, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(f); in total, there are 21 regions. Then for each region Region r ; r A ½1; …; 21, we do max-pooling. Let w z denote an encoded contour fragment in the position of z in shape (position of a contour fragment is defined as its the median point). Maxpooling works as follows:</p><formula xml:id="formula_5">fðS; rÞ ¼ maxðw z jz A Region r Þ;<label>ð4Þ</label></formula><p>where the max function works in row-wise, returns a feature vector of Region r , fðS; rÞ, with the same size as w ij . For each codeword, we take the max value of all shape codes in a region for shape representation, so we called this method as max-pooling.</p><p>Max-pooling procedure is well established by biophysical evidence in visual cortex (V1) <ref type="bibr" target="#b43">[44]</ref>. Its correctness empirically verified by many algorithms applied into image classification, such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b9">10]</ref>, etc. It also works well with linear classifiers. Final representation fðSÞ for shape S is a concatenation of the feature vectors for all regions.</p><p>fðSÞ ¼ ½fðS; 1Þ T ; …; fðS;</p><formula xml:id="formula_6">21Þ T T :<label>ð5Þ</label></formula><p>It is easy to know the dimension of fðSÞ is 21 Â M. SPM can encode spatial information among the short-range contour fragments in a coarse-to-fine way. We train a classifier on training shape to decide whether the classifier fires on a coarse level (1 Â 1 region) or a fine level (2 Â 2 and 4 Â 4 regions). More specifically, if the training shapes are well aligned, it contains similar contour fragments in each small grid, so the classifier will fire on a fine level. On the other hand, if the training shapes are rotated in different directions, it contains different contour fragments in a single small grid; but all contour fragments are contained in the coarse level; so the classifier will fire on a coarse level. Thus, SPM is a very flexible strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Shape classification using linear SVM</head><p>Since our shape representation is a simple vector, we directly adopt SVM for shape classification. For multi-class SVM, we use the formulation proposed by Crammer and Singer in <ref type="bibr" target="#b45">[46]</ref>. Given a set of training shapes ff i g with labels fy i A ½1; …; Ng where N is the number of shape classes. Crammer and Singer 0 s multi-class SVM can be used to solve the following optimization problem: min</p><formula xml:id="formula_7">ω 1 ;…;ωN ∑ N n ¼ 1 J ω n J 2 þ λ∑ i maxð0; 1 þ ω T r i f i À ω T y i f i Þ;<label>ð6Þ</label></formula><p>where r i ¼ arg max n A ½1;…;N;n a y i ω T n f i . In Eq. ( <ref type="formula" target="#formula_7">6</ref>), the left part is a regularization term; the right part is multi-class hinge-loss; parameter λ controls the relative weight of the regularization term. To solve <ref type="bibr" target="#b5">(6)</ref>, we use the off-shelf SVM solver, LibLinear developed by Fan et al. <ref type="bibr" target="#b46">[47]</ref>. In the testing stage, shape label is predicted by</p><formula xml:id="formula_8">b y ¼ arg max n A ½1;…;N ω T n f:<label>ð7Þ</label></formula><p>Learning with SVM is a process of selecting support vectors, during which certain contour fragments important for recognition are selected in every shape. In Fig. <ref type="figure" target="#fig_4">4</ref>, we show some examples. For a shape, we find top 20 values in all f i Á ω y i . We then find the contour fragments that contribute to the top 20 entries in f i ; in other words, we find the contour fragments which have maximal code value in the top 20 entries in f i . As shown in Fig. <ref type="figure" target="#fig_4">4</ref>, the selected contour fragments by coding (with LLC), max-pooling and SVM are meaningful. There are a few trivial fragments in top 20, such as, the 17th and 19th in (c), because the values of their codes are large as it is easier for them to be precisely encoded. But their corresponding value in ω y i is small. Time consuming kernels, such as the RBF kernel and intersection kernel, and so on can further improve the shape classification performance. But for faster speed, we use linear SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we test our method for shape classification on a variety of shape datasets and compare results of our method with the state-of-the-art shape classification approaches, available for those datasets in the literature. We also study the robustness of our method. First of all, we give implementation details as following. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Extracting contour fragments: We use DCE to extract about 400 contour fragments per shape; max curvature τ of DCE is set to 0.5.</p><p>When computing shape context for contour fragment, we have 5 reference points given in Section 3.1, and set number of bins of shape context to be 60 (10 for dividing angle space and 6 for dividing radius space). Thus, dimension of our shape context descriptor for a contour fragment is 300. Besides, positions of contour fragments in shape are also recorded which are used for pooling with spatial pyramid.</p><p>Learning shape codebook: Standard k-means clustering is adopted for training the codebook. The number of the contour segments collected from the dataset could be enormous. As a result, the codebook training can be very time consuming and computationally expensive. Therefore, we randomly select 1000 images and for each image only 300 shape context features are picked for training the codebook. The number of clustering centers set to 1500 if it is not specified. In addition, we will study the performance of BCF with different number of clustering centers.</p><p>Coding, pooling and classification: In the coding scheme, the approximated LLC with 5 nearest neighbors is adopted. When pooling, a shape is divided into 1 Â 1, 2 Â 2, and 4 Â 4, in total 21 regions. The final feature vector for shape representation is normalized by its ℓ 2 norm. For shape classification, a fast offshelf linear SVM toolbox, LibLinear <ref type="bibr" target="#b46">[47]</ref>, is used.</p><p>Datesets: We evaluate our BCF method on shape classification benchmark dataset which is the MPEG-7 dataset <ref type="bibr" target="#b36">[37]</ref>, and use BCF for 70 classes animal classification on Animal dataset <ref type="bibr" target="#b5">[6]</ref>, for leaf classification on Swedish Leaf dataset <ref type="bibr" target="#b2">[3]</ref>, and for multi-view object classification on ETH-80 image dataset <ref type="bibr" target="#b47">[48]</ref>. In the rest of this section, we give experimental results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MPEG-7 dataset</head><p>The MPEG-7 dataset is widely used for shape analysis in the field of computer vision. It has 1400 silhouette images divided into 70 classes with high shape variability. Each class has 20 different shapes (see Fig. <ref type="figure" target="#fig_5">5</ref> for some typical images). We use two strategies for evaluating shape classification performance: (1) half training, we randomly select 10 shapes in each class for training and use the rest shapes for testing in each round; this procedure is repeated for 10 times; average classification accuracy and standard derivation of classification accuracies are reported; (2) leave-oneout, for each shape, we use all shapes except the current one for training and use the current one for testing; average classification accuracy is reported.</p><p>BCF is compared with other shape classification methods in Table <ref type="table">1</ref>. In <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref> contour fragments are used for shape classification. BCF outperforms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref> by over 6% when using half of shapes for training. The superior performance may be attributed to the fact that the discriminative learning via SVM in our approach can maximize the margins between different shape categories and find very informative contour fragments for each category. In <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>,   Table <ref type="table">1</ref> Classification accuracy comparison on Mpeg-7 dataset <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Classification accuracy</head><p>Half training Leave one out Class segment set <ref type="bibr" target="#b15">[16]</ref> 90.9% 97.93% Contour segments <ref type="bibr" target="#b5">[6]</ref> 91.1% -Skeleton paths <ref type="bibr" target="#b5">[6]</ref> 86.7% -ICS <ref type="bibr" target="#b5">[6]</ref> 96.6% -Polygonal multi-resolution <ref type="bibr" target="#b49">[50]</ref> -97.57% String of symbols <ref type="bibr" target="#b48">[49]</ref> -97.36% Robust symbolic <ref type="bibr" target="#b16">[17]</ref> -98.57% Kernel-edit distance <ref type="bibr" target="#b17">[18]</ref> -98.93% BCF 97.16 70.79% 98.93%</p><p>however, all contour fragments have equal weights. Preconditions of discriminative learning with SVM are that BCF provides a compact shape representation and LLC precisely preserves information of contour In <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, shape is described based on the symbolic representation. BCF achieves the state-of-the-art performance when using leave one out for testing which is the same as the result in the most recent work in <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Animal dataset</head><p>The animal dataset was introduced in <ref type="bibr" target="#b5">[6]</ref>, it contains 2000 shapes describing 20 kinds of animals, including horse, rabbit, monkey, and so on. Each category has 100 animals images. Some of shapes form the two most difficult classes (Cat and Monkey) and the easiest class (Spider) in this dataset are shown in Fig. <ref type="figure">6</ref>. The dataset has much more intra-class variability since the same kind of animals may have various gestures. We use 50 shapes randomly selected per class for training and the rest of shapes for testing. We run experiments for 10 times and average classification accuracy of our method is compared with that of other methods in Table <ref type="table">2</ref>.</p><p>As shown in Table <ref type="table">2</ref>, the proposed BCF method obtains a classification accuracy of 83.40% which significantly outperforms the classical shape descriptor, inner distance shape context <ref type="bibr" target="#b2">[3]</ref>, and the previous state-of-the-art method <ref type="bibr" target="#b5">[6]</ref> which integrates contour segments and skeleton paths for shape classification. Average classification accuracy for each of the 20 classes in Animal dataset is reported in Table <ref type="table" target="#tab_1">3</ref>. BCF dramatically improves classification accuracy in Cat and Monkey classes. This shows that BCF can capture the intra-class partial similarity within the highly deformed objects from Animal dataset. Bag of SIFT method <ref type="bibr" target="#b37">[38]</ref> directly uses texture feature for shape classification, obtains a classification accuracy of 74.9% which is much lower than BCF 0 s accuracy. This shows that our contour fragment feature is more suitable for shape classification than SIFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Swedish Leaf dataset</head><p>In this subsection, we use BCF for leaf image recognition on the Swedish Leaf Dataset <ref type="bibr" target="#b50">[51]</ref>. The Swedish leaf dataset comes from a leaf classification project at Linköping University and Swedish Museum of Natural History. The dataset contains isolated leaves from 15 different Swedish tree species, with 75 leaves per species. Some typical binary shapes of leaf images are shown in Fig. <ref type="figure">7</ref>. Note that some species are indistinguishable to the untrained eye, e.g., the 1st, 3rd, 9th, 11th and 15th species. We follow the experimental setting in <ref type="bibr" target="#b2">[3]</ref>. In each species, 25 shapes are randomly selected for training and the rest of shapes are used for testing. We run training and testing for 10 times and report the average and standard deviation of the classification accuracies. We compare classification accuracy of our method with other pure shape-based recognition methods in Table <ref type="table">4</ref>. The methods compared include a preliminary work <ref type="bibr" target="#b50">[51]</ref> using some simple features like moments, area and curvature and so on, the Fourier descriptor, the shape context with dynamic programming (SCþ DP), the inner distance shape context with dynamic programming (IDSCþ DP), the multi-scale matrix distance matrix <ref type="bibr" target="#b51">[52]</ref>, the morphological strategy method in <ref type="bibr" target="#b52">[53]</ref>, a robust symbolic representation method <ref type="bibr" target="#b16">[17]</ref> and the shape-tree method in <ref type="bibr" target="#b29">[30]</ref>. BCF obtains the state-of-the-art performance among these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">ETH-80 dataset</head><p>The ETH-80 dataset <ref type="bibr" target="#b47">[48]</ref> contains 80 3-D high resolution objects (Fig. <ref type="figure" target="#fig_6">8</ref>) from eight categories. For each object, there are 41 color images from different viewpoints. So the dataset contains Fig. <ref type="figure">6</ref>. Some shapes form Animal dataset <ref type="bibr" target="#b5">[6]</ref>. The first, second and last row show 8 shapes from Cat (1st row), Monkey (2nd row) and Spider (3rd row) class from this dataset respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Classification accuracy comparison on Animal dataset <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Classification accuracy (%)</p><p>Class segment set <ref type="bibr" target="#b15">[16]</ref> 69.7 IDSC <ref type="bibr" target="#b2">[3]</ref> 73.6 Bag of SIFT <ref type="bibr" target="#b37">[38]</ref> 74.9 Contour segments <ref type="bibr" target="#b5">[6]</ref> 71.7 Skeleton paths <ref type="bibr" target="#b5">[6]</ref> 67.9 ICS <ref type="bibr" target="#b5">[6]</ref> 78.4 BCF 83.407 1.30 Fig. <ref type="figure">7</ref>. Typical shape of images from the Swedish leaf dataset <ref type="bibr" target="#b50">[51]</ref>. One image from each species.</p><p>3280 images in total. Segmentation masks of all images are provided to evaluate shape-based object recognition approaches with this dataset. The test mode of this is leave-oneobject-out cross-validation <ref type="bibr" target="#b47">[48]</ref>. Specifically, in each round images from 79 objects are used for training and images from the remaining one object are used for testing. We compare the average classification accuracy of BCF to many other previous approaches in Table <ref type="table">5</ref>. BCF gets a classification accuracy of 91.49% which outperforms pervious state-of-the-art approach in <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Robustness to noise</head><p>In the above experiments, the shapes are quite smooth in these datasets. To evaluate the performance of our descriptor under noisy conditions, we add Gaussian noise to shape boundaries and carry out image classification using BCF. We use the whole Mpeg-7 dataset <ref type="bibr" target="#b36">[37]</ref> as the original shape boundaries. Noise is added by perturbing all pixels on each shape contour in both x-and y-coordinates by values drawn from a Gaussian random variable with zero mean and standard derivation s. As the parameter s increases, we add increasing Gaussian noise to the shape boundaries. Fig. <ref type="figure">9</ref> shows an example of shape boundaries with increasing Gaussian noise. We report the classification accuracies using half for training and leaving one out for testing as noise ratio s varying from 0 to 1 in Fig. <ref type="figure" target="#fig_0">10</ref>. Classification accuracy using half training drops about 4% when s increases from 0 to 1, which shows that BCF is robust to noise. This is due to the fact that both DCE and the shape context are robust to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">The effect of codebook size</head><p>In this experiment, we do shape classification using shape codebooks of different sizes on the full Mpeg-7 dataset. Shape classification accuracies of BCF using codebooks of different sizes are reported in Fig. <ref type="figure" target="#fig_0">11</ref>. Generally, shape classification accuracy improves as the size of codebook increases, but gets saturated when codebook size increases to 1500. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Classification accuracy comparison on ETH-80 dataset <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Classification accuracy (%)</p><p>Color histogram <ref type="bibr" target="#b47">[48]</ref> 64.86 PCA gray <ref type="bibr" target="#b47">[48]</ref> 82.99 PCA masks <ref type="bibr" target="#b47">[48]</ref> 83.41 SC þDP <ref type="bibr" target="#b47">[48]</ref> 86.40 IDSC þDP <ref type="bibr" target="#b2">[3]</ref> 88.11 IDSC þMorphological strategy <ref type="bibr" target="#b52">[53]</ref> 88.04 Height function <ref type="bibr" target="#b53">[54]</ref> 88.72 Robust symbolic <ref type="bibr" target="#b16">[17]</ref> 90.28 Kernel-edit <ref type="bibr" target="#b17">[18]</ref> 91.33 BCF 91.49 Fig. <ref type="figure">9</ref>. An example of shape boundaries with increasing Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Classification accuracy comparison on Swedish leaf dataset <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Classification accuracy (%)</head><p>Moment þ AreaþCurvature <ref type="bibr" target="#b50">[51]</ref> 82 Fourier <ref type="bibr" target="#b2">[3]</ref> 89.6 SC þ DP <ref type="bibr" target="#b2">[3]</ref> 88.12 IDSC þ DP <ref type="bibr" target="#b2">[3]</ref> 94.13 MDM <ref type="bibr" target="#b51">[52]</ref> 93.60 IDSC þ Morphological strategy <ref type="bibr" target="#b52">[53]</ref> 94.80 Robust symbolic <ref type="bibr" target="#b16">[17]</ref> 95.47 Shape-tree <ref type="bibr" target="#b29">[30]</ref> 96.28 BCF 96.567 0.67</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Generalization ability of shape codebook</head><p>In this experiment, we investigate the generalization ability of the shape codebook learned by k-means. As the space of contour fragments of is much smaller than the space of local features of natural images, e.g., SIFT and HOG, we investigate whether it is possible to learn a universal codebook of contour fragments for shape classification. Therefore, we use the codebook learned from Mpeg-7 dataset for descriptor coding, building shape representation and performing shape classification on Animal dataset. We also use the codebook learned from Animal dataset for descriptor coding, building shape representation and performing shape classification on the Mpeg-7 dataset. The sizes of both codebooks are 1500. Except the codebook, all other experimental settings are the same. We call this experiment "codebook exchanging". Shape classification results (half shapes for training for both datasets) are shown in Table <ref type="table">6</ref>. The results show that there is only about one percentage drops in classification accuracy after codebook exchanging on both datasets. These results show that the generalization ability of our shape codebook is very good. The reason why codebook exchanging can work is that different datasets share lots of common contour fragments. For example, the legs of a horse are very similar to the ones of a dog, and the leaf of an apple may be very similar to the wing of a bat. The success of codebook exchanging implies that we may use a universal shape codebook for all codebook-based shape recognition system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Image classification on Caltech 101 dataset</head><p>The Caltech 101 dataset contains 9144 images in 101 object classes including animals, vehicles, flowers, and so on, with significant variance in shape, color and texture, and a background class. The number of images per category varies from 31 to 800. We follow the common experiment setup for Caltech 101, training on 30 images per class and testing on the rest, and measure the performance using average accuracy over the 102 classes.</p><p>The color/gray images in Caltech 101 dataset are different from the binary shapes we tested in the previous experiments. Now we show how to use the proposed BCF approach to build an image representation for a color/gray image. Given a color/gray image, we first compute its edge map using the gPB algorithm in <ref type="bibr" target="#b0">[1]</ref> (some of the edge maps are shown in Fig. <ref type="figure" target="#fig_7">12(b</ref>) and (e)), and set all the pixels on the edge map with their values larger than 0.1n255 as edge pixels. Then, the edge-linking algorithm in <ref type="bibr" target="#b54">[55]</ref> is applied on the binary edge image to retrieve a set of contours shown in Fig. <ref type="figure" target="#fig_7">12(c</ref>) and (f). Finally, steps (b)-(g) in Fig. <ref type="figure" target="#fig_0">1</ref> are taken to build image representation. Similar to shape classification, we use linear SVM for image classification.</p><p>Comparison with SIFT-based method: We directly compare our contour fragment feature in BCF with dense SIFT feature in <ref type="bibr" target="#b8">[9]</ref> Table <ref type="table">6</ref> Classification accuracy before and after shape codebook exchanging.  using the same coding method (LLC), the same pooling method (SPM), the codebooks of the same size (1024) in Table <ref type="table" target="#tab_2">7</ref>, and the same classifier (linear SVM). The results of LLC <ref type="bibr" target="#b8">[9]</ref> and RBC [56] are obtained running the source code released the authors. The performance of BCF with SPM is 54.5%, which is worse than 71.7% of the SIFT feature with LLC and SPM. Contour fragment feature performs worse for two reasons: (1) some object contours (e.g., the outline of car in Fig. <ref type="figure" target="#fig_7">12</ref>) and some object parts (e.g., the noses of person in Fig. <ref type="figure" target="#fig_7">12</ref>) are missing in the edge maps; even though the edge maps are obtained by the state-of-the-art edge detector;</p><p>(2) contextual information, such as, the ground in car image and the grass and tree in the elephant image, cannot be captured by our contour fragment feature. All this information is useful for recognition and can be capture by SIFT feature. Although, BCF performs worse than SIFT, we show BCF and SIFT feature are complementary to each other in Table <ref type="table" target="#tab_2">7</ref>. LLC <ref type="bibr" target="#b8">[9]</ref> and RBC <ref type="bibr">[56]</ref> are two SIFT based approaches; by combining BCF with them using the simple LP-β method in <ref type="bibr" target="#b56">[57]</ref>, the average image classification accuracy can be improved by 3.7% and 2.2%, respectively.</p><p>Comparison with previous shape-based method: We implemented shape context <ref type="bibr" target="#b43">[44]</ref> method for image classification by setting 16 reference points in binary edge image 960-dimensional feature vector. Then we use a linear SVM for image classification based on the shape context feature vector. The average image classification accuracy of shape context feature is only 3%. Both shape context and BCF are pure shape-based method. By diving contour into fragments and encoding their shape context features, BCF can obtain an average classification accuracy of 54.5% which is a significant improvement. It means that BCF is more robust to occlusions/edgebroken in real image than the previous shape descriptor.</p><p>The effectiveness of spatial pyramid: In Table <ref type="table" target="#tab_2">7</ref>, we show that from level 1 Â 1 to 4 Â 4 the accuracy of BCF improves from 23.9% to 51.7%; by combining the four levels, the accuracy of BCF (denoted as "pyramid") is 54.5%. This shows that SPM is effective for BCF in image classification.</p><p>We also quote some results from very recent literatures, e.g., <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>, and a classical method called SVM-KNN in <ref type="bibr" target="#b57">[58]</ref> in Table <ref type="table" target="#tab_2">7</ref>. In summary, we give comprehensive studies of BCF for real image classification and show good performance by combining BCF with SIFT-based methods, which is better than the most recent results in <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we present a novel shape representation called BCF for shape classification. To the best of our knowledge, this is the first paper that introduces the idea of BoW together with LLC and SPM for shape representation. Since BCF is a part-based model, it is intrinsically robust to occlusion and deformation of shape. In the experiments, we have extensively tested the performance of BCF; all these experimental results on shape benchmarks show that BCF is able to achieve the state-of-the-art performance; moreover, we have tested BCF for image classification on the real image dataset and stress it can dramatically outperform the other shape-based method and is complementary to the texture descriptor. In the future, we will study how to use BCF for object recognition in real image; for example, on edge map extracted from real image, BCF can either do object recognition by combining with sliding window method, or provide shape cue for other off-shelf object detectors. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pipeline of building shape representation using BCF. (a) Shows contour of a shape; (b) shows critical points detected using DCE method; (c) shows some contour fragments in blue color; (d) shows that we use shape context [2] to describe each contour fragment; (e) shows shape codes; (f) shows we use 1 Â 1, 2 Â 2, and 4 Â 4 spatial pyramid for max-pooling; (g) shows the histogram for shape representation. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc><graphic coords="2,122.54,58.64,360.00,119.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 1(b) shows the critical points extracted by DCE for an input contour S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Three exemplar contour fragments containing short-range (left), middlerange (middle) and long-range (right) information in shape.</figDesc><graphic coords="3,337.10,648.83,180.35,66.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Shape context feature for contour fragment c ij . Note that circles in this figure are plotted for showing the positions to compute shape context which do not stand for the area in which to compute shape context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top 20 contour fragments which contribute the most for recognition in a butterfly shape (a), an elephant shape (b), a camel shape (c), and a bat shape (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Typical shapes from the Mpeg-7 dataset [37]. One image from each class.</figDesc><graphic coords="5,130.73,372.46,324.00,72.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. 80 3-D objects from ETH-80 image set. Each row shows one category.</figDesc><graphic coords="7,130.73,474.75,324.00,257.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Example images in Caltech 101 dataset in (a) and (d) together with their gPB edge maps [1] in (b) and (e) and binary shapes obtained by post processing in (c) and (f).</figDesc><graphic coords="8,122.54,595.95,360.00,136.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .Fig. 10 .</head><label>1110</label><figDesc>Fig. 11. Classification accuracies of using half for training and leaving one out for testing are reported as size of codebook changing from 100 to 1800.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Detailed classification accuracy on Animal dataset<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell>Method</cell><cell>Bird (%)</cell><cell>Butterfly (%)</cell><cell>Cat (%)</cell><cell>Cow (%)</cell><cell>Crocodile (%)</cell><cell>Deer (%)</cell><cell>Dog (%)</cell><cell>Dolphin (%)</cell><cell>Duck (%)</cell><cell>Elephant (%)</cell></row><row><cell>CS [6]</cell><cell>76</cell><cell>89</cell><cell>39</cell><cell>70</cell><cell>54</cell><cell>69</cell><cell>69</cell><cell>87</cell><cell>83</cell><cell>95</cell></row><row><cell>ICS [6]</cell><cell>76</cell><cell>93</cell><cell>48</cell><cell>80</cell><cell>66</cell><cell>79</cell><cell>75</cell><cell>89</cell><cell>89</cell><cell>97</cell></row><row><cell>BCF</cell><cell>87.6</cell><cell>92.2</cell><cell>73.8</cell><cell>77.4</cell><cell>76.8</cell><cell>90.4</cell><cell>82.6</cell><cell>89.0</cell><cell>87.0</cell><cell>95.2</cell></row><row><cell>Method</cell><cell>Fish (%)</cell><cell>Fly-bird (%)</cell><cell>Hen (%)</cell><cell>Horse (%)</cell><cell>Leopard (%)</cell><cell>Monkey (%)</cell><cell>Rabbit (%)</cell><cell>Rat (%)</cell><cell>Spider (%)</cell><cell>Tortoise (%)</cell></row><row><cell>CS [6]</cell><cell>70</cell><cell>57</cell><cell>89</cell><cell>96</cell><cell>56</cell><cell>21</cell><cell>81</cell><cell>52</cell><cell>98</cell><cell>81</cell></row><row><cell>ICS [6]</cell><cell>74</cell><cell>65</cell><cell>94</cell><cell>97</cell><cell>65</cell><cell>33</cell><cell>87</cell><cell>84</cell><cell>100</cell><cell>90</cell></row><row><cell>BCF</cell><cell>79.8</cell><cell>72.0</cell><cell>94.2</cell><cell>95.4</cell><cell>66.4</cell><cell>58.4</cell><cell>85.8</cell><cell>70.6</cell><cell>99.2</cell><cell>93.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 7</head><label>7</label><figDesc>Classification accuracy on Caltech 101 dataset.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Average accuracy (%)</cell></row><row><cell>SVM-KNN [58]</cell><cell></cell><cell>66.27 0.4</cell></row><row><cell>SLRR [59]</cell><cell></cell><cell>73.6</cell></row><row><cell>LSGC [60]</cell><cell></cell><cell>75.1</cell></row><row><cell>LLC [9]</cell><cell></cell><cell>71.7 70.8</cell></row><row><cell>RBC [56]</cell><cell></cell><cell>75.6 7 0.8</cell></row><row><cell>Shape context [56]</cell><cell></cell><cell>3.0 7 0.7</cell></row><row><cell>BCF</cell><cell>Level 1 Â 1</cell><cell>23.9 7 0.8</cell></row><row><cell></cell><cell>Level 2 Â 1</cell><cell>40.9 7 0.7</cell></row><row><cell></cell><cell>Level 3 Â 3</cell><cell>49.87 0.7</cell></row><row><cell></cell><cell>Level 4 Â 4</cell><cell>5 1 . 7 7 1.2</cell></row><row><cell></cell><cell>Pyramid</cell><cell>54.5 7 1.5</cell></row><row><cell></cell><cell>Pyramidþ LLC</cell><cell>75.4 7 0.8</cell></row><row><cell></cell><cell>Pyramidþ RBC</cell><cell>77.8 7 1.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X. Wang et al. / Pattern Recognition 47 (2014) 2116-2125</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>MATLAB code of these experiments is available at https://bitbucket.org/ xinggangw/bcf X. Wang et al. / Pattern Recognition 47 (2014)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>[2116][2117][2118][2119][2120][2121][2122][2123][2124][2125] </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewer for their helpful suggestions. This work was supported by National Natural Science Foundation of China (NSFC) Grants 61222308 and 61173120, the Program for New Century Excellent Talents in University in China, National Science Foundation (NSF) Grants OIA-1027897 and IIS-1302164, and Fundamental Research Funds for the Central Universities (HUST 2013TS115). We thank Mr. Ho Simon Wang for assisting to improve the language of this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>None declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape classification using the inner-distance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="286" to="299" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient and robust retrieval by shape content through curvature scale space</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mokhtarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ser. Softw. Eng. Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shape retrieval using triangle-area representation and dynamic space warping</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alajlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">El</forename><surname>Rube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1911" to="1920" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating contour and skeleton for shape classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="360" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
		<title level="m">Visual categorization with bags of keypoints, in: Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europe Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convexity rule for shape decomposition based on discrete contour evolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lakämper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="441" to="454" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classification of contour shapes using class segment sets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Super</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="727" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust symbolic representation for shape recognition and retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daliri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1782" to="1798" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shape recognition based on kernel-edit distance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daliri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1097" to="1103" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shape classification using treeunions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-G</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="983" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning using a mixture of tree-unions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="954" to="967" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A similarity-based approach for shape classification using Aslan skeletons</title>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2024" to="2032" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shock graphs and shape matching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13" to="32" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A skeletal measure of 2d shape similarity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Path similarity skeleton graph matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1282" to="1292" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dissimilarity between two skeletal trees in a context</title>
		<author>
			<persName><forename type="first">E</forename><surname>Baseski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="370" to="385" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A region-based shape descriptor using Zernike moments, Signal Process</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="95" to="102" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generic fourier descriptor for shape-based image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="425" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A multiscale representation method for nonrigid shapes with a single closed contour</title>
		<author>
			<persName><forename type="first">T</forename><surname>Adamek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="742" to="753" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical procrustes matching for shape retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mcneill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="885" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical matching of deformable shapes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tang, 2d shape matching by contour flexibility</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="180" to="186" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptually motivated shape evolution with shape-preserving property</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lewin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clausing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lost in quantization: improving particular object retrieval in large scale image databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: discriminative classification with sets of image features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1458" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The shape Boltzmann machine: a strong model of object shape</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="406" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shape descriptors for non-rigid shapes with a single closed contour</title>
		<author>
			<persName><forename type="first">L</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lakamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eckhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="424" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shape classification using local and global features</title>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Symposium on Image and Video Technology</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="115" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detection and recognition of contour parts based on shape similarity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2189" to="2199" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Randomized clustering forests for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1632" to="1646" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supervised translation-invariant sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3517" to="3524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern classification and scene analysis</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object recognition with features inspired by visual cortex</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="994" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">LIBLINEAR: a library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Analyzing appearance and contour based methods for object categorization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">409</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shape recognition and retrieval using string of symbols</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daliri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust shape similarity retrieval based on contour segmentation polygonal multiresolution and elastic matching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Attalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2229" to="2241" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Söderkvist</surname></persName>
		</author>
		<title level="m">Computer vision classification of leaves from Swedish trees</title>
		<meeting><address><addrLine>Linköping</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multiscale distance matrix for fast plant leaf recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4667" to="4672" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Perceptually motivated morphological strategies for shape retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">6839</biblScope>
			<biblScope unit="page" from="105" to="111" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Shape matching and classification using height functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="134" to="143" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Kovesi</surname></persName>
		</author>
		<ptr target="〈http://www.csse" />
		<title level="m">MATLAB and Octave Functions for Computer Vision and Image Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Centre for Exploration Targeting, School of Earth and Environment, The University of Western Australia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Feature context for image classification and object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Svm-knn: discriminative nearest neighbour classification for visual category recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2126" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning structured low-rank representations for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="676" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">From local similarity to global coding; an application to image classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rabiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2794" to="2801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">to present, he is with the University of California, Los Angeles, as a visiting graduate researcher. He is a reviewer of Pattern Recognition. His research interests include object recognition and shape analysis in computer vision, and machine learning</title>
		<imprint>
			<date type="published" when="2009-05">2009. May 2010 to July 2011. February 2013</date>
			<pubPlace>Wuhan, China; Philadelphia, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Technology and Science ; Electronic Information Engineering from Huazhong University of Technology and Science ; Department of Computer and Information Science, Temple University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently working toward the Ph.D. degree in Department of</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">He is currently an Associate Professor with the Department of Electronics and Information Engineering</title>
		<author>
			<orgName type="collaboration">Bin Feng received the B.S. and Ph.D</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">) as a visiting researcher. His research interests include computer vision and intelligent video processing</title>
		<meeting><address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2006. February 2009 to January 2010</date>
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Science and Technology (HUST) ; Electronics and Computer Science, Hongkong University of Science and Technology (HKUST</orgName>
		</respStmt>
	</monogr>
	<note>respectively, all in electronics and information engineering. he was with the Department of</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">He is currently an Associate Professor with the Department of Electronics and Information Engineering</title>
		<editor>Xiang Bai received the B.S., M.S., and Ph.D</editor>
		<imprint>
			<date type="published" when="2003">2003. 2005. 2009. January 2006 to May 2007. October 2007 to October 2008</date>
			<pubPlace>Wuhan, China; Philadelphia, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Science and Technology (HUST) ; Department of Computer and Information Science, Temple University ; University of California, Los Angeles</orgName>
		</respStmt>
	</monogr>
	<note>respectively, all in electronics and information engineering. he was with the. he was with the. Ph.D. student. His research interests include computer graphics, computer vision, and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">He is currently a Professor and Associate Dean of the Department of Electronics and Information Engineering, HUST. His current research interests include multimedia information processing and computer vision</title>
	</analytic>
	<monogr>
		<title level="m">1986 and the M.S. and Ph.D. degrees in electronics and information engineering from Huazhong University of Science and Technology (HUST)</title>
		<meeting><address><addrLine>Beijing, China; Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991 and 2001</date>
		</imprint>
		<respStmt>
			<orgName>Wenyu Liu received the B.S. degree in computer science from Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>respectively</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">He has published 200 research papers and books. He is an editorial board member of Pattern Recognition and the International Journal of Mathematical Imaging. He received the annual Pattern Recognition Society Award, together with Azriel Rosenfeld, for the best article published in the journal Pattern Recognition in 1998</title>
	</analytic>
	<monogr>
		<title level="m">Longin Jan Latecki received the Ph</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>D. degree in computer science from Hamburg University</orgName>
		</respStmt>
	</monogr>
	<note>He is a professor of computer science at Temple University, Philadelphia. His main research interests include shape representation and similarity, object detection and recognition in images, robot perception, machine learning, and digital geometry. He is the recipient of the 2000 Olympus Prize, the main annual award from the German Society for Pattern Recognition (DAGM). He is a senior member of the IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
