<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<email>dongchao@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite that convolutional neural networks (CNN)   have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27]  and EnhanceNet <ref type="bibr" target="#b37">[38]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution aims at recovering a highresolution (HR) image from a single low-resolution (LR) one. The problem is ill-posed since a multiplicity of solutions exist for any given low-resolution pixel. To overcome this problem, contemporary methods such as those based on deep convolutional neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> constrain the solution space through learning mapping functions from external low-and high-resolution exemplar pairs. To push the solution closer to the natural manifold, new losses are proposed to replace the conventional pixel-wise mean squared error (MSE) loss <ref type="bibr" target="#b6">[7]</ref> that tends to encourage blurry and overly-smoothed results. Specifically, perceptual loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> is introduced to optimize a superresolution model in a feature space instead of pixel space. Ledig et al. <ref type="bibr" target="#b26">[27]</ref> and Sajjadi et al. <ref type="bibr" target="#b37">[38]</ref> further propose adversarial loss to encourage the network to favor solutions building x4 plant x4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Without prior Without prior</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With plant prior</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With building prior</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With building prior</head><p>With plant prior that look more like natural images. With these loss functions the overall visual quality of reconstruction is significantly improved. Though great strides have been made, texture recovery in SR remains an open problem. Examples are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. A variety of different HR patches could have very similar LR counterparts, as shown by the building and plant examples. Generating realistic textures faithful to the inherent class is non-trivial. The results obtained by using perceptual and adversarial losses (without prior) do add fine details to the reconstructed HR image. But if we examine closely, these details are not reminiscent of the textures one would usually observe. Without stronger prior information, existing methods struggle in distinguishing these LR patches and restoring natural and realistic textures thereon.</p><p>We believe that the categorical prior, which characterizes the semantic class of a region in an image (e.g., sky, building, plant), is crucial for constraining the plausible solution space in SR. We demonstrate the effectiveness of categorical prior using the same example in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, we try to restore the visually ambiguous plant and building pairs using two different CNN models, each of which is specially trained on a plant dataset and a building dataset. It is observed that generating realistic textures faithful to  the inherent class can be better achieved by selecting the correct class-dedicated model. This phenomenon is previously documented by Timofte et al. <ref type="bibr" target="#b46">[47]</ref>. They train specialized models for each semantic category on exemplar-based methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46]</ref> and show that SR results can be improved by semantic priors.</p><p>In this study, we wish to investigate class-conditional image super-resolution with CNN. This problem is challenging especially when multiple segments of different classes and sizes co-exist in a single image. No previous work has investigated how categorical priors can be obtained and further incorporated into the reconstruction process. We make this attempt by exploring the possibility of using semantic segmentation maps as the categorical prior. Our experiments embrace this choice and show that segmentation maps encapsulate rich categorical prior up to pixel level. In addition, semantic segmentation results on LR images are satisfactory given a contemporary CNN <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28]</ref> that is fine-tuned on LR images. The remaining question is to find a formulation that allows factorized texture generation in an SR network conditioned on segmentation maps. This is a non-trivial problem. Training a separate SR model for each semantic class is neither scalable nor computationally efficient. Combining LR images with segmentation maps as inputs, or concatenating segmentation maps with intermediate deep features cannot make an effective use of segmentation.</p><p>In this work, we present a novel approach known as Spatial Feature Transform (SFT) that is capable of altering the behavior of an SR network through just transforming the features of some intermediate layers of the network. Specifically, an SFT layer is conditioned on semantic segmentation probability maps, based on which it generates a pair of modulation parameters to apply affine transformation spatially on feature maps of the network. The advantages of SFT are three-fold: (1) It is parameter-efficient. Reconstruction of an HR image with rich semantic regions can be achieved with just a single forward pass through transforming the intermediate features of a single network. <ref type="bibr" target="#b1">(2)</ref> SFT layers can be easily introduced to existing SR network structures. The layers can be trained end-to-end together with the SR network using conventional loss functions. (3) It is extensible. While we consider categorical prior in our study, other priors such as depth maps can also be applied using the proposed SFT layer. We demonstrate the effectiveness of our approach, named as SFT-GAN, in Fig. <ref type="figure" target="#fig_1">2</ref>. More results, a user study, and ablation experiments are provided in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single image super-resolution. Many studies have introduced prior information to help address the ill-posed SR problem. Early methods explore a smoothing prior such as bicubic interpolation and Lanczos resampling <ref type="bibr" target="#b10">[11]</ref>. Image priors such as edge features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>, statistics <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1]</ref> and internal patch recurrence <ref type="bibr" target="#b15">[16]</ref> are employed to improve performance. Dong et al. <ref type="bibr" target="#b9">[10]</ref> train domain specific dictionaries to better recover local structures in a sparse representation framework. Sun et al. <ref type="bibr" target="#b41">[42]</ref> propose context-constrained super-resolution by learning from texturally similar training segments. Timofte et al. <ref type="bibr" target="#b46">[47]</ref> investigate semantic priors by training specialized models separately for each semantic category on exemplar-based methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46]</ref>. In contrast to these studies, we explore categorical priors in the form of segmentation probability maps in a CNN framework.</p><p>Contemporary SR algorithms are mostly learning-based methods, including neighbor embedding <ref type="bibr" target="#b3">[4]</ref>, sparse coding <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> and random forest <ref type="bibr" target="#b38">[39]</ref>. As an instantiation of learning-based methods, Dong et al. <ref type="bibr" target="#b6">[7]</ref> propose SRCNN for learning the mapping of LR and HR images in an end-to-end manner. Later on, the field has witnessed a variety of network architectures, such as a deeper network with residual learning <ref type="bibr" target="#b21">[22]</ref>, Laplacian pyramid structure <ref type="bibr" target="#b25">[26]</ref>, residual blocks <ref type="bibr" target="#b26">[27]</ref>, recursive learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>, and densely connected network <ref type="bibr" target="#b43">[44]</ref>. Multiscale guidance structure has also been proposed for depth map super-resolution <ref type="bibr" target="#b17">[18]</ref>. Different losses have also been proposed. Pixel-wise loss functions, like MSE and Charbonnier penalty <ref type="bibr" target="#b25">[26]</ref>, encourage the network to find an average of many plausible solutions and lead to overly-smooth results. Perceptual losses <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> are proposed to enhance the visual quality by minimizing the error in a feature space. Ledig et al. <ref type="bibr" target="#b26">[27]</ref> introduce an adversarial loss, generating images with more natural details. Sajjadi et al. <ref type="bibr" target="#b37">[38]</ref> develop a similar approach and further explore the local texture matching loss, partly reducing visually unpleasant artifacts. We use the same losses but encourage the network to find solutions under the categorical priors. Enforcing category-specific priors in CNN has been attempted in Xu et al. <ref type="bibr" target="#b47">[48]</ref> but they only focus on two classes of images, i.e., faces and text. Prior is assumed at image-level rather than pixel-level. We take a further step to assume multiple categorical classes to co-exist in an image, and propose an effective layer that enables an SR network to generate rich and realistic textures in a single forward pass conditioned on the prior provided up to the pixel level.</p><p>Network conditioning. Our work is inspired by previous studies on feature normalization. Batch normalization (BN) is a widely used technique to ease network training by normalizing feature statistics <ref type="bibr" target="#b18">[19]</ref>. Conditional Normalization (CN) applies a learned function of some conditions to replace parameters for feature-wise affine transformation in BN. Some variants of CN have proven highly effective in image style transfer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>, visual question answering <ref type="bibr" target="#b5">[6]</ref> and visual reasoning <ref type="bibr" target="#b34">[35]</ref>. Perez et al. <ref type="bibr" target="#b35">[36]</ref> develop a feature-wise linear modulation layer (FiLM), to exploit linguistic information for visual reasoning. This layer can be viewed as a generalization of CN. <ref type="bibr">Perez et al.</ref> show that the affine transformation in CN needs not be placed after normalization. Features can be directly modulated. FiLM shows promising results in visual reasoning. Nonetheless, the formulation cannot handle conditions with spatial information (e.g., semantic segmentation maps) since FiLM accepts a single linguistic input and outputs one scaling and one shifting parameter for each feature map, agnostic to spatial location. Preserving spatial information is crucial for low-level tasks, e.g., SR, since these tasks usually require adaptive processing at different spatial locations of an image. Applying FiLM in SR will result in homogeneous spatial feature modulation, hurting SR quality. The proposed SFT layer addresses this shortcoming. It is capable of converting spatial conditions for not only feature-wise manipulation but also spatial-wise transformation.</p><p>Semantic guidance. In image generation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5]</ref>, semantic segments are used as input conditions to generate natural images. Gatys et al. <ref type="bibr" target="#b13">[14]</ref> use semantic maps to control perceptual factors in neural style transfer. <ref type="bibr" target="#b36">[37]</ref> uses semantic segmentation for video deblurring. Zhu et al. <ref type="bibr" target="#b51">[52]</ref> propose an approach to generate new clothing on a wearer. It first generates human segmentation maps and then uses them to render plausible textures by enforcing region-specific texture rendering. Our work differs from these works mainly in two aspects. First, we use semantic maps to guide texture recovery for different regions in SR domain. Second, we utilize probability maps to capture delicate texture distinction instead of simple image segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given a single low-resolution image x, super-resolution aims at estimating a high-resolution one ŷ, which is as similar as possible to the ground truth image y. Current CNNbased methods use feed-forward networks to directly learn a mapping function G θ parametrized by θ as</p><formula xml:id="formula_0">ŷ = G θ (x).<label>(1)</label></formula><p>In order to estimate ŷ, a specific loss function L is designed for SR to optimize G θ on the training samples,</p><formula xml:id="formula_1">θ = argmin θ i L( ŷi , y i ),<label>(2)</label></formula><p>where (x i , y i ) are training pairs. Perceptual loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> and adversarial loss <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> are introduced to solve the regression-to-the-mean problem that is usually caused by conventional MSE-oriented loss functions. These new losses greatly improve the perceptual quality of reconstructed images. However, the generated textures tend to be monotonous and unnatural, as seen in Fig. <ref type="figure" target="#fig_0">1</ref>. We argue that the semantic categorical prior, i.e., knowing which region belongs to the sky, water, or grass, is beneficial for generating richer and more realistic textures. The categorical prior Ψ can be conveniently represented by semantic segmentation probability maps P, Ψ = P = (P 1 , P 2 , . . . , P k , . . . , P K ),</p><p>where P k is the probability map of k th category and K is the total number of considered categories. To introduce priors in SR, we reformulate Eqn. (1) as</p><formula xml:id="formula_3">ŷ = G θ (x|Ψ),<label>(4)</label></formula><p>where Ψ defines the prior upon which the mapping function can condition. Note that apart from categorical priors, the proposed formulation is also applicable to other priors such as depth information, which could be helpful to the recovery of texture granularity in SR. And one could easily extend the formulation to consider multiple priors simultaneously.</p><p>In the following section, we focus on categorical priors and the way we use them to influence the behavior of an SR network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Feature Transform</head><p>A Spatial Feature Transform (SFT) layer learns a mapping function M that outputs a modulation parameter pair  </p><formula xml:id="formula_4">ŷ = G θ (x|γ, β), (γ, β) = M(Ψ).<label>(5)</label></formula><p>After obtaining (γ, β) from conditions, the transformation is carried out by scaling and shifting feature maps of a specific layer:</p><formula xml:id="formula_5">SFT(F|γ, β) = γ ⊙ F + β,<label>(6)</label></formula><p>where F denotes the feature maps, whose dimension is the same as γ and β, and ⊙ is referred to element-wise multiplication, i.e., Hadamard product. Since the spatial dimensions are preserved, the SFT layer not only performs feature-wise manipulation but also spatial-wise transformation. Figure <ref type="figure" target="#fig_2">3</ref> shows an example of implementing SFT layers in an SR network. We provide more details of the SR branch in Sec. 3.2. Here we focus on the conditioning part. The mapping function M can be arbitrary functions. In this study, we use a neural network for M so that it can be optimized end-to-end with the SR branch. To further share parameters among multiple SFT layers for efficiency, we use a small condition network to generate shared intermediate conditions that can be broadcasted to all the SFT layers. Meanwhile, we still keep few parameters inside each SFT layer to further adapt the shared conditions to the specific parameters γ and β, providing fine-grained control to the features. Segmentation probability maps as prior. We provide a brief discussion on the segmentation network we used. The details are provided in the supplementary material. The LR image is first upsampled to the desired HR size with bicubic interpolation. It is then fed into a segmentation network <ref type="bibr" target="#b30">[31]</ref> as the input. The network is pretrained on the COCO dataset <ref type="bibr" target="#b29">[30]</ref> and then fine-tuned on the ADE dataset <ref type="bibr" target="#b50">[51]</ref> with additional animal and mountain images. We train the network separately from the main SR network.</p><p>For a sanity check, we study the accuracy of segmentation maps obtained from LR image. In a typical setting of SR studies, LR images are downsampled with a scaling factor of ×4 from HR images. We find that under this resolution, satisfactory segmentation results can still be obtained even on LR images given a modern CNN-based segmentation model <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref>. Some LR images and the corresponding segmentation results are depicted in Fig. <ref type="figure" target="#fig_4">4</ref>. As can be observed in Fig. <ref type="figure" target="#fig_4">4</ref>, LR segmentation is close to that of HR. We have not yet tried segmentation on small objects as this remains a challenging problem in the image segmentation community. During testing, classes that fall outside the pre-defined K segmentation classes will be categorized as 'background' class. In this case, our method would still generate a set of default γ and β, degenerating itself as SR-GAN, i.e., treating all classes equally. Discussion. There are alternative ways to introduce categorical priors to an SR network. For instance, one can concatenate the segmentation probability maps with the input LR image as a joint input to the network. We find that this method is ineffective in altering the behavior of CNN (see Sec. 4.3). Another method is to directly concatenate the probability maps with feature maps in the SR branch. This approach resembles the multi-texture synthesis network <ref type="bibr" target="#b28">[29]</ref> <ref type="foot" target="#foot_0">1</ref> . This method, though not as parameter efficient as SFT, amounts to simply adding a post-layer for feature-wise conditional bias. It is thus a special case of  SFT. Another more brute-force approach is to first decompose the LR image based on the predicted semantic class and process each region separately using a model trained specifically for that class. These models may share features to save computation. The final output is generated by combining the output of each model class-wise. This method is computationally inefficient as we need to perform forward passes with several CNN models for a single input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture</head><p>Our framework is based on adversarial learning, inspired by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>. Specifically, it consists of one generator G θ and one discriminator D η , parametrized by θ and η respectively. They are jointly trained with a learning objective given below:</p><formula xml:id="formula_6">min θ max η E y∼pHR log D η (y)+E x∼pLR log(1−D η (G θ (x))),</formula><p>where p HR and p LR are the empirical distributions of HR and LR training samples, respectively.</p><p>The architecture of G θ is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. It consists of two streams: a condition network and an SR network. The condition network takes segmentation probability maps as input, which are then processed by four convolution layers. It generates intermediate conditions shared by all the SFT layers. To avoid interference of different categorical regions in one image, we restrict the receptive field of the condition network by using 1×1 kernels for all the convolution layers.</p><p>The SR network is built with 16 residual blocks with the proposed SFT layers, which take the shared conditions as input and learn (γ, β) to modulate the feature maps by applying affine transformation. Skip connection <ref type="bibr" target="#b26">[27]</ref> is used to ease the training of deep CNN. We upsample features by using nearest-neighbor upsampling followed by a convolution layer. The upsampling operation is performed in the latter part of the network and thus most computation is done in the LR space. Although we have not tried other architectures for the SR network, we believe many contemporary models such as DRRN <ref type="bibr" target="#b42">[43]</ref> and MemNet <ref type="bibr" target="#b43">[44]</ref> are applicable and can equally be benefited from the SFT layer.</p><p>For discriminator D η , we apply a VGG-style <ref type="bibr" target="#b39">[40]</ref> network of strided convolutions to gradually decrease the spatial dimensions. The full architecture and details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>We draw inspiration from <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> and apply perceptual loss and adversarial loss in our model. The perceptual loss measures the distance in a feature space. To obtain the feature maps, we use a pre-trained 19-layer VGG network <ref type="bibr" target="#b39">[40]</ref>, denoted as φ,</p><formula xml:id="formula_7">L P = i φ( ŷi ) − φ(y i ) 2 2 . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>Similar to <ref type="bibr" target="#b26">[27]</ref>, we use the feature maps obtained by the forth convolution before the fifth max-pooling layer and compute the MSE on their feature activations.</p><p>The adversarial loss L D from GAN is also used to encourage the generator to favor the solutions in the manifold of natural images,</p><formula xml:id="formula_9">L D = i log(1 − D η (G θ (x i ))). (<label>8</label></formula><formula xml:id="formula_10">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Implementation details. In this work, we focus on outdoor scenes since their textures are rich and well-suited for our study. For example, the sky is smooth and lacks sharp edges, while the building is rich of geometric patterns. The water presents smooth surface with waves, while the grass has matted textures. We assume seven categories, i.e., sky, mountain, plant, grass, water, animal and building. A 'background' category is used to encompass regions that do not appear in the aforementioned categories. Following <ref type="bibr" target="#b26">[27]</ref>, all experiments were performed with a scaling factor of ×4 between LR and HR images. We initialized the SR network by parameters pre-trained with perceptual loss and GAN loss on ImageNet dataset. After removing low resolution images of size below 30kB, we obtained roughly 450k training images. During training, we followed existing studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> to obtain LR images by downsampling HR images using MATLAB bicubic kernel. The mini-batch size was set to 16. The spatial size of cropped HR and LR sub-images were 96 × 96 and 24 × 24, respectively.</p><p>After initialization, with the same training setting, we fine-tuned our full network on outdoor scenes condition-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic SRCNN[7] VDSR[22] LapSRN[26] DRRN[43] MemNet[44] EnhanceNet[38] SRGAN[27] SFT-GAN (ours) GT</head><p>Figure <ref type="figure">5</ref>. GAN-based methods (SRGAN <ref type="bibr" target="#b26">[27]</ref>, EnhanceNet <ref type="bibr" target="#b37">[38]</ref> and ours) clearly outperform PSNR-oriented approaches in term of perceptual quality. Our proposed SFT-GAN is capable of generating richer and more realistic textures among different The first second images show that our method captures the characteristics of animal fur and building brick. In the third image, SRGAN tends to produce unpleasant water waves. (Zoom in for best view).</p><p>ally on the input segmentation probability maps. In particular, we collected a new outdoor dataset by querying images from search engines using the defined categories as keywords. The dataset was divided into training and test partitions (OutdoorSceneTrain and OutdoorSceneTest) <ref type="foot" target="#foot_1">2</ref> . For OutdoorSceneTrain, we cropped each image so that only one category exists, resulting in 1k to 2k images for each category. Background images were randomly sampled from ImageNet. The total number of training images is 10,324. The OutdoorSceneTest partition consists of 300 images and they are not pre-processed. Segmentation probability maps were generated by the segmentation network (Sec. 3.1).</p><p>For optimization, we used Adam <ref type="bibr" target="#b24">[25]</ref> with β 1 = 0.9. The learning rate was set to 1 × 10 −4 and then decayed by a factor of 2 every 100k iterations. We alternately optimized the generator and discriminator until the model converged at about 5 × 10 5 iterations. Inspired by <ref type="bibr" target="#b33">[34]</ref>, our discriminator not only distinguishes whether the input is real or fake, but also predicts which category the input belongs to. This is possible since our training images were cropped to contain only one category. This restriction was not applied on test images. We find this strategy facilitates the generation of images with more realistic textures.</p><p>We did not conduct our main evaluation on standard benchmarks such as Set5 <ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b49">[50]</ref> and BSD100 <ref type="bibr" target="#b32">[33]</ref> since these datasets are lack of regions with well-defined categories. Nevertheless, we will show that our method still perform satisfactorily on out-of-category examples in Sec. 4.3. Results (PSNR, SSIM) on standard benchmarks are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative Evaluation</head><p>Figure <ref type="figure">5</ref> shows the qualitative results of different models including PSNR-oriented methods, such as SRCNN <ref type="bibr" target="#b6">[7]</ref>, VDSR <ref type="bibr" target="#b21">[22]</ref>, LapSRN <ref type="bibr" target="#b25">[26]</ref>, DRRN <ref type="bibr" target="#b42">[43]</ref>, MemNet <ref type="bibr" target="#b43">[44]</ref>, and GAN-based methods, such as SRGAN <ref type="bibr" target="#b26">[27]</ref> and En-hanceNet <ref type="bibr" target="#b37">[38]</ref>. More results are provided in the supplementary material. For SRGAN, we re-implemented their method and fine-tuned the model with the same setting as ours. We directly used the released test code of EnhanceNet since no training code is available. Despite of preserving sharp edges, PSNR-oriented methods always produce blurry textures. SRGAN and EnhanceNet largely improve the high-frequency details, however, they tend to generate monotonous and unnatural textures, like water waves in Fig. <ref type="figure">5</ref>. Our method employs categorical priors to help capture the characteristics of each category, leading to more natural and realistic textures.  <ref type="bibr">Figure 6</ref>. User study results of ranking SRCNN <ref type="bibr" target="#b6">[7]</ref>, MemNet <ref type="bibr" target="#b43">[44]</ref>, SFT-GAN (ours), and the original HR image. Our method outperforms PSNR-oriented methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">User Study</head><p>We performed a user study to quantify the ability of different approaches to reconstruct perceptually convincing images. To better compare our method against PSNRoriented baselines and GAN-based approaches, we divided the evaluations into two sessions. In the first session, we focused on PSNR-oriented baselines. The users were requested to rank 4 versions of each image: SRCNN <ref type="bibr" target="#b6">[7]</ref>, MemNet <ref type="bibr" target="#b43">[44]</ref> (the state-of-the-art PSNR-oriented method), our SFT-GAN, and the original HR image according to their visual quality. used random images from OutdoorSceneTest and all images were presented in a randomized fashion. In the second session, we focused on GAN-based methods so that the user can concentrate on the texture quality. The subjects were shown the super-resolved image pairs (enlarged texture patches were depicted to facilitate the comparison). Each pair consists of an image of the proposed SFT-GAN and the counterpart generated by SRGAN <ref type="bibr" target="#b26">[27]</ref> or EnhanceNet <ref type="bibr" target="#b37">[38]</ref>. The users were asked to pick the image with more natural and realistic textures. This session involved 96 randomly selected images in total.</p><p>We asked 30 users to finish our user study. The results of the first and second sessions are presented in Fig. <ref type="figure">6</ref> and Fig. <ref type="figure" target="#fig_5">7</ref>, respectively. The results of the first session show that SFT-GAN outperforms the PSNR-oriented methods by a large margin. This is not surprising since PSNR-oriented methods always produce blurry results especially in texture regions. Our method sometimes generates good-quality images comparable to HR causing confusion within the users. In the second session, our method is ranked higher than SRGAN <ref type="bibr" target="#b26">[27]</ref> and EnhanceNet <ref type="bibr" target="#b37">[38]</ref>, especially in building, animal, and grass categories. Comparable performance is found on sky and plant categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>From segmentation probability maps to feature modulation parameters. Our method modulates intermediate features based on segmentation probability maps. We investigate the relationship between the probability and feature modulation parameters, as depicted in Fig. <ref type="figure">8</ref>. All SFT layers exhibit similar behavior so that we only present its . First row: the results of user studies, comparing our method with SRGAN <ref type="bibr" target="#b26">[27]</ref> and EnhanceNet <ref type="bibr" target="#b37">[38]</ref>. Second row: our methods produce visual results that are ranked higher in all categories in comparison with SRGAN <ref type="bibr" target="#b26">[27]</ref>. first layer for this analysis. In the top row, we show an image where building and grass co-exist. It is observed that modulation parameters γ and β are different for various categorical regions to exert meaningful spatial-wise transformation. From the heat map of γ and β, we can see that the modulation parameter maps are closely related to the segmentation probability maps. The boundary of building and grass is clear with a sharp transition. With the guidance of the probability map, our model is able to generate building and grass textures simultaneously without interference of different categories. Some classes, like plant and grass, may not be clearly distinguishable by their visual appearance. They interlace with each other without a clear boundary. Despite the ambiguity, the probability maps are still capable of capturing the semantics to certain extent and the SFT layers reflect the subtle differences between categories in its spatial transformation. In Fig. <ref type="figure">9</ref>, the upper row shows the probability map and modulation parameters activated for plant while the lower row shows those for grass. Distinct activations with smooth transition can be observed. As a result, textures generated by SFT-GAN become more realistic. Robustness to out-of-category examples. Our model mainly focuses on outdoor scenes and it is effective given segmentation maps of the pre-defined K classes. Despite the assumption, it is also robust to other scenes where segmentation results are not available. As shown in Fig. <ref type="figure" target="#fig_7">10</ref>, the SFT-GAN can still produce comparative results with SRGAN when all the regions are deemed as 'background'. More results are provided in the supplementary material. Comparison with other conditioning methods. We qualitatively compare with several alternatives for conditioning SR network, which are already discussed in Sec. 3.1. 1) Input concatenation -This method concatenates the segmentation probability maps with the LR image as a joint input to the network. This is equivalent to adding SFT conditional bias at the input layer.</p><p>2) Compositional mapping -This method is identical to Zhu et al. <ref type="bibr" target="#b51">[52]</ref>. It decomposes an LR image based on the predicted semantic classes and processes each region separately using a specific model for that class. Different models share parameters at lower layers.</p><p>3) FiLM <ref type="bibr" target="#b35">[36]</ref> -This method predicts one parameter for each feature map without spatial information and then uses these parameters to modulate the feature maps. As can be observed from Fig. <ref type="figure" target="#fig_8">11</ref>, the proposed SFT-GAN yields outputs that are perceptually more convincing. Naive input concatenation is not sufficient to exert the necessary condition for class-specific texture generation. Compositional mapping produces good results but it is not parameter efficient (×2.5 parameters as ours). It is also computationally inefficient as we need to forward several times for a single input image. FiLM <ref type="bibr" target="#b35">[36]</ref> cannot handle the situations where multiple categorical classes co-exist in an image since it predicts one parameter for each feature map, agnostic to spatial information. For example, in the first image of Fig. <ref type="figure" target="#fig_8">11</ref>, the road and sky interfere with the building's structure and thus noisy bricks are generated. Similarly in the second image, the animal's fine texture is severely affected by the grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>We have explored the use of semantic segmentation maps as categorical prior for constraining the plausible solution space in SR. A novel Spatial Feature Transform (SFT) layer has been proposed to efficiently incorporate the categorical conditions into a CNN-based SR network. Thanks to the SFT layers, our SFT-GAN is capable of generating distinct and rich textures for multiple semantic regions in a super-resolved image in just a single forward pass. Extensive comparisons and a user study demonstrate the capability of SFT-GAN in generating realistic and visually pleasing textures, outperforming previous GAN-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Our work currently focuses on SR of outdoor scenes. Despite robust to out-of-category images, it does not consider priors of finer categories, especially for indoor scenes, e.g., furniture, appliance and silk. In such a case, it puts forward challenging requirements for segmentation tasks from an LR image. Future work aims at addressing these shortcomings. Furthermore, segmentation and SR may benefit from each other and jointly improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The extracted building and plant patches from two lowresolution images look very similar. Using adversarial loss and perceptual loss without prior could add details that are not faithful to the underlying class. More realistic results can be obtained with the correct categorical prior. (Zoom in for best view).</figDesc><graphic url="image-1.png" coords="1,284.94,220.47,133.51,99.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparing different SR approaches with downsampling factor ×4: SRCNN [7], SRGAN [27], EnhanceNet [38], our proposed SFT-GAN and the original HR image. SRGAN, EnhanceNet, and SFT-GAN clearly outperform SRCNN in terms of perceptual quality, although they yield lower peak signal-to-noise ratio (PSNR) values. SRGAN and EnhanceNet result in more monotonous textures across different patches while SFT-GAN is capable of generating richer and visually pleasing textures. (Zoom in for best view).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The proposed SFT layers can be conveniently applied to existing SR networks. All SFT layers share a condition network. The role of the condition network is to generate intermediate conditions from the prior, and broadcast the conditions to all SFT layers for further generation of modulation parameters.(γ, β) based on some prior condition Ψ. The learned parameter pair adaptively influences the outputs by applying an affine transformation spatially to each intermediate feature maps in an SR network. During testing, only a single forward pass is needed to generate the HR image given the LR input and segmentation probability maps.More precisely, the prior Ψ is modeled by a pair of affine transformation parameters (γ, β) through a mapping function M : Ψ → (γ, β). Consequently,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Some examples on segmentation. First row: LR images. Second row: segmentation results on LR images. Third row: segmentation results on HR images. Forth row: Ground-truth segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>Figure 7. First row: the results of user studies, comparing our method with SRGAN<ref type="bibr" target="#b26">[27]</ref> and EnhanceNet<ref type="bibr" target="#b37">[38]</ref>. Second row: our methods produce visual results that are ranked higher in all categories in comparison with SRGAN<ref type="bibr" target="#b26">[27]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. The modulation parameters γ and β have a close relationship with probability maps P and contain spatial information. The boundary of different categories is clear without interference. Ci denotes the i th channel of the first SFT layer. (Zoom in for best view)</figDesc><graphic url="image-148.png" coords="7,308.86,371.11,54.20,72.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. When facing with other scenes or the absence of segmentation probability maps, our model degenerates itself as SR-GAN and produces comparative results with SRGAN. (Zoom in for best view)</figDesc><graphic url="image-152.png" coords="8,119.86,305.32,65.46,65.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Comparison with other conditioning methods -input concatenation, compositional mapping and FiLM [36].</figDesc><graphic url="image-166.png" coords="8,309.71,222.16,52.80,52.80" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The approach in<ref type="bibr" target="#b28">[29]</ref> is not applicable in our work since it can only generate outputs with a fixed size due to the upsampling operation from one-hot vector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">All data, codes and models can be downloaded from http:// mmlab.ie.cuhk.edu.hk/projects/SFTGAN/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is supported by SenseTime Group Limited and the General Research Fund sponsored by the Research Grants Council of the Hong Kong SAR (CUHK 14241716, 14224316. 14209217).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image up-sampling using totalvariation regularization with a new observation model</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1647" to="1659" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution with deep convolutional sufficient statistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00683</idno>
		<title level="m">Modulating early visual processing by language</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2007">2014. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1838" to="1857" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Behboodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lemic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wolisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Molinaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hirche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bagan</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG. ACM</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<title level="m">Controlling perceptual factors in neural style transfer</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the structure of a real-time, arbitrary neural artistic stylization network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth map superresolution by deep multi-scale guidance</title>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2017. 1, 2, 3, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2017. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diversified texture synthesis with feed-forward networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning markov random field for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier GANs</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03017</idno>
		<title level="m">Learning visual reasoning without strong priors</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">FiLM: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07871</idno>
		<imprint>
			<date type="published" when="2008">2017. 3, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video deblurring via semantic segmentation and pixel-wise non-linear kernel</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">EnhanceNet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2008">2017. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Context-constrained hallucination for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2017. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MemNet: A persistent memory network for image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2007">2017. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Semantic superresolution: When and where is it useful?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to super-resolve blurry face and text images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image superresolution as sparse representation of raw image patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Curves and Surfaces</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Be your own prada: Fashion synthesis with structural coherence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2008">2017. 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
