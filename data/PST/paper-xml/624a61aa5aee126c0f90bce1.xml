<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Enhanced Contrastive Learning for Radiology Findings Summarization</title>
				<funder>
					<orgName type="full">Guangdong Provincial Key Laboratory of Big Data Computing</orgName>
				</funder>
				<funder ref="#_asXG9uX">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_KfryXWm">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-01">1 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinpeng</forename><surname>Hu</surname></persName>
							<email>jinpenghu@link.cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data ? Pazhou Lab</orgName>
								<address>
									<postCode>510330</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuo</forename><surname>Li</surname></persName>
							<email>zhuoli3@link.cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data ? Pazhou Lab</orgName>
								<address>
									<postCode>510330</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
							<email>zhihongchen@link.cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data ? Pazhou Lab</orgName>
								<address>
									<postCode>510330</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">?</forename><forename type="middle">?</forename><surname>Xiang</surname></persName>
							<email>wanxiang@sribd.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data ? Pazhou Lab</orgName>
								<address>
									<postCode>510330</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tsung-Hui</forename><surname>Chang</surname></persName>
							<email>changtsunghui@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data ? Pazhou Lab</orgName>
								<address>
									<postCode>510330</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kong</forename><forename type="middle">(</forename><surname>Shenzhen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data ? Pazhou Lab</orgName>
								<address>
									<postCode>510330</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Enhanced Contrastive Learning for Radiology Findings Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-01">1 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.00203v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians. Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists, and thus automatic impression generation has attracted substantial attention. With the encoder-decoder framework, most previous studies explore incorporating extra knowledge (e.g., static pre-defined clinical ontologies or extra background information). Yet, they encode such knowledge by a separate encoder to treat it as an extra input to their models, which is limited in leveraging their relations with the original findings. To address the limitation, we propose a unified framework for exploiting both extra knowledge and the original findings in an integrated way so that the critical information (i.e., key words and their relations) can be extracted in an appropriate way to facilitate impression generation. In detail, for each input findings, it is encoded by a text encoder, and a graph is constructed through its entities and dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is adopted to model relation information in the constructed graph. Finally, to emphasize the key words in the findings, contrastive learning is introduced to map positive samples (constructed by masking non-key words) closer and push apart negative ones (constructed by masking key words). The experimental results on OpenI and MIMIC-CXR confirm the effectiveness of our proposed method. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Radiology reports document critical observation in a radiology study and play a vital role in commu- nication between radiologists and physicians. A radiology report usually consists of a findings section describing the details of medical observation and an impression section summarizing the most prominent observation. The impression is the most critical part of a radiology report, but the process of summarizing findings is normally time-consuming and could be prone to errors for inexperienced radiologists. Therefore, automatic impression generation (AIG) has drawn substantial attention in recent years, and there are many methods proposed in this area <ref type="bibr" target="#b37">(Zhang et al., 2018;</ref><ref type="bibr" target="#b7">Gharebagh et al., 2020;</ref><ref type="bibr" target="#b22">MacAvaney et al., 2019;</ref><ref type="bibr" target="#b25">Shieh et al., 2019)</ref>.</p><p>Most existing studies focus on incorporating extra knowledge on the general encoder-decoder framework. For example, <ref type="bibr" target="#b37">Zhang et al. (2018)</ref> utilized the background section in the radiology report through a separate encoder and then used it to guide the decoding process to enhance impression generation. Similarly, <ref type="bibr" target="#b22">MacAvaney et al. (2019)</ref> and <ref type="bibr" target="#b7">Gharebagh et al. (2020)</ref> proposed to extract the ontology information from findings and used an encoder to encode such information to promote the decoding process. Although these approaches have brought significant improvements, they only lever- age extra knowledge and findings separately (i.e., through an extra encoder). Thus, their performance relies heavily on the quality of extra knowledge, and the further relationships between extra knowledge and findings are not explored. In this paper, we propose a unified framework to exploit both findings and extra knowledge in an integrated way so that the critical information (i.e., key words and their relations in our paper) can be leveraged in an appropriate way. In detail, for each input findings, we construct a word graph through the automatically extracted entities and dependency tree, with its embeddings, which are from a text encoder. Then, we model the relation information among key words through a graph encoder (e.g., graph neural networks (GNNs)). Finally, contrastive learning is introduced to emphasize key words in findings to map positive samples (constructed by masking non-key words) closer and push apart negative ones (constructed by masking key words), as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In such a way, key words and their relations are leveraged in an integrated way through the above two modules (i.e., contrastive learning and the graph encoder) to promote AIG. Experimental results on two datasets (i.e., OpenI and MIMIC-CXR) show that our proposed approach achieves state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We follow the standard sequence-to-sequence paradigm for AIG. First, we utilize WordPiece <ref type="bibr" target="#b30">(Wu et al., 2016)</ref> to tokenize original findings and obtain the source input sequence</p><formula xml:id="formula_0">X = x 1 , x 2 , ? ? ? , x N ,</formula><p>where N is the number of tokens in X . The goal is to find a sequence Y = {y 1 , ...y i , ..., y L } that summarizes the most critical observations in findings, where L is the length of impression and y i ? V are the generated tokens and V is the vocabulary of all possible tokens. The generation process can be formalized as:</p><formula xml:id="formula_1">p(Y | X ) = L t=1 p (y t | y 1 , . . . , y t-1 , X ) (1)</formula><p>The model is then trained to maximize the negative conditional log-likelihood of Y given the X :</p><formula xml:id="formula_2">? * = arg max ? L t=1 log p (y t | y 1 , ..., y t-1 , X , A; ?) (2)</formula><p>where ? is the parameters of the model, and A represents edges in the relation graph. An overview of our proposed method is presented in Figure <ref type="figure" target="#fig_1">2</ref>. Our model contains three main components, i.e., the graph enhanced encoder, the contrastive learning module, and the decoder. The details are described in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relation Graph</head><p>The impression usually describes critical abnormalities with more concise descriptions summarized from the corresponding findings and sometimes uses key phrases to express observations. For example, a sentence in findings texts "There is a left pleural effusion which is small in size.", is simplified as a key phrase "Small left pleural effusion" in the impression, where the relation between "small" and "effusion" is vital for describing the corresponding observation. Thus, the relation information in findings plays an essential role in accurate key phrase generation. Four types of medical entities, anatomy, observation, anatomy modifier, and observation modifier, are recognized from findings, which compose a majority of important medical knowledge in impression <ref type="bibr" target="#b9">(Hassanpour and Langlotz, 2016)</ref>. With WordPiece tokenization, we represent each entity by frequent subwords and connect any two subwords if they are adjacent in the same entity to enhance internal relations for keeping the entity complete. For example, the entity "opacity" is represented as "op ##acity" and then these two subwords connect to each other with both from "op" to "##acity" and from "##acity" to "op". Besides, we need to consider the semantic relation between entities and other words, such as words used to describe the location and degree of symptoms, which is necessary for accurately recording abnormalities. For example, in a text span "bilateral small pleural effusions", relations in &lt;"bilateral","effusions"&gt;, &lt;"small","effusions"&gt; are also important to describe the observation "effusions" and they can be extracted from the dependency tree. Therefore, we construct a dependency tree to extract the semantic relations between entities and other words, with the direction from their head words to themselves. We also employ the WordPiece to split these words as subwords and connect all the source subwords to the corresponding target words with the original direction. The constructed subword graph is then used to extract relation information, with edges represented by A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Enhanced Encoder</head><p>In recent years, pre-trained models have dominated not only general summarization tasks but also multi-modal tasks because of their strong ability in feature representation <ref type="bibr" target="#b29">(Wu et al., 2021;</ref><ref type="bibr">Zhang et al., 2020a;</ref><ref type="bibr" target="#b33">Yuan et al., 2021</ref><ref type="bibr" target="#b32">Yuan et al., , 2022))</ref>. Thus, in our method, we utilize the pre-trained model BioBERT <ref type="bibr" target="#b16">(Lee et al., 2020)</ref> trained on a large biomedical corpus as our text encoder. The hidden state h i for each token x i is generated by the text encoder </p><formula xml:id="formula_3">[h 1 , h 2 , ? ? ? , h n ] = f te (x 1 , x 2 , ? ? ? , x n ) (3)</formula><formula xml:id="formula_4">s p ? s, s n ? s m = [1e -6] ? R d 1: N , d = size(s) 2: V key = Extract_subword_index(A) 3: for j = 0 to N do 4: if j in V key then 5: s n j ? m 6: else: 7: s p j ? m 8:</formula><p>end if 9: end for Herein, f te (?) refers to the pre-trained Transformerbased text encoder (i.e., BioBERT <ref type="bibr" target="#b16">(Lee et al., 2020)</ref>), and h i is a d-dimensional feature vector for representing corresponding tokens x i . Since GNNs are well known for extracting features from graph structure and have been shown promising in text generation tasks <ref type="bibr" target="#b12">(Jia et al., 2020;</ref><ref type="bibr" target="#b10">Hu et al., 2021)</ref>, we employ a GNN-based encoder to capture relation information from the corresponding subword graph. This process can be formulated as:</p><formula xml:id="formula_5">z = f ge (h, A),<label>(4)</label></formula><p>where f ge (?) is the graph encoder, and z is the feature vector extracted from the graph. Next, to incorporate relation information into token representation, we concatenate z and h and utilize a fully connected layer to reduce it to the same dimensions as z and h:</p><formula xml:id="formula_6">s = MLP([h 1 ? z 1 , h 2 ? z 2 , ? ? ? , h n ? z n ]), (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where s is the final token representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning</head><p>Only relying on a GNN encoder to capture relation information still lacks the capability to fully grasp important word information from findings since the graph is pre-defined before training or testing. Recently, contrastive learning has shown strong power in learning and distinguishing significant knowledge by concentrating positive samples and contrasting with negative samples, and brought significant improvements in many tasks, such as improving the faithfulness of summarization and discriminating vital information to enhance repre-sentation <ref type="bibr" target="#b1">(Cao and Wang, 2021;</ref><ref type="bibr" target="#b34">Zeng et al., 2021)</ref>. We expect our model to be more sensitive to critical words contained in findings. For this purpose, we apply a contrastive learning module to concentrate positive pairs and push negative ones apart, which aims to help the model differentiate essential information from secondary information. We regard tokens with edges in the relation graph as critical tokens since they contain important information for describing key observations, as discussed in 2.1. To construct a positive example, we mask each non-key token representation in s as the constant vectors m ? R d , with all elements 1e -6, so that this instance can consolidate the critical information and remove unimportant words. Meanwhile, we utilize a similar way to mask important token representations in s as m to obtain a negative example s n . The details of generating positive and negative examples are shown in Algorithm 1. Note that in our model, we do not consider the other instances in the same mini-batch as the negative examples, which is different from many existing approaches <ref type="bibr" target="#b14">(Kim et al., 2021;</ref><ref type="bibr" target="#b8">Giorgi et al., 2020)</ref> since we aim to identify the critical content in X instead of expanding differences between various findings in one mini-batch. In addition, radiology reports are not as diverse as ordinary texts, and they are mainly composed of fixed medical terms and some attributive words, where the former is used to record critical information and the latter is to keep sentences fluent and grammatically correct. Afterward, we employ a randomly initialized Transformer-based encoder to model s, s p , s n , respectively, which can be formulated as: We follow <ref type="bibr" target="#b23">Robinson et al. (2020)</ref> to formulate the training objective of contrastive module: set to 1 in this paper.</p><formula xml:id="formula_8">b = f ce (s), (6) b p = f ce (s p ), (7) b n = f ce (s n ),<label>(8)</label></formula><formula xml:id="formula_9">l con = -log e sim(b i ,b p )/? N j=1 e sim(b i ,b p )/? + e sim(b i ,b n )/? ,<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Decoder</head><p>The decoder in our model is built upon a standard Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>, where the representation s is functionalized as the input of the decoder so as to improve the generation process. In detail, s is sent to the decoder at each decoding step, jointly with the generated tokens from previous steps, and thus the current output y t can be computed by</p><formula xml:id="formula_10">y t = f e (s 1 , s 2 , ? ? ? , s n , y 1 , ? ? ? , y t-1 ),<label>(10)</label></formula><p>where f e (?) refers to the Transformer-based decoder and this process is repeated until the complete impression is obtained. Besides, to effectively incorporate the critical word information into the decoding process, we sum the losses from the impression generation and contrastive objectives as</p><formula xml:id="formula_11">L = l ge + ?l con ,<label>(11)</label></formula><p>where l ge is the basic sequence-to-sequence loss, and ? is the weight to control the contrastive loss.</p><p>3 Experimental Setting  <ref type="bibr" target="#b37">Zhang et al., 2018)</ref> to filter the reports by deleting the reports in the following cases: (1) no findings or no impression sections; (2) the findings have fewer than ten words, or the impression has fewer than two words. For OPENI, we follow <ref type="bibr" target="#b10">(Hu et al., 2021)</ref> to randomly divide it into train/validation/test set by 2400:292:576 in our experiments. For MIMIC-CXR, we apply two types of splits, including an official split and a random split with a ratio of 8:1:1 similar to <ref type="bibr" target="#b7">(Gharebagh et al., 2020)</ref>. We report the statistics of these two datasets in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline and Evaluation Metrics</head><p>To explore the performance of our method, we use the following ones as our main baselines:</p><p>? BASE (Liu and Lapata, 2019): this is a backbone sequence-to-sequence model, i.e., a pre-trained encoder and a randomly initialized Transformerbased decoder. ? BASE+GRAPH and BASE+CL: these have the same architecture as BASE, where the former incorporates an extra graph encoder to enhance relation information, and the latter introduces a contrastive learning module to help the model distinguish critical words. Besides, we also compare our method with those existing studies, including both extractive summarization methods, e.g., LEXRANK <ref type="bibr" target="#b6">(Erkan and Radev, 2004)</ref>, TRANSFORMEREXT <ref type="bibr" target="#b20">(Liu and Lapata, 2019)</ref>, and the ones proposed for abstractive models. e.g., TRANSFORMERABS <ref type="bibr" target="#b20">(Liu and Lapata, 2019)</ref>, <ref type="bibr">ONTOLOGYABS (Gharebagh et al., 2020)</ref>, WGSUM (TRANS+GAT), and WGSUM (LSTM+GAT) <ref type="bibr" target="#b10">(Hu et al., 2021)</ref>.</p><p>Actually, factual consistency (FC) is critical in radiology report generation <ref type="bibr">(Liu et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>. Following <ref type="bibr">Zhang et al. (2020c)</ref>; <ref type="bibr" target="#b10">Hu et al. (2021)</ref>, we evaluate our model and three baselines by two types of metrics: summarization and FC metrics. For summarization metrics, we report F 1 scores of ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L). Besides, for FC metrics, we utilize CheXbert <ref type="bibr" target="#b26">(Smit et al., 2020)</ref> <ref type="foot" target="#foot_0">2</ref> to detect 14 observations related to diseases from reference impressions and generated impressions and then calculate the precision, recall, and F1 score between these two identified results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>In our experiments, we utilize biobert-base-cased-v1.1<ref type="foot" target="#foot_1">3</ref> as our text encoder and follow its default model settings: we use 12 layers of self-attention with 768-dimensional embeddings. Besides, we employ stanza <ref type="bibr">(Zhang et al., 2020d)</ref> to extract medical entities and the dependence tree, which is used to construct the graph and generate positive and negative examples. Our method is implemented based on the code of BertSum <ref type="bibr" target="#b20">(Liu and Lapata, 2019)</ref> <ref type="foot" target="#foot_2">4</ref> . In addition, we use a 2-layer graph attention networks (GAT) <ref type="bibr" target="#b28">(Veli?kovi? et al., 2017)</ref>  <ref type="foot" target="#foot_3">5</ref> with the hidden size of 768 as our graph encoder and a 6-layer Transformer with 768 hidden sizes and 2048 feed-forward filter sizes for the contrastive encoder. The decoder is also a 6-layer Transformer with 768 dimensions, 8 attention heads, and 2048 feed-forward filter sizes. Note that ? is set 1 in all experiments, and more detailed hyperparameters are reported in A.1. During the training, we use Adam (Kingma and Ba, 2014) to optimize the trainable parameters in our model.</p><formula xml:id="formula_12">MODEL OPENI MIMIC-CXR RANDOM SPLIT OFFICIAL SPLIT RANDOM SPLIT R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L</formula><p>LEXRANK <ref type="bibr" target="#b6">(Erkan and Radev, 2004)</ref>   <ref type="bibr">(Hu et al., 2021) 61.63 50.98 61.73 48.37 33.34 46.68 56.38 44.75 55.32 OURS 64.97 55.59 64.45 49.13 33.76 47.12 57.38 45.52 56.13</ref> Table <ref type="table">3</ref>: Comparisons of our proposed models with previous study on the OPENI and MIMIC-CXR with respect to ROUGE metric. ? refers to that the results is directly cited from the original paper.</p><p>4 Results and Analyses</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effect of Graph and Contrastive learning</head><p>To explore the effectiveness of our proposed method, we conduct experiments on two benchmark datasets, with the results reported in Table <ref type="table" target="#tab_2">2</ref>, where BASE+GRAPH+CL represents our complete model. We can obtain several observations from the results. First, both BASE+GRAPH and BASE+CL achieve better results than BASE with respect to R-1, R-2, and R-L, which indicates that graph and contrastive learning can respectively promote impression generation. Second, BASE+GRAPH+CL outperforms all baselines with significant improvement on two datasets, confirming the effectiveness of our proposed method in combining graph and contrastive learning. This might be because graphs and contrastive learning can provide valuable information from different aspects, the former mainly record relation information, and the latter brings critical words knowledge, so that an elaborate combination of them can bring more improvements. Third, when comparing these two datasets, the performance gains from our full model over three baselines on OpenI are more prominent than that on MIMIC-CXR. This is perhaps because compared to MIMIC-CXR, OpenI dataset is relatively smaller and has a shorter averaged word-based length, such that it is easier for the graph to record relation and more accessible for contrastive learning to recognize key words by comparing positive and negative examples. Fourth, we can find a similar trend on the FC metric on the MIMIC-CXR dataset, where a higher F1 score means that our complete model can generate more accurate impressions thanks to its more substantial power in key words discrimination and relationship information extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Previous Studies</head><p>In this subsection, we further compare our models with existing models on the aforementioned datasets, and the results are reported in Table <ref type="table">3</ref>.</p><p>There are several observations. First, the comparison between our model and ONTOLOGYABS shows the effectiveness of our design in this task, where our model achieves better performance, though both of them enhance impression generation by incorporating crucial medical information. This might be because by comparing positive and negative examples for each findings, our model is more sensitive to critical information and more intelligent in distinguishing between essential information and secondary information, contributing to more accurate and valuable information embedded in the model. Second, we can observe that our model outperforms all existing models in terms of R-1, R-2, and R-L. On the one hand, effectively combining contrastive learning and graph into the sequence to sequence model is a better solution to improve feature extraction and thus promote the decoding process robustly. On the other hand, the pre-trained model (i.e., BioBERT) used in our model is a more powerful feature extractor in modeling biomedical text than those existing studies, e.g., TRANSFORMERABS, ONTOL-OGYABS, and PGN, which utilize randomly initialized encoders. Third, when compared to those complicated models, e.g., WGSUM utilize stanza to extract entities and construct two extra graph encoders to extract features from a word graph, which are then regarded as background information and dynamic guiding information to enhance the decoding process for improving impression generation, our model can achieve better performance through a somewhat more straightforward method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human Evaluation</head><p>We further conduct a human evaluation to understand the quality of the generated impression better and alleviate the limitation of the ROUGE metric.</p><p>One hundred generated impressions on MIMIC-CXR from BASE and BASE+GRAPH+CL, along with their corresponding reference impressions, are randomly selected for expert evaluation <ref type="bibr" target="#b7">(Gharebagh et al., 2020)</ref>. Besides, we follow <ref type="bibr" target="#b10">Hu et al. (2021)</ref> to utilize four metrics: Key, Readability, Accuracy, and Completeness, respectively. We invite three medical experts to score these generated impressions based on these four metrics, with the results shown in Figure <ref type="figure" target="#fig_3">3</ref>. On the one hand, compared to BASE, we can find that our model outperforms it on all four metrics, where 16%, 25%, 18%, and 8% of impressions from our model obtain higher quality than BASE. On the other hand, comparing our model against reference impressions, our model obtains close results on key, accuracy, and completeness, with 86%, 78%, and 92% of our model outputs being at least as good as radiologists, while our model is less preferred for readability with a 10% gap. The main reason might be that many words removed in positive examples are used to keep sequence fluently, and our model tends to identify them as secondary information, leading that our model obtains relatively worse results on the readability metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analyses</head><p>We conduct further analyses on Findings Length and Case Study.</p><p>[0, 25) <ref type="bibr">[25, 45) [45, 65) [65, 85) [85, 105) [105, 125)[125, max]</ref> Findings Length Findings Length To test the effectiveness of the word-based length of findings, we categorize the findings on the MIMIC-CXR test set into seven groups and present the R-1 score for each group in Figure <ref type="figure">4</ref>. We have the following observations. First, as the findings length becomes long, the performance of BASE and our model tend to decrease, except for the second group, i.e., <ref type="bibr">[25,</ref><ref type="bibr">45]</ref>, since short text are more accessible for the encoder to capture valid features, which is consistent with previous studies <ref type="bibr" target="#b4">(Dai et al., 2019)</ref>. Second, our model outperforms BASE in all the groups, further illustrating the effectiveness of our model regardless of the findings length. Third, we can observe a grey line with a downward trend from the incremental chart in the upper right corner of Figure <ref type="figure">4</ref>, indicating that our model (i.e., BASE+GRAPH+CL) tends to gain better improvements over BASE on shorter findings than that on longer ones. This is because longer findings usually contain relatively more secondary information such that it is more challenging for contrastive learning to distinguish critical knowledge.</p><p>Case study To further demonstrate how our approach with graph and contrastive learning helps the generation of findings, we perform qualitative analysis on two cases, and the results are shown in Figure <ref type="figure" target="#fig_4">5</ref>, where different colors on the texts indicate different critical information. Compared to BASE model, our model can generate more complete impressions which cover almost all the crucial abnormalities. In contrast, the BASE model fails to identify all the key information, e.g., ("moderate cardiomegaly" in the left example and "possible small left pleural effusion" in the right case). Besides, our model can generate more accurate impressions with an appropriate word to represent possibility and a better modifier to describe the observation. On the one hand, in Figure <ref type="figure" target="#fig_4">5</ref>, "suggestive of" in the left example and "may" in the right example imply a type of uncertainty, which means that doctors wonder whether the abnormal observation exists when writing findings, so that the corresponding word (i.e., "likely") is used to describe this sensitive information. On the other hand, in the left case, according to the phrase "Frontal and lateral" in its original findings, our model can generate the synonym "bilateral" to depict the symptom "pleural effusions" more specifically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Recently, NLP technology has broadly applied in the medical domain, such as medical entity recognition <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr" target="#b40">Zhao et al., 2019)</ref>, radiology report generation <ref type="bibr" target="#b2">(Chen et al., 2021;</ref><ref type="bibr">Zhang et al., 2020b;</ref><ref type="bibr">Liu et al., 2021a)</ref>, AIG, etc. Impression generation can be regarded as a type of summarization task that has drawn substantial attention in recent years, and there are many studies for addressing general abstractive summarization <ref type="bibr" target="#b24">(See et al., 2017;</ref><ref type="bibr" target="#b17">Li et al., 2020;</ref><ref type="bibr" target="#b31">You et al., 2019;</ref><ref type="bibr" target="#b11">Huang et al., 2020)</ref>. <ref type="bibr" target="#b31">You et al. (2019)</ref> designed a novel focus-attention mechanism and saliencyselection network, equipped in the encoder and decoder to enhance summary generation. <ref type="bibr" target="#b17">Li et al. (2020)</ref> proposed an abstractive sentence summarization method guided by the key words, which utilized a dual-attention and a dual-copy mechanism to integrate the semantics of both original sequence and key words. Many methods propose to introduce specific designs on the general summarization model to address radiology impression generation <ref type="bibr" target="#b37">(Zhang et al., 2018;</ref><ref type="bibr" target="#b7">Gharebagh et al., 2020;</ref><ref type="bibr" target="#b22">MacAvaney et al., 2019;</ref><ref type="bibr" target="#b10">Hu et al., 2021;</ref><ref type="bibr" target="#b0">Abacha et al., 2021)</ref>.  <ref type="formula">2021</ref>) further introduced pre-defined word graphs to record salient words as well as their internal relation and then employed two separate graph encoders to leverage graphs for guiding the decoding process. Most of these approaches exploit separate encoders to encode predefined knowledge (e.g., ontology terms and word graph), which are then utilized to enhance impression generation. However, they tend to over-rely on the quality of pre-extracted ontologies and word graphs and lack sensitivity to vital information of findings themselves. Compared to these models, our method offers an alternative solution to robustly improve key information extraction with the help of both graphs and contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose to combine graphs and contrastive learning to better incorporate valuable features for promoting impression generation. Specifically, we utilize the graph encoder to extract relation information from the graph, constructed by medical entities and the dependence tree, for enhancing the representation from the pre-trained text encoder. In addition, we employ contrastive learning to assist the model in distinguishing between critical and secondary information, simultaneously improving sensitivity to important word represen-tation by comparing positive and negative examples. Furthermore, we conduct experiments on two benchmark datasets, and the results illustrate the effectiveness of our proposed method, where new state-of-the-art results are achieved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of the findings and corresponding impression, where the relation information, as well as positive and negative examples, are also shown in the figure. Note that represents the removed word.</figDesc><graphic url="image-1.png" coords="1,306.43,212.60,209.84,154.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall architecture of our proposed method with graph and contrastive learning. An example input and output at t-1 and t step are shown in the figure, where the top is the backbone sequence-to-sequence paradigm with a graph to store relation information between critical words and the bottom is the contrastive learning module with specific positive and negative examples. m refer to a mask vector.</figDesc><graphic url="image-2.png" coords="2,75.40,70.87,444.47,223.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where f ce (?) represents the contrastive encoder. b, b p and b n are intermediate states extracted from the encoder, which are also d-dimensional vectors. Then, we calculate cosine similarity sim(b 1 , b 2 ) = b 1 b 2 b 1 ? b 2 for positive and negative pairs, denoted as sim(b, b p ) and sim(b, b n ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The results of human evaluation, where forward and backslash represent that BASE+GRAPH+CL versus the reference and BASE, respectively. Yellow, green and blue represent that our model loses, equal to competitors and wins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of the generated impressions from BASE and BASE+GRAPH+CL as well as reference impressions. The yellow nodes in the graph indicate that these words are contained in entities.</figDesc><graphic url="image-3.png" coords="8,75.40,70.87,444.48,166.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>MacAvaney et al. (2019); Gharebagh et al. (2020) extracted the salient clinical ontology terms from findings and then incorporated them into the summarizer through a separate encoder for enhancing AIG. Hu et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the two benchmark datasets with random split for OPENI and official split for MIMIC-CXR, including the numbers of report, the averaged sentence-based length (AVG. SF, AVG. SI), the averaged word-based length (AVG. WF, AVG. WI) of both IMPRESSION and FINDINGS.</figDesc><table><row><cell>9)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of baselines and our method on OPENI and MIMIC-CXR datasets. R-1, R-2 and R-L refer to ROUGE-1, ROUGE-2 and ROUGE-L, respectively. P, R and F-1 represent precision, recall, and F1 score.</figDesc><table><row><cell>3.1 Dataset</cell></row><row><cell>Our experiments are conducted on two following</cell></row><row><cell>datasets: OPENI (Demner-Fushman et al., 2016)</cell></row><row><cell>and MIMIC-CXR (Johnson et al., 2019) respec-</cell></row></table><note><p><p>tively, where the former contains 3268 reports collected by Indiana University and the latter is a larger dataset containing 124577 reports. Note that the number of reports we introduced is counted after pre-processing. We follow</p><ref type="bibr" target="#b10">(Hu et al., 2021;</ref>    </p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>FC is only applied to MIMIC-CXR since the CheXbert is designed for MIMIC-CXR. We obtain it from https:// github.com/stanfordmlgroup/CheXbert</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We obtain BioBERT from https://github.com/ dmis-lab/biobert</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We obtain the code of BertSum from https:// github.com/nlpyang/PreSumm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Since previous study<ref type="bibr" target="#b10">(Hu et al., 2021)</ref> has shown that GAT<ref type="bibr" target="#b28">(Veli?kovi? et al., 2017)</ref> is more effective in impression generation, we select GAT as our graph encoder.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), <rs type="funder">NSFC</rs> under the project "<rs type="projectName">The Essential Algorithms and Technologies for Standardized Analytics of Clinical Texts</rs>" (<rs type="grantNumber">12026610</rs>) and the <rs type="funder">Guangdong Provincial Key Laboratory of Big Data Computing</rs>, <rs type="affiliation">The Chinese University of Hong Kong, Shenzhen</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_asXG9uX">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funded-project" xml:id="_KfryXWm">
					<idno type="grant-number">12026610</idno>
					<orgName type="project" subtype="full">The Essential Algorithms and Technologies for Standardized Analytics of Clinical Texts</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Hyper-parameter Settings </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overview of the Mediqa 2021 Shared Task on Summarization in the Medical Domain</title>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M'</forename><surname>Yassine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Rabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Workshop on Biomedical</title>
		<meeting>the 20th Workshop on Biomedical</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="74" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.09209</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-modal Memory Networks for Radiology Report Generation</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5904" to="5914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating Radiology Reports via Memory-driven Transformer</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1439" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Preparing a Collection of Radiology Examinations for Distribution and Retrieval</title>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonya</forename><forename type="middle">E</forename><surname>Marc B Rosenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laritza</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><forename type="middle">J</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based Lexical Centrality as Salience in Text Summarization</title>
		<author>
			<persName><forename type="first">G?nes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Sajad</forename><surname>Sotudeh Gharebagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Filice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1899" to="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Osvald</forename><surname>John M Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03659</idno>
		<title level="m">Declutr: Deep Contrastive Learning for Unsupervised Textual Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Information Extraction from Multi-institutional Radiology Reports</title>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Word Graph Guided Summarization for Radiology Findings</title>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hui</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4980" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward</title>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5094" to="5107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network</title>
		<author>
			<persName><forename type="first">Ruipeng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3622" to="3631" />
		</imprint>
	</monogr>
	<note>Fang Fang, Cong Cao, and Shi Wang</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">R</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Ying</forename><surname>Matthew P Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07042</idno>
		<title level="m">MIMIC-CXR-JPG, a Large Publicly Available Database of Labeled Chest Radiographs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07345</idno>
		<title level="m">Self-guided Contrastive Learning for BERT Sentence Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Biobert: a Pre-trained Biomedical Language Representation Model for Biomedical Text Mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Keywords-Guided Abstractive Sentence Summarization</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8196" to="8203" />
		</imprint>
	</monogr>
	<note>Chengqing Zong, and Xiaodong He</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2021a. Medicalvlbert: Medical visual language bert for covid-19 ct report generation with alternate learning</title>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuixing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3786" to="3797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clinically Accurate Chest X-Ray Report Generation</title>
		<author>
			<persName><forename type="first">Guanxiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Ming Harry</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mc-Dermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="249" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text Summarization with Pretrained Encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3721" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring Word Segmentation and Medical Concept Recognition for Chinese Medical Texts</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhe</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Workshop on Biomedical Language Processing</title>
		<meeting>the 20th Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ontology-aware Clinical Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajad</forename><surname>Sotudeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ish</forename><surname>Talati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">W</forename><surname>Filice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1013" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04592</idno>
		<title level="m">Contrastive Learning with Hard Negative Samples</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Get To the Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation</title>
		<author>
			<persName><forename type="first">Alexander Te-Wei</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</title>
		<meeting>the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chexbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1500" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph Attention Networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BASS: Boosting Abstractive Summarization with Unified Semantic Graph</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the gap between Human and Machine Translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving Abstractive Document Summarization with Salient Information Modeling</title>
		<author>
			<persName><forename type="first">Yongjian</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenmian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2132" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Xtrans2cap: Cross-modal knowledge transfer using transformer for 3d dense captioning</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00843</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multilevel contextual referring</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huixing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14289</idno>
		<title level="m">Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with Ex-tracted Gap-Sentences for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Alan Yuille, and Daguang Xu. 2020b. When Radiology Report Generation Meets Knowledge Graph</title>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12910" to="12917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to Summarize Radiology Findings</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Yi Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianpei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</title>
		<meeting>the Ninth International Workshop on Health Text Mining and Information Analysis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Merck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5108" to="5120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14640</idno>
		<title level="m">Biomedical and Clinical English Model Packages in the Stanza Python NLP Library</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization</title>
		<author>
			<persName><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
