<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Stream Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oliverio</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
							<email>osantana@ac.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Departament d&apos;Arquitectura de Computadors</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Ramirez</surname></persName>
							<email>aramirez@ac.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Departament d&apos;Arquitectura de Computadors</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departament d&apos;Arquitectura de Computadors</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Barcelona Supercomputing Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Stream Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>microarchitecture</term>
					<term>branch prediction</term>
					<term>access latency</term>
					<term>instruction stream</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The next stream predictor is an accurate branch predictor that provides stream level sequencing. Every stream prediction contains a full stream of instructions, that is, a sequence of instructions from the target of a taken branch to the next taken branch, potentially containing multiple basic blocks. The long size of instruction streams makes it possible for the stream predictor to provide high fetch bandwidth and to tolerate the prediction table access latency. Therefore, an excellent way for improving the behavior of the next stream predictor is to enlarge instruction streams.</p><p>In this paper, we provide a comprehensive analysis of dynamic instruction streams, showing that there are several kinds of streams according to the terminating branch type. Consequently, focusing on particular kinds of stream is not a good strategy due to Amdahl's law. We propose the multiple stream predictor, a novel mechanism that deals with all kinds of streams by combining single streams into long virtual streams. We show that our multiple stream predictor is able to tolerate the prediction table access latency without requiring the complexity caused by additional hardware mechanisms like prediction overriding, also reducing the overall branch predictor energy consumption.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High performance superscalar processors require high fetch bandwidth to exploit all the available instruction-level parallelism. The development of accurate branch prediction mechanisms has provided important improvements in the fetch engine performance. However, it has also increased the fetch architecture complexity. Our approach to achieve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>This fetch engine design is based on the next stream predictor, an accurate branch prediction mechanism that uses instruction streams as the basic prediction unit. We call stream to a sequence of instructions from the target of a taken branch to the next taken branch, potentially containing multiple basic blocks. Figure <ref type="figure" target="#fig_0">1</ref> shows an example control flow graph from which we will find the possible streams. The figure shows a loop containing an if-then-else structure. Let us suppose that our profile data shows that A → B → D is the most frequently followed path through the loop. Using this information, we lay out the code so that the path A → B goes through a not-taken branch, and falls-through from B → D. Basic block C is mapped somewhere else, and can only be reached through a taken branch at the end of basic block A.</p><p>From the resulting code layout we may encounter four possible streams composed by basic blocks ABD, A, C, and D. The first stream corresponds to the sequential path starting at basic block A and going through the frequent path found by our profile. Basic block A is the target of a taken branch, and the next taken branch is found at the end of basic block D. Neither the sequence AB, nor the sequence BD can be considered streams because the first one does not end in a taken branch, and the second one does not start in the target of a taken branch. The infrequent case follows the taken branch at the end of A, goes through C, and jumps back into basic block D.</p><p>Although a fetch engine based on streams is not able to fetch instructions beyond a taken branch in a single cycle, streams are long enough to provide high fetch bandwidth. In addition, since streams are sequentially stored in the instruction cache, the stream fetch engine does not need a special-purpose storage, nor a complex dynamic building engine. However, taking into account current technology trends, accurate branch prediction and high fetch bandwidth is not enough. The continuous increase in processor clock frequency, as well as the larger wire delays caused by modern technologies, prevent branch prediction tables from being accessed in a single cycle <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. This fact limits fetch engine performance because each branch prediction depends on the previous one, that is, the target address of a branch prediction is the starting address of the following one.</p><p>A common solution for this problem is prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. A small and fast predictor is used to obtain a first prediction in a single cycle. A slower but more accurate predictor provides a new prediction some cycles later, overriding the first prediction if they differ. This mechanism partially hides the branch predictor access latency. However, it also causes an increase in the fetch architecture complexity, since prediction overriding requires a complex recovery mechanism to discard the wrong speculative work based on overridden predictions.</p><p>An alternative to the overriding mechanism is using long basic prediction units. A stream prediction contains enough instructions to feed the execution engine during multiple cycles <ref type="bibr" target="#b15">[16]</ref>. Therefore, the longer a stream is, the more cycles the execution engine will be busy without requiring a new prediction. If streams are long enough, the execution engine of the processor can be kept busy during multiple cycles while a new prediction is being generated. Overlapping the execution of a prediction with the generation of the following prediction allows to partially hide the access delay of this second prediction, removing the need for an overriding mechanism, and thus reducing the fetch engine complexity.</p><p>Since instruction streams are limited by taken branches, a good way to obtain longer streams is removing taken branches through code optimizations. Code layout optimizations have a beneficial effect on the length of instruction streams <ref type="bibr" target="#b15">[16]</ref>. These optimizations try to map together those basic blocks that are frequently executed as a sequence. Therefore, most conditional branches in optimized code are not taken, enlarging instruction streams. However, code layout optimizations are not enough for the stream fetch engine to completely overcome the need for an overriding mechanism <ref type="bibr" target="#b16">[17]</ref>.</p><p>Looking for novel ways of enlarging streams, we present a detailed analysis of dynamic instruction streams. Our results show that most of them finalize in conditional branches, function calls, and return instructions. As a consequence, it would seem that these types of branches are the best candidates to apply techniques for enlarging instruction streams. However, according to Amdahl's law, focusing on particular branch types is not a good approach to enlarge instruction streams. If we focus on a particular type of stream, the remainder streams, which do not benefit from the stream enlargement, will limit the achievable performance improvement. This leads to a clear conclusion: the correct approach is not focusing on particular branch types, but trying to enlarge all dynamic streams. In order to achieve this, we present the multiple stream predictor, a novel predictor that concatenates those streams that are frequently executed as a sequence. This predictor does not depend on the type of the branch terminating the stream, making it possible to generate very long virtual streams.</p><p>The remainder of this paper is organized as follows. Section 2 describes previous related work. Section 3 presents our experimental methodology. Section 4 provides an analysis of dynamic instruction streams. Section 5 describes the multiple stream predictor. Section 6 evaluates the proposed predictor. Finally, Section 7 presents our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The prediction table access latency is an important limiting factor for current fetch architectures. The processor front-end must generate the fetch address in a single cycle because this address is needed for fetching instructions in the next cycle. However, the increase in processor clock frequency, as well as the slower wires in modern technologies, cause branch prediction tables to require multi-cycle accesses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The trace predictor <ref type="bibr" target="#b4">[5]</ref> is a latency tolerant mechanism, since each trace prediction is potentially a multiple branch prediction. The processor front-end can use a single trace prediction to feed the processor back-end with instructions during multiple cycles, while the trace predictor is being accessed again to obtain a new prediction. Overlapping the prediction table access with the fetch of instructions from a previous prediction allows to hide the branch predictor access delay. Our next stream predictor has the same ability <ref type="bibr" target="#b16">[17]</ref>, since a stream prediction is also a multiple branch prediction able to provide enough instructions to hide the prediction table access latency.</p><p>Using a fetch target queue (FTQ) <ref type="bibr" target="#b11">[12]</ref> is also helpful for taking advantage of this fact. The FTQ decouples the branch prediction mechanism and the instruction cache access. Each cycle, the branch predictor generates the fetch address for the next cycle, and a fetch request that is stored in the FTQ. Since the instruction cache is driven by the requests stored in the FTQ, the fetch engine is less likely to stay idle while the predictor is being accessed again.</p><p>Another promising idea to tolerate the prediction table access latency is pipelining the branch predictor <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. Using a pipelined predictor, a new prediction can be started each cycle. Nevertheless, this is not trivial, since the outcome of a branch prediction is needed to start the next prediction. Therefore, each branch prediction can only use the information available in the cycle it starts, which has a negative impact on prediction accuracy. In-flight information could be taken into account when a prediction is generated, as described in <ref type="bibr" target="#b19">[20]</ref>, but this also involves an increase in the fetch engine complexity. It is possible to reduce this complexity in the fetch engine of a simultaneous multithreaded processor by pipelining the branch predictor and interleaving prediction requests from different threads each cycle <ref type="bibr" target="#b1">[2]</ref>. Nevertheless, analyzing the accuracy and performance of pipelined branch predictors is out of the scope of this work.</p><p>A different approach is the overriding mechanism described by Jimenez et al. <ref type="bibr" target="#b6">[7]</ref>. This mechanism provides two predictions, a first prediction coming from a fast branch predictor, and a second prediction coming from a slower, but more accurate predictor. When a branch instruction is predicted, the first prediction is used while the second one is still being calculated. Once the second prediction is obtained, it overrides the first one if they differ, since the second predictor is considered to be the most accurate. A similar mechanism is used in the Alpha EV6 <ref type="bibr" target="#b2">[3]</ref> and EV8 <ref type="bibr" target="#b18">[19]</ref> processors.</p><p>The problem of prediction overriding is that it requires a significant increase in the fetch engine complexity. An overriding mechanism requires a fast branch predictor to obtain a prediction each cycle. This prediction should be stored for being compared with the main prediction. Some cycles later, when the main prediction is generated, the fetch engine should determine whether the first prediction is correct or not. If the first prediction is wrong, all the speculative work done based on it should be discarded. Therefore, the processor should track which instructions depend on each prediction done in order to allow the recovery process. This is the main source of complexity of the overriding technique.</p><p>Moreover, a wrong first prediction does not involve that all the instructions fetched based on it are wrong. Since both the first and the main predictions start in the same fetch address, they will partially coincide. Thus, the correct instructions based on the first prediction should not be squashed. This selective squash will increase the complexity of the recovery mechanism. To avoid this complexity, a full squash could be done when the first and the main predictions differ, that is, all instructions depending on the first prediction are squashed, even if they should be executed again according to the main prediction. However, a full squash will degrade the processor performance and does not remove all the complexity of the overriding mechanism. Therefore, the challenge is to develop a technique able to achieve the same performance than an overriding mechanism, but avoiding its additional complexity, which is the objective of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Methodology</head><p>The results in this paper have been obtained using trace driven simulation of a superscalar processor. Our simulator uses a static basic block dictionary to allow simulating the effect of wrong path execution. This model includes the simulation of wrong speculative predictor history updates, as well as the possible interference and prefetching effects on the instruction cache. We feed our simulator with traces of 300 million instructions collected from the SPEC 2000 integer benchmarks<ref type="foot" target="#foot_1">1</ref> using the reference input set. To find the most representative execution segment we have analyzed the distribution of basic blocks as described in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Since previous work <ref type="bibr" target="#b15">[16]</ref> has shown that code layout optimizations are able to enlarge instruction streams, we present data for both a baseline and an optimized code layout. The baseline code layout was generated using the Compaq C V5.8-015 compiler on Compaq UNIX V4.0. The optimized code layout was generated with the Spike tool shipped with Compaq Tru64 Unix 5.1. Optimized code generation is based on profile information collected by the Pixie V5.2 tool using the train input set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simulator Setup</head><p>Our simulation setup corresponds to an aggressive 8-wide superscalar processor. The main values of this setup are shown in Table <ref type="table" target="#tab_0">1</ref>. We compare our stream fetch architecture with three other state-of-the-art fetch architectures: a fetch architecture using an interleaved BTB and a 2bcgskew predictor <ref type="bibr" target="#b18">[19]</ref>, the fetch target buffer (FTB) architecture <ref type="bibr" target="#b11">[12]</ref> using a perceptron predictor <ref type="bibr" target="#b7">[8]</ref>, and the trace cache fetch architecture using a trace predictor <ref type="bibr" target="#b4">[5]</ref>. All these architectures use an 8-entry fetch target queue (FTQ) <ref type="bibr" target="#b11">[12]</ref> to decouple branch prediction from the instruction cache access. We have found that larger FTQs do not provide additional performance improvements.</p><p>Our instruction cache setup uses wide cache lines, that is, 4 times the processor fetch width <ref type="bibr" target="#b10">[11]</ref>, and 64KB total hardware budget. The trace fetch architecture is actually evaluated using a 32KB instruction cache, while the remainder 32KB are devoted to the trace cache. This hardware budget is equally divided into a filter trace cache <ref type="bibr" target="#b12">[13]</ref> and a main trace cache. In addition, we use selective trace storage <ref type="bibr" target="#b9">[10]</ref> to avoid trace redundancy between the trace cache and the instruction cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fetch Models</head><p>The stream fetch engine <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> model is shown in Figure <ref type="figure" target="#fig_2">2</ref>.a. The stream predictor access is decoupled from the instruction cache access using an FTQ. The stream predictor generates requests, composed by a full stream of instructions, which are stored in the FTQ. These requests are used to drive the instruction cache, obtain a line from it, and select which instructions from the line should be executed. In the same way, the remainder three fetch models use an FTQ to decouple the branch prediction stage from the fetch stage.</p><p>Our interleaved BTB fetch model (iBTB) is inspired by the EV8 fetch engine design described in <ref type="bibr" target="#b18">[19]</ref>. This iBTB model decouples the branch prediction mechanism from the instruction cache with an FTQ. An interleaved BTB is used to allow the prediction of multiple branches until a taken branch is predicted, or until an aligned 8-instruction block is completed. The branch prediction history is updated using a single bit for prediction block, which combines the outcome of the last branch in the block with path information <ref type="bibr" target="#b18">[19]</ref>. Our FTB model is similar to the one described in <ref type="bibr" target="#b11">[12]</ref> but using a perceptron branch predictor <ref type="bibr" target="#b7">[8]</ref> to predict the direction of conditional branches.  Our trace cache fetch model is similar to the one described in <ref type="bibr" target="#b13">[14]</ref> but enhanced using an FTQ <ref type="bibr" target="#b11">[12]</ref> to decouple the trace predictor from the trace cache, as shown in Figure <ref type="figure" target="#fig_2">2</ref>.c. Trace predictions are stored in the FTQ, which feeds the trace cache with trace identifiers. An interleaved BTB is used to build traces in the case of a trace cache miss. This BTB uses 2-bit saturating counters to predict the direction of conditional branches when a trace prediction is not available. In addition, an aggressive 2-way interleaved instruction cache is used to allow traces to be built as fast as possible. This mechanism is able to obtain up to a full cache line in a cycle, independent of PC alignment.</p><p>The four fetch architectures evaluated in this paper use specialized structures to predict return instructions. The iBTB, the FTB, and the stream fetch architecture use a return address stack (RAS) <ref type="bibr" target="#b8">[9]</ref> to predict the target address of return instructions. There are actually two RAS, one updated speculatively in prediction stage, and another one updated non-speculatively in commit stage, which is used to restore the correct state in case of a branch misprediction. The iBTB and FTB fetch architectures also use a cascaded structure <ref type="bibr" target="#b14">[15]</ref> to improve the prediction accuracy of the rest of indirect branches. Both the stream predictor and the trace predictor are accessed using correlation, and thus they are already able to correctly predict indirect jumps and function calls.</p><p>The trace fetch architecture uses a return history stack (RHS) <ref type="bibr" target="#b4">[5]</ref> instead of a RAS. This mechanism is more efficient than a RAS in the context of trace prediction because the trace predictor is indexed using a history of previous trace identifiers instead of trace starting addresses. There are also two RHS, one updated speculatively in prediction stage, and another one updated nonspeculatively in commit stage. However, the RHS in the trace fetch architecture is less accurate predicting return instructions than the RAS in the rest of evaluated architectures. Trying to alleviate this problem, we also use a RAS to predict the target address of return instructions during the trace building process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Branch Prediction Setup</head><p>We have evaluated the four simulated fetch engines varying the size of the branch predictor from small and fast tables to big and slow tables. We use realistic prediction table access latencies calculated using the CACTI 3.0 tool <ref type="bibr" target="#b21">[22]</ref>. We modified CACTI to model tagless branch predictors, and to work with setups expressed in bits instead of bytes. Data we have obtained corresponds to 0.10μm technology. For translating the access time from nanoseconds to cycles, we assumed an aggressive 8 fan-out-of-four delays clock period, that is, a 3.47 GHz clock frequency as reported in <ref type="bibr" target="#b0">[1]</ref>. It has been claimed in <ref type="bibr" target="#b3">[4]</ref> that 8 fan-out-of-four delays is the optimal clock period for integer benchmarks in a high performance processor implemented in 0.10μm technology.</p><p>We have found that the best performance is achieved using three-cycle latency tables <ref type="bibr" target="#b16">[17]</ref>. Although bigger predictors are slightly more accurate, their increased access delay harms processor performance. On the other hand, predictors with a lower latency are too small and achieve poor performance. Therefore, we have chosen to simulate all branch predictors using the bigger tables that can be accessed in three cycles. Table <ref type="table" target="#tab_1">2</ref> shows the configuration of the simulated predictors. We have explored a wide range of history lengths, as well as DOLC index <ref type="bibr" target="#b4">[5]</ref> configurations, and selected the best one found for each setup. Table <ref type="table" target="#tab_1">2</ref> also shows the approximate hardware budget for each predictor. Since we simulate the larger three cycle latency tables<ref type="foot" target="#foot_2">2</ref> , the total hardware budget devoted to each predictor is different. The stream fetch engine requires less hardware resources because it uses a single prediction mechanism, while the other evaluated fetch architectures use some separate structures.</p><p>Our fetch models also use an overriding mechanism <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> to complete a branch prediction each cycle. A small branch predictor, supposed to be implemented using very fast hardware, generates the next fetch address in a single cycle. Although being fast, this predictor has low accuracy, so the main predictor is used to provide an accurate back-up prediction. This prediction is obtained three </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of Dynamic Instruction Streams</head><p>Fetching a single basic block per cycle is not enough to keep busy the execution engine of wide-issue superscalar processors during multiple cycles. In this context, the main advantage of instruction streams is their long size. A stream can contain multiple basic blocks, whenever only the last one ends in a taken branch. This makes it possible for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>. However, having high average length does not involve that most streams are long. Some streams could be long, providing high fetch bandwidth, while other streams could be short, limiting the potential performance. Therefore, in the search for new ways of improving the stream fetch engine performance, the distribution of dynamic stream lengths should be analyzed.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> shows an histogram of dynamic streams classified according to their length. It shows the percentage of dynamic streams that have a length ranging from 1 to 30 instructions. The last bar shows the percentage of streams that are longer than 30 instructions. Data is shown for both the baseline and the optimized 2% 4% 6% 8% 10% 12% 14% <ref type="table" target="#tab_1">1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 0 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 3</ref>  code layout. In addition, streams are divided according to the terminating branch type: conditional branches, unconditional branches, function calls, and returns.</p><p>Using the baseline code layout, most streams are shorter than the average length: 70% of the dynamic streams have 12 or less instructions. Using the optimized code layout, the average length is higher. However, most streams are still shorter than the average length: 70% of the dynamic streams have 15 or less instructions. Therefore, in order to increase the average stream length, research should be focused in those streams that are shorter than the average length. For example, if we consider an 8-wide execution core, research effort should be devoted to enlarge streams shorter than 8 instructions. Using optimized codes, the percentage of those streams is reduced from 40% to 30%. Nevertheless, there is still room for improvement.</p><p>Most dynamic streams finish in taken conditional branches. They are 60% when using the baseline code and 52% when using the optimized code. The percentage is lower in the optimized codes due to the higher number of not taken conditional branches, which never finish instruction streams. There also is a big percentage of streams terminating in function calls and returns. They are 30% of all dynamic streams in the baseline code. The percentage is larger in the optimized code: 36%. This happens because code layout optimizations are mainly focused on conditional branches. Since the number of taken conditional branches is lower, there is a higher percentage of streams terminating in other types of branches, although the total number is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multiple Stream Prediction</head><p>According to the analysis presented in the previous section, one could think that, in order to enlarge instruction streams, the most promising field for research are conditional branches, function calls, and return instructions. However, we have found that techniques for enlarging the streams finalizing in particular branch types achieve poor results <ref type="bibr" target="#b17">[18]</ref>. This is due to Amdahl's law: although these techniques enlarge a set of instructions streams, there are other streams that are not enlarged, limiting the achievable benefit. Therefore, we must try to enlarge not particular stream types, but all instruction streams. Our approach to achieve this is the multiple stream predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Multiple Stream Predictor</head><p>The next stream predictor <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>, which is shown in Figure <ref type="figure">4</ref>.a, is a specialized branch predictor that provides stream level sequencing. Given a fetch address, i.e., the current stream starting address, the stream predictor provides the current stream length, which indicates where is the taken branch that finalizes the stream. The predictor also provides the next stream starting address, which is used as the fetch address for the next cycle. The current stream starting address and the current stream length form a fetch request that is stored in the FTQ. The fetch requests stored in the FTQ are then used to drive the instruction cache.</p><p>Actually, the stream predictor is composed by two cascaded tables: a first level table indexed only by the fetch address, and a second level table indexed using path correlation. A stream is only introduced in the second level if it is not accurately predicted by the first level. Therefore, those streams that do not need correlation are kept in the first level, avoiding unnecessary aliasing. In order to generate a prediction, both levels are looked up in parallel. If there is a second level table hit, its prediction is used. Otherwise, the prediction of the first level table is used. The second level prediction is prioritized because it is supposed to be more accurate than the first level due to the use of path correlation.</p><p>The objective of our multiple stream predictor is predicting together those streams that are frequently executed as a sequence. Unlike the trace cache, the instructions corresponding to a sequence of streams are not stored together in a special purpose buffer. The instruction streams belonging to a predicted sequence are still separate streams stored in the instruction cache. Therefore, the multiple stream predictor does not enable the ability of fetching instructions beyond a taken branch in a single cycle. The benefit of our technique comes from grouping predictions, allowing to tolerate the prediction table access latency. which corresponds to the starting address of the stream sequence. However, the rest of the fields should be duplicated. The tag and length fields determine the first stream that should be executed. The target of this stream, determined by the next stream field, is the starting address of the second stream, whose length is given by the second length field. The second next stream field is the target of the second stream, and thus the next fetch address.</p><p>In this way, a single prediction table lookup provides two separate stream predictions, which are supposed to be executed sequentially. After a multiple stream prediction, every stream belonging to a predicted sequence is stored separately in the FTQ, which involves that using the multiple-stream predictor does not require additional changes in the processor front-end. Extending this mechanism for predicting three or more streams per sequence would be straightforward, but we have found that sequences having more than two streams do not provide additional benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multiple Stream Predictor Design</head><p>Providing two streams per prediction needs duplicating the prediction table size. In order to avoid a negative impact on the prediction table access latency and energy consumption, we only store multiple streams in the first-level table of the cascaded stream predictor, which is smaller than the second-level table. Since the streams belonging to a sequence are supposed to be frequently executed together, it is likely that, given a fetch address, the executed sequence is always the same. Consequently, stream sequences do not need correlation to be correctly predicted, and thus keeping them in the first level table does not limit the achievable benefit.</p><p>In order to take maximum advantage of the available space in the first level table, we use hysteresis counters to detect frequently executed stream sequences. Every stream in a sequence has a hysteresis counter associated to it. All hysteresis counters behave like the counter used by the original stream predictor to decide whether a stream should be replaced from the prediction table <ref type="bibr" target="#b15">[16]</ref>. When the predictor is updated with a new stream, the corresponding counter is increased if the new stream matches with the stream already stored in the selected entry. Otherwise, the counter is decreased and, if it reaches zero, the whole predictor entry is replaced with the new data, setting the counter to one. If the decreased counter does not reach zero, the new data is discarded. We have found that 3bit hysteresis counters, increased by one and decreased by two, provide the best results for the multiple stream predictor.</p><p>When the prediction table is looked up, the first stream is always provided. However, the second stream is only predicted if the corresponding hysteresis counter is saturated, that is, if the counter has reached its maximum value. Therefore, if the second hysteresis counter is not saturated, the multiple stream predictor provides a single stream prediction as it would be done by the original stream predictor. On the contrary, if the two hysteresis counters are saturated, then a frequently executed sequence has been detected, and the two streams belonging to this sequence are introduced in the FTQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation of the Multiple Stream Predictor</head><p>Our multiple stream predictor is able to provide a high amount of instructions per prediction. Figure <ref type="figure" target="#fig_5">5</ref> shows an histogram of instructions provided per prediction. It shows the percentage of predictions that provide an amount of instructions ranging from 1 to 30 instructions. The last bar shows the percentage of predictions that provide more than 30 instructions. Data are shown for both the baseline and the optimized code layout. In addition, data are shown for the original single-stream predictor, described in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>, and a 2-stream multiple predictor.</p><p>The main difference between both code layouts is that, as can be expected, there is a lower percentage of short streams in the optimized code. Besides, it is clear that our multiple stream predictor efficiently deals with the most harmful problem, that is, the shorter streams. Using our multiple stream predictor, there is an important reduction in the percentage of predictions that provide a small number of instructions. Furthermore, there is an increase in the percentage of predictions that provide more than 30 instructions, especially when using optimized codes. The lower number of short streams points out that the multiple stream predictor is an effective technique for hiding the prediction table access latency by overlapping table accesses with the execution of useful instructions.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> shows the average processor performance achieved by the four evaluated fetch architectures, for both the baseline and the optimized code layout. We have evaluated a wide range of predictor setups and selected the best one found for each evaluated predictor. Besides the performance of the four fetch engines using overriding, the performance achieved by the trace cache fetch architecture and the stream fetch engine not using overriding is also shown. In the latter case, the stream fetch engine uses a 2-stream multiple predictor instead of the original single-stream predictor.</p><p>The main observation from Figure <ref type="figure" target="#fig_6">6</ref> is that the multiple stream predictor without overriding provides a performance very close to the original single-stream predictor using overriding. The performance achieved by the multiple stream predictor without overriding is enough to outperform both the iBTB and the 0% 5% 10% 15% 20% 25% <ref type="table" target="#tab_1">1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 0 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 3</ref>   FTB fetch architectures, even when they do use overriding. The performance of the multiple stream predictor without overriding is also close to a trace cache using overriding, while requiring lower complexity.</p><p>It should be taken into account that this improvement is achieved by increasing the size of the first level table. Fortunately, the tag array is unmodified and no additional access port is required. We have checked using CACTI <ref type="bibr" target="#b21">[22]</ref> that the increase in the predictor area is less than 12%, as well as that the prediction table access latency is not increased. Moreover, our proposal not only avoids the need for a complex overriding mechanism, but also reduces the predictor energy consumption. Although the bigger first level table consumes more energy per access, it is compensated with the reduction in the number of prediction table accesses. The ability of providing two streams per prediction causes 35% reduction in the total number of prediction table lookups an updates, which leads to 12% reduction in the overall stream predictor energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Current technology trends create new challenges for the fetch architecture design. Higher clock frequencies and larger wire delays cause branch prediction tables to require multiple cycles to be accessed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, limiting the fetch engine performance. This fact has led to the development of complex hardware mechanisms, like prediction overriding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, to hide the prediction table access delay.</p><p>To avoid this increase in the fetch engine complexity, we propose to use long instruction streams <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> as basic prediction unit, which makes it possible to hide the prediction table access delay. If instruction streams are long enough, the execution engine can be kept busy executing instructions from a stream during multiple cycles, while a new stream prediction is being generated. Therefore, the prediction table access delay can be hidden without requiring any additional hardware mechanism.</p><p>In order to take maximum advantage of this fact, it is important to have streams as long as possible. We achieve this using the multiple stream predictor, a novel predictor design that combines frequently executed instruction streams into long virtual streams. Our predictor provides instruction streams long enough for allowing a processor not using overriding to achieve a performance close to a processor using prediction overriding, that is, we achieve a very similar performance at a much lower complexity, also requiring less energy consumption.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of instruction streams</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2.b shows a diagram representing these two fetch architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Fetch models evaluated</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Histograms of dynamic streams classified according to their length and the terminating branch type. The results presented in these histograms are the average of the eleven benchmarks used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Fig. 4 .</head><label>44</label><figDesc>Fig. 4. The next stream predictor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Histograms of dynamic predictions, classified according to the amount of instructions provided, when using a single-stream predictor and a 2-stream multiple predictor. The results presented in these histograms are the average of the eleven benchmarks used.</figDesc><graphic url="image-155.png" coords="14,92.94,392.87,247.72,123.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Processor performance when using (full bar) and not using (shadowed bar) overriding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Configuration of the simulated processor</figDesc><table><row><cell cols="2">fetch, rename, and commit width</cell><cell>8 instructions</cell></row><row><cell cols="2">integer and floating point issue width</cell><cell>8 instructions</cell></row><row><cell>load/store issue width</cell><cell></cell><cell>4 instructions</cell></row><row><cell>fetch target queue</cell><cell></cell><cell>8 entries</cell></row><row><cell>instruction fetch queue</cell><cell></cell><cell>32 entries</cell></row><row><cell cols="2">integer, floating point, and load/store issue queues</cell><cell>64 entries</cell></row><row><cell>reorder buffer</cell><cell></cell><cell>256 entries</cell></row><row><cell cols="2">integer and floating point registers</cell><cell>160</cell></row><row><cell>L1 instruction cache</cell><cell cols="2">64/32 KB, 2-way, 128 byte block, 3 cycle latency</cell></row><row><cell>L1 data cache</cell><cell cols="2">64 KB, 2-way, 64 byte block, 3 cycle latency</cell></row><row><cell>L2 unified cache</cell><cell cols="2">1 MB, 4-way, 128 byte block, 16 cycle latency</cell></row><row><cell>main memory latency</cell><cell cols="2">350 cycles</cell></row><row><cell>maximum trace size</cell><cell cols="2">32 instructions (10 branches)</cell></row><row><cell>filter and main trace caches</cell><cell cols="2">128 traces, 4-way associative</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Configuration of the simulated branch predictors</figDesc><table><row><cell cols="3">iBTB fetch (approx. 95KB)</cell></row><row><cell>2bcgskew predictor</cell><cell>interleaved BTB</cell><cell>1-cycle predictor</cell></row><row><cell>four 64K entry tables</cell><cell>1024 entry, 4-way, first level</cell><cell>64 entry gshare</cell></row><row><cell>16 bit history</cell><cell>4096 entry, 4-way, second level</cell><cell>6-bit history</cell></row><row><cell>(bimodal 0 bits)</cell><cell>DOLC 14-2-4-10</cell><cell>32 entry, 1-way, BTB</cell></row><row><cell cols="3">FTB fetch architecture (approx. 50KB)</cell></row><row><cell>perceptron predictor</cell><cell>FTB</cell><cell>1-cycle predictor</cell></row><row><cell>256 perceptrons</cell><cell>1024 entry, 4-way, first level</cell><cell>64 entry gshare</cell></row><row><cell>4096x14 bit local and</cell><cell>4096 entry, 4-way, second level</cell><cell>6-bit history</cell></row><row><cell>40 bit global history</cell><cell>DOLC 14-2-4-10</cell><cell>32 entry, 1-way, BTB</cell></row><row><cell cols="3">Stream fetch architecture (approx. 32KB)</cell></row><row><cell cols="2">next stream predictor</cell><cell>1-cycle predictor</cell></row><row><cell cols="2">1024 entry, 4-way, first level</cell><cell>32 entry, 1-way, spred</cell></row><row><cell cols="2">4096 entry, 4-way, second level</cell><cell>DOLC 0-0-0-5</cell></row><row><cell cols="2">DOLC 16-2-4-10</cell><cell></cell></row><row><cell cols="3">Trace fetch architecture (approx. 80KB)</cell></row><row><cell>next trace predictor</cell><cell>interleaved BTB</cell><cell>1-cycle predictor</cell></row><row><cell>2048 entry, 4-way, first level</cell><cell cols="2">1024 entry, 4-way, first level 32 entry, 1-way, tpred</cell></row><row><cell cols="2">4096 entry, 4-way, second level 4096 entry, 4-way, second level</cell><cell>DOLC 0-0-0-5</cell></row><row><cell>DOLC 10-4-7-9</cell><cell>DOLC 14-2-4-10</cell><cell>perfect BTB override</cell></row></table><note>cycles later and compared with the prediction provided by the single-cycle predictor. If both predictions differ, the new prediction overrides the previous one, discarding the speculative work done based on it. The configuration of the singlecycle predictors used is shown in Table2.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">J. Labarta, K. Joe, and T. Sato (Eds.): ISHPC 2005 and ALPS 2006, LNCS 4759, pp. 1-16, 2008. c Springer-Verlag Berlin Heidelberg 2008</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">We excluded 181.mcf because its performance is very limited by data cache misses, being insensitive to changes in the fetch architecture.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">The first level of the trace and stream predictors, as well as the first level of the cascaded iBTB and FTB, is actually smaller than the second one because larger first level tables do not provide a significant improvement in prediction accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by the Ministry of Education of Spain under contract TIN-2004-07739-C02-01, the HiPEAC European Network of Excellence, the Barcelona Supercomputing Center, and an Intel fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clock rate versus IPC: The end of the road for conventional microarchitectures</title>
		<author>
			<persName><forename type="first">V</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hrishikesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tolerating branch predictor latency on SMT</title>
		<author>
			<persName><forename type="first">A</forename><surname>Falcón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on High Performance Computing</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Digital 21264 sets new standard</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gwennap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The optimal useful logic depth per pipeline stage is 6-8 fo4</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hrishikesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Path-based next trace prediction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Microarchitecture</title>
		<imprint>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reconsidering complex branch predictors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Intl. Conf. on High Performance Computer Architecture</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The impact of delay on the design of branch predictors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Microarchitecture</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic branch prediction with perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Intl. Conf. on High Performance Computer Architecture</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Branch history table prediction of moving target branches due to subroutine returns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Emma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trace cache redundancy: red &amp; blue traces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on High Performance Computer Architecture</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fetching instruction streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Microarchitecture</title>
		<imprint>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A scalable front-end architecture for fast instruction delivery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on Computer Architecture</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Filtering techniques to improve trace cache efficiency</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Intl. Conf. on Parallel Architectures and Compilation Techniques</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A trace cache microarchitecture and evaluation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comprehensive analysis of indirect branch prediction</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Falcón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on High Performance Computing</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A low-complexity fetch architecture for high-performance superscalar processors</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latency tolerant branch predictors</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Workshop on Innovative Architecture for Future Generation High-Performance Processors and Systems</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Techniques for enlarging instruction streams</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<idno>UPC-DAC-RR-2005-11</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Departament d&apos;Arquitectura de Computadors, Universitat Politècnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Design tradeoffs for the Alpha EV8 conditional branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective ahead pipelining of instruction block address generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fraboulet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Basic block distribution analysis to find periodic behavior and simulation points in applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Parallel Architectures and Compilation Techniques</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">CACTI 3.0: an integrated cache timing, power and area model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<publisher>Western Research Laboratory</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
