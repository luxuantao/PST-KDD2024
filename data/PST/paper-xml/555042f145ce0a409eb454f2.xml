<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">Chen</forename><forename type="middle">M</forename><surname>Xilin</surname></persName>
						</author>
						<author role="corresp">
							<persName><surname>Bevilacqua</surname></persName>
							<email>marco.bevilacqua@imtlucca.it</email>
						</author>
						<author>
							<persName><forename type="first">M.-L</forename><forename type="middle">Alberi</forename><surname>Morel</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institut National de Recherche en Informatique et en Automatique-Alcatel Lucent</orgName>
								<orgName type="laboratory">Joint Research Laboratoire</orgName>
								<orgName type="institution">Bell Labs</orgName>
								<address>
									<settlement>Nozay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IMT Institute for Advanced Studies Lucca</orgName>
								<address>
									<postCode>55100</postCode>
									<settlement>Lucca</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institut National de Recherche en Informatique et en Automatique</orgName>
								<address>
									<postCode>35042</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Centre de Villarceaux</orgName>
								<orgName type="institution">Bell Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Alcatel-Lucent</orgName>
								<address>
									<postCode>91620</postCode>
									<settlement>Nozay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A0639A48D6B221C8388F8BABCB3B9BDE</idno>
					<idno type="DOI">10.1109/TIP.2014.2364116</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single-Image Super-Resolution via Linear Mapping of Interpolated Self-Examples Marco Bevilacqua, Aline Roumy, Member, IEEE, Christine Guillemot, Fellow, IEEE, and Marie-Line Alberi Morel</p><p>Abstract-This paper presents a novel example-based single-image superresolution procedure that upscales to high-resolution (HR) a given low-resolution (LR) input image without relying on an external dictionary of image examples. The dictionary instead is built from the LR input image itself, by generating a double pyramid of recursively scaled, and subsequently interpolated, images, from which self-examples are extracted. The upscaling procedure is multipass, i.e., the output image is constructed by means of gradual increases, and consists in learning special linear mapping functions on this double pyramid, as many as the number of patches in the current image to upscale. More precisely, for each LR patch, similar self-examples are found, and, because of them, a linear function is learned to directly map it into its HR version. Iterative back projection is also employed to ensure consistency at each pass of the procedure. Extensive experiments and comparisons with other state-of-the-art methods, based both on external and internal dictionaries, show that our algorithm can produce visually pleasant upscalings, with sharp edges and well reconstructed details. Moreover, when considering objective metrics, such as Peak signal-to-noise ratio and Structural similarity, our method turns out to give the best performance. Index Terms-Super resolution, example-based, regression, neighbor embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S UPER-RESOLUTION (SR) refers to a family of techniques that aim at increasing the resolution of given images. Nowadays, many applications, e.g. video surveillance and remote sensing, require the display of images at a considerable resolution, that may not be easy to obtain given the limitations of physical imaging systems or environmental conditions. Moreover, with the spread of digital technologies, there is a tremendous amount of user-produced images collected in the years, that are valuable but may be affected by a poor quality. Therefore, techniques to augment the resolution of an image, i.e. the total number of pixels, and contextually improve its visual quality, are particularly appealing.</p><p>SR methods are traditionally categorized according to the number of input images: when several low-resolution (LR) input images are available we speak about multi-frame SR algorithm; vice versa, when the LR input image is unique, we have the single-image SR problem. In both cases, as an output, a unique high-resolution (HR) super-resolved image is produced.</p><p>Multi-frame SR methods, of which a good overview is given in <ref type="bibr" target="#b0">[1]</ref>, are the first ones that have been studied, since the SR problem first appeared in the scientific community <ref type="bibr" target="#b1">[2]</ref>. Here, the multiple LR input images are considered as different views of the same scene, taken with sub-pixel misalignment, i.e. each image is seen as a degraded version of an underlying HR image to be estimated, where the degradation processes can include blurring, geometrical transformations, and downsampling. Single-image SR, instead, aims at constructing the HR output image from as little as a single LR input image. The problem stated is an inherently ill-posed problem, as there can be several HR images generating the same LR image. Single-image SR is deeply connected with traditional "analytical" interpolation, since they share the same goal. Traditional interpolation methods, e.g. bicubic interpolation, by computing the missing pixels in the HR grid as averages of known pixels, implicitly impose a "smoothness" prior. However, natural images often present strong discontinuities, such as edges and corners, and thus the smoothness prior results in producing ringing and blurring artifacts in the output image. Single-image SR algorithms can be broadly classified into two main approaches: interpolation-based methods <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, which, possibly in a nonparametric fashion, follow the interpolation approach by posing more sophisticated statistical priors; and machine learning (ML) based methods.</p><p>In particular, the latter, by taking advantage of the powerfulness of ML techniques, have shown to give very challenging results, and many algorithms appeared in the literature in the recent years. ML-based algorithms can consist in pixel-based procedures, where each value in the HR output image is singularly inferred via statistical learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, or patch-based procedures, where the HR estimation is performed thanks to a dictionary of correspondences of LR and HR patches (i.e. squared blocks of image pixels). ML-based SR that makes use of patches is also referred to as example- based SR <ref type="bibr" target="#b7">[8]</ref>. In the upscaling procedure the LR input image itself is divided into patches, and for each LR input patch a single HR output patch is reconstructed, by observing the "examples" contained in the dictionary.</p><p>Example-based methods mainly vary in two aspects: the patch reconstruction method used and the typology of dictionary. As for the method to perform the single patch reconstructions, we have again two main categories: codingbased reconstruction methods and direct mapping (DM). Neighbor embedding (NE) belongs to coding-based methods. In NE-based SR <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, for each LR input patch we select K similar LR examples in the dictionary by nearest neighbor search (NNS), and a linear combination of these neighbors is computed to possibly approximate the input patch. The corresponding HR neighbors are then similarly combined, i.e. by using the same weights computed with the LR patches, to generate the HR output patch. This approach relies on what in <ref type="bibr" target="#b11">[12]</ref> is called "manifold assumption": the linear combination weights are meant to capture the local geometry of a manifold on which the LR patches are supposed to lay; by applying the same weights to reconstruct the HR patches, we implicitly assume that they too lay on a manifold with similar local structures. In order to enforce the assumption of manifold similarity between the two distinct spaces represented by the LR and HR patches, <ref type="bibr">Gao et al.</ref> propose in <ref type="bibr" target="#b12">[13]</ref> to compute the weights in a subspace common to the LR and HR patches. The method therefore assumes the existence of a common low-dimensional space that preserves some meaningful characteristics of the patches.</p><p>Example-based SR via sparse representations <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> also falls within the family of coding-based methods, and can be considered very close to NE-based SR: however, here, the weights are not computed on neighbors found by NNS, but typical sparse coding algorithms are employed. The method presented in <ref type="bibr" target="#b15">[16]</ref> bridges the two approaches (NE and sparse representations), by proposing a "sparse neighbor embedding" algorithm. In <ref type="bibr" target="#b16">[17]</ref>, instead, another sparse-representation-based SR algorithm is presented, which,  in addition to a sparse example-based term, uses several regularization terms to globally optimize the image generation process.</p><p>All the methods so far require manifold similarity between the LR and HR spaces. A different patch reconstruction approach that does not rely on this assumption is given instead by direct mapping (DM). Example-based SR via DM <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> aims at finding, in fact, for each patch reconstruction, a function that directly maps a given LR patch into its HR version. The mapping function is commonly found with traditional regression methods.</p><p>As for the second discriminating aspect of example-based SR, the typology of the dictionary, we have mainly two choices: an external dictionary, built from a set of external training images, and an "internal" one, built without using any other image than the LR input image itself. This latter case exploits the so called "self-similarity" property, typical of natural images, according to which image structures tend to repeat within and across different image scales: therefore, patch correspondences can be found in the input image itself Operating schemes of the local learning based reconstruction procedure, for the two approaches described (NE and DM). From the image it is clear how NE exploits the intra-space relationships, by learning a model among the LR patches and applying it on the HR patches. DM, instead, exploits the "horizontal" relationships, by attempting to learn the mapping between the two spaces. (a) Neighbor Embedding. (b) Direct Mapping. and possibly scaled versions of it. To learn these patch correspondences, that specifically take the name of self-examples, we can have one-step schemes <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> or schemes based on a pyramid of recursively scaled images starting from the LR input image <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. Clearly, the advantage of having an external dictionary instead of an internal one lies in the fact that it is built in advance, while the internal one is generated online and updated at each run of the algorithm. However, external dictionaries have a considerable drawback: they are fixed and thus non-adapted to the input image. The study conducted in <ref type="bibr" target="#b25">[26]</ref> also confirms the benefit of using internal statistics in patch-based image processing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Main Contributions</head><p>In this paper we present a novel example-based SR algorithm that makes use of an internal dictionary. The contributions are twofold. First, we build a "double pyramid", where the traditional image pyramid of <ref type="bibr" target="#b22">[23]</ref> is juxtaposed with a pyramid of interpolated images. Second, HR patches are reconstructed through DM, whereas in <ref type="bibr" target="#b22">[23]</ref> the HR patches are reconstructed via NE. Therefore, in the proposed algorithm, a linear mapping is learned on the double pyramid from the LR interpolated patches to the HR patches, and then applied to each interpolated LR input patch. As said before, unlike NE, DM does not rely on any theoretical assumption (i.e. a manifold similarity between the LR and HR spaces), and is therefore preferable.</p><p>The rationale for the interpolation operation is to make the computation of the single mapping functions via regression more robust (LR and HR patches, in fact, turn out to have the same sizes). Moreover, a Tikhonov regularization is added to the problem to provide numerical stability in the computation, since the linear mapping requires a matrix inversion. A doublepyramid-like scheme has also been introduced in <ref type="bibr" target="#b21">[22]</ref>. However, our proposed algorithm differs from <ref type="bibr" target="#b21">[22]</ref> in two ways. First, the reconstruction in <ref type="bibr" target="#b21">[22]</ref> is based on a neighbor embedding technique analyzed in <ref type="bibr" target="#b26">[27]</ref> (NE with a sum-to-one constraint). This NE technique was shown to have a performance highly dependent on the number of neighbors chosen. More precisely, when this number is equal to the size of the LR input vectors, the performance of the algorithms dramatically drops. As a second discriminating aspect, while <ref type="bibr" target="#b21">[22]</ref> initializes the pyramid by down-scaling the LR input image only once, we instead initialize the pyramid with many sub-levels by recursively down-scaling the LR input image. We show that many sub levels are needed in order to better reconstruct a HR image. More precisely, we have that at the first iteration of the algorithm around 35% of the selected patches come from the pyramid levels below the first one (see Fig. <ref type="figure" target="#fig_2">3</ref> for details). The patches initially selected play a crucial role: in fact, since the whole algorithm is by nature recursive, it is very sensitive to its initialization.</p><p>All the listed contributions are possible at the cost of a slight increase in the complexity of the algorithm (i.e. +10.7% for a scale factor of 3 and +8.1% for a scale factor of 4; see Section IV-B for details), while having a beneficial effect on the quality performance, both in terms of objective metrics Peak signal-to-noise ratio (PSNR) and Structural similarity (SSIM) and visual results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Organization of the Paper</head><p>The rest of the paper is organized as follows. Section II reviews the fundamentals of example-based SR using an internal dictionary, by giving also a general introduction of NE and DM methods. Then, in Section III, our algorithm is described in detail, by explaining how the internal dictionary is trained and the whole upscaling procedure. Before drawing the conclusions, Section IV presents some extensive experiments done: the different implementation choices are here validated and the algorithm is compared with other state-of-the-art methods, by showing visual and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXAMPLE-BASED SR WITH SELF-EXAMPLES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Principles and Notations</head><p>Single-image SR is the problem of estimating an underlying HR image, given only one observed LR image. The generation process of the LR image from the original HR image, that we consider, can be written as Given a reference patch in the interpolated version of the starting image U (I 0 ), x l i , 3 LR neighbors are found in the "side pyramid" of interpolated levels (y l 1 , y l 2 , y l 3 ). Thanks to these and the corresponding HR neighbors (y h 1 , y h 2 , y h 3 ), a linear function M is meant to be learned to directly map x l i into its corresponding HR output patch x h i .</p><formula xml:id="formula_0">I L = (I H * B) ↓ s ,<label>(1)</label></formula><p>where I L and I H are respectively the LR and HR image, B is a blur kernel the original image is convoluted with, which is typically modeled as a Gaussian blur <ref type="bibr" target="#b27">[28]</ref>, and the expression ↓ s denotes a downsampling operation by a scale factor of s.</p><p>The LR image is then a blurred and down-sampled version of the original HR image.</p><p>Example-based single-image SR aims at reversing the image generation model <ref type="bibr" target="#b0">(1)</ref>, by means of a dictionary of image examples that map locally the relation between an HR image and its LR counterpart, the latter obtained with the model <ref type="bibr" target="#b0">(1)</ref>. For general upscaling purposes, the examples used are typically in the form of patches, i.e. squared blocks of pixels (e.g. 3 × 3 or 5 × 5 blocks). The dictionary is then a collection of patches, which, two by two, form pairs. A pair specifically consists of a LR patch and its HR version with enriched high frequency details.</p><p>Example-based SR algorithms comprise two phases:</p><p>1) A training phase, where the above-mentioned dictionary of patches is built; 2) The proper super-resolution phase, that uses the dictionary created to upscale the input image.</p><p>As for the training phase, in the next section we discuss how to build an "internal" dictionary of patches, i.e. starting from as little as the LR input image. As for the SR phase, instead, in example-based algorithms the patch is also the reconstruction unit used in the upscaling procedure. In fact, the LR input image is partitioned into patches; for each single LR input patch, then, by using the LR-HR patch correspondences in the dictionary, a HR output patch is constructed. The HR output image is finally built by re-assembling all the reconstructed HR patches. In Section II-C we revise the two main patch reconstruction approaches: neighbor embedding and direct mapping.</p><p>Hereinafter in this paper we will use the following notation to indicate the different kinds of patches. X l = {x l i } N x i=1 will denote the set of LR patches into which the LR input image is partitioned; similarly, X h = {x h i } N x i=1 will denote the set of reconstructed HR patches, that will form the HR output image. Each patch is expressed in vector form, i.e. its pixels are concatenated to form a unique vector. As for the dictionary,</p><formula xml:id="formula_1">Y l = {y l i } N y i=1 and Y h = {y h i } N y</formula><p>i=1 will be respectively the sets of LR and HR example patches. In each patch reconstruction, then, the goal is to predict an HR output patch x h i , given the related LR input patch x l i , and the two coupled sets of patches, Y l and Y h , that form the dictionary. Fig. <ref type="figure" target="#fig_0">1</ref> shows in a simple manner the operating diagram of an examplebased SR algorithm, where the input image is partitioned into LR patches, a dictionary of patch correspondences is exploited, and new HR patches are generated and subsequently re-assembled to create the super-resolved output image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Searching for Self-Examples</head><p>In example-based SR, when performing the training phase, we speak about an internal learning when, instead of making use of external training images, we derive the patches directly from the input image and processed versions of it. Local image structures, that can be captured in the form of patches, tend to recur across different scales of an image, especially for small scale factors. We can then use the input image itself, conveniently up-sampled or down-sampled into one or several differently re-scaled versions, and use pairs of these images to learn correspondences of LR and HR patches, that will constitute our internal dictionary. We call the patches learned in this way self-examples. In this respect, there are two main kinds of learning schemes described in the literature: one-step schemes like in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and <ref type="bibr" target="#b21">[22]</ref>, and schemes involving the construction of a pyramid of recursively scaled images <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>.</p><p>One-step schemes are meant to reduce as much as possible the size of the internal dictionary, i.e. the number of self-examples to be examined at each patch reconstruction, and thus the computational time of the SR algorithm. In fact, from the input image only one pair of training images is constructed and thus taken into account for the construction of the dictionary. This approach is motivated by the fact that the most relevant patches correspondences can be found when the rescale factor employed is rather small. Only one rescaling is then sufficient to obtain a good amount of self-examples. Let D denote an image downscaling operator, s.t. D(I ) = (I ) ↓ p , where p is a conveniently chosen small scale factor; and let U denote the dual upscaling operator, s.t. U(I ) = (I ) ↑ p . Let I L still indicate the LR input image. In <ref type="bibr" target="#b21">[22]</ref>, for example, J L = U(D(I L )), which represents a low-pass filtered version of the LR input image I L , is used as source for the LR patches, whereas the HR example patches are directly sampled from the input image (J H = I L ). Freedman and Fattal propose in <ref type="bibr" target="#b20">[21]</ref> an equivalent approach, except that a high-pass version of I L ( J H = I L -J L ) is used to get HR training patch. In <ref type="bibr" target="#b17">[18]</ref>, instead, we have J L = (I L * B) and J H = U(I L ): the LR examples patches are taken again from a low-pass filtered version of I L , obtained by blurring the original image with a Gaussian blur kernel B, and the corresponding HR patches are taken from an upscaled version of I L , which does not alter its frequency spectrum content.</p><p>The method described in <ref type="bibr" target="#b22">[23]</ref> paved the way to several SR algorithms (see <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>), based on self-examples derived in an image pyramid. Differently from one-step learning methods, here several training images are constructed by recursively down-scaling the LR input image, thus forming a sort of overturned pyramid. Given a LR input patch, all the levels of this pyramid can be used to find similar selfexamples, usually by performing a nearest neighbor search (NNS) (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>To test the effective usefulness of constructing a full pyramid of images, we built a pyramid with 6 sub-levels as in <ref type="bibr" target="#b22">[23]</ref> (the original LR image is recursively down-scaled 6 times). For each LR input patch, then, the K = 9 most similar self-examples (the K nearest neighbors) have been searched throughout the whole pyramid, and the histogram of all the selected neighbors has been drawn, where the histogram classes are the 6 sub-levels of the pyramid. Fig. <ref type="figure" target="#fig_2">3</ref> presents the histograms of the selected neighbors for two different images.</p><p>As we can observe from Fig. <ref type="figure" target="#fig_2">3</ref>, it is clear that the first sublevel, i.e. the image obtained with only one rescaling operation, is the most relevant source of self-examples. Nevertheless, in both cases nearly 35 percent of the neighbors still come from the other sub-levels, which is not a negligible percentage. Pyramidal scheme like in <ref type="bibr" target="#b22">[23]</ref> are then preferable than one-step scheme like <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Patch Reconstruction Methods Based on Local Learning</head><p>Once the dictionary of self-examples is built, i.e. we have the two dictionary sets Y l and Y h containing, respectively, LR and HR example patches, the proper SR upscaling procedure is ready to start. In this respect, example-based SR algorithms consist in patch-based procedures, where the HR output image is built by means of single patch reconstructions, as many as the number of LR patches the LR input image is partitioned into.</p><p>For example-based SR, two main approaches to reconstruction are possible: neighbor embedding (NE) and direct mapping (DM). In both cases, for each input patch </p><formula xml:id="formula_2">x l i , LR</formula><formula xml:id="formula_3">M i = M(x l i , Y l i , Y h i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Prediction:</head><p>The model M i is applied to actually predict the HR output patch. NE and DM differ in steps 2 and 3 of the local learning procedure above. In NE, the reconstruction model M i consists of a vector of weights w i ∈ R K (where K is the number of neighbors chosen), that identifies a linear combination of the LR neighbors Y l i . The weights are computed w.r.t. the LR input patch and its neighbors (w i = M(x l i , Y l i )). In <ref type="bibr" target="#b22">[23]</ref>, e.g., the single weight w i ( j ), related to the neighbor y l j found in the pyramid, is an exponential function of the distance between the latter and the LR input patch, according to the non-local means (NLM) model <ref type="bibr" target="#b28">[29]</ref>.</p><formula xml:id="formula_4">w i ( j ) = 1 C e - x l i -y l j 2 2 t , (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where t is a parameter to control the decaying speed and C is a normalizing constant to make the weights sum up to one.</p><p>In other NE methods, instead, the weights are meant to describe a linear combination that approximates the LR input patch (i.e. x l i ≈ Y l i w i ). In <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, e.g., the weights are computed as the result of a least squares problem with a sum-to-one constraint:</p><formula xml:id="formula_6">w i = arg min w x l i -Y l i w 2 s.t. w T 1 = 1.<label>(3)</label></formula><p>The formula (3) recalls the method used in Locally Linear Embedding (LLE) <ref type="bibr" target="#b29">[30]</ref> to describe a high-dimensional point lying on a manifold through its neighbors. In <ref type="bibr" target="#b26">[27]</ref>, instead, the sum-to-one constraint is replaced by a nonnegative condition, thus leading to nonnegative weights.</p><formula xml:id="formula_7">w i = arg min w x l i -Y l i w 2 s.t. w ≥ 0.<label>(4)</label></formula><p>In <ref type="bibr" target="#b12">[13]</ref>, finally, the weight computation is performed in an appropriate subspace: the LR input patch and its HR neighbors are expressed as lower-dimensional vectors, thanks to convenient projection matrices. The weights are then found by minimizing the error in the new subspace with no explicit constraint:</p><formula xml:id="formula_8">w i = arg min w P l i x l i -P h i Y h i w 2 . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>Once the vector of weights w i is computed, the prediction step (Step 3) of the NE learning procedure consists in generating the HR output patch x h i as a linear combination of the HR neighbors Y l i , by using the same weights:</p><formula xml:id="formula_10">x h i ≈ Y h i w i . (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>Fig. <ref type="figure" target="#fig_3">4a</ref> depicts the scheme of the patch reconstruction procedure in the NE case. The model, i.e. the weights, is totally learned in the LR space, and then applied to the HR local training set to generate the HR output patch.</p><p>In direct mapping (DM) methods <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>, instead, the model is a regression function f i that directly maps the LR input patch into the HR output patch. The function is learned by taking into account the two local training sets</p><formula xml:id="formula_12">( f i = M(Y l i , Y h i ))</formula><p>, by minimizing the empirical fitting error between all the pairs of examples. An appropriate regularization term is placed to make the problem well-posed. We then have:</p><formula xml:id="formula_13">f i = arg min f ∈H K j =1 y h j -f (y l j ) 2 2 + λ f 2 H , (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>where H is a desired Hilbert function space and λ ≥ 0 a regularization parameter. Examples similar to x l i and their corresponding HR versions are then used to learn a unique mapping function, which is afterwards simply applied to x l i to predict the HR output patch (Step 3):</p><formula xml:id="formula_15">x h i = f i (x l i )<label>(8)</label></formula><p>Fig. <ref type="figure" target="#fig_3">4b</ref> depicts the scheme of the patch reconstruction procedure also in the DM case. An advantage of DM w.r.t. NE is that it can enable fast procedures, while presenting only slightly degraded performance, as done in <ref type="bibr" target="#b30">[31]</ref>. By allowing a mismatch in the neighborhood computation (neighborhood of the closest atom in the dictionary rather than neighborhood of the input patch), the mappings can be pre-computed and stored. Hence the fast implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED ALGORITHM</head><p>In this section we present our novel SR algorithm, based on an internal dictionary of self-examples. Starting from the image pyramid described in Section II-B, we propose a modified scheme with a "double pyramid", presented in Section III-A. The self-examples found in this scheme are used to gradually upscale the LR input image up to the final super-resolved image, according to the cross-level scale factor chosen for the pyramid. The upscaling procedure employed falls within the local learning based reconstruction methods described in Section II-C. In particular, it is a direct mapping method, where each LR patch is mapped into its HR version by means of a specifically learned linear function. The whole upscaling procedure and a summary of the whole algorithm are given in Section III-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Building the Double Pyramid</head><p>The goal of our SR algorithm is to retrieve the underlying HR image I H from a degraded LR version of its I L , which is Listing 1 Single-Image SR via Linear Mapping of Interpolated Self-Examples supposed to be originated according to the image generation model <ref type="bibr" target="#b0">(1)</ref>. We choose the blur kernel B to be a Gaussian kernel with a given variance σ 2 B . The value s is instead an integer scale factor (e.g. 3 or 4), which is the factor by which we want the LR input image I L to be magnified; i.e. if I L is of size N × M, the final super-resolved image ÎH will have a size of s N × s M.</p><p>For complexity reason, the SR algorithm later described is applied only on the luminance component Y of the input image I L (a colorspace transformation from the RG B to the YIQ model is then possibly performed at the beginning), whereas the color components I and Q are simply upsized by Bicubic interpolation to the final desired size s N × s M. In fact, since humans are more sensitive to changes in the brightness of the image rather than changes in color, it is a common belief that the SR procedure is worthy to be performed only on Y, so reducing the complexity of the algorithm by one third. Hereafter, then, all the image matrices and patch vectors must be intended as collections of pixel luminance values.</p><p>As a starting point for our internal dictionary learning procedure, we take the single pyramid depicted in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Here, the top-level is represented by the LR input image itself (I 0 = I L ). From it, a finite number of "sub-levels" is created, according to the following relation:</p><formula xml:id="formula_16">I -n = (I L * B n ) ↓ p n , (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>where p, the pyramid cross-level scale factor, is typically a "small" number (e.g. p = 1.25). The sub-level image I -n is then a particular rescaled version of the original image I L (the total rescale factor amount to p n ). As for the variance of the Gaussian kernel B n to which it is subjected, it can be computed according to the following formula, which is explained in <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_18">σ 2 B n = n • σ 2 B • log( p)/ log(s). (<label>10</label></formula><formula xml:id="formula_19">)</formula><p>Once the single pyramid is created, we propose now to interpolate each sub-level I -n by the factor p. The so obtained  <ref type="figure">U(I -n</ref> ), where U is an upscaling operator s.t. U(I ) = (I ) ↑ p , is an image with the same size as the original non-interpolated level located just above in the pyramid I -n+1 (except for possible 1-pixel differences, due to the non-integer interpolation factors). We can then consider the pair constituted by U(I -n ) and I -n+1 a pair of, respectively, LR and HR training images, from which derive a set of self-examples. By using all the pairs {U(I -n ), I -n+1 } for n = 1, . . . N L, where N L is the chosen number of sublevels, and sampling at corresponding locations pairs of, respectively, LR and HR patches of equal size √ D × √ D, we can then form our LR and HR internal dictionary sets:</p><formula xml:id="formula_20">Y l = {y l i ∈ R D } N y i=1 and Y h = {y h i ∈ R D } N y</formula><p>i=1 . Fig. <ref type="figure" target="#fig_4">5</ref> reports the scheme described, with the "double pyramid" formed by the traditional image cascade and, next to it, a side pyramid of interpolated levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gradual Upscalings</head><p>Once the LR and HR dictionary sets, Y l and Y h , are formed, by populating them with the correspondences of self-examples found in the double pyramid described in Section III-A, the proper SR reconstruction algorithm starts.</p><p>The algorithm consists in a multi-pass procedure, where the input image is gradually magnified by an upscale factor equal to the cross-level scale factor p. Given s as the total scale factor to be achieved, the number of necessary passes is then:</p><formula xml:id="formula_21">N P = log p s . (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>If s is not a power of p, the image super-resolved after N P passes will be over-sized w.r.t. the targeted dimension (s M × s N); which means that an extra resizing operation is needed. I 0 will be super-resolved into I 1 , I 1 into I 2 , and so until obtaining I N P , which will be possibly resized to obtain the SR estimated image ÎH with the desired dimension. The multi-pass SR procedure is illustrated graphically in Fig. <ref type="figure" target="#fig_5">6</ref>.</p><p>When generally upscaling the image I n , this is first interpolated into the image U(I N ) from which a set of overlapping patches X l = {x l i } N x i=1 is formed, by scanning it with a sliding window of dimension √ D × √ D. Each input patch x l i is processed singularly and, after the learning based patch reconstruction procedure, a corresponding HR output patch x h i is produced. By iterating for all patches, we have at the end a set of HR reconstructed patches X h = {x h i } N x i=1 , which will be finally re-assembled to form the upper-level image I n+1 . In the following paragraph we describe the patch reconstruction method adopted.</p><p>1) Direct Mapping of the Self-Examples via Multi-Linear Regression: While most of the "pyramid-based" SR algorithms in the literature <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref> make use of a neighbor embedding (NE) based procedure to express each input and output patch in terms of combinations of self-examples, we follow instead the direct mapping (DM) approach. As explained in Section II-C, DM aims at learning, thanks to the LR and HR local training sets, a mapping to directly derive the single HR output patch as a function of the related LR input patch.</p><p>DM has been employed in SR example-based algorithms using external dictionaries <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, by using in particular the Kernel Ridge Regression (KRR) solution. In this case, the function space H is what is called a reproducing kernel Hilbert space (RKHS), and the single regression function f i is seen as an expansion of kernel functions, where Gaussian kernels are typically chosen.</p><p>For the sake of simplicity, we believe instead that, since in the case of internal learning the dictionary patches are more pertinent training examples, a simple linear mapping "can do the job". With this goal, H is taken as a linear function space</p><formula xml:id="formula_23">H = f (x) = Mx | M ∈ R D×D , x ∈ R D (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>and the regularized empirical error (7) can be re-expressed as follows:</p><formula xml:id="formula_25">M i = arg min M∈R D×D K j =1 y h j -My l j 2 2 + λ M 2 F = arg min M∈R D×D Y h i -MY l i 2 2 + λ M 2 F ,<label>(13)</label></formula><p>where Y l i and Y h i are the usual, respectively LR and HR, local training sets related to the patch x l i . In other words, we are looking for a linear transformation (i.e. the matrix M i ) to be directly applied to the LR input patch. This linear transformation is learned by observing the relations between the LR and HR dictionary patches which are neighbors with x l i , according to the machine learning pattern and a regression model, where Y l i is the matrix of the predictor variables or regressors, and Y h i is the matrix of the response variables. As the response variables are vectors and not scalars, we properly speak about multi-variate regression (MLR) <ref type="bibr" target="#b31">[32]</ref>. The solution to ( <ref type="formula" target="#formula_25">13</ref>) is known, and can be written in a closedform formula:</p><formula xml:id="formula_26">M i = Y h i Y l i T Y l i Y l i T + λI -1<label>(14)</label></formula><p>where I is the identity matrix. The equation ( <ref type="formula" target="#formula_26">14</ref>) corresponds exactly to what we called "model generation" (Step 2 of  the local learning procedure described in Section II-C). The prediction of the HR unknown patch (Step 3) is the straight application of the linear mapping learned:</p><formula xml:id="formula_27">x h i = M i x l i . (<label>15</label></formula><formula xml:id="formula_28">)</formula><p>Fig. <ref type="figure" target="#fig_4">5</ref> gives a rough depiction of how the DM reconstruction method works in the double pyramid.</p><p>2) Patch Aggregation and IBP: By learning for each input patch x l i ∈ X l a linear function M i with the equation ( <ref type="formula" target="#formula_26">14</ref>), and by applying this function to generate the related output patch, we end up with a collection of HR patches X h = {x h i } N x i=1 that need to be assembled to generate the current upscaling. Before the patch aggregation, the set of reconstructed HR patches X h and the equivalent set of LR patches from which they have been originated, X l , are added to the dictionary as new correspondences of patches to be used in future upscalings: the patches of the two sets, in fact, are equal in number, and a LR-HR relation stands. The update of the dictionary is performed by simple set union, i.e.</p><formula xml:id="formula_29">Y l = Y l ∪ X l and Y h = Y h ∪ X h .</formula><p>The patches of the input image, and consequently also in the equally-sized output image, are taken with a certain overlap; that means that, when they are re-placed in the original positions, at each pixel location we have a set of different candidate values. We can see the image at this stage as a 3D image, where the multiple values per pixel are the results of different local observations of the input image (i.e. different patches taken), and therefore contain different partial information about the unknown HR image. We obtain a single output image by convexly combining these candidates at each pixel location: the convex combination is done by simply taking uniform weights, i.e. each candidate is weighted by 1/N p , where N p is the number of overlapping patches contributing to that particular position.</p><p>After the image of the new level is formed, by overlapping and averaging, before it is used as a starting image for the next upscaling, it is further refined in an iterative fashion by the iterative back-projection (IBP) procedure. IBP is an additional operation adopted by several SR algorithms (see <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>), for which the output super-resolved image, once reconstructed, is "back-projected" to the LR dimension in order to assure it to be consistent with the LR input image, i.e. to assure it to be a plausible estimation of the underlying HR image, and conveniently corrected if errors are observed.</p><p>At the iteration t of this refining procedure, the generic reconstructed n-th level I t n is first back-projected into an estimated LR image Î t L :</p><formula xml:id="formula_30">Î t L = I t n * B n ↓ p n (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>where B n is a Gaussian blur with variance as expressed in <ref type="bibr" target="#b9">(10)</ref>.</p><p>The deviation between this LR image found by back-projection and the original LR image is then used to further correct the HR estimated image:</p><formula xml:id="formula_32">I t +1 n = I t n + I L -Î t L ↑ p n * b, (<label>17</label></formula><formula xml:id="formula_33">)</formula><p>where b is a back-projection filter that locally spreads the differential error.</p><p>In Listing 1, our proposed SR procedure is described in a simplified manner, by reporting the pseudocode for the two main routines of the algorithm: "InternalLearning", where the double pyramid is constructed and the dictionary sets of LR and HR patches are initially formed, and "SingleUpscale", that reports the procedure to upscale a generic level I n to the upper level. To be noted, in particular, on line 15 the for loop, which consists of 3 instructions and implements the 3 steps of the local learning based patch reconstruction procedure described in Section II-C: nearest neighbor search, model generation, and prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section we conduct some experiments on the proposed single-image SR algorithm. In particular, in Section IV-A we evaluate the different contributions, in terms of implementation choices, that led to its final formulation summarized in Listing 1. Section IV-B provides some considerations about the complexity, intended both as time and space complexity, of the proposed algorithm. In particular, we evaluate the amount of extra complexity possibly brought by the double pyramid. In Section IV-C, finally, we compare our algorithm with other state-of-the-art methods, by both showing visual comparisons on super-resolved images and reporting quantitative results, according to the PSNR and SSIM metrics. PSNR and SSIM values are obtained as measures of the distance between the HR original image, from which the LR input image, for test purposes, has been originated, and the super-resolved image. The image generation model adopted is the one expressed in Equation ( <ref type="formula" target="#formula_0">1</ref>), with the variance of the Gaussian blur σ 2 B set to 1 in all experiments. The interpolation method used is Bicubic interpolation.</p><p>As for the various parameters of the algorithms, they have been "tuned" by empirically looking for their optimal values. Notably, the cross-level scale factor p is taken as p = 1.25; as for the patch size, instead, 5 × 5 patches are sampled from the internal images with a 4-pixel overlap. For each input patch, then, K = 12 neighbors are selected in the dictionary via NNS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation of the Different Contributions</head><p>In this section we want to assess the different contributions of our algorithm, which have been discussed in Section III. With respect to the well-known single-image SR algorithm in <ref type="bibr" target="#b22">[23]</ref>, two main contributions have been presented:</p><p>1) The employment of a DM reconstruction method (i.e. MLR), instead of NE; 2) The introduction of a different training and upscaling scheme, i.e. the "double" pyramid. To singularly evaluate the two "ingredients" above, we test three different procedures (summarized in Table <ref type="table">I</ref>), which all fall in the category of example-based single-image SR algorithms employing an internal dictionary.</p><p>We first start with an algorithm, where a single pyramid is constructed and NE with non-local means (NLM) weights ( <ref type="formula" target="#formula_4">2</ref>) is used as patch reconstruction method ("Algorithm 1"). This algorithm is very close in the spirit to the method in <ref type="bibr" target="#b22">[23]</ref>, and thus its implementation can be considered as a reference for the mentioned method, except for possible slight differences in the code configuration and the choice of the parameters. With respect to Algorithm 1, "Algorithm 2" uses a DM patch reconstruction method instead of NE, i.e. the MLR method described in Section III-B that linearly maps each LR patch into the related HR patch. The finally proposed algorithm introduces then the double pyramid scheme, featuring the side pyramid of interpolated levels. Table <ref type="table">II</ref> presents the PSNR and SSIM values for all the considered algorithms, when tested on seven input images and for two different scale factors (s = 3, 4).</p><p>As we can observe from the table, from Algorithm 1 to the Proposed algorithm we have almost always a progressive consistent improvement in the SR performance, with our finally proposed procedure presenting an average gain of about 0.22 dB w.r.t. to the traditional scheme based on a single pyramid and NE (Algorithm 1). This gain can be appreciated when observing the output images. Fig. <ref type="figure" target="#fig_6">7</ref> shows in fact the visual results obtained with the three different procedures, for two super-resolved images.</p><p>As we can observe from the zoomed-in areas of the images, with the finally proposed algorithm, we are able to produce finer details (see the branch of the tree behind the bird, or the texture of the hat of the woman). In particular, by observing the woman's hat, we can appreciate a sort of progress in the outcome of the three algorithms: Algorithm 1 gives a pretty blurred result; Algorithm 2, thanks to the use of DM in the place of NE, shows instead more regular structures; the edges and the shapes of these structures look even sharper with the Proposed algorithm, where the DM functions have been computed on the interpolated patches of the double pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Considerations on the Algorithm Complexity</head><p>In order to evaluate the time complexity of the algorithm, we ran two versions of it, featuring the usual single pyramid and our double pyramid, for two different scale factors <ref type="bibr">(3 and 4)</ref>. The single-pyramid implementation gives an idea of the complexity of several algorithms in the literature (see <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>). The goal of this test is to evaluate the penalty brought by the use of our double pyramid, in terms of increased complexity. Table <ref type="table" target="#tab_2">III</ref> summarizes the results of the tests made.</p><p>As we can see from Table <ref type="table" target="#tab_2">III</ref>, the double pyramid leads to an extra complexity in the order of about 10%. The increase in the computational cost is mainly due to the fact that bigger LR vectors are processed (LR patches have the same size as HR patches). Nevertheless, we believe that the complexity increase is still within a tolerable extent.</p><p>To evaluate the space complexity, i.e. memory storage requirement, we can instead compute an estimation of the size of the internal dictionary. Here, the only discriminating factor between the single and the double pyramid is the size of the LR patches. In fact, in the case of double pyramid, both patches have the same size, whereas the LR patches are p 2 smaller in the case of single pyramid (where p is the small scale factor used at each pass). Given this, it is easy to compute the ratio between the two dictionary sizes:</p><formula xml:id="formula_34">R = 2 1 + 1 p 2 . (<label>18</label></formula><formula xml:id="formula_35">)</formula><p>In our tests p was set to 1.25, which corresponds to a 22% increase of the storage requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison With State-of-the-Art Algorithms</head><p>In this section we perform a comparative assessment of our method with other single-image SR algorithms in the state-of-the art, by extending the comparison also to SR methods based on external dictionaries. For this purpose, we consider Bicubic interpolation as a reference for traditional analytical interpolation methods, and four other example-based SR algorithms. The characteristic of the latter, as well as those ones of our proposed method, are summarized in Table <ref type="table" target="#tab_3">IV</ref>. For the first three methods in the table, the original code of the respective authors, possibly slightly modified to make the comparison fair, has been used. For the "pyramid" method of <ref type="bibr" target="#b22">[23]</ref>, instead, the third-party code of the authors of <ref type="bibr" target="#b23">[24]</ref> has been adopted. Table V reports the performance results of the algorithms considered (PSNR and SSIM values), with the usual set of seven test images, and 3 and 4 as scale factors.</p><p>Table <ref type="table" target="#tab_3">V</ref> shows clearly that our method outperforms the other algorithms in terms of objective quality of the superresolved images, for all the images and the two scale factors considered. The results are confirmed when observing the visual comparisons (Fig. <ref type="figure" target="#fig_7">8</ref>, Fig. <ref type="figure" target="#fig_8">9</ref>, and Fig. <ref type="figure" target="#fig_9">10</ref>).</p><p>From the visual results presented, we can see that methods based on external dictionaries ("LLE-based NE", "NN NE", and "Sparse SR") often present blurring and ringing artifacts (see e.g. the Bike images in Fig. <ref type="figure" target="#fig_8">9</ref>). Results for the two algorithms based on an internal dictionary (the "pyramid" algorithm of <ref type="bibr" target="#b22">[23]</ref> and ours), instead, are certainly more pleasant at sight, presenting sharp edges and smooth artifact-free results in the regions with no texture. However, the algorithm of <ref type="bibr" target="#b22">[23]</ref> in some cases shows results that look somehow "artificial", with over-smoothed areas or unnatural edges (see Lena's eye in Fig. <ref type="figure" target="#fig_9">10e</ref>, whose shape is deformed). Our algorithm succeeds also in avoiding these undesired effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper we presented a novel single-image super-resolution (SR) method, which belongs to the family of example-based SR algorithms, using a dictionary of patches in the upscaling procedure. The algorithm originally makes use of a "double pyramid" of images, built starting from the input image itself, to extract the dictionary patches (thus called "self-examples"), and employs a regression-based method to directly map the low-resolution (LR) input patches into their related high-resolution (HR) output patches. When compared to other state-of-the-art algorithms, our proposed algorithm shows the best performance, both in terms of objective metrics and subjective visual results. As for the former, it presents considerable gains in PSNR and SSIM values. When observing the super-resolved images, also, it turns out to be the most capable in producing fine artifact-free HR details. The algorithm does not rely on extra information, since making use of an internal dictionary automatically "self-adapted" to the input image content, and also the few parameters have proven to be easy to tune. This makes it a particularly attractive method for SR upscaling purposes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Operating diagram of the example-based SR procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pyramid of recursively scaled images (in this case with 4 sub-levels), where the top level (I 0 ) is represented by the LR input image I L . Given a LR input patch x l i , a desired number of "neighbors" (y l 1 , y l 2 , . . . ) can be found at any level of the pyramid.</figDesc><graphic coords="2,314.15,370.97,125.66,94.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Percentage of selected neighbors for each sub-level of the pyramid, for two different images.</figDesc><graphic coords="2,439.67,375.68,115.19,88.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.Operating schemes of the local learning based reconstruction procedure, for the two approaches described (NE and DM). From the image it is clear how NE exploits the intra-space relationships, by learning a model among the LR patches and applying it on the HR patches. DM, instead, exploits the "horizontal" relationships, by attempting to learn the mapping between the two spaces. (a) Neighbor Embedding. (b) Direct Mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Creation of the "double pyramid" and search of self-examples throughout it. The figure concerns the upscaling of I 0 to I 1 . Given a reference patch in the interpolated version of the starting image U (I 0 ), x li , 3 LR neighbors are found in the "side pyramid" of interpolated levels (y l 1 , y l 2 , y l 3 ). Thanks to these and the corresponding HR neighbors (y h 1 , y h 2 , y h 3 ), a linear function M is meant to be learned to directly map x l i into its corresponding HR output patch x h i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Estimation of the HR image by gradual upscalings. The original image I L , by 4 upscalings by a factor of p, is super-resolved into I 4 , that exceeds in dimension the desired scale factor s. ÎH is obtained by a finally resizing operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Super-resolved images with zoomed-in areas for the three different internal-dictionary procedures. The image and the scale factors considered are: Bird x4 (left) and Woman x4 (right).</figDesc><graphic coords="7,312.47,380.93,138.14,138.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparative results with zoomed-in areas for Butterfly magnified by a factor of 3. The methods considered are: (a) Bicubic interpolation, (b) LLE-based NE, (c) NN NE, (d) Sparse SR, (e) Pyramid, (f) Proposed.</figDesc><graphic coords="9,102.95,485.45,128.54,128.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparative results with zoomed-in areas for Bike magnified by a factor of 3. The methods considered are: (a) Bicubic interpolation, (b) LLE-based NE, (c) NN NE, (d) Sparse SR, (e) Pyramid, (f) Proposed.</figDesc><graphic coords="11,102.95,209.81,128.54,128.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Comparative results with zoomed-in areas for Lena magnified by a factor of 3. The methods considered are: (a) Bicubic interpolation, (b) LLE-based NE, (c) NN NE, (d) Sparse SR, (e) Pyramid, (f) Proposed.</figDesc><graphic coords="12,102.95,209.81,128.54,128.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Model Generation: A model M i for the local reconstruction is computed. M i generally depends on both the LR input patch and the local training sets:</figDesc><table><row><cell>and HR</cell></row><row><cell>local training sets are formed, by performing a nearest neigh-</cell></row><row><cell>bor search (NNS). A desired number of neighbors of the</cell></row><row><cell>LR input patch x l i is searched among the LR self-examples Y l , and consequently an equal number of HR neighbors is</cell></row><row><cell>determined. Let Y l i indicate the matrix collecting, column by column, the selected LR neighbors, and let Y h i indicate</cell></row><row><cell>the matrix of the corresponding HR neighbors. Thanks to</cell></row><row><cell>the local sets Y l i and Y h i , the unknown HR patch x h i is</cell></row><row><cell>then predicted. The whole local learning procedure can be</cell></row><row><cell>summarized in 3 steps.</cell></row><row><cell>1) Nearest Neighbor Search (NNS): The local training</cell></row><row><cell>sets Y l i and Y h i are determined.</cell></row><row><cell>2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III AVERAGE</head><label>III</label><figDesc>RUNNING TIME OF THE ALGORITHM, FOR TWO SCALE FACTORS, WITH SINGLE OR DOUBLE PYRAMID. TIME IS EXPRESSED</figDesc><table><row><cell>IN SECONDS</cell></row><row><cell>TABLE IV</cell></row><row><cell>SUMMARY OF OUR METHOD AND THE FOUR EXAMPLE-BASED</cell></row><row><cell>SR ALGORITHMS CONSIDERED FOR THE COMPARISON</cell></row><row><cell>interpolated level</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON (PSNR AND SSIM VALUES) WITH OTHER STATE-OF-THE-ART METHODS, WHEN SUPER-RESOLVING SEVERAL IMAGES FOR SCALE FACTORS OF 3 AND 4</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Super-resolution image reconstruction: A technical overview</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="36" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiframe image restoration and registration</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Vis. Image Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="317" to="339" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">New edge-directed interpolation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1521" to="1527" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting the sparse derivative prior for super-resolution and image demosaicing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Workshop Statist. Comput. Theories Vis</title>
		<meeting>3rd IEEE Int. Workshop Statist. Comput. Theories Vis</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007-07">Jul. 2007</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single image super-resolution using Gaussian process regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single image superresolution with non-local means and steering kernel regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4544" to="4556" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Example-based superresolution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002-04">Mar./Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004-07">Jun./Jul. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image hallucination using neighbor embedding over visual primitive manifolds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neighbor embedding based super-resolution algorithm through edge detection and feature selection</title>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="494" to="502" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Locality preserving constraints for super-resolution with neighbor embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th IEEE Int. Conf. Image Process</title>
		<meeting>16th IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="page" from="1189" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint learning for single-image super-resolution via a coupled constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="469" to="480" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resolution enhancement based on learning the sparse association of image patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image super-resolution with sparse neighbor embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3194" to="3205" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-scale dictionary for single image super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1114" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-image super-resolution via local learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single image super-resolution with multiscale similarity learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1648" to="1659" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting self-similarities for single frame super-resolution</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Asian Conf. Comput. Vis. (ACCV)</title>
		<meeting>10th Asian Conf. Comput. Vis. (ACCV)</meeting>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
			<biblScope unit="page" from="497" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning context-aware sparse representation for single image super-resolution</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th IEEE Int. Conf. Image Process</title>
		<meeting>18th IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
			<biblScope unit="page" from="1349" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Internal statistics of a single natural image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><forename type="middle">A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf. (BMVC)</title>
		<meeting>Brit. Mach. Vis. Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="135" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1167" to="1183" />
			<date type="published" when="2002-09">Sep. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multivariate regression-Techniques and tools</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hyötyniemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">125</biblScope>
			<pubPlace>Helsinki, Finland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Helsinki Univ. Technology, Control Engineering Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Superresolution using neighbor embedding of back-projection residuals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><forename type="middle">A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Digit. Signal Process. (DSP)</title>
		<meeting>Int. Conf. Digit. Signal ess. (DSP)<address><addrLine>Santorini, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-07">Jul. 2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
