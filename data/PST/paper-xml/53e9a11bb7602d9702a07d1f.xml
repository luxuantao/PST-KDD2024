<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminant sparse neighborhood preserving embedding for face recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-02-18">18 February 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jie</forename><surname>Gui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hefei Institute of Intelligent Machines</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>230031</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
							<email>znsun@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Hefei Institute of Intelligent Machines</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>230031</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rongxiang</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Hefei Institute of Intelligent Machines</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>230031</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingke</forename><surname>Lei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Electronic Engineering Institute</orgName>
								<address>
									<postCode>230037</postCode>
									<settlement>Hefei Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Old Dominion University</orgName>
								<address>
									<settlement>Norfolk</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminant sparse neighborhood preserving embedding for face recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-02-18">18 February 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">66B1D06F10F95FE1F983645D56E4C74F</idno>
					<idno type="DOI">10.1016/j.patcog.2012.02.005</idno>
					<note type="submission">Received 5 January 2011 Received in revised form 5 January 2012 Accepted 8 February 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Sparse neighborhood preserving embedding Sparse subspace learning Discriminant learning Maximum margin criterion Discriminant sparse neighborhood preserving embedding Face recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse subspace learning has drawn more and more attentions recently. However, most of the sparse subspace learning methods are unsupervised and unsuitable for classification tasks. In this paper, a new sparse subspace learning algorithm called discriminant sparse neighborhood preserving embedding (DSNPE) is proposed by adding the discriminant information into sparse neighborhood preserving embedding (SNPE). DSNPE not only preserves the sparse reconstructive relationship of SNPE, but also sufficiently utilizes the global discriminant structures from the following two aspects: (1) maximum margin criterion (MMC) is added into the objective function of DSNPE; (2) only the training samples with the same label as the current sample are used to compute the sparse reconstructive relationship. Extensive experiments on three face image datasets (Yale, Extended Yale B and AR) demonstrate the effectiveness of the proposed DSNPE method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past two decades, appearance-based face recognition has attracted considerable interests in computer vision and pattern recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. It is well known that the dimension of face images is usually very high. For example, a 100-by-100 pixel face image can be viewed as a 10,000-dimensional vector. High dimensionality of feature vector has become a critical problem in practical pattern recognition applications. The data in the high-dimensional space is usually redundant and may degrade the performance of pattern classifiers when the number of training samples is much smaller than the dimensionality of the input data. A common way to solve these problems is to adopt dimensionality reduction methods. So far, an enormous volume of literature has been devoted to investigate various data-dependent dimensionality reduction methods for projecting the high-dimensional data into low-dimensional feature spaces. These traditional dimensionality reduction methods can be classified into four categories as follows.</p><p>The first category is linear dimensionality reduction algorithm (also named as subspace learning algorithm), among which principal component analysis (PCA) and linear discriminant analysis (LDA) are two of the most popular ones <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Generally, PCA projects the original data into a low-dimensional space which is spanned by the eigenvectors associated with the largest eigenvalues of the covariance matrix of all the data points. However, PCA does not take into consideration the label information of the input data. As a result, PCA will probably lose much useful information which is critical for pattern classification tasks <ref type="bibr" target="#b3">[4]</ref>. Unlike PCA, LDA is a supervised method which takes full consideration of the class labels for patterns. It is generally believed that the class information can make the recognition algorithm more discriminative. Thus, LDA has been shown to be more effective than PCA in many applications. One limitation of PCA and LDA is that they only exploit the linear global Euclidean structure. Recent research shows that the face images may reside on a nonlinear submanifold <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, which makes PCA and LDA inefficient. In order to overcome the problem, many nonlinear feature extraction methods such as kernel-based approaches and manifold learning-based ones have been developed.</p><p>The second category is the kernel-based algorithm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, which uses a linear classifier algorithm to solve non-linear problems by mapping the original non-linear observations into a higherdimensional space. It is based on the assumption that the nonlinear structure data will be linearly separable in the kernel space. The most popular kernel methods are kernel principal component analysis (KPCA) <ref type="bibr" target="#b7">[8]</ref> and kernel Fisher discriminant analysis (KFDA)</p><p>Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/pr Pattern Recognition 0031-3203/$ -see front matter &amp; 2012 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2012.02.005 <ref type="bibr" target="#b6">[7]</ref>, which are the kernel versions of PCA and LDA. KPCA and KFDA have been proved to be effective in some real world applications. However, the choice of the kernel, which is crucial to the success of these algorithms, has been traditionally entirely left to the user. So many research works are conducted on multiple kernel learning to solve the problem of kernel determination <ref type="bibr" target="#b8">[9]</ref>.</p><p>The third category is manifold learning-based algorithm, which is based on the idea that the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. The representative algorithms include locally linear embedding (LLE) <ref type="bibr" target="#b4">[5]</ref>, isometric feature mapping (ISOMAP) <ref type="bibr" target="#b9">[10]</ref>, Laplacian eigenmaps (LE) <ref type="bibr" target="#b10">[11]</ref>, Hessian-based locally linear embedding (HLLE) <ref type="bibr" target="#b11">[12]</ref>, maximum variance unfolding (MVU) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, local tangent space alignment (LTSA) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, Riemannian manifold learning (RML) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, and local spline embedding (LSE) <ref type="bibr" target="#b18">[19]</ref>, etc. Each manifold learning algorithm attempts to preserve a different geometrical property of the underlying manifold. Local approaches such as LLE and LE, the first step of which is graph construction based on k-nearest-neighbor and e-ball based methods, aim to preserve the locality proximity relationship among the data, while global approaches like ISOMAP aim to preserve the metrics at all scales. These nonlinear methods do yield impressive results on some benchmark artificial and real world data sets due to their nonlinear nature, geometric intuition, and computational feasibility. However, all these manifold learning algorithms have the out of sample problem <ref type="bibr" target="#b19">[20]</ref>. The reason is that they can only yield an embedding of the training data set. Nevertheless, when applied to a new sample, they cannot easily find the sample's image in the embedding space by utilizing the low-dimensional embedding results of the training data set because of the implicitness of the nonlinear map. Thus a dozen of methods have been proposed to solve this problem, e.g., incremental manifold learning <ref type="bibr" target="#b20">[21]</ref>, lowrank matrix approximation <ref type="bibr" target="#b21">[22]</ref>, locality preserving projections (LPP) <ref type="bibr" target="#b22">[23]</ref>, discriminant locality preserving projections based on maximum margin criterion (DLPP/MMC) <ref type="bibr" target="#b23">[24]</ref>, null space discriminant locality preserving projections (NDLPP) <ref type="bibr" target="#b24">[25]</ref>, locality preserving discriminant projections (LPDP), etc. <ref type="bibr" target="#b25">[26]</ref>.</p><p>The last one is matrix and tensor embedding algorithm <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> which represents patterns as matrixes or high-order tensors instead of vectors. The aforementioned subspace learning algorithms, kernel based algorithm and manifold learning-based algorithm all consider a vector representation of samples. However, the extracted features from many real world vision problems may contain higher-order structure. For example, a captured image is a second-order tensor, i.e., a matrix, and sequential data such as video sequences for event analysis is in the form of a third-order tensor. Thus it is necessary to derive the multilinear forms of these traditional linear feature extraction methods to handle the data as tensors directly. Recently this research field has received a lot of attention from the image processing and computer vision community, and these methods <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> have been shown to be much more efficient than the traditional vector-based methods.</p><p>Recently, some new methods integrating the theory of sparse representation, compressed sensing and subspace learning (linear dimensionality reduction methods) have been proposed, and have been successfully applied in many practical applications <ref type="bibr">[35-37, 61, 62]</ref>. Sparse subspace learning (SSL) <ref type="bibr" target="#b37">[38]</ref> is a special family of dimensionality reduction methods which consider ''sparsity''. It has either of the following two characteristics: (1) finding a subspace spanned by sparse base vectors. The sparsity is enforced on the projection vectors and associated with the feature dimension. The representative methods include sparse principal component analysis (SPCA) <ref type="bibr" target="#b38">[39]</ref>, sparse nonnegative matrix factorization <ref type="bibr" target="#b35">[36]</ref>, and nonnegative sparse PCA <ref type="bibr" target="#b39">[40]</ref>, etc. (2) Aiming at the sparse reconstructive weight which is associated with the sample size. The representative methods include sparse neighborhood preserving embedding (SNPE) <ref type="bibr" target="#b40">[41]</ref>. In fact, SNPE is identical to sparsity preserving projections (SPP) <ref type="bibr" target="#b41">[42]</ref>, which has achieved higher recognition rates than PCA and neighborhood preserving embedding (NPE) for face recognition. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> proposed a sparse representation-based classifier (SRC) <ref type="bibr" target="#b34">[35]</ref> oriented unsupervised dimensionality reduction algorithm which combines SRC and PCA in its objective function. Yang and Chu <ref type="bibr" target="#b43">[44]</ref> proposed the SRC steered discriminative projection (SRC-DP). The basic idea of SRC-DP is to seek a linear transformation such that in the transformed low-dimensional space, the within-class reconstruction residual is as small as possible and simultaneously the between-class reconstruction residual is as large as possible.</p><p>However, SNPE suffers from a limitation that it does not encode discriminant information, which is very important for recognition tasks. In this paper, we propose a discriminant sparse neighborhood preserving embedding (DSNPE) algorithm by combining SNPE and maximum margin criterion (MMC) methods, which can be viewed as a new algorithm integrating Fisher criterion and sparsity criterion. It is well known that MMC is a method proposed to maximize the trace of the difference of the betweenclass scatter matrix and within-class scatter matrix from which LDA can be derived by incorporating some constraints. Thus, DSNPE is proposed by introducing MMC into the objective function of SNPE, which has two advantages: (1) it retains the sparsity characteristic of SNPE; (2) it emphasizes the discriminative information by incorporating MMC, which can make the class mean vectors have a wide spread and make every class scatter in a small space. Furthermore, to further increase the discriminative power of DSNPE, we integrate additional discriminant information. More concretely, to compute the sparse reconstructive relationship, we only use the training samples with the same label as the current sample instead of using all of the training samples. The reason behind this decision is based on the following observation: taking face images into account, the most compact expression of a certain face image is generally given by the face images from the same class <ref type="bibr" target="#b34">[35]</ref>. The proposed method is applied to face biometrics and is examined using the Yale, Extended Yale B, and AR face image databases. Experimental results show that it is more suitable for recognition tasks than SNPE.</p><p>The remainder of this paper is organized as follows: In Section 2 we will introduce our DSNPE method in details. A theoretical analysis of DSNPE is given in Section 3. The experimental results for applying our method to face recognition will be presented in Section 4, followed by the conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Discriminant sparse neighborhood preserving embedding (DSNPE)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph construction based on sparse representation</head><p>Instead of considering k-nearest-neighbor and e-ball based methods as in typical graph construction, we attempt to automatically construct a graph G and make it well preserve the discriminative information based on sparse representation (SR). In the past few years, SR has received a great deal of attentions, which was initially proposed as an extension of traditional signal processing methods such as Fourier and wavelet. The problem solved by sparse representation is to search for the most compact representation of a signal in terms of linear combination of patterns in an over-complete dictionary. SR has been successfully used in image super-resolution <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>, image denoising <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>, signal reconstruction <ref type="bibr" target="#b49">[50]</ref>, signal recovery <ref type="bibr" target="#b50">[51]</ref>, etc.</p><p>SR has compact mathematical expression. Given a signal (or an image with vector pattern) xAR D , and a matrix X¼[x 1 ,x 2 ,y,x n ]AR n Â D containing the elements of an over-complete dictionary <ref type="bibr" target="#b51">[52]</ref> in its columns, the goal of SR is to represent x using as few entries of X as possible. The objective function can be described as follows:</p><formula xml:id="formula_0">min s i Js i J 0 s:t: x i ¼ Xs i<label>ð1Þ</label></formula><formula xml:id="formula_1">or min s i Js i J 0 s:t: Jx i ÀXs i J!e<label>ð2Þ</label></formula><p>where</p><formula xml:id="formula_2">s i ¼[s i,1 ,y,s i,iÀ 1 ,0,s i,i þ 1 ,y,s in ]</formula><p>T is an n-dimensional vector in which the ith element is equal to zero (implying that the x i is removed from X), and the elements s i,j ,jai denote the contribution of each x j to reconstructing x i . Unfortunately, this criterion is not convex, and finding the sparsest solution of Eq. ( <ref type="formula" target="#formula_0">1</ref>) is NP-hard. This difficulty can be bypassed by convexizing the problem and using l 1 instead of l 0 .</p><p>The l 1 minimization problem can be solved by LASSO <ref type="bibr" target="#b52">[53]</ref> or LARS <ref type="bibr" target="#b53">[54]</ref>. After repeating l 1 minimization problem to all the points, the sparse weight matrix can be expressed as S¼[s 1 ,y,s n ] T . Then, the new constructed graph is G¼ {X,S}, where X is the training sample set and S is the edge weight matrix.</p><p>In the following, we give two reasons why SR is more suitable to graph construction than k-nearest-neighbor and e-ball based methods.</p><p>(1) Parameter-free. SR does not need to determine the model parameters such as the neighborhood size k of k-nearest-</p><p>neighbor and e of e-ball based methods, which are generally difficult to set in practice. In contrast, the advantage of being parameter-free makes SR easy to use in practice. In fact, the data distribution probability may vary greatly at different areas of the data space, which results in distinctive neighborhood structure for each instance. However, both k-nearest-neighbor and e-ball based methods use a predefined parameter to determine the neighborhoods for all the data. It seems to be unreasonable that all data points share the same parameter for k-nearest-neighbor and e-ball based methods, which may not characterize the manifold structure well, especially in under sampling case. Obviously, compared to k-nearest-neighbor and e-ball based methods, SR has the merit of being parameter-free.</p><p>(2) Robustness to data noise. The data noise is inevitable especially for visual data, and the robustness is a desirable property for a satisfying graph construction method. The graph constructed by k-nearest-neighbor and e-ball based methods is based on pair-wise Euclidean distance, which is very sensitive to data noise. It means that the graph structure is easy to change when unfavorable noise comes in. However, SR has been shown to be robust to data noise in <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sparse neighborhood preserving embedding (SNPE)</head><p>In <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, the objective function of SNPE is defined as min</p><formula xml:id="formula_3">X n i ¼ 1 Jw T x i Àw T Xs i J 2<label>ð3Þ</label></formula><p>where w is the projection matrix. By some simple algebra formulations (see the appendix), the objective function can be reduced to</p><formula xml:id="formula_4">X n i ¼ 1 Jw T x i Àw T Xs i J 2 ¼ w T XðIÀSÀS T þ S T SÞX T w<label>ð4Þ</label></formula><p>For compact expression, the objective function can be further transformed to an equivalent form as follows:</p><formula xml:id="formula_5">X n i ¼ 1 Jw T x i Àw T Xs i J 2 ¼ w T XðIÀSÀS T þ S T SÞX T w ¼ w T XS a X T w<label>ð5Þ</label></formula><p>where S a ¼I À S À S T þS T S In addition, to avoid degenerate solutions, a constraint is added</p><formula xml:id="formula_6">w T XX T w ¼ I<label>ð6Þ</label></formula><p>Therefore, the minimization problem is reduced to arg min w T XS a X T w</p><formula xml:id="formula_7">s:t: w T XX T w ¼ I<label>ð7Þ</label></formula><p>Therefore, the transformation matrix that minimizes the objective functions is given by the minimum eigenvalues solution to the generalized eigenvalues problem</p><formula xml:id="formula_8">XS a X T w ¼ lXX T w<label>ð8Þ</label></formula><p>It is easy to show that the matrices XS a X T and XX T are symmetric and positive semidefinite. The vectors w i that minimize the objective function are given by minimum eigenvalues solutions to the generalized eigenvalues problem. Let the column vectors w 0 ,w 1 ,y,w d À 1 be the solutions of Eq. ( <ref type="formula" target="#formula_8">8</ref>), ordered according to their eigenvalues, l 0 ,l 1 ,y,l d À 1 .Thus, the embedding is written as follows:</p><formula xml:id="formula_9">x i -y i ¼ w T x i , w ¼ ½w 0 ,w 1 ,. . .,w dÀ1 ð<label>9Þ</label></formula><p>where y i is a d-dimensional vector, and w is a D Â d matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Maximizing margin criterion (MMC)</head><p>Maximum margin criterion (MMC) <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> is proposed to maximize the (average) margin between classes after dimensionality reduction. MMC can represent class separability better than PCA. Furthermore, LDA can be derived from MMC by incorporating some constraints. However, MMC does not suffer from the small sample size problem, which is known to cause serious stability problems for LDA.</p><p>In <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>, the objective function of MMC is written as</p><formula xml:id="formula_10">J 1 ¼ max X ij p i p j ðdðm i ,m j ÞÀsðm i ÞÀsðm j ÞÞ 8 &lt; : 9 = ;<label>ð10Þ</label></formula><p>where p i and p j are the prior probability of class i and class j, m i and m j are the mean vectors of class i and class j. Here d(m i ,m j ), s(m i ), and s(m j ) are defined as</p><formula xml:id="formula_11">dðm i ,m j Þ ¼ Jm i Àm j J<label>ð11Þ</label></formula><p>Table <ref type="table">1</ref> Discriminant sparse neighborhood preserving embedding.</p><formula xml:id="formula_12">Input: training set X ¼ fðx i ,y i Þg N i ¼ 1</formula><p>Output: D Â d feature matrix w extracted from X 1. Project the image set {x i } into the PCA subspace by throwing away the smallest principal components 2. Construct weight matrix S using Eqs. ( <ref type="formula" target="#formula_15">15</ref>) or (16) 3. Perform eigenvalue decomposition using Eq. ( <ref type="formula" target="#formula_20">20</ref>), construct D Â d feature matrix w whose columns consist of the eigenvectors corresponding to its d smallest eigenvalues. </p><formula xml:id="formula_13">sðm i Þ ¼ trðS i Þ ð 12Þ sðm j Þ ¼ trðS j Þ ð<label>13Þ</label></formula><p>where S j is the covariance matrix of class j.</p><p>Thus the optimized function can be derived as follows:</p><formula xml:id="formula_14">J 2 ¼ maxtrðS b ÀS w Þ ð 14Þ</formula><p>The matrix S b is called the between-class scatter matrix and S w is called the within-class scatter matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Discriminant sparse neighborhood preserving embedding (DSNPE)</head><p>In this section, we will discuss the solution of DSNPE. Inspired by the observation that the most compact expression of a certain face image is generally given by the face images from the same class <ref type="bibr" target="#b34">[35]</ref>, we modify the original sparse representation as min</p><formula xml:id="formula_15">s i Js i J 1 s:t: x i ¼ X k s i labelðx i Þ ¼ k<label>ð15Þ</label></formula><p>or min</p><formula xml:id="formula_16">s i :s i : 1 s:t: Jx i ÀX k s i J!e labelðx i Þ ¼ k<label>ð16Þ</label></formula><p>where X k denote the set of training samples whose label is the same as x i . That is to say, to compute the sparse reconstructive relationship, we only use the training samples with the same label as the current sample instead of using all of the training samples. Furthermore, if a linear transformation Y ¼w T X can maximize J 2 , an optimal subspace for pattern classification will be explored. This is because the linear transformation aims to project a pattern closer to patterns in the same class but farther from those in different classes, which is exactly the goal for classification. That is to say, to find an optimal linear subspace for classification means to maximize the following optimized function:</p><formula xml:id="formula_17">J 3 ¼ max trðw T ðS b ÀS w ÞwÞ ð<label>17Þ</label></formula><p>If the linear transformation obtained by SNPE can satisfy J 3 simultaneously, the discriminability of the data will be improved greatly. Thus the solution for DSNPE can be represented as the following multi-object optimization problem: mintrðw T XS a X T wÞ max trðw T ðS b ÀS w ÞwÞ ( s:t:</p><formula xml:id="formula_18">w T XX T w ¼ I<label>ð18Þ</label></formula><p>The solution to the constrained multi-object optimization problem is to find a subspace which preserves the sparsity property and maximizes the margin between different classes simultaneously, so it can be changed into the following constrained problem: min trðw T ðXS a X T ÀgðS b ÀS w ÞÞwÞ</p><formula xml:id="formula_19">s:t: w T XX T w ¼ I<label>ð19Þ</label></formula><p>where g is a parameter to balance the sparsity and the discriminant information.</p><p>Eq. ( <ref type="formula" target="#formula_19">19</ref>) can be solved by Lagrangian multiplier method: @ @w trðw T ðXS a X T ÀgðS b ÀS w ÞÞwÀl i ðw T XX T wÀIÞÞ ¼ 0 where l i is the Lagrangian multiplier. Then, we can get</p><formula xml:id="formula_20">ðXS a X T ÀgðS b ÀS w ÞÞw i ¼ l i XX T w i<label>ð20Þ</label></formula><p>where w i is the generalized eigenvector of XS a X T À g(S b À S w ) and XX T ; l i is the corresponding eigenvalue.</p><p>Let the column vectors w 0 ,w 1 ,y,w d À 1 be the solutions of Eq. ( <ref type="formula" target="#formula_20">20</ref>), ordered according to their first d smallest eigenvalues l 0 ,l 1 ,y,l d À 1 . Thus, the embedding is written as follows:</p><p>x i -y i ¼ w T x i , w ¼ ½w 0 ,w 1 ,. . .,w dÀ1 where y i is a d-dimensional vector and w is a D Â d matrix.</p><p>The main procedure for the discriminant sparse neighborhood preserving embedding algorithm is summarized in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Time complexity analysis</head><p>In this section, we theoretically analyze the time complexity of our algorithm. We omit the time complexity analysis of sparse learning, because there are a number of software packages to realize the algorithm of sparse learning and different package has different time complexities. For convenience, we give a notation that the number of principal components in the PCA step of DSNPE is q. The DSNPE contains the PCA step and the eigendecomposition step using Eq. ( <ref type="formula" target="#formula_20">20</ref>). Since the PCA step of DSNPE is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The maximal average recognition rates (percent) across 20 runs on the Yale database and the corresponding standard deviations (std) and dimensions (shown in parentheses).  the same as the one often used in the other algorithms such as the classical LDA (i.e., PCAþLDA) and LPP, we focus on the time complexity of the eigendecomposition step using Eq. ( <ref type="formula" target="#formula_20">20</ref>), which is o(q 3 ). Hence, the time complexity of DSNPE is o(q 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theoretical analysis of DSNPE</head><p>In this section, we give some theoretical analyses to better reveal the characteristic of DSNPE. At first, a lemma is presented as follows: Lemma 1. <ref type="bibr" target="#b56">[57]</ref>. For symmetric matrix A A R nÂn , EA R nÂn , let A¼QLQ T be the eigen-decomposition of A and A þE¼B ¼PL 1 P T be the eigen-decomposition of B. Write Q ¼[q 1 ,q 2 ,y,q n ], P¼[p 1 ,p 2 ,y, p n ], where q i and p i are the normalized eigenvectors of A and B, respectively. Let y denote the acute angle between q i and p i , then sinðyÞ r aJEJ 2 where a is a constant that only depends on A.</p><p>The following Theorem 1 characterizes the solution of DSNPE when the parameter g is approaching infinity. Note that Theorem 1 requires the positive definiteness of XX T , which always holds for our algorithm since we use PCA to preprocess the data. Without loss of generality, we further assume the data matrix X has been centered.</p><p>Theorem 1. When g-N, the w i obtained by the proposed DSNPE method converges to the generalized eigenvector m i of the betweenclass scatter matrix S b and the within-class scatter matrix S w , i.e., g-N, there exists a constant b such that</p><formula xml:id="formula_21">w i Àbm i ¼ 0</formula><p>Proof. If the both sides of Eq. ( <ref type="formula" target="#formula_20">20</ref>) are divided by g, we have</p><formula xml:id="formula_22">1 g XS a X T ÀðS b ÀS w Þ w i ¼ l i g XX T w i<label>ð21Þ</label></formula><p>Eq. ( <ref type="formula" target="#formula_22">21</ref>) is equivalent to</p><formula xml:id="formula_23">ðXX T Þ À0:5 1 g XS a X T ÀðS b ÀS w Þ ðXX T Þ À0:5 ðXX T Þ 0:5 w i ¼ l i g ðXX T Þ 0:5 w i<label>ð22Þ</label></formula><p>Therefore (XX T ) 0.5 w i is the eigenvector of (XX T ) À 0.5 ((1/g)XS a X T À (S b À S w ))(XX T ) À 0.5 .</p><p>On the other hand, from the definition of m i , we obtain</p><formula xml:id="formula_24">S b m i ¼ c i S w m i<label>ð23Þ</label></formula><p>where c i is the corresponding eigenvalue of m i . Since X has been centered, then we have XX T ¼nS t ¼n(S b þS w ). With <ref type="bibr" target="#b22">(23)</ref>, we then have</p><formula xml:id="formula_25">ÀðXX T Þ À0:5 ðS b ÀS w ÞðXX T Þ À0:5 ðXX T Þ 0:5 m i ¼ d i ðXX T Þ 0:5 m i<label>ð24Þ</label></formula><p>Table <ref type="table">3</ref> The maximal average recognition rates (percent) across 20 runs on the Extended Yale B database and the corresponding standard deviations (std) and dimensions (shown in parentheses).  where d i ¼ À1þc i /nþ nc i . Therefore (XX T ) 0.5 m i is the eigenvector of À(XX T ) À 0.5 (S b ÀS w )(XX T ) À 0.5 . Let y denote the acute angle between (XX T ) 0.5 m i and (XX T ) 0.5 w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>By directly applying Lemma 1 we have sinðyÞ r a g</p><formula xml:id="formula_26">JðXX T Þ À0:5 ðXS a X T ÞðXX T Þ À0:5 J 2 :</formula><p>when g goes to infinity, it is easy to see that sin(y)-0. Therefore, there exists a constant b such that</p><formula xml:id="formula_27">ðXX T Þ 0:5 ðw i Àbm i Þ ¼ 0<label>ð25Þ</label></formula><p>Since we assume that XX T g0, we have</p><formula xml:id="formula_28">w i Àbm i ¼ 0<label>ð26Þ</label></formula><p>This completes the proof of Theorem 1. &amp;</p><p>When the parameter g assumes the value of zero, DSNPE degenerates into SNPE. From this point it can be concluded that SNPE is a special case of DSNPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In this section, we conducted a set of experiments to verify the effectiveness of the proposed DSNPE method. Three face databases were used, including Yale, Extended Yale B and the AR face image database.</p><p>In each experiment, the image set was partitioned into a training set and test set with different numbers. For ease of representation, the experiments were named as p-train, which means that p images per individual were selected for training and the remaining images for test. To robustly evaluate the performance of different algorithms in different training and testing conditions, we selected images randomly and repeated the experiment 20 times in each condition. We exhibited the results in the form of mean recognition rate with standard deviation.</p><p>We compare DSNPE with several representative dimensional reduction methods such as PCA <ref type="bibr" target="#b1">[2]</ref>, LDA <ref type="bibr" target="#b1">[2]</ref>, LPP <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b57">58]</ref>, DLPP/ MMC <ref type="bibr" target="#b23">[24]</ref>, LPDP <ref type="bibr" target="#b25">[26]</ref>, NDLPP <ref type="bibr" target="#b24">[25]</ref>, SNPE1 <ref type="bibr" target="#b41">[42]</ref>, and SNPE2 <ref type="bibr" target="#b41">[42]</ref>. The nearest neighbor classifier is employed for classification. For LPP, the number of nearest neighbors k is taken to be p À1 as done in <ref type="bibr" target="#b58">[59]</ref> where p is the number of images per individual selected for training. For DSNPE, we simply set the value of g as 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental results on the Yale database</head><p>The Yale face database was constructed at the Yale Center for Computation Vision and Control. There are 165 images of 15 individuals (each person providing 11 different images). The images demonstrate variations in lighting condition (leftlight, center-light and right-light), facial expression (normal, happy, sad, sleepy, surprised, and wink), and with or without glasses. All images were also in grayscale and cropped and resized to the resolution of 32 Â 32 pixels. We pre-processed the data by normalizing each face vector to the unit. Shown in Fig. <ref type="figure" target="#fig_0">1</ref> is one object from Yale database. The top ten Eigenfaces, Fisherfaces, DSNPE1faces, and DSNPE2faces of Yale images are shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>For each person, p images (p varying from 2 to 8) were randomly selected for training, and the rest were used as test samples. The training set was used to learn a face subspace. Recognition was then performed in the subspaces. In general, the recognition rates vary with the dimension of the face subspace. Table <ref type="table">2</ref> shows the maximal average recognition rates across 20 runs of each method under nearest neighbor classifier and their corresponding standard deviations (std) and dimensions, where the best results are highlighted in bold.</p><p>The recognition rate curves of different algorithms are drawn in Fig. <ref type="figure">3</ref> where SNPE1 denotes the SNPE algorithm based on Eq. ( <ref type="formula" target="#formula_0">1</ref>), SNPE2 denotes the SNPE based on Eq. ( <ref type="formula" target="#formula_1">2</ref>).</p><p>DSNPE1 denotes the DSNPE algorithm based on Eq. ( <ref type="formula" target="#formula_15">15</ref>) and DSNPE2 denotes the DSNPE based on Eq. ( <ref type="formula" target="#formula_16">16</ref>). Due to the space limitation, we only draw the recognition rate curves of some representative methods in Fig. <ref type="figure">3</ref>. For the baseline method, we simply performed face recognition in the original 1024-dimensional image space. Note that the upper bound of the dimensionality of LDA is c À 1 where c is the number of individuals <ref type="bibr" target="#b1">[2]</ref>.</p><p>As can be seen, our algorithm DSNPE1 outperformed all other methods except for 2 train while the PCA method performed the worst in all cases. It is very interesting that the PCA method and the baseline method have the same performance when p varies from 2 to 5 which is consistent with the results in many publications such as <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental results on the Extended Yale B database</head><p>The Extended Yale B database <ref type="bibr" target="#b49">[50]</ref> contains 2414 front-view face images of 38 individuals. For each individual, about 64 pictures were taken under various laboratory-controlled lighting conditions. In our experiments, we use the cropped images with the resolution of 32 Â 32. We pre-processed the data by normalizing each face vector to the unit. Thirty two cropped sample images of one person in the Extended Yale B database after the scale normalization are displayed in Fig. <ref type="figure">4</ref>. For each individual, p images (p equals to 5 or 10) were randomly selected for training and the rest were used for test. The experimental design is the same as in Section 4.1. The maximal average recognition rate, the corresponding dimensionality and the standard deviations across 20 runs of tests of each method are shown in Table <ref type="table">3</ref>. The best results are highlighted in bold face font. In addition, we draw the recognition rate curves of some representative algorithms in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>As can be seen, our DSNPE algorithm performed the best in all cases. Moreover, the PCA method and the baseline method have nearly the same performance as the Yale face database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental results on the AR face database</head><p>The last experiment was tested using the AR face database (http://rvl1.ecn.purdue.edu/$ aleix/aleix_face_DB.html) which contains over 4000 color face images of 126 individuals (70 men and 56 women), including frontal views of faces with different facial expressions, illumination conditions and occlusions. The pictures of most persons were taken in two sessions (separated by two weeks). Each section contains 13 color images. In our experiments here, we use a subset of the AR face database provided and preprocessed by <ref type="bibr">Martinez [4]</ref>. This subset contains 1400 face images corresponding to 100 person (50 men and 50 women), where each person has 14 different images with illumination change and expressions. The original resolution of these image faces is 165 Â 120. Here, for computational convenience, we manually cropped the face portion of the image and then normalized it to 62 Â 44 pixels. The normalized images of one person are shown in Fig. <ref type="figure">6</ref>, where the images in the first row in Fig. <ref type="figure">6</ref>   <ref type="figure">6h</ref>-n were taken under the same conditions as Fig. <ref type="figure">6a-g</ref>. Since AR database has naturally been partitioned into two sessions, we also consider this case in our experiments. In this experiment, images from the first session (i.e., Fig. <ref type="figure">6a-g</ref>) were used for training, and images from the second session (i.e., Fig. <ref type="figure">6h-n</ref>) were used for test.</p><p>As the training set and test set are fixed, we only give the recognition rates of different algorithms. The maximal recognition rate of each method and the corresponding reduced dimension are listed in Table <ref type="table" target="#tab_2">4</ref>. The recognition rate curves of some representative algorithms vs. the variation of dimensions are shown in Fig. <ref type="figure">7</ref>. From the experimental results, we can see that DSNPE1 achieves the highest recognition rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we developed a new sparse dimensionality reduction method called discriminant sparse neighborhood preserving embedding (DSNPE). Our proposed method combines sparsity criterion and maximum margin criterion (MMC) together to project the input high-dimensional image into a low-dimensional feature vector. Therefore both the robustness advantage of sparse representation and distinctiveness advantage of MMC are integrated to develop a good pattern recognition solution.</p><p>The proposed DSNPE method is applied for face recognition. The testing results on three face image databases, i.e., Yale, Extended Yale B and AR face database demonstrate that DSNPE is more effective than some popular dimensionality reduction algorithms. These experiments are mainly designed to prove the effectiveness of DSNPE for pattern recognition task oriented feature dimensionality reduction. Because the DSNPE algorithm is directly applied on the original facial images rather than local features such as Gabor, LBP features, the accuracy of face recognition in our experiments has still a big gap from practical face recognition applications. However, we argue that DSNPE provides a useful tool of dimensionality reduction which may benefit state-of-the-art pattern recognition algorithms. Our future work will apply DSNPE algorithm on advanced visual features rather than the original pixel intensity values. And some useful strategies (e.g., localizing the training images, synthesizing virtual samples) will also be incorporated into the pattern recognition scheme to achieve much higher pattern recognition accuracy for face, iris and palmprint recognition. Appendix. The formulation for deriving Eq. ( <ref type="formula" target="#formula_4">4</ref>) Similarly, Jw T x i Àw T Xs i J 2 ¼ w T XðIÀSÀS T þS T SÞX T w</p><formula xml:id="formula_29">X n i ¼ 1 Jw T x i Àw T Xs i J 2 ¼ X n i ¼ 1 w T ðx i ÀXs i Þðx i ÀXs i Þ T w ¼ w T X n i ¼ 1 ðx i ÀXs i Þðx i ÀXs i Þ T ! w Âw T X n i ¼ 1 x i x T i À X n i ¼ 1 x i s T i ! X T ÀX X n i ¼ 1 s i x T i ! þ X X n i ¼ 1 s i s T i ! X T ! w Â X n i ¼ 1 x i x T i ¼ ½x 1 x 2 Á Á Á x n x T</formula><formula xml:id="formula_30">X n i ¼ 1 s i s T i ¼ S T S</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Eleven cropped and resized samples of one person in Yale face database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Top 10 Eigenfaces, Fisherfaces, DSNPE1faces and DSNPE2faces of Yale dataset.</figDesc><graphic coords="4,72.08,447.18,192.24,97.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Recognition accuracy vs. dimensionality on Yale database with 2,3,4,5,6,7,8 images for each individual randomly selected for training.</figDesc><graphic coords="5,130.76,648.80,324.00,81.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Recognition accuracy vs. dimensionality on Extended Yale B database with 5, 10 images for each individual randomly selected for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig.6. Some typical samples of the cropped images found in the AR face image database.</figDesc><graphic coords="7,304.27,382.92,246.24,110.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>are from Session 1, and the images in the second row are from Session 2. The details of the images are: Fig. 6a neutral expression, Fig. 6b smile, Fig. 6c anger, Fig. 6d scream, Fig. 6e left light on; Fig. 6f right light on, Fig. 6g all sides light on, and Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Cooperation Program of China (Grant no. 2010DFB14110)the Knowledge Innovation Program of the Chinese Academy of Sciences (Grant nos. Y023A61121 and Y023A11292).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Maximal recognition rates (percent) on the AR face database and the corresponding dimensions.</figDesc><table><row><cell>Methods</cell><cell>Baseline</cell><cell>PCA</cell><cell>LPP</cell><cell>LDA</cell><cell>NDLPP</cell><cell>LPDP</cell><cell>DLPP/MMC</cell><cell>SNPE1</cell><cell>SNPE2</cell><cell>DSNPE1</cell><cell>DSNPE2</cell></row><row><cell>Recognition rate</cell><cell>78.14</cell><cell>78.14</cell><cell>75.42</cell><cell>82.42</cell><cell>83.43</cell><cell>83.43</cell><cell>84</cell><cell>75.42</cell><cell>74.28</cell><cell>84.28</cell><cell>82</cell></row><row><cell>Dimension</cell><cell>2520</cell><cell>455</cell><cell>243</cell><cell>93</cell><cell>99</cell><cell>229</cell><cell>70</cell><cell>243</cell><cell>238</cell><cell>243</cell><cell>243</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J.Gui  et al. / Pattern Recognition 45 (2012) 2884-2893</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Basic Research Program of China (Grant no. 2012CB316300), the National Science Foundation of China (Grant nos. 61100161, 61175022, 60736018, 60905023, 61075024, and 61005007), the International S&amp;T</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual learning and recognition of 3-D objects from appearance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997-07">1997. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-step dimensionality reduction and semisupervised graph-based tumor classification using gene expression data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="181" to="191" />
			<date type="published" when="2010-11">2010. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001-02">2001. February</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000-12-22">2000. December 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cognition-the manifold ways of perception</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page">2268</biblScope>
			<date type="published" when="2000-12-22">2000. December 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized discriminant analysis using a kernel approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Baudat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Anouar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2385" to="2404" />
			<date type="published" when="2000-10">2000. October</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998-07-01">1998. July 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML 2010)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Two-stage learning kernel algorithms</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2324" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003-06">2003. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hessian eigenmaps: locally linear embedding techniques for high-dimensional data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="5591" to="5596" />
			<date type="published" when="2003-05-13">2003. May 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relaxed maximum-variance unfolding</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<date type="published" when="2008-07">2008. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-linear dimensionality reduction procedures for certain large-dimensional multi-objective optimization problems: employing correntropy and a novel maximum variance unfolding, Evolutionary Multi-Criterion Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<biblScope unit="volume">4403</biblScope>
			<biblScope unit="page" from="772" to="787" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nonlinear dimension reduction via local tangent space alignment, Intelligent Data Engineering and Automated Learning 2690</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="477" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust local tangent space alignment, Neural Information Processing</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<biblScope unit="volume">5863</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="293" to="301" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Riemannian manifold learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="796" to="809" />
			<date type="published" when="2008-05">2008. May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Riemannian manifold learning for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3951</biblScope>
			<biblScope unit="page" from="44" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction with local spline embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2009-09">2009. September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Outof-sample extensions for LLE, isomap, mds, eigenmaps, and spectral clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Paiement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouimet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems 16 (NIPS&apos;03</title>
		<meeting>Advances in Neural Information Processing Systems 16 (NIPS&apos;03</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incremental nonlinear dimensionality reduction by manifold learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H C</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="377" to="391" />
			<date type="published" when="2006-03">2006. March</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A scalable kernel-based semisupervised metric learning algorithm with out-of-sample generalization ability</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2839" to="2861" />
			<date type="published" when="2008-11">2008. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face recognition using Laplacianfaces</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005-03">2005. March</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face recognition using discriminant locality preserving projections based on maximum margin criterion</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3572" to="3579" />
			<date type="published" when="2010-10">2010. October</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Null space discriminant locality preserving projections for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="3644" to="3649" />
			<date type="published" when="2008-10">2008. October</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Locality preserving discriminant projections for face and palmprint recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="2696" to="2707" />
			<date type="published" when="2010-08">2010. August</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multilinear discriminant analysis for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">O</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="212" to="220" />
			<date type="published" when="2007-01">2007. January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two-dimensional PCA: a new approach to appearance-based face representation and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="131" to="137" />
			<date type="published" when="2004-01">2004. January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tensor subspace analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tensor locality sensitive discriminant analysis and its complexity</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Wavelets Multiresolution and Information Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="865" to="880" />
			<date type="published" when="2009-11">2009. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Uncorrelated multilinear discriminant analysis with regularization and aggregation for tensor object recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="103" to="123" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tensor rank one discriminant analysis-a convergent method for discriminative multilinear subspace selection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1866" to="1882" />
			<date type="published" when="2008-06">2008. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminant locally linear embedding with high-order tensor data</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems Man and Cybernetics Part B-Cybernetics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2008-04">2008. April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">General tensor discriminant analysis and Gabor features for gait recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1700" to="1715" />
			<date type="published" when="2007-10">2007. October</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009-02">2009. February</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004-11">2004. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face recognition using Fisher non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks-ISNN 2005</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="112" to="117" />
		</imprint>
	</monogr>
	<note>Part</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spectral regression: a unified approach for sparse subspace learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining (ICDM&apos;07)</title>
		<meeting><address><addrLine>Omaha, NE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="2006-06">2006. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nonnegative sparse PCA</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1561</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning with l(1)-graph for image analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="858" to="866" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparsity preserving projections with applications to face recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="331" to="341" />
			<date type="published" when="2010-01">2010. January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the dimensionality reduction for sparse representation based face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 International Conference on Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
		<title level="m">Sparse representation classifier steered discriminative projection, in: 2010 International Conference on Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010-06">2010. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="2378" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries in wavelet domain</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Image and Graphics</title>
		<meeting>the Fifth International Conference on Image and Graphics</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="754" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image sequence denoising via sparse and redundant representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006-12">2006. December</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Automatic hard thresholding for sparse signal reconstruction from Nde measurements, Review of Progress in Quantitative Nondestructive Evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogandzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sparse signal recovery with exponential-family noise</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grabarnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 47th Annual Allerton Conference on Communication, Control, and Computing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="60" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual recognition and inference using dynamic overcomplete sparse learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreutz-Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2301" to="2352" />
			<date type="published" when="2007-09">2007. September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the Lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B-Methodological</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Solution of L1 minimization problems by LARS/homotopy methods</title>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 31th International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="636" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient and robust feature extraction by maximum margin criterion</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="157" to="165" />
			<date type="published" when="2006-01">2006. January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Comments on efficient and robust feature extraction by maximum margin criterion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1862" to="1864" />
			<date type="published" when="2007-11">2007. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Algorithms: Eigensystems: Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University City Science Center</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="153" to="160" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Globally maximizing, locally minimizing: unsupervised discriminant projection with applications to face and palm biometrics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="650" to="664" />
			<date type="published" when="2007-04">2007. April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sch lkopf, Local learning projections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1039" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robust principal component analysis based on maximum correntropy criterion</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1485" to="1494" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Maximum correntropy criterion for robust face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1561" to="1576" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Zhenan Sun received the BE degree in industrial automation from Dalian University of Technology, the MS degree in system engineering from Huazhong University of Science and Technology, and the PhD degree in pattern recognition and intelligent systems from the Institute of Automation, Chinese Academy of Sciences (CASIA)</title>
	</analytic>
	<monogr>
		<title level="m">2007, and the Ph.D. degree in pattern recognition and intelligence system from University of Science and Technology of China</title>
		<title level="s">the M.Sc. degree in Computer Applied technology from Anhui Institute of Optics and Fine Mechanics, Chinese Academy of Science</title>
		<meeting><address><addrLine>Nanjing, China; Hefei, China; Hefei, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">2004. 2010. 1999. 2002, and 2006. March 2006</date>
		</imprint>
		<respStmt>
			<orgName>Jie Gui received the B.Sc. degree in Computer Science from Hohai University ; Center of Biometrics and Security Research (CBSR) at the National Laboratory of Pattern</orgName>
		</respStmt>
	</monogr>
	<note>He is an associate professor at CASIA. Recognition (NLPR) of CASIA as a faculty. He is a member of the IEEE and the IEEE Computer Society. His current research focuses on biometrics, pattern recognition, and computer vision</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">He is currently an associate professor in Hefei Institute of Intelligent Machines</title>
	</analytic>
	<monogr>
		<title level="m">2004, and the Ph.D. degree in pattern recognition and intelligence system from University of Science and Technology of China</title>
		<meeting><address><addrLine>Wuhan, China; Hefei, China; Hefei, China</addrLine></address></meeting>
		<imprint>
			<publisher>Chinese Academy of Science</publisher>
			<date type="published" when="2007-02">2008. June 2007 to February 2008</date>
		</imprint>
		<respStmt>
			<orgName>Center of China Normal University ; Hong Kong Polytechnic University</orgName>
		</respStmt>
	</monogr>
	<note>1998, the M.Sc. degree in computer science from Hefei University of technology. His research interests include palmprint recognition, pattern recognition, and image processing</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">His research interests include pattern recognition, machine learning and image processing</title>
		<author>
			<persName><forename type="first">Rong-Xiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-09">2006. September 2006</date>
			<pubPlace>Hefei, China; Hefei, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer science from Hefei University of Technology ; University of Science and Technology of China</orgName>
		</respStmt>
	</monogr>
	<note>he is a Master-Doctoral Program student in Department of Automation</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Lei received the BE degree in Communication Countermeasure Engineering from the Electronic Engineering Institute, China, in 1998, the Ph.D. degree in the Department of Automation, the University of Science and Technology of China. Now he is a lecturer in the Electronic Engineering Institute</title>
		<author>
			<persName><forename type="first">Ying-Ke</forename></persName>
		</author>
		<imprint>
			<pubPlace>Hefei, China</pubPlace>
		</imprint>
	</monogr>
	<note>His research interests are machine learning, pattern recognition. and communication signal processing</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">His research interests include machine learning, data mining, and bioinformatics. He received the Outstanding Ph</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<pubPlace>Tempe, AZ; Norfolk, VA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Shuiwang Ji received the Ph.D. degree in computer science from Arizona State University ; Computer Science, Old Dominion University ; D. Student Award from Arizona State University</orgName>
		</respStmt>
	</monogr>
	<note>Currently, he is an Assistant Professor in the Department of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
