<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Contrastive Learning Automated</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-10">10 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
							<email>&lt;yshen@tamu.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Zhangyang Wang</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Contrastive Learning Automated</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-10">10 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.07594v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on adhoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bilevel optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous "best practices" observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentationaware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-ofthe-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious datasetspecific tuning on augmentation selection. We release the code at https://github.com/ Shen-Lab/GraphCL_Automated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-supervised learning on graph-structured data has raised significant interests recently <ref type="bibr" target="#b15">(Hu et al., 2019;</ref><ref type="bibr" target="#b60">You et al., 2020b;</ref><ref type="bibr" target="#b21">Jin et al., 2020;</ref><ref type="bibr" target="#b17">Hu et al., 2020b;</ref><ref type="bibr" target="#b19">Hwang et al., 2020;</ref><ref type="bibr" target="#b31">Manessi &amp; Rozza, 2020;</ref><ref type="bibr" target="#b64">Zhu et al., 2020a;</ref><ref type="bibr" target="#b37">Peng et al., 2020a;</ref><ref type="bibr" target="#b44">Rong et al., 2020;</ref><ref type="bibr" target="#b22">Jin et al., 2021b;</ref><ref type="bibr" target="#b54">Wu et al., 2021;</ref><ref type="bibr" target="#b45">Roy et al., 2021;</ref><ref type="bibr" target="#b18">Huang et al., 2021;</ref><ref type="bibr" target="#b27">Li et al., 2021)</ref>. Among many others, graph contrastive learning methods extend the contrastive learning idea <ref type="bibr" target="#b14">(He et al., 2020;</ref><ref type="bibr" target="#b7">Chen et al., 2020c)</ref>, originally developed in the computer vision domain, to learn generalizable, transferable and robust representations from unlabeled graph data <ref type="bibr" target="#b51">(Veličković et al., 2018;</ref><ref type="bibr" target="#b50">Sun et al., 2019;</ref><ref type="bibr" target="#b59">You et al., 2020a;</ref><ref type="bibr" target="#b39">Qiu et al., 2020;</ref><ref type="bibr">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b65">Zhu et al., 2020b;</ref><ref type="bibr">c;</ref><ref type="bibr" target="#b5">Chen et al., 2020b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b40">Ren et al., 2019;</ref><ref type="bibr" target="#b35">Park et al., 2020;</ref><ref type="bibr" target="#b38">Peng et al., 2020b;</ref><ref type="bibr" target="#b20">Jin et al., 2021a;</ref><ref type="bibr" target="#b52">Wang &amp; Liu, 2021)</ref>.</p><p>Nevertheless, unlike images, graph datasets are abstractions of diverse nature (e.g. pandemics, citation networks, biochemical molecules, or social networks). Such a unique diversity challenge was not fully addressed by prior graph self-supervised learning approaches <ref type="bibr" target="#b15">(Hu et al., 2019;</ref><ref type="bibr" target="#b59">You et al., 2020a;</ref><ref type="bibr">b)</ref>. For example, the state-of-the-art graph contrastive learning framework, GraphCL <ref type="bibr" target="#b59">(You et al., 2020a)</ref>, constructs specific contrastive views of graph data via handpicking ad-hoc augmentations for every dataset <ref type="bibr" target="#b59">(You et al., 2020a;</ref><ref type="bibr" target="#b63">Zhao et al., 2020;</ref><ref type="bibr" target="#b25">Kong et al., 2020)</ref>. The choice of augmentation follows empirical rules of thumb, typically summarized from many trial-and-error experiments per dataset. That seriously prohibits GraphCL and its variants from broader applicability, considering the tremendous heterogeneity of practical graph data. Moreover, even such trial-and-error selection of augmentations relies on a labeled validation set for downstream evaluation, which is not always available <ref type="bibr" target="#b9">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b16">Hu et al., 2020a)</ref>.</p><p>Contributions. Given a new and unseen graph dataset, can our graph contrastive learning methods automatically select their data augmentation, avoiding ad-hoc choices or tedious tuning? This paper targets at overcoming this crucial, unique, and inherent hurdle. We propose joint augmentation optimization (JOAO), a principled bi-level optimization framework that learns to select data augmentations for the first time. To highlight, the selection framework by JOAO is: (i) automatic, completely free of human labor of trial-and-error on augmentation choices; (ii) adaptive, generalizing smoothly to handling diverse graph data; and (iii) dynamic, allowing for augmentation types varying at different training stages. Compared to previous ad-hoc, per-dataset and prefixed augmentation selection, JOAO achieves an unprecedented degree of flexibility and ease of use. We summarize our contributions:</p><p>• Leveraging GraphCL <ref type="bibr" target="#b59">(You et al., 2020a)</ref> as the baseline model, we introduce joint augmentation optimization (JOAO) as a plug-and-play framework. JOAO is the first to automate the augmentation selection when performing contrastive learning on specific graph data. It frees GraphCL from expensive trial-and-errors, or empirical ad-hoc rules, or any validation based on labeled data.</p><p>• JOAO can be formulated as a unified bi-level optimization framework, and be instantiated as min-max optimization.</p><p>It takes inspirations from adversarial perturbations as data augmentations <ref type="bibr" target="#b55">(Xie et al., 2020)</ref>, and can be solved by an alternating gradient-descent algorithm.</p><p>• In accordance with diverse and dynamic augmentations enabled by JOAO, we design a new augmentation-aware projection head for graph contrastive learning. The rationale is to avoid too many complicated augmentations distorting the original data distribution. The idea is to keep one nonlinear projection head per augmentation pair, and each time using the single head corresponding to the augmentation currently selected by JOAO.</p><p>• Extensive experiments demonstrate that GraphCL with JOAO performs on par with or even sometimes better than state-of-the-art (SOTA) competitors, across multiple graph datasets of various types and scales, yet without resorting to tedious dataset-specific manual tuning or domain knowledge. We also show the augmentation selections made by JOAO are in general informed and often aligned with previous "best practices".</p><p>We leave two additional remarks: (1) JOAO is designed to be flexible and versatile. Although this paper mainly demonstrates JOAO on GraphCL, they are not tied with each other. The general optimization formulation of JOAO allows it to be easily integrated with other graph contrastive learning frameworks too.</p><p>(2) JOAO is designed for automating the tedious and ad-hoc augmentation selection. It intends to match the state-of-the-art results achieved by exhaustive manual tuning, but not necessarily to surpass them all. To re-iterate, our aim is to scale up graph contrastive learning to numerous types and scales of graph data in the real world, via a hassle-free framework rather than tuning one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries and Notations</head><p>Graph neural networks (GNNs) have grown into powerful tools to model non-Euclidean graph-structured data arising from various fields <ref type="bibr" target="#b56">(Xu et al., 2018;</ref><ref type="bibr" target="#b61">You et al., 2020c;</ref><ref type="bibr" target="#b58">You &amp; Shen, 2020;</ref><ref type="bibr" target="#b28">Liu et al., 2020;</ref><ref type="bibr" target="#b62">Zhang et al., 2020)</ref>. Let G = {V, E}<ref type="foot" target="#foot_0">1</ref> denote an undirected graph in the space G with V and E being the set of nodes and edges, respectively, and X v ∈ R D for v ∈ V being node features. A GNN is defined as the mapping f : G → R D that encodes a sample graph G into an D -dimensional vector.</p><p>Self-supervised learning on graphs is shown to learn more generalizable, transferable and robust graph representations, through exploiting vast unlabelled data <ref type="bibr" target="#b21">(Jin et al., 2020;</ref><ref type="bibr" target="#b17">Hu et al., 2020b;</ref><ref type="bibr" target="#b19">Hwang et al., 2020;</ref><ref type="bibr" target="#b31">Manessi &amp; Rozza, 2020;</ref><ref type="bibr" target="#b64">Zhu et al., 2020a;</ref><ref type="bibr" target="#b37">Peng et al., 2020a;</ref><ref type="bibr" target="#b44">Rong et al., 2020;</ref><ref type="bibr" target="#b22">Jin et al., 2021b;</ref><ref type="bibr" target="#b54">Wu et al., 2021;</ref><ref type="bibr" target="#b45">Roy et al., 2021;</ref><ref type="bibr" target="#b18">Huang et al., 2021;</ref><ref type="bibr" target="#b27">Li et al., 2021)</ref>. However, earlier selfsupervised tasks often need to be carefully designed with domain knowledge <ref type="bibr" target="#b60">(You et al., 2020b;</ref><ref type="bibr" target="#b15">Hu et al., 2019)</ref> due to the intrinsic complicacy of graph datasets.</p><p>Graph contrastive learning recently emerges as a promising direction <ref type="bibr" target="#b51">(Veličković et al., 2018;</ref><ref type="bibr" target="#b50">Sun et al., 2019;</ref><ref type="bibr" target="#b39">Qiu et al., 2020;</ref><ref type="bibr">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b65">Zhu et al., 2020b;</ref><ref type="bibr">c;</ref><ref type="bibr" target="#b5">Chen et al., 2020b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b40">Ren et al., 2019;</ref><ref type="bibr" target="#b35">Park et al., 2020;</ref><ref type="bibr" target="#b38">Peng et al., 2020b;</ref><ref type="bibr" target="#b20">Jin et al., 2021a;</ref><ref type="bibr" target="#b52">Wang &amp; Liu, 2021)</ref>. For example, the SOTA GraphCL framework <ref type="bibr" target="#b59">(You et al., 2020a)</ref> enforces the perturbation invariance in GNNs through maximizing agreement between two augmented views of graphs: an overview is illustrated in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive loss optimization</head><p>Figure <ref type="figure">1</ref>: Overview of the GraphCL pipeline in <ref type="bibr" target="#b59">(You et al., 2020a)</ref>.</p><p>Specifically, we denote the input graph-structured sample G from certain empirical distribution P G . Its samples two random augmentation operators A 1 , A 2 from a given pool of augmentation types as A = {NodeDrop, Subgraph, EdgePert, AttrMask, Identical} <ref type="bibr" target="#b59">(You et al., 2020a)</ref> and A ∈ A : G → G. GraphCL <ref type="bibr" target="#b59">(You et al., 2020a)</ref> optimizes the following loss:</p><formula xml:id="formula_0">min θ L(G, A 1 , A 2 , θ) = min θ (−E P G ×P (A 1 ,A 2 ) sim( Positive pairs T θ,1 (G), T θ,2 (G))<label>(1)</label></formula><formula xml:id="formula_1">+ E P G ×P A 1 log(E P G ×P A 2 exp(sim(T θ,1 (G), T θ,2 (G ) Negative pairs ))) ,</formula><p>where</p><formula xml:id="formula_2">T θ,i = A i • f θ • g θ (i = 1, 2</formula><p>) is parameterized by θ = {θ , θ }, and f θ : G → R D , g θ : R D → R D are the shared-weight GNN and projection head, respectively, sim(u, v) = u T v u v is the cosine similarity function, P G = P G acts as the negative sampling distribution, and P A1 and P A2 are the marginal distributions. After the contrastive pre-training, the pre-trained f θ * can be further leveraged for various downstream task fine-tuning.</p><p>In the current GraphCL framework, (A 1 , A 2 ) are selected by hand and pre-fixed for each dataset. In other words, P (A1,A2) is a Dirac distribution with the only spike at the selected augmentation pair. Yet given new graph data, how to select (A 1 , A 2 ) relies on no more than loose heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">JOAO: The Unified Framework</head><p>One clear limitation in (1) is that one needs to pre-define the sampling distribution P (A1,A2) based on prior rules, and only a Dirac distribution (i.e., only one pair for each dataset) was explored. Rather, we propose to dynamically and automatically learn to optimize P (A1,A2) when performing GraphCL (1), via the following bi-level optimization framework:</p><formula xml:id="formula_3">min θ L(G, A 1 , A 2 , θ), s.t. P (A1,A2) ∈ arg min P (A 1 ,A 2 ) D(G, A 1 , A 2 , θ),<label>(2)</label></formula><p>We refer to (2) as joint augmentation optimization (JOAO), where the upper-level objective L is the same as the GraphCL objective (or the objective of any other graph contrastive learning approach), and the lower-level objective D optimizes the sampling distribution P (A1,A2) jointly for augmentation-pair selections. Notice that JOAO (2) only exploits the signals from the self-supervised training itself, without accessing downstream labeled data for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instantiation of JOAO as Min-Max Optimization</head><p>Motivated from adversarial training <ref type="bibr" target="#b53">(Wang et al., 2019;</ref><ref type="bibr" target="#b55">Xie et al., 2020)</ref>, we follow the same philosophy to always exploit the most challenging data augmentation of the current loss, hence instantiating the general JOAO framework as a concrete min-max optimization form:</p><formula xml:id="formula_4">min θ L(G, A 1 , A 2 , θ), s.t. P (A1,A2) ∈ arg max P (A 1 ,A 2 ) L(G, A 1 , A 2 , θ) − γ 2 dist(P (A 1 ,A 2 ) , P prior ) ,<label>(3)</label></formula><p>where γ ∈ R ≥0 , P prior is the prior distribution on all possible augmentations, and dist : P × P → R ≥0 is a distance function between the sampling and the prior distribution (P is the probability simplex). Thereby, JOAO's formulation aligns with the idea of model-based adversarial training <ref type="bibr" target="#b43">(Robey et al., 2020)</ref>, where adversarial training is known to boost generalization, robustness and transferability <ref type="bibr" target="#b43">(Robey et al., 2020;</ref><ref type="bibr" target="#b53">Wang et al., 2019)</ref>.</p><p>In this work, we choose P prior as the uniform distribution to promote diversity in the selections, following a common principle of maximum entropy <ref type="bibr" target="#b13">(Guiasu &amp; Shenitzer, 1985)</ref> in Bayesian learning. No additional information is assumed about the dataset or the augmentation pool. In practice, it encourageds more diverse augmentation selections rather than collapsing to few. Comparison between the formulations with and without the prior is shown in Table <ref type="table" target="#tab_7">S5</ref> of Appendix E. We use a squared Euclidean distance for dist(•, •). Accordingly, we have</p><formula xml:id="formula_5">dist(P (A1,A2) , P prior ) = |A| i=1 |A| j=1 (p ij − 1 |A| 2 ) 2 where the probability p ij = Prob(A 1 = A i , A 2 = A j ).</formula><p>We will next present how to optimize (3). Following <ref type="bibr" target="#b53">(Wang et al., 2019)</ref>, we adopt the alternating gradient descent algorithm (AGD), alternating between upper-level minimization and lower-level maximization, as outlined in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 AGD for optimization (3)</head><p>Input: initial parameter θ (0) , sampling distribution</p><formula xml:id="formula_6">P (0) (A1,A2) , optimization step N . for n = 1 to N do 1. Upper-level minimization: fix P (A1,A2) = P (n−1) (A1,A2</formula><p>) , and call equation ( <ref type="formula" target="#formula_7">4</ref>) to update θ (n) . 2. Lower-level maximization: fix θ = θ (n) , and call equation ( <ref type="formula">9</ref>) to update P (n) (A1,A2) . end for Return: Optimized parameter θ (N ) .</p><p>Upper-level minimization. The upper-level minimization w.r.t. θ follows the conventional gradient descent procedure as in the GraphCL optimization (1) given the sampling distribution P (A1,A2) , represented as:</p><formula xml:id="formula_7">θ (n) = θ (n−1) − α θ L(G, A 1 , A 2 , θ),<label>(4)</label></formula><p>where α ∈ R &gt;0 is the learning rate.</p><p>Lower-level maximization. Since it is not intuitive to directly calculate the gradient of the lower-level objective w.r.t. P (A1,A2) , we first rewrite the contrastive loss in (1) as:</p><formula xml:id="formula_8">L(G, A 1 , A 2 , θ) = |A| i=1 |A| j=1 Targeted p ij − E P G sim(T i θ (G), T j θ (G)) + E P G log( |A| j =1 p j Undesired E P G exp(sim(T i θ (G), T j θ (G )))) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">T i θ = A i • f θ • g θ , (i = 1, ...,<label>5</label></formula><p>), and the marginal probabilities p j = p j = Prob(A 2 = A j ). In the equation (5), we expand the expectation on augmentations A 1 , A 2 into the form of weighted summation related to p ij in order to calculate the gradient. However, within the expectation on G of the negative pair term there is the marginal probabilities p j entangled, and therefore we make the following numerical approximation for the lower bound of the negative pair term to disentangle p ij in the equation ( <ref type="formula" target="#formula_8">5</ref>):</p><formula xml:id="formula_10">E P G ×P A 1 log(E P G ×P A 2 exp(sim(T θ,1 (G), T θ,2 (G )))) ≥ E P G ×P A 1 ×P A 2 log(E P G exp(sim(T θ,1 (G), T θ,2 (G )))) ≈ E P G ×P (A 1 ,A 1 ) log(E P G exp(sim(T θ,1 (G), T θ,2 (G )))),<label>(6)</label></formula><p>where the first inequality comes from Jensen's inequality, and the second approximation is numerical. It results in the approximated contrastive loss:</p><formula xml:id="formula_11">L(G, A 1 , A 2 , θ) ≈ |A| i=1 |A| j=1 Targeted p ij (G, A i , A j , θ) = |A| i=1 |A| j=1 p ij − E P G sim(T i θ (G), T j θ (G)) + E P G log(E P G exp(sim(T i θ (G), T j θ (G )))) .<label>(7)</label></formula><p>Through approximating the contrastive loss, the lower-level maximization in the optimization (3) is rewritten as:</p><formula xml:id="formula_12">P (A1,A2) ∈ arg max p∈P,p=[pij ],i,j=1,...,|A| {ψ(p)}, ψ(p) = |A| i=1 |A| j=1 p ij (G, A i , A j , θ) − γ 2 |A| i=1 |A| j=1 (p ij − 1 |A| 2 ) 2 , (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>where ψ(p) is a strongly-concave function w.r.t. p in the probability simplex P. Thus, a projected gradient descent <ref type="bibr" target="#b53">(Wang et al., 2019;</ref><ref type="bibr" target="#b3">Boyd et al., 2004</ref>) is performed to update the sampling distribution P (A1,A2) for selecting augmentation pairs, expressed as:</p><formula xml:id="formula_14">b = p (n−1) + α p ψ(p (n−1) ), p (n) = (b − µ1) + , (9)</formula><p>where α ∈ R &gt;0 is the learning rate, µ is the root of the equation 1 T (b − µ1) = 1, and (•) + is the element-wise non-negative operator. µ can be efficiently found via the bi-jection method <ref type="bibr" target="#b53">(Wang et al., 2019;</ref><ref type="bibr" target="#b3">Boyd et al., 2004)</ref>.</p><p>Even though an optimizer with theoretical guarantee of convergence for non-convex non-concave min-max problems remains an open challenge, we acknowledge that AGD is an approximation of solving the bi-level optimization (3) precisely, which typically costs Bayesian optimization <ref type="bibr" target="#b49">(Srinivas et al., 2010;</ref><ref type="bibr" target="#b49">Snoek et al., 2012)</ref>, automatic differentiation <ref type="bibr" target="#b29">(Luketina et al., 2016;</ref><ref type="bibr" target="#b10">Franceschi et al., 2017;</ref><ref type="bibr" target="#b2">Baydin et al., 2017;</ref><ref type="bibr" target="#b47">Shaban et al., 2019)</ref>, or first-order techniques based on some inner-loop approximated solution <ref type="bibr" target="#b30">(Maclaurin et al., 2015;</ref><ref type="bibr" target="#b36">Pedregosa, 2016;</ref><ref type="bibr" target="#b11">Gould et al., 2016)</ref>. As most of them suffer from high time or space complexity, AGD was adopted as an approximated heuristic mainly for saving computational overhead. It showed some level of empirical convergence as seen in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">SANITY CHECK: JOAO RECOVERS AUGMENTATION-PAIRS ALIGNED WITH PREVIOUS "BEST PRACTICES"</head><p>How reasonable are the JOAO-selected augmentation pairs per dataset? This section pass JOAO through a sanity check, by comparing its selections with the previous trial-and-error findings by manually and exhaustively combining different augmentations (using downstream labels for validation) <ref type="bibr" target="#b59">(You et al., 2020a)</ref>. To examine such alignment, we visualize in the top row of Figure <ref type="figure" target="#fig_1">3</ref> the JOAO-optimized sampling distributions P (A1,A2) , and in the bottom row the GraphCL's manual trialand-error results over various augmentation pairs, for four different datasets (data statistics in Table <ref type="table" target="#tab_0">1</ref>). Please refer to the caption on Figure <ref type="figure" target="#fig_1">3</ref> how to interpret the percentage numbers in the top and bottom rows respectively. Overall, we observe a decent extent of alignments between the two rows' trends and especially the high-value locations, indicating that: if an augmentation pair was manually verified to yield better GraphCL results, it is also more likely to be selected by JOAO. More specifically we can see: (1) augmentation pairs containing EdgePert and AttrMask are more likely to be selected for biochemical molecules and denser graphs, respectively;</p><p>(2) NodeDrop and Subgraph are generally adopted on all four datasets; and (3) the augmentation pairs of two identy transformations are completely abandoned by JOAO, and those pairs with more diverse transformation types are more desired. All those observations are well aligned with the "rules of thumb" summarized in <ref type="bibr" target="#b59">(You et al., 2020a)</ref>. More discussions are provided in Appendix D. Bottom row: GraphCL performance gains (classification accuracy %, see <ref type="bibr" target="#b59">(You et al., 2020a)</ref> for the detailed setting) when exhaustively trying every possible augmentation pair. Note that the percentage numbers in the first and second rows have different meanings and are not apple-to-apple comparable; however, the overall alignments between the two rows' trends and high-value locations indicate that, if an augmentation pair was manually verified to yield better GraphCL results, it is also more likely to be selected by JOAO. Warmer (colder) colors indicate higher (lower) values, and white marks 0.</p><p>Therefore, the selections of augmentations made by JOAO are shown to be generally consistent with previous "best practices" observed from manual tuning -yet now being fully automated, flexible, versatile. It is also achieved without using any downstream task label, while <ref type="bibr" target="#b59">(You et al., 2020a)</ref> would hinge on a labeled set to compare two augmentations by their downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Augmentation-Aware Multi-Projection Heads:</head><p>Addressing A New Challenge from JOAO JOAO conveys the blessing of diverse and dynamic augmentations that are selected automatically during each GraphCL training, which may yield more robust and invariant features. However, that blessing could also bring up a new challenge: compared to one fixed augmentation pair throughout training, those varying and more aggressive augmentations can distort the training distribution more <ref type="bibr">(Lee et al., 2020;</ref><ref type="bibr" target="#b23">Jun et al., 2020)</ref>. Even mild augmentations, such as adding/dropping nodes or edges, could result in graphs very unlikely under the original distribution. Models trained with these augmentations may fit the original distribution poorly.</p><p>To address this challenge arising from using JOAO, we introduce multiple projection heads and an augmentation-aware selection scheme into GraphCL, as glimpsed in Figure <ref type="figure" target="#fig_2">4</ref> (see Figure <ref type="figure" target="#fig_0">S2</ref> in Appendix D for a schematic diagram). Specifically, we construct |A| projection heads each of which corresponds to one augmentation type (|A| denotes the cardinality of the augmentation pool). Then during training, once an augmentation is sampled, it will only go through and update its corresponding projection head. The main idea is to explicitly disentangle the distorted feature distributions caused by various augmentation pairs, and each time we only use the one head corresponding to the augmentation currently selected by JOAO. ,g Θ 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive loss optimization</head><formula xml:id="formula_15">) = P (A 1 ,A 2 ) .</formula><p>In mathematical forms, we route the output features from f through the projection head sampled from P (g Θ 1 ,g Θ 2 ) at each training step, where P (g Θ 1 ,g Θ 2 ) = P (A1,A2) , and Θ 1 , Θ 2 denote the head parameters, resulting in</p><formula xml:id="formula_16">T θ,i = A i • f θ • g Θ i , (i = 1, 2). Denot- ing L v2 (G, A 1 , A 2 , θ , Θ 1 , Θ 2 ) = E P (g Θ 1 ,g Θ 2 ) L(G, A 1 , A 2 , {θ , (Θ 1 , Θ 2 )})</formula><p>, we could then integrate the augmentation-aware projection head mechanism into the JOAO framework, referred to as JOAOv2:</p><formula xml:id="formula_17">min θ L v2 (G, A 1 , A 2 , θ , Θ 1 , Θ 2 ), s.t. P (A1,A2) ∈ arg max P (A 1 ,A 2 ) L v2 (G, A 1 , A 2 , θ , Θ 1 , Θ 2 ) − γ 2 dist(P (A 1 ,A 2 ) , P prior ) , P (g Θ 1 ,g Θ 2 ) = P (A1,A2) .<label>(10)</label></formula><p>Algorithm 1 could be easily adapted to solve (10) (see Algorithm S1 of Appendix A).</p><p>Our preliminary experiments in Table <ref type="table" target="#tab_1">2</ref> show that, without bells and whistles, augmentation-aware projection heads improve the performance upon JOAO under different augmentation strengths. That aligns with the observations in <ref type="bibr">(Lee et al., 2020;</ref><ref type="bibr" target="#b23">Jun et al., 2020)</ref>, showing that disentangling augmented and original feature distributions could have the model benefit more from stronger augmentations. We also plot the learned P (A1,A2) in Figure <ref type="figure">S1</ref> of Appendix D, where we can observe an ever stronger alignment than presented in Figure <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our proposed methods, JOAO and JOAOv2, against state-of-the-art (SOTA) competitors including self-supervised approaches heuristically designed with domain knowledge, and graph contrastive learning (GraphCL) with pre-defined rules for augmentation selection, using the scenarios of datasets originated from diverse sources, and datasets on specific bioinformatics domains. An summary of main results can be found in Table <ref type="table" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experiment Settings</head><p>Datasets. We use datasets of diverse nature from the benchmark TUDataset <ref type="bibr" target="#b32">(Morris et al., 2020)</ref>, including graph data for small molecules &amp; proteins <ref type="bibr">(Riesen &amp; Bunke, 2008;</ref><ref type="bibr" target="#b8">Dobson &amp; Doig, 2003)</ref>, computer vision <ref type="bibr" target="#b34">(Nene et al., 1996)</ref> and various relation networks <ref type="bibr" target="#b57">(Yanardag &amp; Vishwanathan, 2015;</ref><ref type="bibr" target="#b46">Rozemberczki et al., 2020)</ref> of diverse statistics (see Table <ref type="table" target="#tab_0">S1</ref> of Appendix B), under semi-supervised and unsupervised learning. Additionally we gather domain-specific bioinformatics datasets from the benchmark <ref type="bibr" target="#b15">(Hu et al., 2019)</ref> of relatively similar statistics (see Table <ref type="table" target="#tab_1">S2</ref>   <ref type="bibr" target="#b50">(Sun et al., 2019)</ref>, we pre-train using the whole dataset to learn graph embeddings and feed them into a downstream SVM classifier with 10fold cross-validation. (3) In transfer learning <ref type="bibr" target="#b15">(Hu et al., 2019)</ref>, we pre-train on a larger dataset then finetune and evaluate on smaller datasets of the same category using the given training/validation/test split.</p><p>GNN architectures &amp; augmentations. We adopt the same GNN architectures with default hyper-parameters as in the SOTA methods under individual experiment settings. Specifically, (1) in semi-supervised learning, ResGCN <ref type="bibr" target="#b6">(Chen et al., 2019)</ref> is used with 5 layers and 128 hidden dimensions, (2) in unsupervised representation learning, GIN <ref type="bibr" target="#b56">(Xu et al., 2018)</ref> is used with 3 layers and 32 hidden dimensions, and (3) in transfer learning and on large-scale OGB datasets, GIN is used with 5 layers and 300 hidden dimensions. Plus, we adopt the same graph data augmentations as in GraphCL <ref type="bibr" target="#b59">(You et al., 2020a)</ref> with the default augmentation strength 0.2. We tune the hyper-parameter γ controlling the trade-off in the optimization (3) in the range of {0.01, 0.1, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Compared Algorithms.</head><p>Training from scratch (with augmentations) and graph kernels. The naïve baseline training from the random initialization (with same augmentations as in GraphCL <ref type="bibr" target="#b59">(You et al., 2020a)</ref>) is compared, as well as SOTA graph kernel methods including GL <ref type="bibr" target="#b48">(Shervashidze et al., 2009)</ref>, WL <ref type="bibr" target="#b48">(Shervashidze et al., 2011)</ref> and DGK <ref type="bibr" target="#b57">(Yanardag &amp; Vishwanathan, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heuristic self-supervised methods.</head><p>Heuristic self- supervised methods are designed based on certain domain knowledge, which work well when such knowledge is available and benefits downstream tasks. The compared ones include: (1) edge-based reconstruction including GAE <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2016)</ref>, node2vec <ref type="bibr" target="#b12">(Grover &amp; Leskovec, 2016)</ref> and EdgePred <ref type="bibr" target="#b15">(Hu et al., 2019)</ref>, (2) vertex feature masking &amp; recover, namely AttrMasking <ref type="bibr" target="#b15">(Hu et al., 2019)</ref>, (3) sub-structure information preserving such as sub2vec <ref type="bibr" target="#b1">(Adhikari et al., 2018)</ref>, graph2vec <ref type="bibr" target="#b33">(Narayanan et al., 2017)</ref> and ContextPred <ref type="bibr" target="#b15">(Hu et al., 2019)</ref>, and ( <ref type="formula" target="#formula_7">4</ref>) global-local representation consistency such as Infomax <ref type="bibr" target="#b51">(Veličković et al., 2018)</ref> &amp; InfoGraph <ref type="bibr" target="#b50">(Sun et al., 2019)</ref>. We adopt the default hyper-parameters published for these methods.</p><p>GraphCL with pre-fixed augmentation sampling rules.</p><p>For constructing the sampling pool of augmentations, we follow the same rule as in <ref type="bibr" target="#b59">(You et al., 2020a)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">ON DIVERSE DATASETS FROM TUDATASET</head><p>The results of semi-supervised learning &amp; unsupervised representation learning on TUDataset are in Tables <ref type="table" target="#tab_7">4 &amp; 5</ref>, respectively. Through comparisons between (1) JOAO and GraphCL, (2) JOAOv2 and JOAO, and (3) JOAOv2 and heuristic self-supervised methods, we have the following observations.</p><p>(i) With automated selection, JOAO is comparable to GraphCL with ad hoc rules from exhaustive manual tuning. With the automatic, adaptive, and dynamic augmentation selection procedure, JOAO performs comparably to GraphCL whose augmentations are based on empirical ad-hoc rules gained from expensive trial-and-errors on the same TUDatase. Specifically in semi-supervised learning (Table <ref type="table" target="#tab_4">4</ref>), JOAO matches or beats GraphCL in 7 out of 10 experiments, albeit with a slightly worse average rank. Similar observations were made in unsupervised learning (Table <ref type="table" target="#tab_5">5</ref>). These results echo our earlier results in Section 3.2.1 that automated selections of augmentation pairs made by JOAO were generally consistent with GraphCL's "best practices" observed from manual tuning.</p><p>(ii) Augmentation-aware projection heads provide further improvement upon JOAO. With augmentation-aware projection heads introduced into JOAO, JOAOv2 further improves the performance and sometimes even outperforms GraphCL with the pre-defined rules of thumb for augmentation selections. In semi-supervised learning (Table <ref type="table" target="#tab_4">4</ref>), JOAOv2 achieves the best average ranks of 2.0 and 2.8 under 1% and 10% label rate, respectively, and in unsupervised representation learning ( The results of transfer learning on bioinfomatics datasets are in Table <ref type="table">6</ref>. Similarly, through comparisons among JOAO(v2), GraphCL and heuristic self-supervised methods, we make the following findings.</p><p>(iv) Without domain expertise incorporated, JOAOv2 underperforms some heuristic self-supervised methods in specific domains. Nevertheless, converse to that on diverse datasets, on the specific bioinformatics datasets, JOAOv2 underperforms some heuristic self-supervised methods designed with dataset-specific domain knowledge <ref type="bibr" target="#b15">(Hu et al., 2019)</ref> even though it improves average rank against GraphCL and JOAO. As stated in Section 4.2, the specific domain knowledge encoded in the compared heuristically designed methods correlates with the downstream datasets as shown in <ref type="bibr" target="#b15">(Hu et al., 2019)</ref>, which is not made available to benefit JOAOv2 that works well for a general dataset (as shown in Section 4.3.1), which may not suffice to capture the sophisticated domain expertise. Therefore, to make JOAOv2 even more competitive, domain knowledge can be introduced into the framework, for instance through proposing dataset-specific augmentation types and/or priors. We leave this to future work.</p><p>(v) With better generalizability, JOAOv2 outperforms GraphCL on unseen datasets. Different from results on diverse datasets from TUDataset, both JOAO and JOAOv2 outperform GraphCL with empirically pre-defined rules for augmentation selection on the unseen bioinfomatics datasets. Specifically in Table <ref type="table">6</ref> JOAO improves the average rank by 0.1 and JOAOv2 did by 0.3 compared to GraphCL. Note that the sampling rules of GraphCL were empirically derived from TUDataset hence these rules are not necessarily effective for the previously-unseen bioinfomatics datasets. In contrast, JOAOv2 dynamically and automatically learns the sampling distributions during self-supervised training, possessing the better generalizability. (vi) JOAOv2 scales up well for large datasets. Both JOAO and JOAOv2 scale up for large datasets at least as well as GraphCL does. In Table <ref type="table" target="#tab_8">7</ref>, for the ogbg-ppa dataset, JOAO improved even more significantly compared to GraphCL (by reference to earlier, smaller datasets), with &gt;3.49% and &gt;1.55% accuracy gains at 1% and 10% label rates, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Summary of Main Findings</head><p>We briefly summarize the aforementioned results as follows:</p><p>• Across datasets originated from diverse sources, JOAO with adaptive augmentation selection performs comparably to GraphCL, a strong baseline with exhaustively tuned augmentation rules by hand. • With augmentation-aware projection heads, JOAOv2 further boosts the performance and sometimes even outperforms GraphCL. • On datasets from specific bioinformatics domains, JOAOv2 achieves better performance than GraphCL whose empirical rules were not derived from such data, indicating its better generalizability to unseen datasets. • Both JOAO and JOAOv2 outperform heuristic selfsupervised methods with few exceptions. They might be further enhanced by encoding domain knowledge. • JOAOv2 scales up to large datasets as well as GraphCL does, sometimes with even more significant improvement compared with that for smaller datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions &amp; Discussions</head><p>In this paper, we propose a unified bi-level optimization framework to dynamically and automatically select augmentations in GraphCL, named JOint Augmentation Optimization (JOAO). The general framework is instantiated as min-max optimization, with empirical analysis showing that JOAO makes augmentation selections in general accordance with previous "best practices" from exhaustive hand tuning for every dataset. Furthermore, a new augmentation-aware projection head mechanism is proposed to overcome the potential training distribution distortion, resulting from the more aggressive and varying augmentations by JOAO. Experiments demonstrate that JOAO and its variant performs on par with and sometimes better than the state-of-the-art competitors including GraphCL on multiple graph datasets of various scales and types, yet without resorting to tedious dataset-specific manual tuning.</p><p>Although JOAO automates GraphCL in selecting augmentation pairs, it still relies on human prior knowledge in constructing and configuring the augmentation pool to select from. In this sense "full" automation is still desired and will be pursued in future work. Meanwhile, in parallel to the principled formulation of bi-level optimization, a meta-learning formulation can also be pursued.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Empirical training curves of AGD in JOAO on datasets NCI1 and PROTEINS with different γ values.</figDesc><graphic url="image-1.png" coords="4,55.44,67.06,233.98,90.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top row: sampling distributions (%, defined as the percentage of this specific augmentation pair being selected during the entire training process) for augmentation pairs selected by JOAO on four different datasets (NCI1, PROTEINS, COLLAB, and RDT-B ).Bottom row: GraphCL performance gains (classification accuracy %, see<ref type="bibr" target="#b59">(You et al., 2020a)</ref> for the detailed setting) when exhaustively trying every possible augmentation pair. Note that the percentage numbers in the first and second rows have different meanings and are not apple-to-apple comparable; however, the overall alignments between the two rows' trends and high-value locations indicate that, if an augmentation pair was manually verified to yield better GraphCL results, it is also more likely to be selected by JOAO. Warmer (colder) colors indicate higher (lower) values, and white marks 0.</figDesc><graphic url="image-2.png" coords="5,67.59,67.07,461.62,224.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An overview of GraphCL with multiple augmentationaware projection heads where P (g Θ 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 3</head><label>3</label><figDesc>.3. ON LARGE-SCALE OGB DATASETS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics.</figDesc><table><row><cell>Datasets</cell><cell>Category</cell><cell cols="3">Graph Num. Avg. Node Avg. Degree</cell></row><row><cell>NCI1</cell><cell>Biochemical Molecules</cell><cell>4110</cell><cell>29.87</cell><cell>1.08</cell></row><row><cell cols="2">PROTEINS Biochemical Molecules</cell><cell>1113</cell><cell>39.06</cell><cell>1.86</cell></row><row><cell>COLLAB</cell><cell>Social Networks</cell><cell>5000</cell><cell>74.49</cell><cell>32.99</cell></row><row><cell>RDT-B</cell><cell>Social Networks</cell><cell>2000</cell><cell>429.63</cell><cell>1.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experiments with JOAO and JOAOv2 without explicit hyper-parameter tuning under different augmentation strengths on NCI1 and PROTEINS. A.S. is short for augmentation strength.</figDesc><table><row><cell>Datasets</cell><cell>A.S.</cell><cell>JOAO</cell><cell>JOAOv2</cell></row><row><cell>NCI1</cell><cell cols="3">0.2 61.77±1.61 62.52±1.16 0.25 60.95±0.55 61.67±0.72</cell></row><row><cell>PROTEINS</cell><cell cols="3">0.2 71.45±0.89 71.66±1.10 0.25 71.61±1.65 73.01±1.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of JOAO performance.</figDesc><table><row><cell></cell><cell cols="2">v.s. GraphCL v.s. Heuristic methods</cell></row><row><cell cols="2">Across diverse fields Comparable</cell><cell>Better</cell></row><row><cell>On specific domains</cell><cell>Better</cell><cell>Worse</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Semi-supervised learning on TUDataset. Shown in red are the best accuracy (%) and those within the standard deviation of the best accuracy or the best average ranks. -indicates that label rate is too low for a given dataset size. L.R. and A.R. are short for label rate and average rank, respectively. The compared results except those for ContextPred are as published under the same experiment setting.</figDesc><table><row><cell>L.R.</cell><cell>Methods</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>DD</cell><cell>COLLAB</cell><cell>RDT-B</cell><cell>RDT-M5K</cell><cell>GITHUB</cell><cell>A.R.↓</cell></row><row><cell>1%</cell><cell>No pre-train.</cell><cell>60.72±0.45</cell><cell>-</cell><cell>-</cell><cell>57.46±0.25</cell><cell>-</cell><cell>-</cell><cell>54.25±0.22</cell><cell>7.6</cell></row><row><cell></cell><cell cols="2">Augmentations 60.49±0.46</cell><cell>-</cell><cell>-</cell><cell>58.40±0.97</cell><cell>-</cell><cell>-</cell><cell>56.36±0.42</cell><cell>6.6</cell></row><row><cell></cell><cell>GAE</cell><cell>61.63±0.84</cell><cell>-</cell><cell>-</cell><cell>63.20±0.67</cell><cell>-</cell><cell>-</cell><cell>59.44±0.44</cell><cell>4.0</cell></row><row><cell></cell><cell>Infomax</cell><cell>62.72±0.65</cell><cell>-</cell><cell>-</cell><cell>61.70±0.77</cell><cell>-</cell><cell>-</cell><cell>58.99±0.50</cell><cell>3.3</cell></row><row><cell></cell><cell>ContextPred</cell><cell>61.21±0.77</cell><cell>-</cell><cell>-</cell><cell>57.60±2.07</cell><cell>-</cell><cell>-</cell><cell>56.20±0.49</cell><cell>6.6</cell></row><row><cell></cell><cell>GraphCL</cell><cell>62.55±0.86</cell><cell>-</cell><cell>-</cell><cell>64.57±1.15</cell><cell>-</cell><cell>-</cell><cell>58.56±0.59</cell><cell>2.6</cell></row><row><cell></cell><cell>JOAO</cell><cell>61.97±0.72</cell><cell>-</cell><cell>-</cell><cell>63.71±0.84</cell><cell>-</cell><cell>-</cell><cell>60.35±0.24</cell><cell>3.0</cell></row><row><cell></cell><cell>JOAOv2</cell><cell>62.52±1.16</cell><cell>-</cell><cell>-</cell><cell>64.51±2.21</cell><cell>-</cell><cell>-</cell><cell>61.05±0.31</cell><cell>2.0</cell></row><row><cell>10%</cell><cell>No pre-train.</cell><cell cols="7">73.72±0.24 70.40±1.54 73.56±0.41 73.71±0.27 86.63±0.27 51.33±0.44 60.87±0.17</cell><cell>7.0</cell></row><row><cell></cell><cell cols="8">Augmentations 73.59±0.32 70.29±0.64 74.30±0.81 74.19±0.13 87.74±0.39 52.01±0.20 60.91±0.32</cell><cell>6.2</cell></row><row><cell></cell><cell>GAE</cell><cell cols="7">74.36±0.24 70.51±0.17 74.54±0.68 75.09±0.19 87.69±0.40 53.58±0.13 63.89±0.52</cell><cell>4.5</cell></row><row><cell></cell><cell>Infomax</cell><cell cols="7">74.86±0.26 72.27±0.40 75.78±0.34 73.76±0.29 88.66±0.95 53.61±0.31 65.21±0.88</cell><cell>3.0</cell></row><row><cell></cell><cell>ContextPred</cell><cell cols="7">73.00±0.30 70.23±0.63 74.66±0.51 73.69±0.37 84.76±0.52 51.23±0.84 62.35±0.73</cell><cell>7.2</cell></row><row><cell></cell><cell>GraphCL</cell><cell cols="7">74.63±0.25 74.17±0.34 76.17±1.37 74.23±0.21 89.11±0.19 52.55±0.45 65.81±0.79</cell><cell>2.4</cell></row><row><cell></cell><cell>JOAO</cell><cell cols="7">74.48±0.27 72.13±0.92 75.69±0.67 75.30±0.32 88.14±0.25 52.83±0.54 65.00±0.30</cell><cell>3.5</cell></row><row><cell></cell><cell>JOAOv2</cell><cell cols="7">74.86±0.39 73.31±0.48 75.81±0.73 75.53±0.18 88.79±0.65 52.71±0.28 66.66±0.60</cell><cell>1.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Unsupervised representation learning on TUDataset. Red numbers indicate the top-3 accuracy (%) or the top-2 average ranks. The compared results are from the published papers, and -indicates that results were not available in published papers. For MVGRL we report the numbers with the NT-Xent loss to be comparable with GraphCL.</figDesc><table><row><cell>Methods</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>DD</cell><cell>MUTAG</cell><cell>COLLAB</cell><cell>RDT-B</cell><cell>RDT-M5K</cell><cell>IMDB-B</cell><cell>A.R.↓</cell></row><row><cell>GL</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.66±2.11</cell><cell>-</cell><cell cols="3">77.34±0.18 41.01±0.17 65.87±0.98</cell><cell>7.4</cell></row><row><cell>WL</cell><cell cols="2">80.01±0.50 72.92±0.56</cell><cell>-</cell><cell>80.72±3.00</cell><cell>-</cell><cell cols="3">68.82±0.41 46.06±0.21 72.30±3.44</cell><cell>5.7</cell></row><row><cell>DGK</cell><cell cols="2">80.31±0.46 73.30±0.82</cell><cell>-</cell><cell>87.44±2.72</cell><cell>-</cell><cell cols="3">78.04±0.39 41.27±0.18 66.96±0.56</cell><cell>4.9</cell></row><row><cell cols="3">node2vec 54.89±1.61 57.49±3.57</cell><cell>-</cell><cell>72.63±10.20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.6</cell></row><row><cell>sub2vec</cell><cell cols="2">52.84±1.47 53.03±5.55</cell><cell>-</cell><cell>61.05±15.80</cell><cell>-</cell><cell cols="3">71.48±0.41 36.68±0.42 55.26±1.54</cell><cell>9.5</cell></row><row><cell cols="3">graph2vec 73.22±1.81 73.30±2.05</cell><cell>-</cell><cell>83.15±9.25</cell><cell>-</cell><cell cols="3">75.78±1.03 47.86±0.26 71.10±0.54</cell><cell>5.7</cell></row><row><cell>MVGRL</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.40±7.80</cell><cell>-</cell><cell>82.00±1.10</cell><cell>-</cell><cell>63.60±4.20</cell><cell>7.2</cell></row><row><cell cols="9">InfoGraph 76.20±1.06 74.44±0.31 72.85±1.78 89.01±1.13 70.65±1.13 82.50±1.42 53.46±1.03 73.03±0.87</cell><cell>3.0</cell></row><row><cell cols="9">GraphCL 77.87±0.41 74.39±0.45 78.62±0.40 86.80±1.34 71.36±1.15 89.53±0.84 55.99±0.28 71.14±0.44</cell><cell>2.6</cell></row><row><cell>JOAO</cell><cell cols="8">78.07±0.47 74.55±0.41 77.32±0.54 87.35±1.02 69.50±0.36 85.29±1.35 55.74±0.63 70.21±3.08</cell><cell>3.3</cell></row><row><cell>JOAOv2</cell><cell cols="8">78.36±0.53 74.07±1.10 77.40±1.15 87.67±0.79 69.33±0.34 86.42±1.45 56.03±0.27 70.83±0.25</cell><cell>2.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>) its average rank (2.8) is</cell></row><row><cell>only edged by GraphCL (2.6). The performance acquired by</cell></row><row><cell>JOAOv2 echoes our conjecture in sec. 3.3 that such explicit</cell></row><row><cell>disentanglement of the distorted feature distributions caused</cell></row><row><cell>by various augmentation pairs would reel in the benefits</cell></row><row><cell>from stronger augmentations.</cell></row><row><cell>(iii) Across diverse datasets, JOAOv2 generally outper-</cell></row><row><cell>form heuristic self-supervised methods. On datasets orig-</cell></row><row><cell>inated from diverse sources, JOAOv2 generally outperforms</cell></row><row><cell>heuristic self-supervised methods. Specifically in Table 4,</cell></row><row><cell>JOAOv2 achieves no less than 0.3 average ranking gap with</cell></row><row><cell>all heuristic self-supervised methods under 1% label rate,</cell></row><row><cell>and 1.0 with all but Infomax under 10% label rate, which</cell></row><row><cell>outperforms JOAO but still underperforms JOAOv2, and in</cell></row><row><cell>Table 5 only InfoGraph outperforms JOAO but underper-</cell></row><row><cell>forms JOAOv2 where there is no less that 1.5 average rank-</cell></row><row><cell>ing gap between others and JOAOv2. This in general meets</cell></row><row><cell>our expectation that heuristic self-supervised methods can</cell></row><row><cell>work well when guided by useful domain knowledge, which</cell></row><row><cell>is hard to guarantee across diverse datasets. In contrast</cell></row><row><cell>JOAOv2 can dynamically and automatically adapt augmen-</cell></row><row><cell>tation selections during self-supervised training, exploiting</cell></row><row><cell>the signals (knowledge) from data.</cell></row><row><cell>4.3.2. ON SPECIFIC BIOINFORMATICS DATASETS.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Semi-supervised learning on large-scale OGB datasets.Red numbers indicate the top-2 performances (accuracy in % on ogbg-ppa, F1 score in % on ogbg-code).</figDesc><table><row><cell>L.R.</cell><cell>Methods</cell><cell>ogbg-ppa</cell><cell>ogbg-code</cell></row><row><cell cols="4">1% No pre-train. 16.04±0.74 6.06±0.01</cell></row><row><cell></cell><cell>GraphCL</cell><cell cols="2">40.81±1.33 7.66±0.25</cell></row><row><cell></cell><cell>JOAO</cell><cell cols="2">47.19±1.30 6.84±0.31</cell></row><row><cell></cell><cell>JOAOv2</cell><cell cols="2">44.30±1.67 7.74±0.24</cell></row><row><cell cols="4">10% No pre-train. 56.01±1.05 17.85±0.60</cell></row><row><cell></cell><cell>GraphCL</cell><cell cols="2">57.77±1.25 22.45±0.17</cell></row><row><cell></cell><cell>JOAO</cell><cell cols="2">60.91±0.83 22.06±0.30</cell></row><row><cell></cell><cell>JOAOv2</cell><cell cols="2">59.32±1.11 22.65±0.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the sans-serif typeface to denote a random variable (e.g. G). The same letter in the italic font (e.g. G) denotes a sample, and the calligraphic font (e.g. G) denotes the sample space.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Red numbers indicate the top-3 performances (AUC of ROC in %)</title>
	</analytic>
	<monogr>
		<title level="m">Results for SOTA methods are as published. Methods BBBP Tox21 ToxCast SIDER ClinTox MUV HIV BACE PPI A</title>
				<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>Transfer learning on bioinformatics datasets. R.↓ No pre-train</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feature learning for subgraphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>References Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><surname>Sub2vec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="170" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Online learning rate adaptation with hypergradient descent</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cornish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04782</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">COAD: Contrastive pretraining with adversarial fine-tuning for zero-shot expert linking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11336</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distance-wise graph contrastive learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07437</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Are powerful graph neural nets necessary? a dissection on graph classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04579</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020c</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Forward and reverse gradient-based hyperparameter optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1165" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On differentiating parameterized argmin and argmax problems with application to bi-level optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05447</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The principle of maximum entropy. The mathematical intelligencer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guiasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shenitzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05582</idno>
	</analytic>
	<monogr>
		<title level="m">Contrastive multiview representation learning on graphs</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1985">1985. 2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Gpt-Gnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hop-count based self-supervised anomaly detection on attributed networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Menkovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07917</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised auxiliary learning with metapaths for heterogeneous graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-scale contrastive siamese networks for selfsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05682</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-supervised learning on graphs: Deep insights and new direction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automated self-supervised learning for graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5006" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><surname>Flag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<title level="m">Adversarial data augmentation for graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised label augmentation via input transformations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<biblScope unit="page" from="5714" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04883</idno>
		<title level="m">Representation learning for networks in biology and medicine: Advancements, challenges, and opportunities</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable gradient-based tuning of continuous regularization hyperparameters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2952" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradientbased hyperparameter optimization through reversible learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2113" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph-based neural network models with multiple self-supervised auxiliary tasks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Manessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07267</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<title level="m">Learning distributed representations of graphs</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Columbia object image library (coil-100)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised attributed multiplex network embedding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5371" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization with approximate gradient</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Self-supervised graph representation learning via global context prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01604</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020b</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Gcc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08538</idno>
		<title level="m">Heterogeneous deep graph infomax</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Iam graph database repository for graph based pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
				<imprint>
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Model-based robust deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Robey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Pappas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10247</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on largescale molecular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Node embedding using mutual information and selfsupervision based bi-level aggregation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13014</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">An API oriented open-source python framework for unsupervised learning on graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04819</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Truncated back-propagation for bilevel optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009">2009. 2011</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
	<note>Weisfeiler-Lehman graph kernels</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gaussian process optimization in the bandit setting: No regret and experimental design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2012. 2010. 2010</date>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Info-Graph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning graph representation by aggregating subgraphs via mutual information maximization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13125</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Towards a unified min-max framework for adversarial exploration and robustness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fardad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03563</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Selfsupervised on graphs: Contrastive, generative</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07342</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">or predictive. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Cross-modality protein embedding for compound-protein affinity and contact prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00651</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">When does selfsupervision help graph convolutional networks?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="10871" to="10880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">L 2 -GCN: Layerwise and learned efficient training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020c</date>
			<biblScope unit="page" from="2127" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Iterative graph self-distillation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12609</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06830</idno>
		<title level="m">Data augmentation for graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Self-supervised training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02380</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14945</idno>
		<imprint>
			<date type="published" when="2020">2020c</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
