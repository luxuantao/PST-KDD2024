<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperspectral image reconstruction by deep convolutional neural network for classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yunsong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Integrated Service Network</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Joint Laboratory of High Speed Multi-source Image Coding and Processing</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiying</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Integrated Service Network</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Joint Laboratory of High Speed Multi-source Image Coding and Processing</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Hyperspectral</roleName><forename type="first">Huaqing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Integrated Service Network</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Joint Laboratory of High Speed Multi-source Image Coding and Processing</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hyperspectral image reconstruction by deep convolutional neural network for classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ABFFA22685E36B34A9121B0152E0E092</idno>
					<idno type="DOI">10.1016/j.patcog.2016.10.019</idno>
					<note type="submission">Received date: 22 May 2016 Revised date: 22 August 2016 Accepted date: 15 October 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition hyperspectral imagery</term>
					<term>deep convolutional neural network</term>
					<term>extreme learning machine</term>
					<term>reconstruction</term>
					<term>band selection</term>
					<term>pattern classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial features of hyperspectral imagery (HSI) have gained an increasing attention in the latest years. Considering deep convolutional neural network (CNN) can extract a hierarchy of increasingly spatial features, this paper proposes an HSI reconstruction model based on deep CNN to enhance spatial features. The framework proposes a new spatial features-based strategy for band selection to define training label with rich information for the first time. Then, hyperspectral data is trained by deep CNN to build a model with optimized parameters which is suitable for HSI reconstruction. Finally, the reconstructed image is classified by the efficient extreme learning machine (ELM) with a very simple structure. Experimental results indicate that framework built based on CNN and ELM provides competitive performance with small number of training</p><p>samples. Specifically, by using the reconstructed image, the average accuracy of ELM can be improved as high as 30.04%, while performs tens to hundreds of times faster than those state-of-the-art classifiers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Remote sensing imaging instruments are capable of collecting continues data including spatial and spectral information, simultaneously <ref type="bibr" target="#b0">[1]</ref>. Thus, hyperspectral imagery (HSI) is a three-dimensional data cube including two spatial dimensions with space information of pixels and one spectral dimension with high-dimensional reflectance vectors. Generally, HSI has high spectral resolution, which can natively distinguish many different materials. However, some spectrums of different classes may be similar to others and the spatial features are often omitted in traditional way, which are not sufficiently for class classification <ref type="bibr" target="#b1">[2]</ref>. Nowadays, spatial features have been growing more and more important for remote sensing image analysis <ref type="bibr" target="#b2">[3]</ref> so that it has become a significant topic to enhance the spatial features.</p><p>To achieve the aforementioned requirement for effective and accurate HSI classification, several algorithms have been widely used. Kang et al. <ref type="bibr" target="#b3">[4]</ref> applied the transform domain recursive filter for filtering the pixel-wise classification map obtained by SVM of each class, which considered the neighborhood information of HSI. In <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref>, HSI segmentation algorithms were proposed to incorporate the spatial information (the relationship between neighboring pixels) into the classification process, which aims at partition an image into different regions. However, these algorithms assumed that pixels within a local region should share similar spectral characteristics, which ignored the fact that each pixel in HSI with high spectral resolution may represent different materials of interest, resulting in spectral distortion.</p><p>Another category of considering spatial information approach is based upon the injection of details from the panchromatic <ref type="bibr" target="#b6">[7]</ref>, multispectral <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, or other higher spatial resolution images <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> into the HSI. In this situation, detail injection-based methods require these images to be acquired over the same scene and under the same conditions for being fused with the corresponding HSI. From the application point of view, this is a difficult problem. In addition, these methods may produce serious spatial and spectral distortion.</p><p>Commonly, to well represent a pixel in HSI, an integration of spatial features with spectral features for HSI classification is necessary. Rellier et al. <ref type="bibr" target="#b16">[17]</ref> proposed a texture feature analysis using a Gauss-Markov random field model for HSI classification. In <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>, a patch alignment framework of combining multiple features (e.g. spectral, texture, shape, etc.) was proposed for the subsequent classification. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> also considered multiple types of features for HSI classification using the sparse representation methods. All of these methods have given good performance in terms of classification accuracies, which indicate a combination of spectral and spatial features would have positive influence on classification. However, the combination of multiple features increases the number of training samples leading to computational cost.</p><p>According to the important role of spatial features in HSI analysis, our approach is motivated by the current successful super-resolution model-SRCNN <ref type="bibr" target="#b20">[21]</ref> using deep convolutional neural network (CNN). The SRCNN model is a deep CNN model for single image super-resolution that directly learned an end-to-end mapping between low-and high-resolution images. The mapping is represented as a deep CNN that takes the low-resolution image as the input and outputs the high-resolution one. Inspired by this fact, we propose a deeper CNN that directly learns an end-to-end mapping between HSI and the label with rich spatial features for the first time. Considering the difficulty of obtaining the label image acquired over the same scene under the same conditions by different sensors, we propose a new spatial features-based strategy for selecting a band with spatial information enhancement by a deeper CNN model that has not been previously presented in the available literature. This idea is validated in our previous work <ref type="bibr" target="#b21">[22]</ref> that a selected band as a guided image was used to estimate the spectral background and foreground by matting model <ref type="bibr" target="#b22">[23]</ref>. We refer to this approach as HSI reconstruction with deep convolutional neural network (HRCNN), which does not depend on the corresponding multispectral or panchromatic image and is easier for application. The experimental results show that our HRCNN model is good at enhancing spatial features and remaining spectral features, which is useful for the subsequent classification. We present several types of image quality evaluation of both subjective and objective indices supporting this claim. Furthermore, in order to achieve low computational cost and good generalization performance, extreme learning machine (ELM) is utilized to classify the reconstructed HSI under a small training set in this paper.</p><p>In summary, the main contributions of this paper are as follows: 1) The feature enhancement step is introduced in the proposed HRCNN model because that the HSI data is contaminated with noise, which is not proposed in the other schemes; 2) Since the first principle component (PC) represented as training label may bring about spectral distortion, a new band selection method based on spatial features is taken into consideration in this training model for the first time which aims at not only enhancing spatial features, but also remaining spectral information of HSI data; 3) A new exploration to combine CNN with ELM for HSI classification under a small number of training samples is achieved with good generalization and low computational cost for the first time. Specifically, HRCNN model is first employed to enhance the spatial features without spectral distortion.</p><p>In addition, a spatial features-based band selection is first proposed in this paper to find a band with distinctive and informative spatial information as a training label which is not clearly demonstrated in the state-of-the-art schemes.</p><p>The following part of this paper is divided into six sections. Section II reviews the related work about CNN and ELM used in HSI applications. In section III, we describe the flowchart of the proposed methodology.</p><p>Section IV is devoted to experimental results. In Section V, the reconstructed HSI is extended and applied to classification. In the last section, a conclusion is made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Deep learning models can learn a hierarchy of increasingly complex features, which have been widely used in the field of HSI applications. Zabala et al. segmented the spectral domain of HSI into different regions, to which the stacked autoencoder (SAE) is applied individually <ref type="bibr" target="#b23">[24]</ref>. Ma et al. introduced SAE to integrate spectral-spatial features <ref type="bibr" target="#b24">[25]</ref>. Chen et al. utilized the SAE to classify HSI data by choosing 60% of the tagged samples as training set <ref type="bibr" target="#b25">[26]</ref>. Chen et al also introduced deep belief network (DBN) to extract spectral-spatial features for HSI classification by choosing 50% of the tagged samples as training set <ref type="bibr" target="#b26">[27]</ref>. However, the requirement that training samples should be flattened to one-dimension to satisfy the input of the SAE and DBN neglected the spatial information of HSI data. In addition, there are too many parameters in hidden layers of SAE and DBN, while CNN has the advantage of local connections and shared weights that can reduce computational cost. As far as we know, several studies about CNN in the field of HSI applications have tended to concentrate on extract spatial/structural features for improving classification accuracies: Zhao et al. <ref type="bibr" target="#b27">[28]</ref> explored multiscale CNN to transform the original HSI into a pyramid structure containing spatial features for HSI classification with final result in the form of major voting. Zhao and Du exploited CNN to extract spatial features and the logistic regression (LR) classifier was used <ref type="bibr" target="#b28">[29]</ref>, which conducted experiments only on two hyperspectral images (Pavia center and University of Pavia) collected by the same sensors which cannot illustrate generalization. Romero et al. introduced CNN to learn hierarchical features of remote sensing images including multi-and hyperspectral images <ref type="bibr" target="#b29">[30]</ref>. The flaws in this work are complex network structure and relatively poor performance. Makantasis et al. exploited CNN to encode spectral and spatial features and a Multi-Layer Perceptron (MLP) to classify HSI data <ref type="bibr" target="#b30">[31]</ref> by choosing 80% samples as training samples that is unpractical. Tuia et al. substituted the convolution operators by spatial filters with known properties to extract deep features of HSI data <ref type="bibr" target="#b31">[32]</ref> with lower performance. Thus, challenges still remain in achieving robust performance, good generalization and high speed under a small training set of HSI data by using CNN and efficient classifier. In this paper, CNN is mainly equipped with convolutional layers. It learns the feature representation from raw pixels, and can be trained in an end-to-end manner by the back propagation algorithm <ref type="bibr" target="#b32">[33]</ref>, which can be viewed as a transformation from the input map to the output map. In addition, the issues of defining training label to extract effective spatial feature are not obviously demonstrated in the state-of-the-art works. More details are discussed in Sec. 3.</p><p>Although the main idea behind this paper is to introduce HSI reconstruction technique based on deep CNN, it is of interest to see the effectiveness of the novel reconstructed data on classification. Owing to its simple principle, low computational cost, good generalization performance and little human intervention <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>, ELM is applied to final classification unlike LR, SVM and CR based classifiers used in <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. In the field of HSI classification, ELM and its extensions have been recently commanded increasing attention <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. Pal et al.</p><p>[39] employed kernel-based ELM for land cover classification in remote sensing images. Moreno et al. <ref type="bibr" target="#b39">[40]</ref> successfully performed ELM on soybean classification in HSI. In <ref type="bibr" target="#b40">[41]</ref>, Bazi et al. utilized the optimized ELM by differential evolution to support HSI classification. Ensemble ELMs were introduced in <ref type="bibr" target="#b41">[42]</ref> for HSI classification. In <ref type="bibr" target="#b42">[43]</ref>, ELM was employed to classify HSI according to multiple features. Our previous work <ref type="bibr" target="#b43">[44]</ref>, an optimized ELM was presented for HSI classification. All of the aforementioned studies have verified that ELM-based classifiers could provide comparable performance to SVM both in classification accuracies and computational time. ELM randomly generates the connected weights between input nodes and hidden nodes.</p><p>Therefore, only the linearly connected weights in the output layer should be tuned in ELM, which makes the learning of ELM simple and efficient. This leads to the advantages of extremely fast learning speed and good generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed framework</head><p>The mainly four parts of the proposed framework are as follows: normalization, band selection, reconstruction and classification. The following sections present a brief description of these procedures.  Kang et al. adopted the first PC as the guidance image in order to filter the classification map of each class in HSI <ref type="bibr" target="#b3">[4]</ref>. The principle component analysis (PCA) method can ensure that most information is preserved in a small amount of significant PCs, but it cannot ensure that the spectral signatures of interest are emphasized. In addition, it has been proved that using selected bands may offer a slightly better performance than using PCs in <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b45">[46]</ref>. Furthermore, we valid that the first PC considered as guidance image may result spectral distortion with detailed description in Sec. 4. However, there is no denying that PC image contains rich spatial information, and several algorithms extracted spatial features in PC image <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> for HSI classification. Hence, we make emphasis on selecting a band with the first PC image as reference. Due to their wide applicability, six gray level co-occurrence matrix (GLCM) measures, including angular second moment (ASM), contrast, entropy, variance, correlation, dissimilarity, have been adopted to extract spatial features. These features are defined and listed in Table <ref type="table">1</ref>. Each feature of every band is compared to the specific feature of the first PC. We use the following ratio to select band:</p><formula xml:id="formula_0">6 1 , 1, 2,3, , ii i l FF l i i F PC Band k l L PC      (2)</formula><p>where Band l represents the lth band of a hyperspectral data, and F i is the ith feature demonstrated in Table <ref type="table">1</ref>.</p><p>Here, PC represents the first PC. The band with the minimum value of  l is selected as the training label. In other words, spatial features of the selected band and that of the first PC are most similar than that of other bands.</p><p>Table <ref type="table">1</ref> Parameters for texture features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Description Feature</head><p>Description</p><formula xml:id="formula_1">F 1 ASM F 4 variance F 2 contrast F 5 correlation F 3 entropy F 6 dissimilarity</formula><p>Under training with this label, spatial features are enhanced while spectral information are remained which   </p><formula xml:id="formula_2">f f  2 2 f f  3 3 f f  4 4 f f </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Patch extraction and representation</head><p>This process aims at extracting overlapping patches from the original hyperspectral data and representing each patch as a high-dimensional vector, which determines what should be emphasized and restored in the following stages. This layer is expressed as an operation h 1 , which is a set of feature maps with the same size.</p><p>The input image Y is firstly resized to a fixed height (38 pixels throughout our experiments), keeping their aspect ratios. Each feature map is extracted by performing convolution with convolutional filters (W 1 ) and biases (B 1 ):</p><formula xml:id="formula_3">  1 1 1 1 L l l h W B          YY (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where L is the number of spectral bands and Y l is the lth band of the input hyperspectral data. The star operator  indicates the 2-D convolution operation. Here, we use thresholding function max(0,x) as the element-wise non-linearity σ(•), also known as the ReLU <ref type="bibr" target="#b46">[47]</ref>. A positive response indicates the presence of certain discriminative patterns, and is kept, while negative responses are suppressed by setting them to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Feature enhancement</head><p>The first layer extracts an n 1 -dimensional feature for each patch. However, the HSI is contaminated with some noise, which not only decreases the accuracy of the subsequent processing, but also influences visual effect. Hence, the extracted features in the first layer may be noisy features which will be enhanced by the second convolutional layer and combined to form another set of feature maps according to feature enhancement step in <ref type="bibr" target="#b47">[48]</ref>. This operation h 2 is used learned filters to convolve the feature maps with convolutional filters (W 2 )</p><p>and biases (B 2 ) from the preceding layer formulated as:</p><formula xml:id="formula_5">    2 2 1 2 1 L l l h W h B          YY (4)</formula><p>where h 1 (Y l ) represents the feature maps extracted in the first convolutional layer. The first two convolutional layers aim at extracting features with rich information, noise reduction and contrast enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Non-linear mapping</head><p>The remaining levels of the feature are extracted recursively by performing convolution with convolutional filters (W 3 ) and biases (B 3 ) on the feature maps from the preceding hierarchy layer:</p><formula xml:id="formula_6">    3 3 2 3 1 L l l h W h B          YY (5)</formula><p>where h 2 (Y l ) represents the feature maps extracted by the second convolutional layer of the lth band. Inspired by the non-linear mapping step in <ref type="bibr" target="#b20">[21]</ref>, we map each of these n 2 -dimensional vectors into an n 3 -dimensional one in this operation. Thus, we use filters with size of 1×1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Reconstruction</head><p>The last convolutional layer aims at generating the final reconstructed hyperspectral data with spatial feature enhancement:</p><formula xml:id="formula_7">    4 4 3 4 1 L l l h W h B      YY<label>(6)</label></formula><p>where h 3 (Y l ) represents the feature maps extracted by the third convolutional layer of the lth band. To obtain valid reconstruction, we pass the reconstructed HSI through a relu non-linearity, which rectify the feature maps.</p><p>Overall, HRCNN model consists of four layers, which is a deeper convolutional neural network compared with SRCNN <ref type="bibr" target="#b20">[21]</ref> aiming at feature enhancement for specific HSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Model learning</head><p>The aforementioned band selected by spatial features-based strategy is adopted as the training label. All the other bands are trained with this ideal output. In particular, the parameters, weight and bias matrices, are updated as:</p><formula xml:id="formula_8">  1 1 1 0.9 , i i i i i i E                  (7)</formula><p>where  </p><p>1, 2, 3, 4  represents the indices of layers, and i is the iteration, η is the learning rate. The parameters, weight and bias matrices are trained by using the gradient descent method with the standard back propagation, which are realized through minimizing a loss function between the reconstructed hyperspectral data and the original one, and computing the partial derivative of the loss function with respect to each trainable parameter (weight or bias). The loss function used in this work is defined as:</p><formula xml:id="formula_9">    2 1 1 ; L l l E h X L       Y (8)</formula><p>where X is the selected band with distinctive and informative spatial features and L is the number of band. The learning procedure of HRCNN is summarized in Algorithm 1. Since the architecture and all corresponding trainable parameters are specified, we can build the CNN model and reload the saved parameters for HSI reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification</head><p>Knowing a training set {(x i , t i )|x i ∈R n , t i ∈R m , i=1,…,N }, which is randomly chosen from the reconstructed HSI, the procedure of ELM can be summarized as:</p><p>Step 1 Randomly assign hidden node parameters (a i , b i ), i=1, … , N. N is the number of training set.</p><p>Step 2 Calculate the hidden layer output matrix H. The number of hidden nodes is represented by Q, and</p><formula xml:id="formula_10">g(x) is activation function.         1 1 1 1 1 N 1 QQ Q N Q NQ g b g b g b g b                 a x a x H a x a x<label>(9)</label></formula><p>Step 3 Calculate the output weights †   HT <ref type="bibr" target="#b9">(10)</ref> where † H is Moore-Penrose generalized inverse of the matrix H.</p><p>Finally, the classification result can be predicted by ELM according to the following function: In this way, to evaluate the effectiveness of the proposed algorithm, support vector machine (SVM) <ref type="bibr" target="#b48">[49]</ref> and sparse representation <ref type="bibr" target="#b49">[50]</ref> are used as comparison algorithms for the reconstructed HSI classification. Here, the Orthogonal Matching Pursuit (OMP), Simultaneous Orthogonal Matching Pursuit (SOMP) and First-order neighborhood system weighted constraint OMP (FOMP) algorithms are used to solve the sparse optimization problem. </p><formula xml:id="formula_11">      † x h x h x i i i f   HT<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>The weight matrices of each convolutional layer are initialized by drawing randomly from a Gaussian distribution with zero mean and standard deviation 0.001. The biases are initialized to 0. It is empirically found that a momentum term of 0.9 is important for the network to converge. Stochastic gradient descent with a mini-batch size of 128 and the learning rate of 10 -4 in the first three layers and 10 -5 in the last layer were used to update the weight and bias matrices. The settings of HRCNN are</p><formula xml:id="formula_12">f 1 =9, f 2 =7, f 3 =1, f 4 =5, n 1 =64, n 2 =32, n 3 =16, n 4 =1.</formula><p>The training pairs {Y, X} are prepared as 38×38 pixel sub-images. Hyperspectral data of arbitrary sizes can be tested. All the convolutional layers are given sufficient zero-padding during test, and pooling or full connected layers are not adopted, so that the output band is of the same size as the input. The sub-images are extracted with a stride of 10. Some smaller strides are attempted but resulted in time consuming without performance improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mid-level representation</head><p>Fig. <ref type="figure">5</ref> shows some example of leaned filters trained on the hyperspectral data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quality assessment</head><p>In this section, the proposed algorithm is evaluated by the objective indexes through making a comparison between the reconstructed HSI and the original one. Spectral angel mapper (SAM), spectral information divergence (SID) and root-mean-squared error (RMSE) are adopted to evaluate spectral information. Tables <ref type="table" target="#tab_0">2</ref><ref type="table" target="#tab_1">3</ref><ref type="table" target="#tab_2">4</ref>show the quality assessment of the reconstructed Indian Pines, Salinas and Center of Pavia image, respectively.</p><p>Compared to the reconstruction procedure based on the first PC as the training label, the selected band compares favorably for several quantitative indexes. The values of SAM, SID, and RMSE obtained based on the selected band are more closed to the reference value 0, which means the proposed reconstruction method gives better performance in terms of objective quality indexes.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Classification</head><p>For ELM classifier, we make focus on both classification accuracies and computational time.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 10 illustrates the classification maps obtained by the aforementioned classifiers of the original Indian</head><p>Pines image and of the new reconstructed one, respectively. The first two figures of Fig. <ref type="figure" target="#fig_15">10</ref> show the map of the train samples and the map of the test samples. From the results of each individual classifier, the classification performance of the reconstructed data is much better than that of the original data. This means that the reconstructed HSI can preserve distinctive and refined information for accurate classification. informative data for good classification. In addition, the FOMP classifier, proposed by our previous study <ref type="bibr" target="#b39">[40]</ref>,</p><p>can achieve the highest classification accuracy when it classifies the reconstructed Indian Pines image.      We also compare the proposed scheme with the recent works of deep learning on HSI applications. Though different numbers of training samples are employed in different methods, the reported performance is still an important standard to estimate the development and effectiveness of the proposed scheme. As shown in Table <ref type="table" target="#tab_6">8</ref>, Indian Pines and Center of Pavia are the most used. Among all these methods, our classification scheme could obtain comparable performance (OA, AA and Kappa) under a small training set with less computational complexity. More details are shown in the following part.  Indian Pines CNN, LR  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, a novel approach for HSI reconstruction, with a deep CNN model, is introduced for the subsequent classification by ELM under a small training set for the first time. Through experiment on three hyperspectral data sets, we found that the selected band contains more distinctive and informative spatial information and is referred to as training label. By training bands with this ideal output using a deeper CNN, we obtain and save the model including optimized parameters (weights and biases). Then, this model is loaded and used to reconstruct HSI. In addition, the results of HSI reconstruction is used as the input for various classifiers to be considered as a pre-processing method for the subsequent classification. Results confirm that the effectiveness of the reconstructed HSI dramatically increases the OA, AA and Kappa of the widely used SVM</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 . 2 Fig. 1 .</head><label>321</label><figDesc>Figs.1 and 2, the spectral magnitude values and tendency of these classes in different images obtained by</figDesc><graphic coords="8,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Spectrum of an AVIRIS image over Indian Pines: (a) Grass/Pasture; (b) Woods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The histogram of (a) the original 1 st band, (b) the original 25 th band, (c) the reconstructed 1 st band, (d) the reconstructed 25 th band.</figDesc><graphic coords="10,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. 3 HRCNN</head><label>3</label><figDesc>The flowchart of the proposed HRCNN containing four convolutional layers is illustrated in Fig.4. The mainly four parts of the proposed HSI reconstruction algorithm are as follows: patch extraction and representation, feature enhancement, non-linear mapping and reconstruction. The following sections present a brief description of these procedures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The framework of HRCNN. The network consists of four convolutional layers, each of which is responsible for a specific operation (i.e. patch extraction and representation, feature enhancement, non-linear mapping and reconstruction).</figDesc><graphic coords="11,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 .</head><label>1</label><figDesc>Number of hidden layer is what we call the important parameter, and we obtain a better number of hidden layer by doing experiments. Training the HRCNN model Initialize learning rate, number of max iteration, batch size of training, kernel size, number of kernels, type of kernel and so on. Generate random weights with guassian type and biases with 0; cnnModel=InitCNNModel(weight and bias matrices, [n 1-4 ]); while iter&lt; max iteration or err &gt; min error do compute err according to loss function Eq. 12 for iter=1 to iter&lt;=number/(batch size) do cnnModel.train(TrainingData,TrainingLabels), as loss is minimilized with BP; update weight and bias matrices with Eq. 11;Save parameters (weight, bias) of the CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 1</head><label>1</label><figDesc>DatasetThree hyperspectral data sets are used to evaluate the performance of the reconstructed method in our experiments. The first data in our experiments is about the Indian Pines image1 , which was captured by Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) in 1992 over the Indian Pines test site in Northwestern Indiana. The spatial resolution of this image is 20m per pixel. The test image consists of 145×145 pixels, each pixel have 220 bands with the 20 noisy bands (no. 104-108, 150-163, and 220) removed. The second image utilized in our experiments is the Salinas image 2 , which was also collected by AVIRIS sensor over Salinas, Valley, California. The spatial resolution is 3.7 m per pixel. The area comprises 512×217 pixels and has 224 bands with the 20 bands (no. 108-112,154-167, and 224) removed. The last image is the Center of Pavia acquired by the Reflective Optics System Imaging Spectrometer (ROSIS) 3 . This image comprises 1096×492 pixels and has 115 bands ranging from 0.43 to 0.86 μm with a spatial resolution of 1.3 m per pixel. The 13 noisy bands are removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6</head><label>6</label><figDesc>Fig.5shows some example of leaned filters trained on the hyperspectral data. Fig.6shows example</figDesc><graphic coords="16,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. First layer filters learned on HSI.</figDesc><graphic coords="17,85.40,169.95,424.00,100.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7</head><label>7</label><figDesc>Fig.7shows spectrums of some pixels in Indian Pines image. It can be observed from Fig.7that the</figDesc><graphic coords="18,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Spectrums of some pixels in original and reconstructed HSI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figs. 8</head><label>8</label><figDesc>Figs.8 and 9show the reconstructed spectrums of five pixels in Indian Pines and Center of Pavia image,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Reconstructed spectrum of an AVIRIS image over Indian Pines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>The first data is Indian Pines image containing 16 classes and we randomly choose about 10% of the labeled sample training and use the remaining samples for testing as many literatures also used. For Salinas image, the training sets which account for about 10% of the ground truth were chosen randomly. For the Center of Pavia image, the training sets account for about 5% are chosen randomly. In the following section, the CNN based reconstructed data classified by SVM, OMP, SOMP, FOMP and ELM are represented as R-SVM, R-OMP, R-SOMP and R-FOMP, R-ELM, respectively. For SVM classifier, parameters are determined by fivefold cross validation.Classification performance is evaluated by overall accuracy (OA), average accuracy (AA) and kappa coefficient (Kappa).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Reconstructed spectrum of a ROSIS image over the Center of Pavia.</figDesc><graphic coords="20,89.90,527.69,415.00,164.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. For the Indian Pines image: (a) training set and (b) test set. Classification results obtained by (c) SVM, (d) R-SVM, (e) OMP, (f) R-OMP, (g) SOMP, (h) R-SOMP, (i) FOMP, (j) R-FOMP, (k) ELM, (l) R-ELM.</figDesc><graphic coords="20,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. For the Salinas image: (a) training set and (b) test set. Classification results obtained by (c) SVM, (d) R-SVM, (e) OMP, (f) R-OMP, (g) SOMP, (h) R-SOMP, (i) FOMP, (j) R-FOMP, (k) ELM, (l) R-ELM.</figDesc><graphic coords="22,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. For the Center of Pavia image: (a) training set and (b) test set. Classification results obtained by (c) SVM, (d) R-SVM, (e) OMP, (f) R-OMP, (g) SOMP, (h) R-SOMP, (i) FOMP, (j) R-FOMP, (k) ELM, (l) R-ELM.</figDesc><graphic coords="23,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figs. 11 ,</head><label>11</label><figDesc>Figs. 11,12 demonstrate the classification maps obtained by different methods of the Salinas and Center of</figDesc><graphic coords="24,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Effect of different numbers of training samples for: (a) Indian Pines; (b) Salinas; (c) Center of Pavia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>al.<ref type="bibr" target="#b31">[32]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Spectral quality assessment for the reconstructed Indian Pines image.</figDesc><table><row><cell>Method</cell><cell>SAM</cell><cell>SID</cell><cell>RMSE</cell></row><row><cell>Reference values</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Reconstructed with the selected band</cell><cell>1.6083</cell><cell cols="2">0.0045 0.0263</cell></row></table><note><p>Reconstructed with the 1 st PC 10.9380 0.1832 0.2261</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Spectral quality assessment for the reconstructed Salinas image.</figDesc><table><row><cell>Method</cell><cell>SAM</cell><cell>SID</cell><cell>RMSE</cell></row><row><cell>Reference values</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="4">Reconstructed with the selected band 1.2669 0.0010 0.0183</cell></row><row><cell>Reconstructed with the 1 st PC</cell><cell cols="3">6.0193 0.0429 0.1388</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Spectral quality assessment for the reconstructed Center of Pavia image.</figDesc><table><row><cell>Method</cell><cell>SAM</cell><cell>SID</cell><cell>RMSE</cell></row><row><cell>Reference values</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Reconstructed with the selected band</cell><cell>2.3025</cell><cell cols="2">0.1578 0.0178</cell></row><row><cell>Reconstructed with the 1 st PC</cell><cell cols="3">16.6186 0.2215 0.1340</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Classification accuracies (in percent) of the Indian Pines image. Moreover, both the AA and the Kappa of R-OMP, R-SOMP and R-FOMP are also increased significantly. The OA, AA, and Kappa of R-ELM are increased form 73.35%, 67.22%, 69.45% to 97.62%, 97.26%, and 97.29%, respectively. Where, OA was improved 24.27%. It offers 30.04% higher AA and 27.84%</figDesc><table><row><cell>Class</cell><cell cols="10">SVM R-SVM OMP R-OMP SOMP R-SOMP FOMP R-FOMP ELM R-ELM</cell></row><row><cell>1</cell><cell>35.00</cell><cell>90.00</cell><cell>47.50</cell><cell>100</cell><cell>62.50</cell><cell>100</cell><cell>60.00</cell><cell>100</cell><cell>50.00</cell><cell>97.50</cell></row><row><cell>2</cell><cell>70.64</cell><cell>94.55</cell><cell>53.27</cell><cell>93.46</cell><cell>61.84</cell><cell>93.46</cell><cell>71.18</cell><cell>94.94</cell><cell>64.72</cell><cell>97.51</cell></row><row><cell>3</cell><cell>67.69</cell><cell>95.04</cell><cell>51.88</cell><cell>95.71</cell><cell>54.70</cell><cell>95.17</cell><cell>67.29</cell><cell>96.65</cell><cell>54.83</cell><cell>96.11</cell></row><row><cell>4</cell><cell>40.85</cell><cell>95.31</cell><cell>43.66</cell><cell>97.65</cell><cell>46.01</cell><cell>97.18</cell><cell>53.99</cell><cell>99.06</cell><cell>41.31</cell><cell>97.18</cell></row><row><cell>5</cell><cell>84.76</cell><cell>97.46</cell><cell>83.14</cell><cell>99.08</cell><cell>88.45</cell><cell>99.08</cell><cell>90.76</cell><cell>98.15</cell><cell>86.37</cell><cell>99.08</cell></row><row><cell>6</cell><cell>94.81</cell><cell>97.86</cell><cell>92.82</cell><cell>99.09</cell><cell>98.17</cell><cell>99.24</cell><cell>96.79</cell><cell>98.93</cell><cell>94.96</cell><cell>99.54</cell></row><row><cell>7</cell><cell>80.00</cell><cell>100</cell><cell>92.00</cell><cell>96.01</cell><cell>84.00</cell><cell>100</cell><cell>92.00</cell><cell>96.00</cell><cell>68.00</cell><cell>96.00</cell></row><row><cell>8</cell><cell>98.60</cell><cell>97.90</cell><cell>90.44</cell><cell>99.77</cell><cell>95.10</cell><cell>99.53</cell><cell>96.04</cell><cell>100</cell><cell>98.14</cell><cell>98.83</cell></row><row><cell>9</cell><cell>22.22</cell><cell>83.33</cell><cell>27.78</cell><cell>100</cell><cell>44.44</cell><cell>100</cell><cell>44.44</cell><cell>100</cell><cell>0</cell><cell>100</cell></row><row><cell>10</cell><cell>77.94</cell><cell>94.40</cell><cell>61.71</cell><cell>96.91</cell><cell>66.40</cell><cell>97.26</cell><cell>75.31</cell><cell>97.37</cell><cell>64.80</cell><cell>96.00</cell></row><row><cell>11</cell><cell>83.61</cell><cell>97.33</cell><cell>70.33</cell><cell>98.14</cell><cell>74.86</cell><cell>98.32</cell><cell>84.46</cell><cell>98.91</cell><cell>75.18</cell><cell>98.19</cell></row><row><cell>12</cell><cell>65.35</cell><cell>88.70</cell><cell>41.24</cell><cell></cell><cell>47.46</cell><cell>98.31</cell><cell>64.03</cell><cell>97.93</cell><cell>55.74</cell><cell>93.60</cell></row><row><cell>13</cell><cell>94.54</cell><cell>99.45</cell><cell>94.53</cell><cell>98.36</cell><cell>97.81</cell><cell>98.91</cell><cell>97.81</cell><cell>98.91</cell><cell>97.27</cell><cell>100</cell></row><row><cell>14</cell><cell>96.04</cell><cell>99.47</cell><cell>88.98</cell><cell>99.65</cell><cell>92.25</cell><cell>99.47</cell><cell>95.51</cell><cell>100</cell><cell>91.45</cell><cell>99.82</cell></row><row><cell>15</cell><cell>55.46</cell><cell>90.80</cell><cell>31.90</cell><cell>93.10</cell><cell>36.78</cell><cell>93.97</cell><cell>50.57</cell><cell>96.84</cell><cell>45.98</cell><cell>93.97</cell></row><row><cell>16</cell><cell>72.29</cell><cell>92.77</cell><cell>90.36</cell><cell>90.36</cell><cell>96.39</cell><cell>92.77</cell><cell>87.95</cell><cell>90.36</cell><cell>86.75</cell><cell>92.77</cell></row><row><cell>OA</cell><cell>79.72</cell><cell>95.97</cell><cell>67.87</cell><cell>97.28</cell><cell>72.87</cell><cell>97.37</cell><cell>80.39</cell><cell>97.97</cell><cell>73.35</cell><cell>97.62</cell></row><row><cell>AA</cell><cell>71.24</cell><cell>94.65</cell><cell>66.35</cell><cell>97.21</cell><cell>71.70</cell><cell>97.67</cell><cell>76.76</cell><cell>97.75</cell><cell>67.22</cell><cell>97.26</cell></row><row><cell cols="2">Kappa 76.76</cell><cell>95.40</cell><cell>63.33</cell><cell>96.90</cell><cell>69.02</cell><cell>97.00</cell><cell>77.54</cell><cell>97.68</cell><cell>69.45</cell><cell>97.29</cell></row><row><cell cols="11">Furthermore, Table 5 demonstrates the classification accuracies of different methods. In Table 5, the OA,</cell></row><row><cell cols="11">AA and Kappa of R-SVM are increased to 95.97%, 94.65%, and 95.40%, respectively. R-OMP yields 29.41%</cell></row><row><cell cols="11">higher OA than OMP, R-SOMP offers over 24.5% higher OA than SOMP, and R-FOMP yields 17.58% higher</cell></row><row><cell>OA than FOMP.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Classification accuracies (in percent) the of Salinas image.</figDesc><table><row><cell>Class</cell><cell cols="10">SVM R-SVM OMP R-OMP SOMP R-SOMP FOMP R-FOMP ELM R-ELM</cell></row><row><cell>1</cell><cell>99.50</cell><cell>98.01</cell><cell>99.50</cell><cell>98.56</cell><cell>99.78</cell><cell>98.23</cell><cell>100</cell><cell>99.50</cell><cell>99.61</cell><cell>98.12</cell></row><row><cell>2</cell><cell>100</cell><cell>98.42</cell><cell>99.43</cell><cell>97.64</cell><cell>99.52</cell><cell>97.73</cell><cell>99.64</cell><cell>99.08</cell><cell>97.17</cell><cell>98.87</cell></row><row><cell>3</cell><cell>98.99</cell><cell>97.53</cell><cell>96.68</cell><cell>98.09</cell><cell>97.81</cell><cell>98.26</cell><cell>98.31</cell><cell>98.93</cell><cell>91.57</cell><cell>99.94</cell></row><row><cell>4</cell><cell>99.44</cell><cell>94.26</cell><cell>99.60</cell><cell>96.02</cell><cell>99.36</cell><cell>95.86</cell><cell>99.68</cell><cell>98.25</cell><cell>90.52</cell><cell>97.77</cell></row><row><cell>5</cell><cell>99.17</cell><cell>98.71</cell><cell>97.06</cell><cell>98.47</cell><cell>96.43</cell><cell>98.63</cell><cell>98.84</cell><cell>98.76</cell><cell>93.28</cell><cell>98.80</cell></row><row><cell>6</cell><cell>99.94</cell><cell>99.52</cell><cell>99.89</cell><cell>99.92</cell><cell>99.86</cell><cell>99.97</cell><cell>99.94</cell><cell>100</cell><cell>99.55</cell><cell>99.55</cell></row><row><cell>7</cell><cell>99.72</cell><cell>95.93</cell><cell>99.60</cell><cell>96.93</cell><cell>99.41</cell><cell>97.02</cell><cell>99.97</cell><cell>98.88</cell><cell>98.98</cell><cell>98.42</cell></row><row><cell>8</cell><cell>89.79</cell><cell>98.46</cell><cell>78.77</cell><cell>98.17</cell><cell>82.29</cell><cell>98.28</cell><cell>87.51</cell><cell>99.17</cell><cell>81.66</cell><cell>98.80</cell></row><row><cell>9</cell><cell>99.80</cell><cell>98.57</cell><cell>99.12</cell><cell>99.39</cell><cell>99.44</cell><cell>99.46</cell><cell>99.84</cell><cell>99.73</cell><cell>96.96</cell><cell>99.73</cell></row><row><cell>10</cell><cell>95.29</cell><cell>93.70</cell><cell>95.39</cell><cell>96.54</cell><cell>94.71</cell><cell>96.65</cell><cell>98.20</cell><cell>98.27</cell><cell>86.00</cell><cell>97.12</cell></row><row><cell>11</cell><cell>97.51</cell><cell>96.57</cell><cell>97.71</cell><cell>97.71</cell><cell>96.88</cell><cell>97.82</cell><cell>99.79</cell><cell>97.09</cell><cell>93.14</cell><cell>93.66</cell></row><row><cell>12</cell><cell>99.60</cell><cell>98.56</cell><cell>99.65</cell><cell>98.85</cell><cell>100</cell><cell>98.56</cell><cell>100</cell><cell>99.37</cell><cell>99.37</cell><cell>99.37</cell></row><row><cell>13</cell><cell>97.45</cell><cell>97.70</cell><cell>97.58</cell><cell>95.27</cell><cell>96.00</cell><cell>95.52</cell><cell>99.15</cell><cell>97.33</cell><cell>96.73</cell><cell>99.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Classification accuracies (in percent) of the Center of Pavia image.</figDesc><table><row><cell>1</cell><cell>99.92</cell><cell>99.96</cell><cell>99.21</cell><cell>99.26</cell><cell>99.87</cell><cell>99.29</cell><cell>99.97</cell><cell>99.95</cell><cell>98.54</cell><cell>100</cell></row><row><cell>2</cell><cell>97.10</cell><cell>98.39</cell><cell>87.70</cell><cell>92.50</cell><cell>87.93</cell><cell>92.73</cell><cell>87.70</cell><cell>94.69</cell><cell>88.35</cell><cell>98.62</cell></row><row><cell>3</cell><cell>97.01</cell><cell>99.15</cell><cell>95.92</cell><cell>96.58</cell><cell>97.68</cell><cell>96.73</cell><cell>97.15</cell><cell>99.29</cell><cell>92.31</cell><cell>99.43</cell></row><row><cell>4</cell><cell>94.92</cell><cell>97.82</cell><cell>81.27</cell><cell>96.86</cell><cell>73.60</cell><cell>97.34</cell><cell>83.38</cell><cell>92.02</cell><cell>76.37</cell><cell>98.43</cell></row><row><cell>5</cell><cell>97.22</cell><cell>98.64</cell><cell>94.08</cell><cell>96.30</cell><cell>96.67</cell><cell>96.67</cell><cell>95.51</cell><cell>96.25</cell><cell>89.51</cell><cell>98.18</cell></row><row><cell>6</cell><cell>98.03</cell><cell>97.83</cell><cell>80.15</cell><cell>78.47</cell><cell>77.44</cell><cell>78.69</cell><cell>78.66</cell><cell>87.62</cell><cell>94.09</cell><cell>98.64</cell></row><row><cell>7</cell><cell>93.75</cell><cell>97.25</cell><cell>91.09</cell><cell>96.19</cell><cell>94.75</cell><cell>96.42</cell><cell>92.98</cell><cell>95.18</cell><cell>84.32</cell><cell>96.53</cell></row><row><cell>8</cell><cell>99.38</cell><cell>99.86</cell><cell>97.79</cell><cell>95.55</cell><cell>98.48</cell><cell>95.86</cell><cell>98.62</cell><cell>99.48</cell><cell>95.27</cell><cell>99.55</cell></row><row><cell>9</cell><cell>99.85</cell><cell>99.54</cell><cell>74.72</cell><cell>68.43</cell><cell>83.20</cell><cell>68.48</cell><cell>95.53</cell><cell>95.13</cell><cell>46.85</cell><cell>99.54</cell></row><row><cell>OA</cell><cell>98.89</cell><cell>99.39</cell><cell>95.45</cell><cell>96.20</cell><cell>96.20</cell><cell>96.30</cell><cell>96.56</cell><cell>97.98</cell><cell>94.52</cell><cell>99.43</cell></row><row><cell>AA</cell><cell>97.47</cell><cell>98.72</cell><cell>89.10</cell><cell>91.13</cell><cell>89.96</cell><cell>91.36</cell><cell>92.17</cell><cell>95.51</cell><cell>85.07</cell><cell>98.77</cell></row><row><cell cols="2">Kappa 97.98</cell><cell>98.89</cell><cell>91.74</cell><cell>93.08</cell><cell>93.07</cell><cell>93.27</cell><cell>93.73</cell><cell>96.32</cell><cell>90.11</cell><cell>98.95</cell></row></table><note><p>Class SVM R-SVM OMP R-OMP SOMP R-SOMP FOMP R-FOMP ELM R-ELM</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>Classification performance of different methods in literature.</figDesc><table><row><cell>Reference</cell><cell>Datasets</cell><cell>Baseline</cell><cell>Training set</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell></row><row><cell>Zabala et al. [24]</cell><cell>Indian Pines Subset of Pavia center</cell><cell>SAE, SVM</cell><cell>5%</cell><cell>80.66 97.42</cell><cell>73.63 86.26</cell><cell>--</cell></row><row><cell>Ma et al. [25]</cell><cell>Indian Pines Center of Pavia</cell><cell>SAE, CR</cell><cell>10% 5%</cell><cell>99.22 99.90</cell><cell>98.57 99.73</cell><cell>99.11 99.83</cell></row><row><cell>Chen et al. [26]</cell><cell>University of Pavia</cell><cell>SAE, LR</cell><cell>60%</cell><cell cols="2">98.52 97.82</cell><cell>98.07</cell></row><row><cell>Chen et al. [27]</cell><cell>Indian Pines</cell><cell>DBN, LR</cell><cell>50%</cell><cell>95.95</cell><cell>95.45</cell><cell>95.39</cell></row><row><cell></cell><cell>University of Pavia</cell><cell></cell><cell></cell><cell>99.05</cell><cell>98.48</cell><cell>98.75</cell></row><row><cell>Zhao &amp; Du [29]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Proposed algorithm Indian Pines Salinas CNN, ELM 10% 10% 97.62 98.84 97.26 98.60 97.29 98.71 Center of Pavia 5% 99.43 98.77 98.95 5</head><label></label><figDesc>.3 Computational timeMore importantly, ELM is a high-efficient algorithm so that the computational complexity of all the aforementioned classifiers for the reconstructed HSI is reported in Table9. Experiments are performed using MATLAB on a Laptop with 3.4 GHz CPU and 8 GB of RAM. ELM is achieved by MATLAB while SVM is implemented by mixing C and MATLAB. The sparsity-based classifiers are also achieved by MATLAB. It can be seen from Table9that the computational time of ELM is much less than SVM and sparsity-based classifiers, and the sparsity-based classifiers spend too much time.</figDesc><table><row><cell>30 per class</cell><cell>-</cell><cell>-</cell><cell>88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc>Computing time (in seconds) for the classification procedure in the reconstructed images.</figDesc><table><row><cell cols="2">Methods Indian Pines</cell><cell>Salinas</cell><cell>Center of Pavia</cell></row><row><cell>SVM</cell><cell>3.91</cell><cell>28.95</cell><cell>10.68</cell></row><row><cell>OMP</cell><cell>60.02</cell><cell>1736.06</cell><cell>8715.46</cell></row><row><cell>SOMP</cell><cell>97.41</cell><cell>2054.63</cell><cell>22619.49</cell></row><row><cell>FOMP</cell><cell>111.01</cell><cell>2596.14</cell><cell>35037.51</cell></row><row><cell>ELM</cell><cell>0.59</cell><cell>0.624</cell><cell>4.29</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://engineering.purdue.edu/biehl/MultiSpec/hyperspectral.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.ehu.eus/ccwintco/uploads</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to the Editor-in-Chief and Yi Chen for providing the software of SOMP method.</p><p>This work was partially supported by the National Natural Science Foundation of China (nos. 61222101, 61272120, 61301287, 61301291 and 61350110239).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>classifier, three sparsity-based classifiers (i.e. OMP, SOMP, FOMP) and ELM classifier. By using the reconstructed HSI, the AA of the ELM classifier can be improved as high as 30.04%. Among these classification results, R-FOMP, as we previously proposed, can achieve a bit higher classification accuracies than that of R-ELM when it is applied to classify Indian Pines and Salinas image obtained by the same sensor. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Remote Sensing Digital Image Analysis: An Introduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C.-I</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">Hyperspectral Data Processing: Algorithm Design and Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advances in spectral-spatial classification of hyperspectral images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Tilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="652" to="675" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral-spatial hyperspectral image classification with edge-preserving filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2666" to="2677" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmentation and classification of hyperspectral images using watershed transformation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2367" to="2379" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integration of segmentation techniques for classification of hyperspectral images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Couceiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M F</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="346" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Merging hyperspectral and panchromatic image data: qualitative and quantitative analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Musaoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1779" to="1804" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-spectral and hyperspectral image fusion using 3-D wavelet transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese J. Electron</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="224" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noise-resistant wavelet-based Bayesian fusion of multispectral and hyperspectral images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Backer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3834" to="3842" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super-resolution of hyperspectral images: use of optimum wavelet filter coefficients and sparsity regularization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1728" to="1736" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-resolution hyperspectral imaging via matrix factorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Ezra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial and spectral image fusion using sparse matrix factorization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1693" to="1704" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coupled nonnegative matrix factorization unmixing for hyperspectral and multispectral data fusion</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iwasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="528" to="537" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparse spatio-spectral reconstruction for hyperspectral image super-resolution</title>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hyperspectral super-resolution of locally low rank images from complementary multisource data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Veganzones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simões</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Licciardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="274" to="288" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spectral unmixing for the classification of hyperspectral images at a finer spatial resolution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="533" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Texture feature analysis using a Gauss-Markov model in hyperspectral image classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rellier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Falzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1543" to="1551" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On combining multiple features for hyperspectral remote sensing image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="879" to="893" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint collaborative representation with multitask learning for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5923" to="5936" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Class-level joint sparse representation for multifeature-based hyperspectral image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1012" to="1022" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reconstruction of hyperspectral image using matting model for classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">53104</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Novel segmented stacked autoencoder for effective dimensionality reduction and feature extraction in hyperspectral imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zabalza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral image based on deep auto-encoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data based on deep belief network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2381" to="2392" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning multiscale and deep representations for classifying remotely sensed imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="155" to="165" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral-spatial feature extraction for hyperspectral image classification: a dimension reduction and deep learning approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4544" to="4554" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Camps-Valls, Unsupervised deep feature extraction for remote sensing image classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1349" to="1362" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep supervised learning for hyperspectral data classification through convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Makantasis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IGARSS</title>
		<meeting>of IGARSS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4959" to="4962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiclass feature learning for hyperspectral image classification: Sparse and hierarchical solutions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="272" to="285" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extreme learning machine: theory and applications</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Breast tumor detection in digital mammography based on extreme learning machine</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">X</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face recognition based on extreme learning machine</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="2541" to="2551" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Trends in extreme learning machines: a review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="32" to="48" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Breast mass classification in digital mammography based on Extreme learning machine</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="930" to="941" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kernel-based extreme learning machine for remote sensing image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Warner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="852" to="862" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extreme learning machines for soybean classification in remote sensing hyperspectral images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Galvao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Differential evolution extreme learning machine for the classification of hyperspectral images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1066" to="1070" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LMs: ensemble extreme learning machines for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1060" to="1069" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Local binary patterns and extreme learning machine for hyperspectral imagery classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3681" to="3693" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimizing extreme learning machine for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Similarity-based unsupervised band selection for hyperspectral image analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="564" to="568" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gabor-filtering based nearest regularized subspace for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1012" to="1022" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image hallucination with feature enhancement</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Support vector machines in remote sensing: A review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mountrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ogole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="259" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1031" to="1044" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
