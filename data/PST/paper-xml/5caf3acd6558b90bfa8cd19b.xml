<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Survey on Semantic Segmentation using Deep Learning Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-01">January 1, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fahad</forename><surname>Lateef</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>ResNet ENet Cityscapes</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<address>
									<settlement>ResNet ENet Cityscapes</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yassine</forename><surname>Ruichek</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>ResNet ENet Cityscapes</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<address>
									<settlement>ResNet ENet Cityscapes</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Survey on Semantic Segmentation using Deep Learning Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-01">January 1, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">EE6680623134F17A3DECB23E5D9B2F51</idno>
					<idno type="DOI">10.1016/j.neucom.2019.02.003</idno>
					<note type="submission">Received date: 26 October 2018 Revised date: 31 December 2018 Accepted date: 1 February 2019 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Semantic Segmentation</term>
					<term>Recurrent Neural Network</term>
					<term>Semi-weakly supervised networks PASCAL VOC</term>
					<term>Mapillary March 22</term>
					<term>2018 YES Restricted Deformable Convolution (RDC) Network [48] Zoom Augmentation method: Transforming conventional images to fish-eye images. ERFNet CityScapes</term>
					<term>SYNTHIA January 3</term>
					<term>2018 -</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation is a challenging task in computer vision systems. A lot of methods have been developed to tackle this problem ranging from autonomous vehicles, human-computer interaction, to robotics, medical research, agriculture and so on. Many of these methods have been built using the deep learning paradigm that has shown a salient performance. For this reason, we propose to survey these methods by, first categorizing them into ten different classes according to the common concepts underlying their architectures. Second, by providing an overview of the publicly available datasets on which they have been assessed. In addition, we present the common evaluation matrix used to measure their accuracy. Moreover, we focus on some of the methods and look closely at their architectures in order to find out how they have achieved their reported performances. Finally, we conclude by discussing some of the open problems and their possible solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• Extensive survey on deep neural networks for semantic segmentation.</p><p>• 110 neural network models are categorized into 10 different concepts.</p><p>• Review of state-of-the-art datasets and evaluation metrics for semantic segmentation.</p><p>• Models analysis based on structural design and their performance on tested datasets.</p><p>• Pointing out some of the open problems and possible solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent works in deep learning dealing with semantic segmentation have been significantly improved by using neural networks. Neural networks have a long history since the 1940s and they did not get much of the attention of researchers until 1990s <ref type="bibr" target="#b0">[1]</ref>. Neutral networks made huge progress because of large amount of data is available thanks to the rise of digital cameras, cell phone cameras, and the computing power, which is getting faster as GPUs become general purpose computing tools.</p><p>Deep neural networks are very effective in semantic segmentation, that is labeling each region or pixel with a class of objects/non-objects. Semantic segmentation plays an important role in image understanding and essential for image analysis tasks. It has several applications in computer vision &amp; artificial intelligenceautonomous driving <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, robot navigation <ref type="bibr" target="#b3">[4]</ref>, industrial inspection <ref type="bibr" target="#b4">[5]</ref>; remote sensing <ref type="bibr" target="#b5">[6]</ref>; In cognitive and computational sciencessaliency object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>; In Agriculture sciences <ref type="bibr" target="#b8">[9]</ref>; Fashioncategorizing clothing items <ref type="bibr" target="#b9">[10]</ref>; In medical sciencesmedical imaging analysis <ref type="bibr" target="#b10">[11]</ref> etc. The earlier approaches used for semantic segmentation were textonforest <ref type="bibr" target="#b11">[12]</ref>, random-forest based classifiers <ref type="bibr" target="#b12">[13]</ref>, whereas deep learning techniques allowed precise and much faster segmentation <ref type="bibr" target="#b13">[14]</ref>.</p><p>Semantic segmentation requires image classification, object detection, and boundary localization. Figure <ref type="figure" target="#fig_0">1</ref> is an example of object detection, involving bounding box, and classification of each pixel into different classes (car, road, sky, vegetation, terrain etc).</p><p>1 LE2I-CNRS, University of Technology of Belfort-Montbeliard (UTBM), France (fahad.lateef, yassine.ruichek)@ utbm.fr. Deep learning is a new field division of machine learning, which is rapidly growing with the pace making it very difficult to stay up to date, even to keep track of the works dealing with semantic segmentation. These works cover the development of new methods, improvements of existing methods, and their deployment in new application domains. This is the reason that there is a lack of state-of-the-art reviews. Some surveys and review papers have addressed advancements and innovations on the subject of deep learning and semantic segmentation. A Survey by Zhu et al. <ref type="bibr" target="#b14">[15]</ref> covering a wide range of the papers and areas of semantic segmentation topics including, interactive methods, recent development in the super-pixel, object proposals, semantic image parsing, image co-segmentation, semi &amp; weakly supervised, and fully supervised image segmentation. Martin Thoma <ref type="bibr" target="#b15">[16]</ref> presented a taxonomy of segmentation algorithms and overview of completely automatic, passive, semantic segmentation algorithms. Niemeijer et al. <ref type="bibr" target="#b16">[17]</ref> presented a review of neural network based semantic segmentation for scene understanding in the context of the autonomous driving. Guo et al. <ref type="bibr" target="#b17">[18]</ref> provided a review of semantic segmentation approaches, i.e., region-based, FCNbased and weakly supervised approaches. They have summarized the strengths, weaknesses and major challenges in image semantic segmentation. Geng et al. <ref type="bibr" target="#b18">[19]</ref> presented a survey of recent progress in semantic segmentation with CNN's, and newly developed strategies that have achieved promising results on the Pascal VOC 2012 semantic segmentation challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Detail review provided by Garcia et al. <ref type="bibr" target="#b19">[20]</ref> on deep learning methods for semantic segmentation with their contributions and significance in the field. An extensive review presented by Hongshan et al. <ref type="bibr" target="#b20">[21]</ref>, categorized different methods based on hand engineered features, learned features, and weakly supervised learning.</p><p>The main contributions of this paper are as follows:</p><p>1. The existing methods have been categorized into ten different classes according to the common concept that underlays their architectures. This categorization gives a complete summary of the methods both inspire and diverge from one another. 2. More than 100 different models and 33 datasets (publicly available) have been covered, stating the corpus, original architecture, testing benchmark of each model, and the attributes of each dataset. Furthermore, we provide the best performing method yielded top classification accuracy on each dataset until date. 3. An emphasis on how these methods achieved their accomplishments is given by analyzing their structural design and their performance on the assessed datasets. 4. Finally, some of the open problems and possible solutions have been discussed.</p><p>In this survey, all the models are carefully chosen and put into relation to each other according to their architectural design and contribution to the field. This includes improving accuracy, reducing computation complexity, developing new methods, and enhancing existing ones. All the results reported in this paper are taken from the original papers. We have tried to cover most of the works in deep neural networks for semantic segmentation. This survey will help the new researchers to strengthen their understanding of these remarkable works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Deep Learning Architectures for Semantic Segmentation</head><p>This section provides the details of all the reviewed semantic segmentation methods. We have categorized these methods into ten <ref type="bibr" target="#b9">(10)</ref> classes, presented in the tabular form stating each method, its main idea, its architecture origin, testing benchmarks, publication date, and code availability (Table .13 provides links of available source codes).</p><p>The recent success of deep convolutional neural networks (CNNs) has enabled outstanding progress in semantic segmentation. The first successful application of convolutional neural network was developed by LeCun <ref type="bibr" target="#b21">[22]</ref>. They introduced an architecture named LeNet5 to read zip codes, digits, and extract features at multiple locations in the image. Later, Alex Krizhevsky released a large deep convolutional neural network (AlexNet) <ref type="bibr" target="#b23">[23]</ref>, which is regarded as one of the most influential publications in the field. AlexNet is a deeper and wider version of the LeNet, used to learn complex objects and object hierarchies. Zeiler and Fergus <ref type="bibr" target="#b24">[24]</ref> presented the ZFNet, which is a fine-tuning of the AlexNet structure. They proposed a technique of visualizing feature maps at any layer in the network model. This technique uses a multi-layered deconvolutional network to project the feature activations back to the input pixel space. Lin et al. <ref type="bibr" target="#b25">[25]</ref> proposed a Network-In-Network model based on micro neural networks, which is a multilayer perceptron (MLP) <ref type="bibr" target="#b26">[26]</ref>, consisting of multiple fully connected layers with nonlinear activation functions. Szegedy et al. <ref type="bibr" target="#b27">[27]</ref> proposed an efficient deep neural network called GoogleNet. They introduced an inception module as shown in Figure <ref type="figure" target="#fig_1">2</ref>, which is a combination of 1×1, 3×3, and 5×5 convolutional filters and a pooling layer. It reduced the number of features and operations at each layer thus saving the time and computational cost. The same authors proposed in <ref type="bibr" target="#b28">[28]</ref> an algorithm refereed as BN-Inception for constructing, training, and performing inference with Batch Normalization method. Szegedy et al. <ref type="bibr" target="#b29">[29]</ref> further introduced two new modules Inception V2 and Inception V3 with some major modifications (i.e., factorizing convolutions and using grid reduction technique) of their previous module. Later, Szegedy et al. <ref type="bibr" target="#b30">[30]</ref> replaced the filter concatenation stage of the Inception architecture with residual connections in order to increase efficiency and performance. They proposed Inception-ResNet-v1, Inception-ResNet-v2 and a pure Inception variant called Inception V4. Chollet et al. <ref type="bibr" target="#b31">[31]</ref> proposed a module named Xception, meaning extreme inception. They replaced the inception modules with depth wise separable convolutions proposed in <ref type="bibr" target="#b32">[32]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows GoogLeNet Modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature Encoder Based Methods:</head><p>VGG <ref type="bibr" target="#b33">[33]</ref> and ResNet <ref type="bibr" target="#b34">[34]</ref> methods are the most dominant approaches for feature extraction. In this category, we review these methods and their invariants presented in Table <ref type="table" target="#tab_1">2</ref>. The idea behind the concept is to extract feature maps based on stacked convolution layers, ReLu layers and pooling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">VGG Network:</head><p>VGG network <ref type="bibr" target="#b33">[33]</ref> introduced by Oxford's renowned Visual Geometry Group. Unlike LeNet <ref type="bibr" target="#b21">[22]</ref> and AlexNet <ref type="bibr" target="#b23">[23]</ref>, VGGNet uses multiple 3 × 3 convolution in the sequence that can match the effect of larger receptive fields, e.g. 5 × 5 and 7 × 7. However, it required a large number of parameters and learning power due to having large classifiers. Figure <ref type="figure">3</ref> shows a VGGNet with 16 convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Residual Learning Frameworks</head><p>Residual learning frameworks include methods which use residual block <ref type="bibr" target="#b34">[34]</ref> as a fundamental building block in their architecture. Residual Network -ResNet <ref type="bibr" target="#b34">[34]</ref> is the most popular and widely used neural network for semantic segmentation. It is hard to train a deep neural network with large numbers of layers, the more increase in depth, its performance gets saturated or even starts degrading due to vanishing gradient problem. Several solutions were proposed in <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref> but none of them seemed to really tackle the problem. He et al. <ref type="bibr" target="#b34">[34]</ref> resolved the vanishing gradient problem in an effective way by introducing identity shortcut connection (i.e., skipping one or more layers) as shown in Figure <ref type="figure" target="#fig_2">4</ref>. They proposed a pre-activation variant residual block in which the gradients can easily flow through the shortcut connection without obstruction during the back pass of back propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Several architectures are based on ResNet, its variants and interpretations. Paszke et al. <ref type="bibr" target="#b45">[45]</ref> presented an encoder/decoder scheme network called efficient neural network (ENet). This network is similar to the ResNet bottleneck approach, created specifically for tasks requiring low latency operation, i.e., mobile phones or battery-powered devices. In <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50]</ref>, the authors proposed counter-intuitive way of training a deep network by randomly dropping its layers and using the full network in testing time. Wu et al. <ref type="bibr" target="#b38">[38]</ref> presented a neural network called ResNet-38, in which they added and removed layers in residual networks at train/test time. They analyzed the effective depths of residual units, and point out that ResNet behaves as linear ensembles of shallow networks. Pohlen et al. <ref type="bibr" target="#b44">[44]</ref> proposed a full-resolution residual network (FRRN) with strong localization and recognition performance for semantic segmentation. FRRN exhibits the same superior training properties as ResNet, having two processing streams: residual and pooling. Residual stream carries information at the full image resolution and enables precise adherence to segment boundaries. The pooling stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals in order to realize strong recognition and localization performance for semantic segmentation. Xie et al. <ref type="bibr" target="#b41">[41]</ref> proposed a modified ResNet called ResNeXt, following the split-transform-merge strategy as inception modules <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b30">30]</ref>, except the outputs of different paths are concatenated and all paths share the same topology. Thus, this allows the design to extend to any large number of transformations. Adapting the idea of ResNet-50 <ref type="bibr" target="#b34">[34]</ref>, an architecture called Adaptive network or AdapNet is proposed by Valada et al. <ref type="bibr" target="#b40">[40]</ref>. They introduced an additional convolution with a kernel size of 3 × 3 before the first convolution layer in ResNet, which enables the network to learn more high resolution features in less time. They also proposed the convoluted mixture of deep experts (CMoDE) fusion scheme for learning robust kernels from complementary modalities and spectra. The proposed model adaptively weighs class-specific features based on the scene condition. Inspired by ENet, Romera et al. <ref type="bibr" target="#b46">[46]</ref> proposed an efficient residual factorized network ERFNet for real-time semantic segmentation. ERFNet proposes a non-  <ref type="bibr" target="#b41">[41]</ref> and ResNet-38 <ref type="bibr" target="#b38">[38]</ref>.</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>The focus on VGG and ResNet approaches of recent works led to remarkable results in semantic segmentation. The residual learning frameworks follow the core idea "skip connection" which is the main intuition behind their success. However, using it in large scale can lead to memory problem. These groundbreaking works make it possible to train deeper networks with good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Regional Proposal based Methods</head><p>Regional proposal algorithms are very influential in computer vision (for object detection techniques). The core idea is to detect the regions according to the variety of color spaces and similarity metrics, and then perform the classification (region proposals that might contain object) often called Region-wise prediction. Regional Convolutional Neural Network (R-CNN) along with its descendants shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>Girshick et al. <ref type="bibr" target="#b52">[52]</ref> at UC Berkeley proposed a first regionbased convolutional neural network (R-CNN) for object detection tasks. The R-CNN consists of three modules; regional proposal generator in which they used selective search method <ref type="bibr" target="#b53">[53]</ref> performing the function of generating 2000 different regions that have the highest probability of containing an object; convolutional neural network <ref type="bibr" target="#b21">[22]</ref> for extracting features from each region; finally these feature from CNN are used as input to set of class specific linear SVMs. The features are also fed into bounding box regressor to obtain the most accurate coordinates and reduce localization errors. Figure <ref type="figure">5</ref> shows R-CNN architecture.</p><p>Figure <ref type="figure">5</ref>: The architecture of R-CNN <ref type="bibr" target="#b52">[52]</ref> In <ref type="bibr" target="#b54">[54]</ref>, the authors proposed Fast R-CNN in which, a technique called RoIPool (Region of Interest Pooling) is used that improves the training and testing speed and increases the accuracy for object detection. Later a team from Microsoft proposed an Faster-RCNN <ref type="bibr" target="#b55">[55]</ref> architecture. They introduce Region Proposal Network (RPN) which is a kind of fully convolutional network (FCN) constructed by adding a few additional convolutional layers that predict object bounds and objectness (set of object classes vs. background) scores at each position. The RPN generates region proposals (multiple scales and aspect ratios), which are fed into Fast R-CNN for object detection. RPN and Fast R-CNN share their convolutional features which reduce the complexity, increases the speed and overall object detection accuracy. Lin et al. <ref type="bibr" target="#b57">[57]</ref> present Feature Pyramid Networks (FPN), a multi-scale pyramidal hierarchy of deep convolutional network (ConvNet's), and creates feature pyramids having semantics at all levels, that can be used to replace featurized image pyramids with minimal cost (power, speed, or memory). He et al. <ref type="bibr" target="#b56">[56]</ref> proposed a Mask Regional Convolutional Neural Network (Mask-RCNN), extending Faster R-CNN to pixel-level image segmentation. It added a branch (small FCN) on each RoI for predicting object mask in a pixelto pixel manner, in parallel with the existing branch for bounding box recognition (classification and regression). Faster R-CNN has a drawback of misalignment (pixel-to-pixel alignment) between network inputs and outputs. Mask-RCNN fixes this issue by replacing the RoI pooling layer with Region of Interest Alignment (RoIAlign), a quantization-free layer that preserves exact spatial locations as shown in Figure <ref type="figure">6</ref>. Recently, Liu et al. <ref type="bibr" target="#b58">[58]</ref> proposed network built on Mask-RCNN and FPN named Path Aggregation Network (PANet), boosting information flow in proposal-based instance segmentation framework.</p><p>Region proposal based neural networks have the advantage that object detection and segmentation can be achieved at the same time. Proposals are generated by algorithms ( <ref type="bibr" target="#b59">[59]</ref> provide an in-depth analysis) that are semantically meaningful and related to objects. It may contain an object class or several other classes that can help in determining the semantic labels. Furthermore, feeding the wrapped region proposals into a convolutional neural network for classification can reduce the computational cost.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Recurrent Neural Network based Methods</head><p>Recurrent neural networks (RNNs) were genuinely introduced for dealing sequences <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b62">62]</ref>. Beside its accomplishments in handwriting and speech recognition, RNNs are very much successful in computer vision tasks (dealing with images). We have only reviewed network models that adopt RNN in 2D images (integrate the convolution layers with RNNs). The Recurrent neural network made up of Long-Short-Term Memory (LSTM) <ref type="bibr" target="#b63">[63]</ref> blocks. RNN capability to learn long term dependencies from sequential data and ability to keep memory along the sequence makes it applicable in many computer vision tasks including semantic segmentation <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b65">65]</ref> and scene segmentation and labeling <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b67">67]</ref>, based on using RNN CNN combination. Table <ref type="table" target="#tab_3">4</ref> shows RNN based methods.</p><p>Pinheiro et al. <ref type="bibr" target="#b68">[68]</ref> proposed a convolutional neural network, which relies on a recurrent architecture ( R CNN). R CNN is a sequence of shallow networks sharing same weights, at each instance using the downscaled input image and prediction maps from the previous instance of the network, and automatically learns to smooth its predicted labels. Heng et al. <ref type="bibr" target="#b66">[66]</ref> pro-posed the contextual RNNs for scene labeling. The proposed network can capture long-range dependencies (GIST, local and global features) in an image. These features (after upsampling) are fused via an attention model <ref type="bibr" target="#b67">[67]</ref>. Amaia et al. <ref type="bibr" target="#b75">[75]</ref> present an encoder/decoder based recurrent neural network architecture for semantic instance segmentation. The proposed architecture much resembled FCN <ref type="bibr" target="#b77">[77]</ref> architecture (encoder: feature extractor) using skip-connection, except with decoder part that is the recurrent network (convolutional LSTM <ref type="bibr" target="#b76">[76]</ref>), predicting one instance (object in the image) at a time and output them. Byeon et al. <ref type="bibr" target="#b37">[37]</ref> present a two-dimensional (2D) long-short term memory (LSTM) recurrent neural network for scene labeling. 2D LSTM network architecture consists of four LSTM blocks (it propagates surrounding contexts) and a feed forward layer (summing LSTM activations). This method is able to model long-range dependencies (both local and global) in image. Visin et al. <ref type="bibr" target="#b64">[64]</ref> propose an RNN-based architecture for semantic segmentation codenamed ReSeg to model structured information of local generic features extracted from CNNs. The proposed model is a modified and extended version of ReNet <ref type="bibr" target="#b65">[65]</ref>. The proposed recurrent layer is composed of multiple RNNs <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b74">74]</ref> that sweep the image in both directions horizontally and vertically (output of hidden states), encoding local features, and providing relevant global information. ReNet layers are stacked on top of the output of a FCN.   Recurrent neural network (RNN) can be very beneficial in semantic segmentation; it has recurrent connections (ability to retain previous information) and ability to capture context in an image by modeling long-range semantic dependencies for the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Upsampling / Deconvolution based Methods</head><p>Convolution neural network models have the ability to learn automatically high-level features via a layer-to-layer propagation with sacrificing the spatial information. One deep understanding is that spatial information lost during downsampling operation can be regained by upsampling and deconvolution. Second is to develop reconstruction technique for increasing spatial accuracy and refinement technique for fusing the features of a low and high level. Table <ref type="table" target="#tab_4">5</ref> shows Upsampling / Deconvolution based methods.</p><p>Noh et al. <ref type="bibr" target="#b79">[79]</ref> used this idea and developed a network model by learning a deconvolution network. The convolution network reduces the size of activations through feed forwarding, and deconvolution network enlarges the activations through the combination of unpooling and deconvolution operations. Wang et al. <ref type="bibr" target="#b78">[78]</ref> proposed an objectness-aware semantic segmentation framework (OA-Seg) using two networks, object proposal network (OPN), predicting object bounding boxes and their objectness scores, and lightweight deconvolutional neural network (Light-DCNN) for upsampling the feature maps to larger resolution. Long et al. <ref type="bibr" target="#b77">[77]</ref> proposed first Fully Convolutional Network (FCN), and made breakthroughs in deep learning based semantic segmentation. FCN architectures have become the standard in semantic segmentation; most of the methods utilize FCN architecture. FCN coverts the classification network <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b33">33]</ref> into fully convolutional network and produces a probability map for input of arbitrary size. FCN recovers the spatial information from the downsampling layers by adding upsampling layers to standard convolution network. They defined a skip architecture (shallow fine layer) that combines semantic information from a deep coarse layer with ap-pearance information to produce precise and in depth segmentation. The basic idea was to re-architect and fine-tune classification model (image classification) to learn efficiently from whole image inputs and whole image ground truths (prediction of semantic segmentation). This allows hence extending these classification models to segmentation, and improving the architecture with multi-resolution layer combinations. Figure <ref type="figure" target="#fig_6">8</ref> shows FCN architecture. Badrinarayanan et al. <ref type="bibr" target="#b80">[80]</ref> present an encoder decoder structure deep fully convolutional neural network called SegNet. The encoder network has the same topology as VGG <ref type="bibr" target="#b33">[33]</ref> with no fully connected layers followed by a decoder network (from <ref type="bibr" target="#b93">[93]</ref>) for a pixel-wise classification. SegNet obtains higher resolution than that in <ref type="bibr" target="#b77">[77]</ref> by using set of decoders, each one corresponding to each encoder. One key feature of SegNet is that the information transfer is direct instead of convolving them. SegNet was one the best model to use when dealing with image segmentation problems specially scene segmentation tasks.</p><p>Ghiasi et al. <ref type="bibr" target="#b92">[92]</ref> proposed a network called the Laplacian Pyramid Reconstruction and Refinement (LRR) since the architecture uses a Laplacian reconstruction pyramid <ref type="bibr" target="#b94">[94]</ref>. The architecture uses low-resolution feature maps to reconstruct a coarse and low frequency segmentation map, and then refines this map by adding in higher frequency details derived from higher-resolution feature maps. Lin et al. <ref type="bibr" target="#b88">[88]</ref> proposed a multi-path neural network named refinement network (RefineNet). RefineNet is an encoder decoder architecture inspired by residual connection design <ref type="bibr" target="#b34">[34]</ref> and consists of three components; Residual convolution unit (RCU), Multi-resolution fusion and Chained residual pooling. Multi-path network exploits features at multiple levels, it refines low-resolution features with concentrated low-level features in a recursive manner to produce high-resolution feature maps for semantic segmentation. Islam et al. <ref type="bibr" target="#b91">[91]</ref> proposed a refinement structure architecture called Label Refinement Network (LRN). LRN learns to predict segmentation labels at multiple levels in the network and gradually refines the results at finer scale. LRN is an encoder decoder architecture and has supervision at multiple levels (at each stage of the decoder). Zhao et al. <ref type="bibr" target="#b87">[87]</ref> proposed image cascade network (ICNet) which utilizes semantic information in low resolution along with details from high-resolution images efficiently. The network focuses on fusion of features from multiple layers. They proposed a cascade feature fusion (CFF) unit that fuses the low feature maps with high feature </p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T maps. Fu et al. <ref type="bibr" target="#b81">[81]</ref> proposed Stacked Deconvolutional Network (SDN), inspired by <ref type="bibr" target="#b71">[71]</ref>. The basic idea is stacking multiple shallow deconvolutional networks one by one in order to recover high-resolution prediction. Jegou et al. <ref type="bibr" target="#b51">[51]</ref> proposed a Fully Convolutional DenseNet FC-DenseNet, the extension of <ref type="bibr" target="#b71">[71]</ref> by adding an upsampling path and skipping connections to recover the full input resolution. Bilinski and Prisacariu <ref type="bibr" target="#b86">[86]</ref> proposed an architecture following encoder decoder strategy.</p><p>The encoder is based on ResNeXt architecture and decoder is made of blocks (dense decoder shortcut connections), which generate semantic feature maps and allow multi level fusion in single pass inference. Yang et al. <ref type="bibr" target="#b95">[95]</ref> proposed a fully combined convolutional network (FCCN) to improve the upsampling operation of FCN. The network follows layer-by-layer upsampling strategy, and after each upsampling operation the size of input feature map is doubled. They also proposed a soft cost function that further improves training. Recently in <ref type="bibr" target="#b83">[83]</ref>, they extend FCCN with a highly fused network. The proposed network has three major parts: feature downsampling, combined feature upsampling and multiple predictions. The fused network makes use of multiple scale feature information in low layers. Multiple soft cost functions are used to train the proposed model. Inspired by RefineNet, Park et al. <ref type="bibr" target="#b89">[89]</ref> proposed RGB-D fusion network (RDFNet) for semantic segmentation. The proposed architecture is made of two feature fusion blocks: multi-modal feature fusion (MMF) to fuse features (RGB and depth) in different modalities, and multi-level feature refinement block to further refining feature for semantic segmentation. Islam et al. <ref type="bibr" target="#b90">[90]</ref> proposed Gated Feedback Refinement Network (G-FRNet), an encoder-decoder style architecture. The proposed gated mechanism (Gate Unit) takes two feature maps one after another, i.e., low-resolution feature with larger receptive fields and high-resolution feature with smaller receptive fields, and combines them in order to produce contextual information. The feature maps with different spatial dimension generated by encoder network pass through gate unit before feeding to the decoder (feedback refinement network). The refinement network gradually refines the feature label maps. Recently, Nanfack et al. <ref type="bibr" target="#b82">[82]</ref> proposed encoder-decoder based Squeeze-SegNet architecture. Encoder module is a SqueezeNet architecture <ref type="bibr" target="#b96">[96]</ref> (using the fire module and removing the average pooling layer) inspired by SegNet which removes all fully connected layers of VGG. The squeeze-decoder module is the inversion of the fire module and convolutional layers of SqueezeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Increase Resolution of Feature based Methods</head><p>Another type of method is to recover the spatial resolution by using atrous convolution <ref type="bibr" target="#b97">[97]</ref> and dilated convolution <ref type="bibr" target="#b98">[98]</ref> which can generate high-resolution feature maps for dense prediction. The dilated convolution hosts another parameter "dilation rate" (describing space between the values in a kernel) to convolution layer and it has the ability to expand the receptive field without losing resolution. Table <ref type="table" target="#tab_5">6</ref> shows increase resolution of feature based network models.</p><p>Chen et al. <ref type="bibr" target="#b97">[97]</ref> from Google proposed a deep convolutional neural network model named DeepLab. Instead of us-ing deconvolution, they proposed Atrous ('Holes') convolution. The atrous algorithm was originally developed by Holschneider et al. <ref type="bibr" target="#b106">[106]</ref> for computing undecimated wavelet transform (UWT). The DeepLab architecture is similar to the one in <ref type="bibr" target="#b77">[77]</ref> with some modification like, converting fully-connected layers into convolutional layers, using stride of 8 pixels, skip subsampling after last two pooling layers, and modifying convolutional filters in the layers (increasing length of last three convolutional layers by 2x and the first fully connected layer by 4x) by introducing zeros. The proposed method is combined with fully connected conditional random fields (CRF) and is able to produce semantically accurate predictions and detailed segmentation maps efficiently. Yu and Koltun <ref type="bibr" target="#b98">[98]</ref>, developed a convolutional neural network module design for dense prediction using dilated convolutions to combine multiscale contextual information without losing resolution and analyzing rescaled images for semantic segmentation. This module can be plugged into existing architectures at any resolution. Figure <ref type="figure" target="#fig_7">9</ref> shows an example of dilation convolution with different dilation rates, which define spacing between the values in a kernel. Treml et al. <ref type="bibr" target="#b102">[102]</ref> proposed an encoder decoder structured architecture (SQNet). The encoder is a modified SqueezeNet architecture <ref type="bibr" target="#b96">[96]</ref> so-called "Fire", consisting of convolutional and pooling layers. The decoder is based on parallel dilated convolution layer. Wu et al. <ref type="bibr" target="#b105">[105]</ref> present a fully convolutional residual network (FCRN), a new network for generating feature maps of any higher resolution, without changing the weights. They proposed a method to simulate a high resolution network with a low resolution network, and online bootstrapping method for training. In <ref type="bibr" target="#b99">[99]</ref>, Chen and his team proposed atrous spatial pyramid pooling (ASPP) module, consisting of multiple parallel atrous convolutional layers with different sampling rates to strongly segment objects at multiple scales. Figure <ref type="figure" target="#fig_9">10</ref> shows example of ASPP.</p><p>The proposed network is based on the state-of-art ResNet-101 <ref type="bibr" target="#b34">[34]</ref>    layers are replaced by dilation <ref type="bibr" target="#b98">[98]</ref> to increase the resolution. The subsampling removing means removing striding from some of the interior layers, increasing downstream resolution and reducing the receptive field in subsequent layers. They also propose an approach to remove the gridding artifacts introduced by dilation (degridding), which further improves the performance. Later, Chen et al. <ref type="bibr" target="#b100">[100]</ref> revisited atrous convolution and proposed a new system network called DeepLab V3. They designed new modules in which atrous convolution works in cascade or in parallel manner (spatial pyramid pooling as shown in Figure <ref type="figure" target="#fig_11">11</ref> (a)) to capture multi-scale context by adopting multiple atrous rates, and used batch normalization to train. Their main idea was to duplicate several copies of the last block in ResNet <ref type="bibr" target="#b34">[34]</ref> and arrange them in cascade manner. Wang et al. <ref type="bibr" target="#b103">[103]</ref> proposed a method named design dense upsampling convolution (DUC). The basic idea of DUC is to transform the label map into a smaller label map with multiple channels (dividing the label map into equal subparts having same height and width as the incoming feature map). They also proposed a hybrid dilated convolution (HDC) framework in the encoding phase that effectively enlarges the receptive fields of the network to aggregate global information. Recently in <ref type="bibr" target="#b101">[101]</ref> the DeepLab V3+, which is the extended version of DeepLab V3 was presented. Inspired by <ref type="bibr" target="#b107">[107]</ref>, the authors proposed a decoder module, in which the encoder features are upsampled by a factor of 4 instead of 16 as in <ref type="bibr" target="#b100">[100]</ref>, then are concatenated with the corresponding low-level features from network backbone having the same spatial resolution as shown in Figure <ref type="figure" target="#fig_11">11 (b)</ref>. They adopted the Xception model <ref type="bibr" target="#b31">[31]</ref> and applied depth-wise separable convolution (to reduce computation complexity) to both Atrous Spatial Pyramid Pooling (ASPP) and decoder modules.</p><p>Compared to regular convolution with larger filters, atrous convolution allows to effectively enlarging the field of view of filters without increasing the number of parameters or the amount of computation. Dilated convolution is a simple yet powerful alternative to deconvolutional in dense prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Enhancement of Features based Methods</head><p>Enhancement of feature based methods include extraction of feature at multi-scale or from a sequence of nested regions. In deep networks for semantic segmentation, CNNs are applied to image square patches, often called kernel of fixed size  centered at each pixel, labeling each pixel by observing small region around it. The network covering large and wide context (size of receptive field) is essential for better performance, which can be achieved but with increase the computational complexity. Multi-scale feature extraction or extraction from a sequence of nested region strategies can be taken in to account while ensuring computational efficiency. Table <ref type="table" target="#tab_6">7</ref> shows enhancement of features based network models. Alvarez et al. <ref type="bibr" target="#b107">[107]</ref> propose a network algorithm to learn local features at multi-scales and multi-resolutions using different kernel sizes. The features are fused using weighted linear combination (features of each class with different weight) learned at the same time (offline) directly from the training data. Farabet et al. <ref type="bibr" target="#b108">[108]</ref> proposed a method that extracts multiscale features vectors from the image pyramid (Laplacian pyramid version of the input image) using the multi-scale convolutional network shown in Figure <ref type="figure" target="#fig_12">12</ref>. Each feature vector encodes regions of multiple sizes centered on each pixel location, covering wide context. Couprie et al. <ref type="bibr" target="#b109">[109]</ref> adopted a similar approach, and proposed a convolutional network to learn multi-scale features using image depth information. Liu et al. <ref type="bibr" target="#b110">[110]</ref> proposed the strategy named Multi-scale Patch Aggregation (MPA). The proposed network generates multi-scale patches for object parsing, achieves segmentation and classification for each patch at the same time and aggregates them to infer objects. Hariharan et al. <ref type="bibr" target="#b111">[111]</ref> proposed a pixel classification method (multiple levels of abstraction and scale), Hypercolumn. The basic idea is to extract feature information from earlier layers and last layers of the CNN to allow precise localization and high semantics, and then resizing each feature map with bilinear interpolation. Further some or all of the features are concatenated into a single vector for every location.</p><p>Mostajabi et al. <ref type="bibr" target="#b117">[117]</ref> present a feedforward classification method named Zoom-Out using Superpixels (SLIC <ref type="bibr" target="#b118">[118]</ref>). It extracts features from different levels (local level: superpixel itself; distant level: regions large enough to cover fractions of object or entire object; scene level: entire scene) of spatial context around the superpixel to contribute to labeling decision at that superpixel. Then it computes feature representation at each level and combine all the features before feeding them to a classifier. Chen et al. <ref type="bibr" target="#b67">[67]</ref> proposed attention based model, with ability to choose each time, which part of the input to look at in order to perform the task. The proposed attention model learns to weight the multi-scale features according to the object scales presented in the image (e.g. the model learns to put large weights on features at a coarse scale for large objects). Then for each scale, the attention model outputs a weight map which weights features pixel by pixel, and the weighted sum of FCN-produced score maps across all scales is then used for classification.</p><p>Zhao et al. <ref type="bibr" target="#b112">[112]</ref> present pyramid scene parsing network (PSPNet) for semantic segmentation, which allows multi-scale feature ensembling. They have introduced the pyramid pooling module consisting of large kernel pooling layers shown in Figure <ref type="figure" target="#fig_12">12</ref>, which empirically proves to be an effective global contextual prior, containing information with different pyramid scales and varying among different sub-regions. It concatenates the feature maps with the up sampled output of parallel pooling layers. The idea is also called intermediate supervision. The representations are fed into a convolution layer to get the final per-pixel prediction. Figure <ref type="figure" target="#fig_13">13</ref> shows PSPNet Architecture. Vo and Lee <ref type="bibr" target="#b113">[113]</ref> proposed a deep network architecture with multi-scales dilated convolution layers to extract multi scale features from multi resolution input images. The basic idea consists of cascading dilated convolutions (consecutive layers connection), each layer, with a higher rate than the previous one, achieves denser feature maps. All feature maps are then sized to same resolution and fused into a Maxout layer <ref type="bibr" target="#b114">[114]</ref> to get most driven and leading features from all feature maps. Lin et al. <ref type="bibr" target="#b116">[116]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>ates contextual representations, large superpixels for low sceneresolution regions and finer super pixels for regions with higher scene-resolution. Recently, Ding et al. <ref type="bibr" target="#b115">[115]</ref> proposed a context contrasted local (CCL) model to obtain multi-scale features (both context and local). Instead of using simple sum, they proposed Gate-Sum fusion strategy to aggregate appropriate score maps, which allows a network to choose better and more desired scale of features.</p><p>Several methods aimed to capture multi-scale features, higherlayer feature contains more semantic meaning and less location information. Combining the advantages of multi-resolution images and multi-scale feature descriptors to extract both global and local information in an image without losing resolution improves the performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Semi and Weakly Supervised Concept</head><p>The CNN's are becoming deeper and deeper by increasing the depth and width (the number of levels of the network and the number of units at each level). Deep CNN requires large-scale dataset and massive computing power for training. Collecting labeled dataset manually is time consuming and requires enormous human efforts. To comfort these efforts, semi or weakly supervised methods are applied using deep learning techniques. Table <ref type="table" target="#tab_7">8</ref> shows semi and weakly supervised network models used for semantic segmentation.</p><p>Work by Pathak et al. <ref type="bibr" target="#b119">[119]</ref>] is to be the first considering the fine-tuning of CNN pre-trained for object recognition, using image-level labels, within a weakly supervised segmentation context. They introduced a fully convolutional network method, which relies on a Multiple Instance Learning (MIL-FCN) <ref type="bibr" target="#b138">[138]</ref>], i.e., learn pixel-level semantic segmentation from weak image level labels indicating the presence or absence of an object. They proposed a multi-class pixel-level loss inspired by the binary MIL scenario. Pinheiro et al. <ref type="bibr" target="#b120">[120]</ref> proposed a weakly supervised approach to produce pixel level labels from image-level labels using Log-Sum-Exp (LSE) <ref type="bibr" target="#b121">[121]</ref> method, which assigns the same weight to all pixels of the image during the training. Papandreou et al. <ref type="bibr" target="#b123">[123]</ref> presented a weakly and semi-supervised learning method using weak annotations, either alone or in combination with small number of strong annotations. They developed a method called Expectation Maximization (EM) for training DCNN from weakly annotated data. Hong et al. <ref type="bibr" target="#b122">[122]</ref> proposed a semi-supervised method (Decou-pledNet), which uses two separate networks, one for classification (classifies the object label) and the other for segmentation (to obtain figure-ground segmentation of each classified label). Dai et al. <ref type="bibr" target="#b135">[135]</ref> propose a method based on bounding box annotations (BoxSup). The unsupervised region proposal method (selective search <ref type="bibr" target="#b53">[53]</ref>) is used to generate segmentation masks, and these masks are used for training convolutional network. The proposed BoxSup model, trained with a large set of boxes, increases the object recognition accuracy (the accuracy in the middle of an object), and improves object boundaries. Khoreva et al. <ref type="bibr" target="#b139">[139]</ref> proposed a box-driven segmentation technique for semantic segmentation, which generates input labels for training from the bounding box annotations using Grab Cut-like al-gorithm <ref type="bibr" target="#b140">[140]</ref> without modifying the training procedure. Luo et al. <ref type="bibr" target="#b125">[125]</ref> present a weakly and semi-supervised dual image segmentation (DIS) learning strategy, which performs segmentation (capturing the accurate object classes), and reconstruction (accurate object shapes and boundaries). The idea is to predict tags, label maps from an input image, and perform reconstruction of images using predicted label maps.</p><p>Saleh et al. <ref type="bibr" target="#b129">[129]</ref> proposed weakly supervised segmentation network with built-in foreground/background prior. The main idea is to extract the localization information directly from the network itself (extracting foreground/background masks). Later in <ref type="bibr" target="#b130">[130]</ref>, they extended their work to obtain multi-class (class-specific) masks by the fusion of foreground / background ones with information extracted from a weakly supervised localization network inspired by <ref type="bibr" target="#b141">[141]</ref>. Saito et al. <ref type="bibr" target="#b131">[131]</ref> present a method that uses the feature maps extracted from a pre-trained dilated ResNet having built-in priors for semantic segmentation. They proposed a superpixel clustering method to generate road clusters (to select largest cluster at the bottom half of image), that are considered as the label to train CNN for segmentation. Barnes et al. <ref type="bibr" target="#b128">[128]</ref> develop a weakly supervised method for autonomous driving applications for generating a large amount of labelled images (from multiple sensors and data collected during driving) containing path proposals without any manual annotation. Ye et al. <ref type="bibr" target="#b134">[134]</ref> proposed a method for learning convolutional neural network models from images with three different types of annotations, i.e., image-level labels for classification, box-level labels for object detection and pixel-level labels for semantic segmentation. They proposed an annotation-specific loss module (with three branches, each branch with a different loss function), which is designed to train the network for each of the three different annotations.</p><p>Souly et al. <ref type="bibr" target="#b126">[126]</ref> proposed a semi-supervised semantic segmentation method using adversarial learning inspired by Generative Adversarial Networks (GANs) <ref type="bibr" target="#b142">[142]</ref>. Later, Hung et al. <ref type="bibr" target="#b154">[154]</ref> proposed a similar approach which consists of two sub nets; segmentation net (to generate class probability maps) and discriminator net (to generate spatial probability maps with both labeled and unlabeled data). Wei et al. <ref type="bibr" target="#b133">[133]</ref> presented a weakly and semi supervised approach by using multiple dilated convolutions. They proposed augmented classification network with multi-dilated convolutional (MDC) blocks that generate dense object localization maps, which are utilized for semantic segmentation in both weakly and semi supervised manner. Huang et al. <ref type="bibr" target="#b124">[124]</ref> proposed a weakly supervised network, which produces labels using the contextual information within an image. They proposed a seeded region growing module to find small and tiny discriminative regions from the object of interest using image labels to generate complete and precise pixel level labels, which are used to train semantic segmentation network. Wei et al. <ref type="bibr" target="#b143">[143]</ref> proposed a Simple to Complex (STC) network, a weakly supervised approach using image-level annotations. The basic idea is first to learn from simple images (to generate saliency maps using discriminative regional feature integration (DRFI)), and then apply learned network to the complex images (to generate pixel-level segmentation masks of complex images) for semantic segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCG-GrabCut+ [137]</head><p>A weakly supervised approach based on bounding box annotations Uses GrabCut+ Approach <ref type="bibr" target="#b132">[132]</ref>: to estimate object segment. Semi and weakly supervised learning aims to reduce the load for full annotation. These methods improved learning performance using weak annotations in the form of image-level labels (information about which object classes are present) and bounding boxes (coarse object locations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG + DeepLab</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC, MS COOC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.">Spatio-Temporal based Methods:</head><p>In this section, we aim to investigate the deep convolutional networks that use spatial information along with temporal information for semantic segmentation.</p><p>In a video, frames are associated with each other and have temporal information (i.e., features of continuous sequences of frames) that can be useful for interpreting a video semantically. Spatio-temporal structured prediction can prove useful in both supervised and semi-supervised manner. Table <ref type="table" target="#tab_8">9</ref> shows Spatio-Temporal based network models for semantic segmentation.</p><p>Several methods are proposed in the combination of Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) for video segmentation. Fayyaz et al. <ref type="bibr" target="#b145">[145]</ref> presented a full convolutional network Spatio-Temporal Fully Convolutional Network (STFCN) employing spatial and temporal features. They proposed spatio-temporal module that takes the advantage of LSTM in order to define temporal features. The spatial feature maps of the region in single frame fed into LSTM, infers a relation with spatial features of equivalent regions in frames before that frame. Further, spatial and temporal information fed into dilated convolution network ( <ref type="bibr" target="#b98">[98]</ref> with minor modifications) for upsampling and are fused (summing operation) for semantic predictions. He et al. <ref type="bibr" target="#b146">[146]</ref> proposed Spatiotemporal data-driven pooling model (STD2P), which is method to integrate multi-view information by using super pixels and optical flow. The goal of multi-view semantic segmentation is to make use of the potentially richer information from different views with better segmentations than single view. Qiu et al. <ref type="bibr" target="#b148">[148]</ref> proposed 2D/3D FCNs based architectural model named deep spatio-temporal ful-ly convolutional networks (DST-FCN), that utilizes spatial and temporal dependencies among pixels and voxels. The proposed architecture is a two stream network, Sequential frame stream, (2DFCN for spatial and ConvLSTM for temporal information), and clip stream, (3DFCN based on C3D <ref type="bibr" target="#b152">[152]</ref> developed on voxel level). Pavel et al. <ref type="bibr" target="#b153">[153]</ref> present a recurrent convolutional neural network model utilizing spatial and temporal information for processing image sequences. Yurdakul et al. <ref type="bibr" target="#b154">[154]</ref> proposed a network that combines color and depth information in RGBD videos for semantic segmentation using convolutional and recurrent neural network frameworks.</p><p>Some architectures are based on Gated Recurrent Architectures to overcome gradients problem. Ballas et al. <ref type="bibr" target="#b155">[155]</ref> used a term percepts (visual representations extracted from different levels of DCN) to capture spatial-temporal feature information in the video using gated-recurrent-unit recurrent networks. Siam et al. <ref type="bibr" target="#b149">[149]</ref> present a fully convolutional network based on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T gated recurrent architecture (RFCN). Three different architectures were used following two approaches, conventional recurrent units (RFCLeNet) and convolutional recurrent units (RFC VGG, RFC Dilated), learning spatio-temporal features with less number of parameters. Nilsson et al. <ref type="bibr" target="#b151">[151]</ref> present Gated Recurrent Flow Propagation network. They proposed Spatio Temporal Transformer Gated Recurrent Unit (STGRU), combining the strength of spatial transformer (for optical flow warping) with convolutional gated architecture (to adaptively propagate and fuse estimates). Shelhamer et al. <ref type="bibr" target="#b144">[144]</ref> proposed a network named Clockworks, which is a combination of FCN and clockwork recurrent network <ref type="bibr" target="#b156">[156]</ref>, grouping the layers of the network into stages with different rates (either fixed clock rate or adaptive clock) and then fusing them via skip connections. Saleh et al. <ref type="bibr" target="#b150">[150]</ref> proposed a weakly supervised framework for video semantic segmentation that treats both foreground and background classes equally. The basic idea is to manage multiple foreground objects and multiple background objects equally. They propose an approach to extract class-specific heatmaps from classifier that localizes the different classes for both without pixel level or bounding box annotations. Kundu et al. <ref type="bibr" target="#b147">[147]</ref> proposed a model to optimize the feature space used by the fully connected conditional random field for spatiotemporal regularization. Chandra et al. <ref type="bibr" target="#b157">[157]</ref> proposed a Video Gaussian Conditional Random Field approach for spatio-temporal structured prediction, which is an extension of <ref type="bibr" target="#b158">[158]</ref>. The FCN network obtains unary (class score per-pixel), spatial pairwise and temporal pairwise terms, which are fed into G-CRF module that performs inference (linear system) to obtain the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9.">Methods using CRF / MRF:</head><p>Semantic segmentation involves pixelwise classification and such pixelwise classification often produces unsatisfactory results (poor, incorrect and noisy predictions) that are irreconcilable with the actual visual features of the image <ref type="bibr" target="#b159">[159]</ref>.</p><p>Markov random field (MRF) and its variant Conditional Random Fields are classical frameworks that are widely used to overcome these issues. They express both unary term (per-pixel confidence of assigning labels) and pairwise terms (constraints between adjacent pixels). CNNs can be trained to model unary and pairwise potentials in order to capture contextual information. The context provides important information for scene understanding tasks such as spatial context which provides semantic compatibility/incompatibility relation between objects, scenes and situations. CRFs can be a post processing or endto-end, to smooth and refine the pixel prediction in semantic segmentation. They combine class scores from classifiers with the information captured by the local interactions of pixels and edges or superpixels. Table <ref type="table" target="#tab_9">10</ref> shows network models using CRF.</p><p>Krahenbuhl et al. <ref type="bibr" target="#b160">[160]</ref> proposed a fully connected CRF (DenseCRF) model, in which pairwise edge potentials are defined by a linear combination of Gaussian kernels. The method is based on mean field approximation, message passing is performed using Gaussian filtering techniques <ref type="bibr" target="#b161">[161]</ref>. Methods <ref type="bibr" target="#b79">[79,</ref><ref type="bibr" target="#b97">97,</ref><ref type="bibr" target="#b123">123,</ref><ref type="bibr" target="#b135">135,</ref><ref type="bibr" target="#b129">129,</ref><ref type="bibr" target="#b139">139,</ref><ref type="bibr" target="#b133">133,</ref><ref type="bibr" target="#b150">150]</ref> coupled fully connected CRF with their proposed DCNNs to produce accurate predictions and detailed segmentation maps for improving performance. Zheng et al. <ref type="bibr" target="#b162">[162]</ref> formulate mean-field inference algorithm for the dense CRF with Gaussian filtering technique as recurrent neural network (CRF-RNN), that performs CRF-based probabilistic graphical modelling for structured prediction. Figure <ref type="figure" target="#fig_15">14</ref> shows CRF as RNN. Vemulapalli et al. <ref type="bibr" target="#b163">[163]</ref> proposed a model named Gaussian Mean Field (GMF) network that models unary potentials, pairwise potentials and Gaussian CRF inference for the task of semantic segmentation. In the proposed network, output of each of the layer is closer to maximum a posteriori probability (MAP) estimated to its input. Chandra et al. <ref type="bibr" target="#b158">[158]</ref> proposed a Gaussian Conditional Random Field (G-CRF) module using a quadratic energy function that captures unary and pairwise interactions. Lin et al. <ref type="bibr" target="#b169">[169]</ref> propose a model Context CNN CRF jointly learning CNN and CRFs. They formulate CRF with CNN pairwise potential to capture contextual relationship between neighboring patches and sliding pyramid pooling (multi-scale image network input) for capturing patchbackground context that can be combined to improve the segmentation. Instead of learning the potentials, <ref type="bibr" target="#b168">[168]</ref> proposes a method that learns CNN message estimators for the message passing inference for structured Conditional Random Field (CRFs) predictions. Teichmann et al. <ref type="bibr" target="#b164">[164]</ref> proposed convolutional CRFs (ConvCRFs) method that reformulates the message passing inference in terms of convolutions.</p><p>Some methods employed higher-order potentials (based on object detection or superpixels) modelled as CNN layers when using mean field inference and effective in improving semantic segmentation performance. Arnab et al. <ref type="bibr" target="#b165">[165]</ref> proposed a method in which CRF models unary and pairwise potentials together with high-order potentials object detector (to provide semantic cues for segmentation) and superpixel (having label consistency over regions) in an end-to-end trainable CNN. Shen et al. <ref type="bibr" target="#b166">[166]</ref> proposed joint FCN and CRF model (SegModel) that integrates segmentation specified features, which constitutes high order context and boundary guidance (bilateral-filtering based CRF) for semantic segmentation. Liu et al. <ref type="bibr" target="#b167">[167]</ref>     and pairwise terms (i.e., high-order relations and mixture of label contexts) in single CNN that achieve high performance by extending the VGG network, and adding some layers for modeling pairwise terms. Jiang et al. <ref type="bibr" target="#b170">[170]</ref> utilize the depth information as complementary information into conditional random fields. They proposed depth sensitive fully connected conditional random field combined with a fully convolutional network, (DFCN-DCRF). The basic idea is to add the depth information into dilated-FCN and fully connected CRF to improve accuracy for semantic segmentation.</p><p>CRF inference with deep convolutional neural network improves pixel-level label predictions by producing sharp boundaries and dense segmentation. Several methods learn arbitrary potentials in CRFs. It has been used as post processing, end-to-end fashion, formulated as RNN and incorporated as module in existing neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10.">Alternative to CRF:</head><p>Integrating conditional random field into original architecture is a difficult task due to additional parameters and highly computational complexity at training. Moreover, the majority of CRFs uses hand constructed color-based affinities that may lead to spatial false predictions. Several methods have been proposed to overcome these issues and can be used as alternate to CRFs. Table <ref type="table" target="#tab_10">11</ref> shows network models alternate to CRFs.</p><p>Bertasius et al. <ref type="bibr" target="#b173">[173]</ref> proposed a FCN architecture named Boundary Neural Field (BNF) to predict semantic boundaries and produce semantic segmentation maps using global optimization. The BNF combines the unary potentials (prediction by FCN) and pairwise potentials (boundary-based pixel affinities) from the input RGB image in a global manner. The basic idea is to assign pixels to the foreground and background labels for each of the different object classes and apply constraint relaxation. Later in <ref type="bibr" target="#b176">[176]</ref>, they proposed Convolutional Random Walk Network (RWN) addressing same issue, model based on random walk method <ref type="bibr" target="#b177">[177]</ref>. The network model predicts semantic segmentation potentials and pixel level affinities, and combines them through proposed random walk layer that applies spatial smoothing predictions.</p><p>Jampani et al. <ref type="bibr" target="#b171">[171]</ref> propose a network based on Gaussian bilateral filter <ref type="bibr" target="#b178">[178]</ref>, named bilateral neural network (BNN). Bilateral filter inference in fully connected CRF <ref type="bibr" target="#b160">[160]</ref> (by replacing Gaussian potentials with bilateral convolution) to learn pairwise potentials of fully connected CRF. Barron et al. <ref type="bibr" target="#b172">[172]</ref> propose edge-aware smoothness algorithm using bilateral filtering technique name the bilateral solver. Peng et al. <ref type="bibr" target="#b175">[175]</ref> proposed a residual-based boundary refinement model, Global Convolutional network (GCN), for semantic segmentation. They proposed boundary refinement block (FCN structure without fully connected and global pooling layers) to model the boundary alignment as a residual structure. Chen et al. <ref type="bibr" target="#b174">[174]</ref> proposed a model with domain transform (DT) module as a substitute to CRF, an edge preserving filtering method. The model consists of three modules. The first module produces semantic segmentation score prediction based on DeepLab. The second module named Edge Net, predicts edge features from midway layers and the third module is an edge-preserving filter named Domain Transform (recursive filtering), proposed in <ref type="bibr" target="#b179">[179]</ref>.</p><p>Several methods have been proposed that can be used as alternative to CRF with the advantage of fast and less number of parameters. Bilateral filtering techniques can be useful tool in the construction of deep learning frameworks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets and Evaluation for Deep Learning techniques</head><p>One of the hardest problem for any segmentation systems based on deep learning techniques is the collection of data in order to construct suitable dataset. There are four possible ways to get labeled data as shown in Figure <ref type="figure" target="#fig_19">16</ref>. Traditional Supervision: hand label data; Weak supervision: obtained automatically without human annotators using unlabeled data; Semisupervised learning: partially labeled and partially unlabeled data, and transfer learning: using pre-trained model as a start point. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets:</head><p>The dataset acts as the benchmark against which deep learning networks are trained and tested. Several datasets has been constructed over the last few years that are used in deep learning, motivating researchers to create new models and strategies with better generalization abilities. These datasets can be categorized according to the nature of data.</p><p>The automotive datasets includes; CamVid dataset <ref type="bibr" target="#b180">[180]</ref> which is considered as the first with semantically annotated videos, Daimler Urban Segmentation <ref type="bibr" target="#b181">[181]</ref>, CityScapes <ref type="bibr" target="#b182">[182]</ref>, Mapillary Vistas <ref type="bibr" target="#b183">[183]</ref> and the most recent Apolloscape-Scene parsing <ref type="bibr" target="#b184">[184]</ref> which focuses on semantic understanding of urban street scenes. The KITTI <ref type="bibr" target="#b185">[185]</ref> dataset used in various computer vision tasks such as 2D/3D object detection, stereo, optical flow, and tracking. Synthetic datasets <ref type="bibr" target="#b186">[186]</ref> [187] consist of a thousand images extracted from realistic open-world games.</p><p>Data sets generic in nature; PASCAL VOC <ref type="bibr" target="#b188">[188]</ref> is one of the most popular and widely used dataset in deep learning semantic segmentation, CIFAR-10/100 <ref type="bibr" target="#b189">[189]</ref> contains up to 60,000 images, offering 10 and 100 categories of tiny 32 × 32 images. A remarkable ImageNet <ref type="bibr" target="#b190">[190]</ref> dataset contains over 14 million labeled images, SegTrack v2 <ref type="bibr" target="#b191">[191]</ref> is a video segmentation dataset with annotations on multiple objects at each frame, and PASCAL Context <ref type="bibr" target="#b192">[192]</ref> is a set of additional annotations for PASCAL VOC. Microsoft-COCO <ref type="bibr" target="#b193">[193]</ref> is a collection of images of complex everyday scenes contains common natural objects, ADE20K <ref type="bibr" target="#b194">[194]</ref> containing both indoor and outdoor images with large variations, and DAVIS <ref type="bibr" target="#b195">[195]</ref> dataset containing densely annotated videos with pixel accurate ground truth. Recently developed COCO stuff <ref type="bibr" target="#b196">[196]</ref> dataset augments the original COCO dataset with much more comprehensive stuff annotations.</p><p>Indoor environment datasets; NYUDv2 <ref type="bibr" target="#b197">[197]</ref> is composed of RGB-D images and video sequences from a variety of indoor scenes, Cornell RGB-D <ref type="bibr" target="#b198">[198]</ref> contains labeled office and home scene point clouds, ScanNet <ref type="bibr" target="#b199">[199]</ref> comprises more than 1500 scenes annotated with 3D camera pose, surface reconstructions, and semantic segmentations. Stanford 2D-3D <ref type="bibr" target="#b200">[200]</ref> contains mutually registered modalities from 2D/3D domains, with 71,882 RGB images (both regular and 360 • ), along with the corresponding depths, surface normal and semantic annotations. SUN 3D <ref type="bibr" target="#b201">[201]</ref> and SUN RGB-D <ref type="bibr" target="#b202">[202]</ref> datasets contain videos of big spaces for place-centric scene understanding.</p><p>Object datasets; RGB-D Object v2 <ref type="bibr" target="#b203">[203]</ref> containing 25000 images of common household objects in 51 categories, YouTube Dataset <ref type="bibr" target="#b204">[204]</ref> comprises 126 videos.</p><p>Datasets for outdoor environment; Microsoft Cambridge <ref type="bibr" target="#b205">[205]</ref> consists of 591 real outdoor scene photographs of 21 object classes; Graz-02 <ref type="bibr" target="#b206">[206]</ref> is a natural-scene object category dataset created at INRIA. LabelMe <ref type="bibr" target="#b207">[207]</ref> contains outdoor images of 8 different classes that are taken in different cities of Spain; Barcelona dataset <ref type="bibr" target="#b208">[208]</ref> is a subset of LabelMe; Stanford-background <ref type="bibr" target="#b209">[209]</ref> and PASCAL SBD <ref type="bibr" target="#b210">[210]</ref> are collected from PASCAL VOC; Sift-flow <ref type="bibr" target="#b211">[211]</ref> consists of 2688 images of 256 × 256 pixels and 33 classes, and Freiburg Forest <ref type="bibr" target="#b212">[212]</ref> constitute on outdoor forest environment in different condition lighting, shadows and sun angles. The dataset construction is both time consuming and labor intensive, so for the researchers and developers the most practical and workable approach is to use existing standard datasets which are representative enough for the domain of the problem. Some datasets have become standard and commonly used by researchers to compare their work with others using standard metric for evaluation. Dataset selection at a start of research is challenging task, therefore the comprehensive description on dataset can help. In Table <ref type="table" target="#tab_11">12</ref>, we list the datasets used by deep learning networks that are publicly available. Are given different information such as environment nature, the number of classes, training/testing samples, image resolution, year of construction, and best performances achieved till date (to the best of our knowledge) by the models for semantic segmentation. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b198">198,</ref><ref type="bibr" target="#b203">203]</ref> datasets are not accessed for semantics, but they can be used for semantic segmentation. <ref type="bibr" target="#b184">[184,</ref><ref type="bibr" target="#b199">199]</ref> datasets are not evaluated at all. All these datasets provide appropriate pixel-wise or point-wise labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation:</head><p>We describe commonly used evaluation metrics for semantic segmentation. The overall performance of the semantic segmentation systems can be assessed in terms of accuracy, time,  A C C E P T E D M A N U S C R I P T memory, and power consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy:</head><p>The accuracy of the semantic segmentation system is measure of the correctness of the segmentation or is the ratio of the correctly segmented area over the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel wise Accuracy:</head><p>The ratio between the amount of correctly classified pixels and the total number of them. Confusion matrix terminology is used to describe the performance of a classification model.</p><p>Let N cls be the number of classes, N xy is the number of pixels which belong to class x and were labeled as class y. The confusion matrix reports the number of false positives (N xy ), false negatives (N yx ), true positives (N xx ), and true negatives (N yy ).</p><formula xml:id="formula_2">PixelAccuracy = Σ N cls x=1 N xx Σ N cls x=1 Σ N cls y=1 N xy (1)</formula><p>The pixel-wise classification accuracy is not reliable for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (i.e., large regions which have one class or labeled images could have a more coarse labeling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Accuracy:</head><p>The ratio of correct pixels is calculated in per-class basis and then averaged over the total number of classes N cls .</p><formula xml:id="formula_3">MeanAccuracy = 1 N cls Σ N cls x=1 N xx Σ N cls y=1 N xy<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Intersection over Union (MIoU):</head><p>The ratio between the numbers of true positives N xx , (intersection) over the sum of true positives N xx , false negatives N yx , false positives N xy (union). Intersection over Union is computed on a per-class basis and then averaged.</p><formula xml:id="formula_4">MIoU = 1 N cls Σ N cls x=1 N xx Σ N cls y=1 N xy + Σ N cls y=1 N yx -N xx<label>(3)</label></formula><p>The most widely used accuracy measuring strategy is MIoU, due to its easiness and simplicity.</p><p>Frequency Weighted Intersection over Union (FWIoU)</p><formula xml:id="formula_5">FWIoU = 1 Σ N cls x=1 Σ N cls y=1 N yx Σ N cls x=1 Σ N cls y=1 N xy N xx Σ N cls y=1 N xy + Σ N cls y=1 N yx -N xx<label>(4)</label></formula><p>Precision: The relation between true positives N xx , and all elements classified as positives <ref type="bibr" target="#b4">(5)</ref> Recall: measures how good all the positives are found.</p><formula xml:id="formula_6">Precision = N xx N xx + N xy</formula><formula xml:id="formula_7">Recall = N xx N xx + N yx (6)</formula><p>Average Precision: Mean precision at a set of eleven equal space recall levels (0.0, 0.1, 0.2 . . . , 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Average Precision:</head><p>Mean of all the Average Precision values across all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time, Memory and Power:</head><p>The memory and processing time of the system is highly dependent on hardware and the back-end implementation. The usage of hardware accelerators GPUs makes the processing time of these system very fast, however it consumes much of the memory and power. Most of the methods do not provide information, regarding time, memory and hardware, which is very crucial as these network models may be applied in (mobile systems, robotics, autonomous driving etc) where with limited power and memory, extremely accurate image segmentation would be required. Furthermore, these information can help researchers to estimate, make comparisons or choose methods depending on the application and requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ANALYSIS &amp; DISCUSSION</head><p>We analyze some of the network models on the bases of their performance on datasets and their design structure to find out the reasons for their accomplishments. It is difficult to compare these methods due to the majority of them has been evaluated on very few datasets. Some methods used different metrics and also lack information about experimental setup (hardware, time, memory).</p><p>AdapNet <ref type="bibr" target="#b191">[191]</ref>: The model is an adapted version of <ref type="bibr" target="#b69">[69]</ref>, replacing the pooling and convolutional layers of conv4/conv5 with two dilated convolution layers with dilation factors of 2 and 4 respectively. This leads to a decrease in the size of the network and its running time for real-time applications.</p><p>ResNet DUC+HDC <ref type="bibr" target="#b103">[103]</ref>:</p><p>• Achieves a score of It can be noticed, that those methods which achieved the high performance results, are doing so due to the availability of large amount of labeled data. Extra training data is beneficial for increasing the accuracy of the model; several models used large datasets (merging two or three datasets) during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">OPEN PROBLEMS AND CHALLENGES</head><p>In this section, we discuss some of the open problems and their possible solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Open Problems and Possible Solutions</head><p>Techniques for semantic segmentation using deep neural networks (DNNs) are rapidly growing and the following problems are still needed to be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Reducing Complexity &amp; Computation:</head><p>The deep neural networks are not much suitable to be deployed on mobile platforms (e.g. embedded devices) that have limited resources because, DNN are highly memory demanding, time and power consuming. There is also problem with computational complexity that arises due to a great number of operations needed for inference. It is important to investigate how to reduce the complexity of the model to achieve high efficiency without loss of accuracy. Some CNN compression approaches have been proposed to deal with reducing complexity and computational cost. Wang et al. <ref type="bibr" target="#b217">[217]</ref> proposed a method to excavate and decrease the redundancy in feature maps extracted from large number of filters in each layer of network. Kim et al. <ref type="bibr" target="#b218">[218]</ref> proposed a one-shot whole network compression approach, that consists of three steps: Rank selection, Low-rank tensor decomposition, and fine tuning. Andrew et al. <ref type="bibr" target="#b219">[219]</ref> applied model compression techniques to the problem of semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Caffe2 is a portable deep learning framework by Facebook, capable of training large models and allows to build machine learning applications for mobile systems. Compressing and accelerating DNN achieved lots of progress.</p><p>However, there are some potential issues like; compression may cause loss in accuracy; decomposition operation; transfer information to convolutional filters is not suitable on some networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Apply to Adverse Conditions:</head><p>There have been a few of network models which are applied in real challenging environmental conditions or to handle adverse conditions such as direct lighting, reflections from specular surfaces, varying seasons, fog or rain.</p><p>Although, some CNN models used synthetic data together with real data to boost the performance of state-of-the-art methods for semantic segmentation on the challenging environmental conditions. However, using huge amounts of high-quality data from the real world so far remains indispensable. One possible solution is to use synthetic data with the real data. Apparently there are significant visual differences between the two data domains and to narrow this gap, domain adaptation technique may be used. Hoffman et al. <ref type="bibr" target="#b220">[220]</ref> proposed an unsupervised domain adaptation method for transferring semantic segmentation FCNs across image domains. Yang et al. <ref type="bibr" target="#b221">[221]</ref> proposed a curriculum-style learning approach to minimize the domain gap. The authors in <ref type="bibr" target="#b222">[222]</ref> proposed a domain Shift approach based on Generative Adversarial Network (GAN), which transfers the information of the target distribution to the learned embedding using a generator-discriminator pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Need large and high quality labeled data:</head><p>The classification performance of DNNs and dataset size are positively correlated. Current state-of-the-art methods require high quality labeled data, which is not available on large-scale as they are time consuming and labour exhaustive. The effective solution to this problem would be to build large and high quality datasets, which seems hard to achieve. Therefore, the researchers rely on semi and weakly supervised methods making DNNs less reliant on the labeling of large datasets. These methods has considerably improved the semantic segmentation performance by using additional weak annotations either alone or in combination with a small number of strong annotations. However, they are far from fully supervised learning methods in terms of accuracy. Thus, this opens new challenges for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Overfitting:</head><p>As mentioned before, DNNs are data hungry and they do not perform well unless they are fed with large datasets. Majority of the available datasets are relatively small, so as DNN models become very complex to capture all the useful information necessary to solve a problem. The model may run risk of "Overfitting" with limited amount of data. Overfitting occurs when the gap between the training error and test error is too large. Regularization techniques help in overcoming this problem. Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error <ref type="bibr" target="#b60">[60]</ref>. Several of these methods are applied in DNNs to prevent overfitting such as L1 and L2 regularization, Lp norm, dropout, dropConnect. Data Augmentation is also used to reduce overfitting (e.g. increasing the size of the training data -image rotating, flipping, scaling, shifting). However, the regularization may increase training time (e.g. using dropout increases the training time by 2x or 3x than a standard neural network of the same architecture) and there is no standard for regularizing CNNs. Introducing better or improved regularization method would be an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Segmentation in Real-time:</head><p>Real-time semantic segmentation without loosing to much accuracy is of great importance, as it can be useful in autonomous driving, robot interaction, mobile computing where running time is critical to evaluate the performance of the system. DNN methods for semantic segmentation are more focused on accuracy then speed. Majority of the methods are far away from real-time segmentation. One possible solution to the problem could be performing convolutions in an efficient manner. Several works have aimed at developing efficient architectures that can run in real-time based on convolution factorization (disintegrate convolutional operation into multiple steps). Some computationally efficient modules for convolution are introduced. For example, Inception <ref type="bibr" target="#b27">[27]</ref>, Xception <ref type="bibr" target="#b31">[31]</ref>,</p><p>ResNet <ref type="bibr" target="#b34">[34]</ref>, ASP <ref type="bibr" target="#b99">[99]</ref>, ESP <ref type="bibr" target="#b47">[47]</ref>; ShuffleNet <ref type="bibr" target="#b223">[223]</ref> and MobileNet <ref type="bibr" target="#b224">[224]</ref>, are using grouped and depth-wise convolutions. Another possible solution could be to apply network compression using different techniques (e.g. parameter pruning and sharing <ref type="bibr" target="#b225">[225]</ref>, low-rank factorization and sparsity <ref type="bibr" target="#b226">[226]</ref> etc) to reduce the size of the network. However, real-time semantic segmentation still lacks higher accuracy and new methods and approaches must be developed to work-out between runtime and accuracy.</p><p>6. Video / 3D Segmentation: DNNs have been successfully applied for semantic segmentation of 2D images while not much for 3D images and on videos despite their significance. Several video and 3D network models for semantic segmentation have been proposed over the years and progress has been made but some challenges still exist. The lack of large datasets of 3D images and sequence images (videos) make it difficult to progress on 3D and video semantic segmentation. 3D networks are computationally expensive when dealing with high resolution and complex scenes (large number of classes). In 3D semantic segmentation task, using 3D Point cloud information is very effective. Zhang et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T <ref type="bibr" target="#b227">[227]</ref> proposed an efficient large-scale point cloud segmentation method, in which 2D images with 3D point clouds are fused into CNN to segment complex 3D urban scenes. The authors in <ref type="bibr" target="#b228">[228,</ref><ref type="bibr" target="#b229">229]</ref> proposed methods for direct semantic labeling of 3D pointclouds with spectral information. However, 3D segmentation methods face many challenges as compared to 2D segmentation, i.e., High complexity, computational cost, slow processing and most important lack of 3D datasets. In video semantic segmentation, two approaches can be useful, one to improve computational cost (by reducing latency);</p><p>The authors in <ref type="bibr" target="#b144">[144,</ref><ref type="bibr" target="#b230">230]</ref> proposed designed schedule frameworks which reduce the overall cost and maximum latency of video semantic segmentation. However, these approaches are far away to meet the latency requirements in real-time applications. The second approach is to improve accuracy (by exploiting temporal continuity -temporal features and temporal correlations between video frames). Several methods <ref type="bibr" target="#b145">[145,</ref><ref type="bibr" target="#b146">146,</ref><ref type="bibr" target="#b148">148]</ref> have been proposed using temporal information with spatial information for increasing the accuracy of pixel labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we have provided a comprehensive survey of deep learning techniques used for semantic segmentation.</p><p>The surveyed methods have been categorized in ten classes, according to the common concept underlaying their architectures. We have also provided a summary of these methods stating, for each of them, the main idea, the origin of its architecture, testing benchmarks, code availability and the year of publication.</p><p>Thirty five datasets on which these methods have been applied, have been reported and described in details showing their environment nature, number of classes , resolution, number of the images and the method which achieved the best performance on each till date to the best of our knowledge.</p><p>We have mainly analyzed the design and performance of some of these methods which reported that had achieved high scores. The goal was to find out how they do so.</p><p>We have also discussed some of the open problems and tried to suggest some of possible solutions.</p><p>This survey had shown that there is much scope of improvement in terms of accuracy, speed and complexity. So, our future work, would be to take some of these methods and develop a new one by enhancement of the weaknesses and/or combination of the merits. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Object Detection / Bounding Box and Semantic Segmentation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Inception module</figDesc><graphic coords="4,316.04,206.94,208.06,83.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Residual Learning: A building block</figDesc><graphic coords="5,368.05,258.08,104.02,182.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 shows</head><label>7</label><figDesc>Reseg network architecture. Shuai et al. [69] use graphical RNNs (Directed Acyclic Graph-Recurrent Neural Network or DAG-RNN) to model longrange contextual dependencies of local features in the image for semantic segmentation. They proposed a new class weighting function in order to improve the accuracy for recognition of non-frequent classes. Inspired by DenseNet [71], Fan and A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ReSeg Network [64]</figDesc><graphic coords="10,28.79,75.23,244.47,55.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: FCN: Segmentation Network [77]</figDesc><graphic coords="10,329.04,183.02,182.05,105.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Dilated convolution with size of 3 × 3 with different dilation rates. (a) dilation rate = 1, receptive field = 3 × 3 (b) dilation rate = 2, receptive field = 7 × 7.</figDesc><graphic coords="12,323.84,326.51,192.45,121.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>image classification DCNN. They combine the network with a fully connected Conditional Random Field (CRF) in order to improve the localization of object boundaries. Yu and Koltun[104] present another deep neural network named Dilated Residual Network (DRN), a residual network ResNet [34] like architecture, in which subset of interior subsamples A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Atrous Spatial Pyramid Pooling (ASPP)<ref type="bibr" target="#b99">[99]</ref> </figDesc><graphic coords="13,39.20,433.22,223.66,84.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: DeepLabV3 and DeepLabV3+ [101]</figDesc><graphic coords="14,47.00,75.23,208.06,119.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Multiscale CNN for scene parsing<ref type="bibr" target="#b108">[108]</ref> </figDesc><graphic coords="14,36.59,470.91,228.87,114.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Pyramid Scene Parsing Network (PSPNet)<ref type="bibr" target="#b112">[112]</ref> </figDesc><graphic coords="14,300.43,470.00,239.27,87.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>proposed a network called cascaded feature network (CFN). It utilizes depth information, dividing the image into layers representing visual characteristic of objects and scenes (multi-scene resolutions). Proposing context-aware receptive field CaRF (superpixel based), aggregates convolutional features of local context into strong features. The CaRF gener-A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure14: CRF as a recurrent Neural Network<ref type="bibr" target="#b162">[162]</ref> </figDesc><graphic coords="19,305.63,168.92,228.87,160.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>proposed Deep Parsing Network (DPN), which models unary term A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>FCN</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 Figure 15 :</head><label>1515</label><figDesc>Figure<ref type="bibr" target="#b14">15</ref> gives an overview to the readers to have good understanding of the categorization of different methods for semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Getting Label Data</figDesc><graphic coords="23,52.20,215.91,197.66,192.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>GoogLeNet Modules</figDesc><table><row><cell>Model</cell><cell>Corpus</cell><cell cols="2">Original Architecture Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row><row><cell></cell><cell>Inception module: Bottleneck [27]</cell><cell>NIN</cell><cell>ImageNet</cell><cell>September 17, 2014</cell><cell>YES</cell></row><row><cell></cell><cell>Batch Normalization Modified BN-Inception [28]</cell><cell>Inception</cell><cell>ImageNet</cell><cell>March 2, 2015</cell><cell>YES</cell></row><row><cell></cell><cell>Inception V2, V3 [29]</cell><cell>BN-Inception</cell><cell>ImageNet</cell><cell>December 11, 2015</cell><cell>YES</cell></row><row><cell></cell><cell>Inception V4 and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GoogLeNet</cell><cell>Inception-ResNet-v1, 2 Combining the Inception architecture with Residual</cell><cell>Inception V3 ResNet</cell><cell>ImageNet</cell><cell>August 23, 2016</cell><cell>YES</cell></row><row><cell></cell><cell>connections [30]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Xception [31]</cell><cell>Inception V3</cell><cell>ImageNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Depthwise Separable</cell><cell></cell><cell>JFT (Google's)</cell><cell>April 4, 2017</cell><cell>YES</cell></row><row><cell></cell><cell>Convolutions [32]</cell><cell>ResNet</cell><cell>FastEval14k</cell><cell></cell><cell></cell></row></table><note><p>Figure 3: VGG-16 Layer Structure</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Feature Encoder based Methods</head><label>2</label><figDesc></figDesc><table><row><cell>Category</cell><cell></cell><cell cols="3">Strategy / Structure</cell><cell>Corpus</cell><cell>Original Architecture</cell><cell>Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Convolutional Networks</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ConvNets) Used much</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Visual Geometry Group Network</cell><cell>(VGGNet) [33]</cell><cell>smaller 3 × 3 filters in each convolutional layers which match the</cell><cell>AlexNet</cell><cell>ImageNet, PASCAL VOC</cell><cell>April 10, 2015</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>effect of larger receptive</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>R E S</cell><cell></cell><cell></cell><cell>Residual Network (ResNet) [34]</cell><cell>fields e.g. 5 × 5 and 7 × 7. Bottleneck Approach Shortcut Connections Layer Perceptions) are added (MLPs-Multi</cell><cell>VGG</cell><cell>ImageNet, Cityscapes, PASCAL VOC COCO, CIFAR-10,</cell><cell>December 10,</cell><cell>YES</cell></row><row><cell></cell><cell>I</cell><cell></cell><cell></cell><cell></cell><cell>(Shallow Network)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>D I U</cell><cell></cell><cell></cell><cell>ResNet-38 [38]</cell><cell>ReNet for Image classification FCN for semantic image</cell><cell>ResNet + FCN</cell><cell>Cityscapes, PASCAL VOC ADE20K,</cell><cell>November 30,</cell><cell>YES</cell></row><row><cell></cell><cell>A</cell><cell></cell><cell></cell><cell></cell><cell>segmentation.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>L</cell><cell></cell><cell></cell><cell></cell><cell>Combining the strength</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>of FC-ResNet: gradient</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fully Convolutional</cell><cell>flow and iterative</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>.</cell><cell></cell><cell></cell><cell>Dense ResNet</cell><cell>refinement.</cell><cell>ResNet</cell><cell>CamVid</cell><cell>April 30, 2018</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(FC-DRN) [39]</cell><cell>FC-DenseNet: Multi-Scale</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>feature representation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and deep supervision).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Feature Encoder</cell><cell>L</cell><cell></cell><cell></cell><cell>Adaptive Network (AdapNet) [40]</cell><cell>Convoluted Mixture (CMoDE) fusion scheme. of Deep Experts</cell><cell>ResNet</cell><cell>Cityscapes, Freiburg forest Synthia,</cell><cell>May 29, 2017</cell><cell>-</cell></row><row><cell></cell><cell>E A R N</cell><cell></cell><cell></cell><cell>ResNeXt [41]</cell><cell>Hyper-parameter "Cardinality" models capacity. a new way to adjust</cell><cell>ResNet</cell><cell>ImageNet, CIFAR COCO,</cell><cell>April 11, 2017</cell><cell>YES</cell></row><row><cell></cell><cell>I</cell><cell></cell><cell></cell><cell></cell><cell>In-Place Activated Batch</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>N</cell><cell></cell><cell></cell><cell></cell><cell>Normalization module: To</cell><cell></cell><cell>COCO-Stuff,</cell><cell></cell></row><row><cell></cell><cell>G</cell><cell></cell><cell></cell><cell>INPLACE-ABN [42]</cell><cell>reduce the training</cell><cell>DeepLabV3</cell><cell>Cityscapes</cell><cell>December 11,</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>memory footprint of</cell><cell></cell><cell>Mapillary Vistas</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>residual networks.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DSSPN explicitly</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dynamic-Structured</cell><cell>constructs a semantic</cell><cell></cell><cell>ADE20K,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Semantic Propagation Network</cell><cell>neuron graph network by incorporating the</cell><cell>-</cell><cell>COCO-Stuff, Cityscape</cell><cell>March 16, 2018</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(DSSPN) [43]</cell><cell>semantic concept</cell><cell></cell><cell>Mapillary</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hierarchy.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Two Stream Network</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Residual Stream: Carries</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>information at the full</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F R A M E W O R K S</cell><cell>. Encoder Decoder</cell><cell>e R</cell><cell>e S</cell><cell>Full-resolution Residual Networks (FRRN) [44] (ENet) [45] Efficient Neural Network</cell><cell>image resolution, enabling precise adherence to segment boundaries. Pooling Stream: Sequence of pooling operations to obtain robust features for recognition. Presents a different view upsample the output on encoder-decoder architecture The decoder is to</cell><cell cols="2">ResNet + VGG Cityscapes SUN Cityscapes, ResNet CamVid,</cell><cell>December 6, 2016 June 7, 2016</cell><cell>YES YES</cell></row><row><cell></cell><cell></cell><cell>a</cell><cell>g</cell><cell></cell><cell>of the encoder,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>l</cell><cell>m</cell><cell></cell><cell>only to fine-tuning.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>.</cell><cell>-T i</cell><cell>e n t a</cell><cell>Efficient Residual Factorized Network (ERFNet) [46]</cell><cell>A non-bottleneck-1D (non-bt-1D) layer and combines with bottleneck.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>m</cell><cell>t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>e</cell><cell>i</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>n</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p>scale feature representation and deep supervision). Liang et al.</p><ref type="bibr" target="#b43">[43]</ref> </p>proposed a Dynamic Structured Semantic Propagation Network (DSSPN), that builds a large semantic neuron graph by taking in the semantic concept hierarchy into network construction. In semantic propagation graph, each neuron is responsible for segmenting out regions of one concept in the word hierarchy. They proposed dense semantic-enhanced neural block in which the learned features of each neuron are further propagated into its child neurons to evolve features for recognizing more fine-grained concepts. Samuel et al.</p><ref type="bibr" target="#b42">[42]</ref> </p>present In-Place Activated Batch Normalization (INPLACE-ABN) architecture module to reduce the training memory footprint of residual network ResNeXt</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Region Proposal based Methods Cetegory Strategy / Structure Corpus Original Architecture Testing Benchmark Published On Code Available</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Regional proposal generator:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Regional</cell><cell>Selective Search Method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Convolutional Neural Network</cell><cell>CNN: for extracting features from each region</cell><cell>AlexNet VGG-16</cell><cell>PASCAL VOC</cell><cell>October 22, 2014</cell><cell>YES</cell></row><row><cell></cell><cell>(R-CNN) [52]</cell><cell>Set of class specific linear</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>SVMs to score features.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Improvement in R-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fast R-CNN [54]</cell><cell>Region of Interest (RoI)</cell><cell>VGG-16</cell><cell cols="2">PASCAL VOC September 27, 2015</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell>pooling layer.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Regional Proposals</cell><cell>Faster R-CNN [55]</cell><cell>Region Proposal Network (RPN) Merge of RPN and Fast R-CNN.</cell><cell>VGG-16 FCN as RPN ZFNet</cell><cell>PASCAL VOC COCO</cell><cell>June 6, 2016</cell><cell>YES</cell></row><row><cell></cell><cell>Mask R-CNN [56]</cell><cell>Region of Interest Alignment (RoIAlign): for pixel-to-pixel alignment</cell><cell>VGG-16 FCN as RPN ZFNet</cell><cell>Cityscapes, COCO</cell><cell>January 24, 2018</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell>Create feature pyramids</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Feature Pyramid Network (FPN) [57]</cell><cell>having semantics at all levels, that can be used to replace</cell><cell>Fast/Faster R-CNN</cell><cell>COCO</cell><cell>April 18, 2017</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell>featured image pyramids.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Path Aggregation Network (PANet) [58]</cell><cell>Bottom up Path Augmentation Adaptive Feature Pooling: Fully connected Fusion:</cell><cell>Mask R-CNN / FPN</cell><cell>COCO, Mapillary vistas Cityscapes,</cell><cell>March 5, 2018</cell><cell>-</cell></row><row><cell cols="3">Figure 6: The framework of Mask R-CNN [56]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 : Recurrent Neural Network based Methods Cetegory Strategy / Structure Corpus Original Architecture Testing Benchmark Published on Code Available</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Feed-Forward Approach:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Recurrent Convolution Neural Network ( R CNN) [68]</cell><cell>Models non-local class dependencies in a scene from the raw image (Extract</cell><cell>LeNet</cell><cell>Stanford Background SIFT Flow</cell><cell>June 21, 2014</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>contextual information).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Model the contextual</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Directed Acyclic Graph RNNs</cell><cell>DAG-RNNs [69]</cell><cell>dependencies of local features. Class Weighting Function that attends to rare classes. Model contextual</cell><cell>VGGNet + RNN</cell><cell>SiftFlow, CamVid, Barcelona</cell><cell>November 23, 2015</cell><cell>-</cell></row><row><cell></cell><cell>Dense Recurrent Neural Network (DD-RNN) [70]</cell><cell>dependencies through dense connections Inspired by Attention model to focus DenseNet [71].</cell><cell>VGGNet + RNN</cell><cell>PASCAL Context, SiftFlow ADE20K,</cell><cell>January 23, 2018</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>on relevant dependencies.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Model long-range semantic</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>dependencies for graphical</cell><cell></cell><cell>Sift Flow,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>DAG-RNNs [72]</cell><cell>structured images.</cell><cell>VGGNet + RNN</cell><cell>Pascal Context</cell><cell>June 6, 2017</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Class Weighting Function that</cell><cell></cell><cell>COCO Stuff</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>attends to rare classes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Modified ReNet [65]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recurrent Neural Network</cell><cell>ReSeg: Recurrent Segmentation [64]</cell><cell>Recurrent Layer: Composed by multiple RNNs. Gated Recurrent Unit</cell><cell>ReNet + RNN</cell><cell>CamVid, Weizmann Horse Oxford Flower,</cell><cell>June 1, 2016</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell>(GRU) [73] or LSTM [74]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Multi-level Contextual Recurrent Neural Networks (MCRNNs) [66]</cell><cell>CRNNs encode three contextual cues (local, global and GIST). Attention model is effectiveness. adopted to improve</cell><cell>VGGNet + RNN</cell><cell>CamVid, Cityscapes KITTI, Stanford-background, SiftFlow,</cell><cell>January 23, 2018</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Model long-range dependencies</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(Local: Pixel-by -Pixel and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Global: Label-by-Label)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Two-Dimensional LSTM Network (2D-LSTM) [37]</cell><cell>in an image. LSTM blocks: Activation (surrounding contexts</cell><cell>LSTM</cell><cell>Stanford Background SIFT Flow</cell><cell>June 7, 2015</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>in all directions).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feedforward layer: Summing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>LSTM activations.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Encoder/Decoder based</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Recurrent Neural Network</cell><cell>ResNet</cell><cell>Pascal VOC 2012,</cell><cell></cell><cell></cell></row><row><cell cols="2">Recurrent model for semantic instance segmentation [75]</cell><cell>Encoder: Feature extractor Decoder: Convolutional</cell><cell>+ Convolutional</cell><cell>Cityscapes, CVPPP Plant</cell><cell>March 22, 2018</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell>LSTM [76], predicting</cell><cell>LSTM</cell><cell>Leaf Segmentation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>one instance at a time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Upsampling / Deconvolution based Methods</figDesc><table><row><cell>Category</cell><cell cols="3">Strategy / Structure</cell><cell>Corpus</cell><cell>Original Architecture</cell><cell>Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Object Proposal Network</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(OPN) generate object</cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell cols="2">Ojectness-Aware Segmentation (OA-Seg) [78]</cell><cell>proposals Lightweight deconvolutional</cell><cell>VGGNet</cell><cell>PASCAL VOC</cell><cell>October 15, 2016</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>neural network (Light-DCNN)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>for upsampling</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Built from a downsampling</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Fully Convolutional DenseNet (FC-DenseNet) [51]</cell><cell>path, an upsampling path and skip connections. The main goal is to exploit</cell><cell>DenseNet</cell><cell>CamVid Gatech</cell><cell>October 31, 2017</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>the feature reuses</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Convolution Network:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Unpooling of Low Level Features or Score Maps</cell><cell></cell><cell>Encoder Decoder</cell><cell>ConvDeconvNet [79] SegNet [80]</cell><cell>Feature extractor Deconvolution Network: Shape Generartor from the feature extractor Obtain higher resolution encoder. by using a set of decoders one corresponding to each</cell><cell>VGGNet VGGNet, DeconvNet</cell><cell>PASCAL VOC Cityscapes, CamVid KITTI, SUN RGB-D,</cell><cell>May 18, 2015 October 9, 2016</cell><cell>YES YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Stacked Deconvolutional Network (SDN) [81]</cell><cell>SDN Unit: Efficient shallow deconvolutional network stack multiple SDN units one by one with dense connections.</cell><cell>DenseNet</cell><cell>PASCAL VOC CamVid, GATECH</cell><cell>August 16, 2017</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Squeeze-SegNet [82]</cell><cell>DFire Module: Series of concatenation of expand module of SqueezeNet.</cell><cell>SqueezeNet SegNet</cell><cell>CamVid, Cityscapes</cell><cell>April 13, 2018</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Deep filter consisting</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(convolution, pooling,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Fully Convolutional Network (FCN) [77] Skip Layer Architecture</cell><cell>activation functions, deconvolution) layers. Upsampling: end-to-end learning by backpropagation from the pixel-wise loss. Skip (Shallow fine layer) that appearance information to improve from a deep, coarse layer with the combines semantic information</cell><cell>Finetuning of AlexNet, VGGNet, GoogLeNet</cell><cell>Cityscapes, CIFAR10, Freiburg Forest SYNTHIA, KITTI, PASCAL Context, PASCAL VOC, CamVid, ADE20K,</cell><cell>March 8, 2015</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>segmentation.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FCN32s FCN16s FCN8s</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upsampling / Deconvolution</cell><cell></cell><cell>F e a t u r e F u s i o n</cell><cell>Fully Combine Convolutional Network (FCCN) [83] Semantic Motion Segmentation Network (SMSNet)[84] Dense Decoder Shortcut Connections [86]</cell><cell>Fusing and reusing feature maps Layer by Layer Motion feature component: FlowNet2 architecture[85] Semantic Segmentation component: AdapNet architecture Fusion component: combines both the motion and semantic features Encoder: ResNeXt architecture A decoder is made up of blocks Multi-level fusion in single-pass which generate semantic features maps.</cell><cell>FCN-VGG FlowNet, AdapNet ResNeXt</cell><cell>CamVid, PASCAL VOC, ADE20K Cityscapes, KITTI Pascal VOC, NYUD, CamVid Pascal-Context, Pascal Person-Part,</cell><cell>January 4, 2018 September 1, 2017 June 22, 2018</cell><cell>-YES -</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>inference</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Image Cascade Network (ICNet) [87]</cell><cell>Proposed a cascade feature fusion (CFF) unit</cell><cell>Modified PSPNet</cell><cell>Cityscapes</cell><cell>April 27, 2017</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Refine Network (RefineNet) [88]</cell><cell>Three Components 1. Residual convolution unit (RCU) 3. Chained residual pooling 2. Multi-resolution fusion</cell><cell>ResNet</cell><cell>Cityscapes, ADE20K, &amp; Context PASCAL VOC NYUDv2, SUN-RGBD,</cell><cell>November 26, 2016</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RGB-D Multi-level</cell><cell>Multi-modal feature fusion (MMF):</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Residual Feature Fusion Network</cell><cell>the fusion of features (RGB and depth) Multi-level feature refinement:</cell><cell>RefineNet</cell><cell>NYUDv2, SUN RGB-D</cell><cell>December 25, 2017</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(RDFNET) [89]</cell><cell>Refining feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Reconstruction and Refinement</cell><cell>Encoder Decoder</cell><cell>Gated Feedback Refinement Network (G-FRNet) [90]</cell><cell>Gate Unit: Combines low-resolution features and high-resolution features to produce contextual information. maps with larger spatial dimensions. Refinement unit: Generate new label</cell><cell>VGGNet</cell><cell>CamVid, Parsing PASCAL VOC, Horse-Cow</cell><cell>July 1, 2017</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Label Refinement Network (LRN) [91]</cell><cell>Predicts semantic labels at several different resolutions in a coarse-to-fine fashion.</cell><cell>SegNet</cell><cell>CamVid, SUN RGB-D, PASCAL VOC</cell><cell>March 1, 2017</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Laplacian Pyramid Reconstruction and Refinement (LRR) [92]</cell><cell>Boundary mask "inset" used for localizing object boundaries. LRR-32x 16x and 8x layers</cell><cell>ResNet</cell><cell>Cityscapes, PASCAL VOC</cell><cell>July 30, 2016</cell><cell>YES</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Increase Resolution of Features based Methods</figDesc><table><row><cell>Category</cell><cell cols="2">Strategy / Structure</cell><cell>Corpus</cell><cell cols="2">Original Architecture Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row><row><cell>.</cell><cell>.</cell><cell>DeepLab [97]</cell><cell>Atrous ('Holes') Convolution</cell><cell>FCN-VGG</cell><cell>Cityscapes, PASCAL VOC</cell><cell>June 7, 2016</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Atrous Spatial Pyramid</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Atrous Convolution</cell><cell>DeepLabV2 [99]</cell><cell>Pooling (ASPP). Method effectively enlarge filters to incorporate the field of view of</cell><cell>FCN-ResNet</cell><cell>Cityscapes, COCO PASCAL VOC,</cell><cell>May 12, 2017</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell>multi-scale context.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>DeepLabV3 [100]</cell><cell>Rethink Atrous Convolution Augment the Atrous Spatial Pyramid Pooling (ASPP).</cell><cell>DeepLabV2</cell><cell>Cityscapes, PASCAL VOC</cell><cell>December 5, 2017</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>DeepLabV3+ [101]</cell><cell>Encoder Decoder Approach Xception [27]</cell><cell>DeepLabV3</cell><cell>PASCAL VOC</cell><cell>March 8, 2018</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Rectangular Prism</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Increase Resolution of Features</cell><cell>.</cell><cell>Dilated Convolutions Module [98]</cell><cell>convolutional layers, with no pooling or subsampling for multi-scale context aggregation [34]. Fire module: modified SqueezeNet [96]</cell><cell>VGGNet</cell><cell>Cityscapes, PASCAL VOC</cell><cell>April 30, 2016</cell><cell>YES</cell></row><row><cell></cell><cell>Dilated Convolution</cell><cell>SQ Network [102]</cell><cell>Parallel dilated convolution layer.</cell><cell>SqueezeNet</cell><cell>Cityscapes</cell><cell>December 10, 2016</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Refinement module:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>SharpMask approach</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Hybrid Dilated Convolution (HDC) [103]</cell><cell>Dense Upsampling Convolution (DUC) by TuSimple.</cell><cell>ResNet + DUC</cell><cell>KITTI, PASCAL VOC</cell><cell>November 9, 2017</cell><cell>YES</cell></row><row><cell>.</cell><cell></cell><cell>Dilated Residual Network (DRN) [104]</cell><cell>Replacing dilated into ResNet model. convolutions layers</cell><cell>ResNet</cell><cell>Cityscapes</cell><cell>May 28, 2017</cell><cell>YES</cell></row><row><cell></cell><cell cols="2">Fully Convolutional Residual Network</cell><cell></cell><cell>ResNet + FCN DeepLab</cell><cell>Cityscapes, PASCAL VOC</cell><cell>April 15, 2016</cell><cell>-</cell></row></table><note><p><p><p>(FCRN)  </p><ref type="bibr" target="#b105">[105]</ref> </p>Method to simulate a high resolution network with a low resolution network. Enlarge the field-of-view (FoV) of features. Online bootstrapping method for training.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 : Enhancement of Features based Mtheods</head><label>7</label><figDesc></figDesc><table><row><cell>Category</cell><cell>Strategy / Structure</cell><cell>Corpus</cell><cell>Original Architecture</cell><cell>Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 : Semi and Weakly Supervised based Methods</head><label>8</label><figDesc></figDesc><table><row><cell>Category</cell><cell>Strategy / Structure</cell><cell>Corpus</cell><cell>Original Architecture</cell><cell>Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Spatio-Temporal based Methods</figDesc><table><row><cell>Category</cell><cell>Strategy / Structure</cell><cell>Corpus</cell><cell>Original Architecture</cell><cell>Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row><row><cell>.</cell><cell>Clockwork FCN [144]</cell><cell>Clockworks: clock signals that control the learning of different layers with different rates</cell><cell>FCN Clockwork RN</cell><cell>Youtube-Objects, Cityscapes NYUD,</cell><cell>August 11, 2016</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell>Spatial-Temporal Module</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Spatio-Temporal FCN [145]</cell><cell>embedding into FCN LSTM to define relationships between</cell><cell>FCN</cell><cell>Camvid NYUDv2</cell><cell>September 2, 2016</cell><cell>YES</cell></row><row><cell></cell><cell></cell><cell>image frames</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Spatio-Temporal Data-Driven Pooling (STD2P) [146]</cell><cell>Incorporate superpixels and multi-view information into convolutional networks</cell><cell>FCN</cell><cell>NYUDv2 SUN 3D</cell><cell>April 26, 2017</cell><cell>-</cell></row><row><cell></cell><cell>Feature Space Optimization (FSO) [147]</cell><cell>Optimize the mapping of pixels regularization to a Euclidean feature space used by DenseCRF for spatio-temporal</cell><cell>VGG Dilation</cell><cell>CityScapes, Camvid</cell><cell>December 12, 2016</cell><cell>YES</cell></row><row><cell></cell><cell>Deep Spatio-Temporal FCN (DST-FCN) [148]</cell><cell>Learn spatial-temporal dependencies through 2D FCN on pixels and 3D FCN on voxels</cell><cell>VGG C3D</cell><cell>A2D, CamVid</cell><cell>October 5, 2017</cell><cell>-</cell></row><row><cell>Spatio-</cell><cell></cell><cell>Implementation of three gated</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Temporal</cell><cell></cell><cell>recurrent architectures</cell><cell></cell><cell>SegTrack V2,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Gated Recurrent FCN [149]</cell><cell>RFC-LeNet: Conventional Recurrent Units.</cell><cell>FCN</cell><cell>Davis, Cityscapes,</cell><cell>November 21, 2016</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>RFC-VGG and RFC-Dilated:</cell><cell></cell><cell>SYNTHIA</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Convolutional Recurrent Units.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Weakly-Supervised Two-stream</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>WSBF[150]</cell><cell>Network. One stream takes image, and other features. optical flow to extract the</cell><cell>VGG</cell><cell>Cityscapes, YouTube-Objects CamVid,</cell><cell>August 15, 2017</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>RFC-VGG and RFC-Dilated:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Convolutional Recurrent Units.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Spatio-Temporal Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Gated Recurrent Flow Propagation (GRFP) [151]</cell><cell>Gated Recurrent Unit (STGRU) Combining spatial transformer with convolutional-gated</cell><cell>Dilation LRR</cell><cell>CityScapes, Camvid</cell><cell>October 2, 2017</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>architecture.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 : Methods using CRF/MRF</head><label>10</label><figDesc></figDesc><table><row><cell>Category</cell><cell>Strategy / Structure</cell><cell>Corpus</cell><cell>Original Architecture</cell><cell>Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Alternative to CRF based Methods</figDesc><table><row><cell>Category</cell><cell>Strategy / Structure</cell><cell>Corpus</cell><cell>Original Architecture</cell><cell>Testing Benchmark</cell><cell>Published on</cell><cell>Code Available</cell></row><row><cell></cell><cell></cell><cell>Bilateral filter inference in DenseCRF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Bilateral Neural Network (BNN)[171]</cell><cell>Replacing Gaussian potentials with bilateral convolution to learn</cell><cell>DeepLab</cell><cell>Pascal VOC</cell><cell>June 26, 2016</cell><cell>Yes</cell></row><row><cell></cell><cell></cell><cell>pairwise potentials .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fast Bilateral Solver (BS) [172]</cell><cell>Edge-aware smoothness algorithm using bilateral filtering technique.</cell><cell>CRF-RNN</cell><cell>Pascal VOC MS COCO</cell><cell>July 22, 2016</cell><cell>-</cell></row><row><cell></cell><cell>Boundary Neural</cell><cell>Build unary and pairwise potentials</cell><cell></cell><cell>Semantic</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Field</cell><cell>from input RGB image, then</cell><cell>FCN</cell><cell>Boundaries</cell><cell>May 24, 2016</cell><cell>-</cell></row><row><cell></cell><cell>(BNF) [173]</cell><cell>combine them in global manner.</cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Domain transform (DT) Module:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Alternative to CRF Approaches</cell><cell>DT-EdgeNet [174] Global Convolutional Network (GCN) [175]</cell><cell>Edge-preserving filter. Edge Net: Predicts edge features from midway layers. Large kernels used for classification and localization. Boundary Refinement Block: Model the boundary alignment as a</cell><cell>DeepLab FCN ResNet</cell><cell>Pascal VOC Cityscapes PASCAL VOC COCO</cell><cell>December 12, 2016 March 8, 2017</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell>residual structure.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Random Walk Network (RWN) [176]</cell><cell>Random Walk Network Pixel labeling framework</cell><cell>DeepLab-largeFOV</cell><cell>Pascal, Sift Flow SBD-Stanford Background,</cell><cell>July 22, 2017</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 : Summary of Datasets DataSet Environment Nature No of Classes Samples Image Resolution Year Performance Network Model</head><label>12</label><figDesc></figDesc><table><row><cell>Test</cell></row><row><cell>Validation</cell></row><row><cell>Training</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>•</head><label></label><figDesc>Achieves top score of 88.25% IoU on Freiburg Forest and 72.91% IoU on Synthia dataset. The network achieves the score of 69.39% IoU on cityscapes dataset.FCCN proposed a cost function that significantly improves the segmentation performance, very few researchers tried to modify cost function when training their models. FCCN calculates cost function on each pre output layer including the final output layer.The model uses CNN-based pairwise potential functions to capture semantic correlations between neighboring patches which improve the coarse-level prediction. The model applies FCN with sliding pyramid pooling, CNN contextual pairwise, boundary refinement (dense CRF method), and trained model with extra images from the COCO dataset to improve the overall performance of the model.</figDesc><table><row><cell>• Achieves 44.8% IoU on Sift-flow, 31.2% IoU on COCO stuff (171 classes) and 43.7% IoU on PASCAL Context</cell><cell></cell></row><row><cell>dataset.</cell><cell></cell></row><row><cell>Segmentation network uses a pre-trained CNN with DAG-RNN,</cell><cell></cell></row><row><cell>fusing low-level features with DAG-RNN. A new class weighted</cell><cell></cell></row><row><cell>loss function proposed to control the classwise loss during train-DeepLab V3 [100]: ing. The performance of segmentation network increases with</cell><cell></cell></row><row><cell>• Achieves score of 81.3% IoU on cityscapes. increase in DAGs with DAG-RNN. Fully connected CRF is used, which further improves the performance of the network.</cell><cell>Clockwork-FCN [144]:</cell></row><row><cell cols="2">M A N U S C R I P T Improvement mainly comes from changing hyper perimeter: Fine tuning batch normalization, varying batch size, larger crop size, changing output stride, multi scale inputs during inference, add-ing left-right flipped inputs, trained on 3475 finely and ex-tra 20000 coarsely annotated images of cityscapes dataset. Fur-thermore, the use of ResNet-101 model which is pre-trained on ImageNet and JFT dataset, results in the second best score of 86.90 IoU on Pascal VOC. DeepLab V3+ [101]: • Achieves 89.0% IoU on Pascal VOC and 82.1% IoU on cityscapes. DeepLab V3+ is a modified version of DeepLab V3, adapted to output stride = 16 or 8 instead of 32. It is also adapted to Xception module, which further increased the performance. DSSPN [43]: • Achieves top scores on COCO Stuff 38.9% IoU, 43.6% IoU on ADE20K, 58.6% IoU on Pascal Context and 45.01% IoU on Mapillary dataset. • Achieves 68.50% IoU on Youtube Object, 68.40% IoU RefineNet [88]: on Cityscapes, 28.90% IoU on NYUDv2 dataset. The Clockwork-FCN uses different clock schedules; Fixed-rate clock reduces computation by assigning different rates to each stage such that later stages execute less often. Adaptive clock-work updates when the output score maps is predicted to change, thus reducing computation while maintaining accuracy. Residual framework ResNet-38 [38]: • Achieves the highest score of 48.1% IoU on Pascal Con-text, 80.6% IoU on cityscapes and 43.43% IoU on ADE20K. The model introduces residual units into ResNet (17 residual units for 101 layers ResNet) expanding it into a sufficiently large number of sub-networks. Each connection in residual unit shares same kernel sizes and numbers of channels, this re-sults in improving model accuracy. ResNet-38 does not apply any multi-scale testing, model averaging or CRF based post-processing, except for the test set of ADE20K. ESPNet: [47]: M • Achieves a score of 45.90% IoU on SUN-RGB, 46.50% IoU on NYUDv2 and 47.30% IoU on Pascal Context datasets. The results on Pascal VOC, cityscapes, and ADE20K datasets are 83.40% IoU, 73.60% IoU, and 40.70 % IoU respectively. RefineNet applies data augmentation during training (random scaling, cropping and horizontal flipping of image), and multi-scale evaluation (average the predictions on the same image across different scales for the final prediction). Dense CRF method is used only for Pascal VOC. Dilation10 [98]: • Achieves 67.60% IoU on PASCAL VOC, 67.10% IoU on Cityscapes, 32.31% IoU on ADE20K and 65.29% IoU on A N U S C R I P T CamVid dataset.</cell></row><row><cell>Adelaide Context CNN-CRF [169]: not just simply adding extra convolutional filters. use of temporal data is the reason for the boost of performance A C C E P T E D DSSPN constructs a semantic neuron graph in which each neu-ron segments regions of one parent concept in a semantic con-cept hierarchy (combining labels from four datasets) and aims at recognizing between its child concepts. Instead of using a completely large semantic neural graph, DSSPN only acti-vates relative small neural graph for each image during training, which makes DSSPN memory and computation efficient. RFCNet [149]: • Achieves top scores of 81.20% IoU on SYNTHIA, 80.12% IoU on SegTrack and 69.84% IoU on DAVIS dataset. The model uses different FCN architectures as a recurrent node to utilize temporal information, deconvolution layer for upsam-pling and supports skip architecture for finer segmentation. The A C C E P T E D</cell><cell>The improvement can be credited to the highly representational multi-scale features learned by the model, which enable the seg-mentation of very distant objects present in Synthia and Citys-capes. AdapNet model approach is based on a mixture of con-volutional neural network (CNN) experts (Convoluted Mixture of Deep Experts -CMoDE) and incorporates multiple modali-ties including appearance, depth and motion. PSPNet [112]: • Achieves the best results on ADE20K with 44.8% IoU, promising results are obtained on cityscapes and Pascal VOC with 80.2% IoU and 85.4% IoU respectively. PSPNet developed an effective optimization strategy for deep ResNet-101 [34] based on deeply supervised loss; two loss func-tation is performed. The performance is increased by transferring pre-trained classi-ments different depths of pre-trained ResNet and data augmen-CamVid dataset. learning process. PSPNet applies multi scale testing, experi-TEXT, 65.24% IoU on SYNTHIA, and 57.00% IoU on iary loss applied after the fourth stage, this helps optimizing the 29.39% IoU on ADE20K, 35.10% IoU on PASCAL CON-tions: main softmax loss to train the final classifier and auxil-• Efficient real-time segmentation network, achieves 60.2% IoU on cityscape, 40.0% IoU on Mapillary dataset with 0.364M parameters, 63.01% IoU on Pascal VOC test set with 0.364M parameters. Efficient Spatial Pyramid (ESP) network is an efficient neural network in terms of speed and memory. ESP, based on factor-ized form of convolutions (point-wise convolution and spatial pyramid of dilated convolutions), reduces the number of pa-rameters, memory, with large receptive field. FCN-8s [77]: • Achieves the score of 77.46% IoU on Freiburg Forest, 67.20% IoU on PASCAL VOC, 65.30% IoU on CIFAR-10, 65.30% IoU on Cityscapes, 56.10% IoU on KITTI,</cell></row><row><cell></cell><cell>fier weights, fusing different layer representations, and learning FCCN [83]: end-to-end on whole images.</cell></row><row><cell></cell><cell>• Achieves a top scores of 69.94% IoU on CamVid and score of 44.23% IoU on ADE20K dataset. DAG-RNN [72]:</cell></row></table><note><p>• Achieves score of 40.6% IoU on NYUDv2, 42.30% IoU on SUN-RGB, 78.00% IoU on Pascal VOC, 66.40% IoU on CIFAR-100, 71.60% IoU on Cityscapes, and 43.30% IoU on Pascal Context dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>In STFCN model, no post processing required, the spatial temporal module is embedded on top of the final convolutional layer. LSTM blocks are used for inferring the relations between spatial features that provide valuable information and improve the accuracy of the segmentation. Furthermore, applying dilated convolutions for multi-scale contextual information archives better results.</figDesc><table><row><cell>STGRU (GRFP + Dilation) [151]:</cell></row><row><cell>• Achieves the score of 66.10 IoU on CamVid dataset. Model GRFP + Dilation scores 67.80% IoU and model GRFP +</cell></row><row><cell>LRR-4x achieves the score of 72.80% IoU on Cityscapes</cell></row><row><cell>dataset.</cell></row><row><cell>The model combines the power of both convolutional-gated ar-</cell></row><row><cell>chitecture and spatial transformers (CNN). The model GRFP is</cell></row><row><cell>trained with Dilation 10 [88] and LRR [70] network that im-</cell></row><row><cell>prove performance for video. The model improves semantic</cell></row><row><cell>video segmentation and labeling accuracy by propagating infor-</cell></row><row><cell>mation from labeled video frames to nearby unlabeled frames</cell></row><row><cell>with slight computation.</cell></row><row><cell>80.10% IoU on Cityscapes, 83.10%</cell></row><row><cell>IoU on PASCAL VOC, 39.40% IoU on ADE20K dataset.</cell></row><row><cell>DUC provides the dense pixel-wise predictions, HDC uses ar-</cell></row><row><cell>bitrary dilation rates which enlarge the receptive fields of the</cell></row><row><cell>network. ResNet with different depths are experimented, data</cell></row><row><cell>augmentation is performed (for cityscapes, each image of the</cell></row><row><cell>training set is partitioned into twelve 800 × 800 patches mak-ing 35700 images). The model is trained using the combination</cell></row><row><cell>of MS-COCO dataset, augmented PASCAL VOC 2012 train-</cell></row><row><cell>ing and trainval sets. ResNet DUC+HDC is also evaluated on</cell></row><row><cell>KITTI dataset achieving the average precision of 92.88% for</cell></row><row><cell>road segmentation using ResNet 101-DUC model, pre-trained</cell></row><row><cell>from ImageNet during training.</cell></row><row><cell>ST-Dilation [145]:</cell></row><row><cell>• Achieves the score of 65.90% IoU on CamVid dataset. Model ST-FCN32s scores 50.60% IoU on Camvid dataset</cell></row><row><cell>and Model ST-FCN8s scores 30.90% IoU on NYUDv2</cell></row><row><cell>dataset.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>VGG-16PASCAL VOC December 2, 2014 -</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>November 9, 2017 YES</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors express their gratitude to University Technology Belfort-Montbeliard and Higher Education Commission of Pakistan for providing the support and necessary requirement for completion of work. The authors would also like to acknowledge Zhi Yan and Abdellatif El Idrissi for helpful discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX: Table .13 REFERENCES</head></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Link Network Model</head><p>Code Link Inception <ref type="bibr" target="#b27">[27]</ref> RSIS <ref type="bibr" target="#b75">[75]</ref> https://github.com/imatge-upc/rsis BN-Inception <ref type="bibr" target="#b28">[28]</ref> FC-DenseNet <ref type="bibr" target="#b51">[51]</ref> https://github.com/SimJeg/FC-DenseNet Inception V2, V3 <ref type="bibr" target="#b29">[29]</ref> https://github.com/Microsoft/CNTK/tree/master/ examples/Image/Classification/GoogLeNet ConvDeconvNet <ref type="bibr" target="#b79">[79]</ref> https://github.com/HyeonwooNoh/DeconvNet Inception V4 <ref type="bibr" target="#b30">[30]</ref> https://github.com/titu1994/Inception-v4 SegNet <ref type="bibr" target="#b80">[80]</ref> https://github.com/alexgkendall/caffe-segnet Xception <ref type="bibr" target="#b31">[31]</ref> https://github.com/kwotsin/TensorFlow-Xception FCN <ref type="bibr" target="#b77">[77]</ref> https://github.com/shelhamer/fcn.berkeleyvision.org VGGNet <ref type="bibr" target="#b33">[33]</ref> https://github.com/machrisaa/tensorflow-vgg SMSNet <ref type="bibr" target="#b84">[84]</ref> https://github.com/JohanVer/SMSnet ResNet <ref type="bibr" target="#b34">[34]</ref> https://github.com/KaimingHe/deep-residual-networks ICNet <ref type="bibr" target="#b87">[87]</ref> https://github.com/hszhao/ICNet ResNet-38 <ref type="bibr" target="#b38">[38]</ref> https://github.com/itijyou/ademxapp RefineNet <ref type="bibr" target="#b88">[88]</ref> https://github.com/guosheng/refinenet ResNeXt <ref type="bibr" target="#b41">[41]</ref> https://github.com/facebookresearch/ResNeXt RDFNET <ref type="bibr" target="#b89">[89]</ref> https://github.com/SeongjinPark/RDFNet/blob/master INPLACE-ABN <ref type="bibr" target="#b42">[42]</ref> https://github.com/mapillary/inplace abn G-FRNet <ref type="bibr" target="#b90">[90]</ref> https://github.com/mrochan/gfrnet FRRN <ref type="bibr" target="#b44">[44]</ref> https://github.com/TobyPDE/FRRN LRN <ref type="bibr" target="#b91">[91]</ref> https://github.com/golnazghiasi/LRR ENet <ref type="bibr" target="#b45">[45]</ref> https://github.com/TimoSaemann/ENet DeepLab <ref type="bibr" target="#b97">[97]</ref> https://bitbucket.org/deeplab/deeplab-public ERFNet <ref type="bibr" target="#b46">[46]</ref> https://github.com/Eromera/erfnet DeepLabV2 <ref type="bibr" target="#b99">[99]</ref> https://bitbucket.org/aquariusjay/deeplab-public-ver2</p><p>ESPNet <ref type="bibr" target="#b47">[47]</ref> https://github.com/sacmehta/ESPNet Dilation <ref type="bibr" target="#b98">[98]</ref> https://github.com/tensorflow/models /tree/master/research/deeplab R-CNN <ref type="bibr" target="#b52">[52]</ref> https://github.com/rbgirshick/rcnn DeepLabV3+ <ref type="bibr" target="#b101">[101]</ref> https://github.com/fyu/dilation Fast R-CNN <ref type="bibr" target="#b54">[54]</ref> https://github.com/rbgirshick/fast-rcnn HDC <ref type="bibr" target="#b103">[103]</ref> https://github.com/TuSimple/TuSimple-DUC Faster R-CNN <ref type="bibr" target="#b55">[55]</ref> https://github.com/ShaoqingRen/faster rcnn DRN <ref type="bibr" target="#b104">[104]</ref> https://github.com/fyu/drn Mask R-CNN <ref type="bibr" target="#b56">[56]</ref> https://github.com/matterport/Mask RCNN PSPNet <ref type="bibr" target="#b112">[112]</ref> https://github.com/hszhao/PSPNet FPN <ref type="bibr" target="#b57">[57]</ref> https://github.com/unsky/FPN DenseCRF <ref type="bibr" target="#b160">[160]</ref> https://github.com/lucasb-eyer/pydensecrf</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural networks and neuroscience-inspired computer vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="921" to="R929" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time object detection and semantic segmentation for autonomous driving</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIPPR 2017: Automatic Target Recognition and Navigation</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10608</biblScope>
			<biblScope unit="page">106080</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combination of computer vision detection and segmentation for autonomous driving</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Jan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Position, Location and Navigation Symposium (PLANS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1047" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Road segmentation for all-day outdoor robot navigation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">314</biblScope>
			<biblScope unit="page" from="316" to="325" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic metallic surface defect detection and recognition with convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1575</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Salvaggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph modelbased salient object detection using objectness and multiple saliency cues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="188" to="202" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient object detection via multi-scale attention cnn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page" from="130" to="140" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in cnns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lottes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2229" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clothingout: a category-supervised gan model for clothing segmentation and retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Medical image semantic segmentation based on deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jifara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Adil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1257" to="1265" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantic texton forests for image categorization and segmentation, in: Computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semantic segmentation deep learning review</title>
		<ptr target="www.blog.qure.ai" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond pixels: A comprehensive survey from bottom-up to semantic image segmentation and cosegmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="27" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Thoma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06541</idno>
		<title level="m">A survey of semantic segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A review of neural network based semantic segmentation for scene understanding in context of the self driving car</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Fouopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Knake-Langhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<imprint>
			<publisher>BioMedTec Studierendentagung</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of semantic segmentation using deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Survey of recent progress in semantic image segmentation with cnns</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">51101</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Methods and datasets on semantic segmentation: A review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="page" from="82" to="103" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Decouplednet</surname></persName>
		</author>
		<ptr target="https://github.com/fvisin/resegFSO[147]https://bitbucket.org/infinitei/videoparsing" />
		<imprint>
			<biblScope unit="volume">164</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks, in: Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Principles of neurodynamics. perceptrons and the theory of brain mechanisms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CORNELL AERONAUTICAL LAB INC BUFFALO NY</title>
		<imprint>
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<title level="m">Xception: Deep learning with depthwise separable convolutions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplifying convnets for fast learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep belief networks</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5947</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3547" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On the iterative refinement of densely connected representation levels for semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11332</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adapnet: Adaptive semantic segmentation in adverse environmental conditions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="4644" to="4651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<idno>CoRR, abs/1712.02616</idno>
		<imprint>
			<date>December 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName><surname>Enet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<title level="m">A deep neural network architecture for real-time semantic segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06815</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Restricted deformable convolution based road scene semantic segmentation using surround view cameras</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00708</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
	<note>Deep networks with stochastic depth</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1175" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international</title>
		<meeting>the IEEE international</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="814" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient sequence learning with group recurrent networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="799" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Reseg: A recurrent neural network-based model for semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Renet: A recurrent neural network based alternative to convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00393</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-level contextual rnns with attention model for scene labeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno>no. EPFL-CONF-199822</idno>
	</analytic>
	<monogr>
		<title level="m">st International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Dag-recurrent neural networks for scene labeling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3620" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06831</idno>
		<title level="m">Dense recurrent neural networks for scene labeling</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Scene segmentation with dagrecurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1480" to="1493" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baradad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00617</idno>
		<title level="m">Recurrent neural networks for semantic instance segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Objectness-aware semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="307" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>CoRR abs/1511.00561</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04943</idno>
		<title level="m">Stacked deconvolutional network for semantic segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Squeeze-segnet: a new fast deep convolutional neural network for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nanfack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elhassouny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O H</forename><surname>Thami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Conference on Machine Vision</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="volume">10696</biblScope>
			<biblScope unit="page">106962</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Semantic segmentation via highly fused convolutional network with multiple soft cost functions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
		<imprint>
			<publisher>Cognitive Systems Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Semantic motion segmentation using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><surname>Smsnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="582" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Dense decoder shortcut connections for singlepass semantic segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6596" to="6605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08545</idno>
		<title level="m">Icnet for real-time semantic segmentation on high-resolution images</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cvpr</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4877" to="4885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Label refinement network for coarse-to-fine semantic segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00551</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Computer Vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="671" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Fully combined convolutional network with soft cost function for traffic scene parsing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="725" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MLITS, NIPS Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08502</idno>
		<title level="m">Understanding convolution for semantic segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04339</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Semantic road segmentation via multi-scale ensembles of learned features</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3572</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Multi-scale patch aggregation (mpa) for simultaneous detection and segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3141" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Semantic image segmentation using fully convolutional neural networks with multi-scale images and multi-scale dilated convolutions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Multimedia Tools and Applications</publisher>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<title level="m">Maxout networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2393" to="2402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7144</idno>
		<title level="m">Fully convolutional multiclass multiple instance learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semisupervised semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1495" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Deep dual learning for semantic image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Semi and weakly supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09695</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07934</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Find your own way: Weaklysupervised segmentation of path proposals for urban autonomy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Built-in foreground/background prior for weakly-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="413" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Incorporating network built-in priors in weakly-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1382" to="1396" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Superpixel clustering with deep features for unsupervised road segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsutsui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05998</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00509</idno>
		<title level="m">Learning semantic segmentation with diverse supervision</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Boxsup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Bridging category-level and instancelevel semantic image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="570" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004">2004</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets, in: Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="852" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Stfcn: spatio-temporal fcn for semantic video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05971</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Std2p: Rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Campus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7158" to="7167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Learning deep spatio-temporal dependence for semantic video segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="939" to="949" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Convolutional gated recurrent networks for video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3090" to="3094" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Bringing background into the foreground: Making all classes equal in weakly-supervised video semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A</forename><surname>Akbarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="2125" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.088712</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Object class segmentation of rgb-d video using recurrent convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="105" to="113" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Semantic segmentation of rgbd videos with recurrent fully convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Yurdakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yemez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06432</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3511</idno>
		<title level="m">A clockwork rnn</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal random fields for efficient video segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Camille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8915" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="402" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Conditional random fields meet deep neural networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Fast high-dimensional filtering using the permutohedral lattice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="753" to="762" />
			<date type="published" when="2010">2010</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Gaussian conditional random field network for semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3224" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04777</idno>
		<title level="m">Convolutional crfs for semantic segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Higher order conditional random fields in deep neural networks, in: European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Semantic segmentation via structured patch prediction, context crf and guidance crf</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Deeply learning the messages in message passing inference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Incorporating depth into both cnn and crf for indoor semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Software Engineering and Service Science</title>
		<imprint>
			<biblScope unit="page" from="525" to="530" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">The fast bilateral solver</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="617" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3602" to="3610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4545" to="4554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Large kernel matters improve semantic segmentation by global convolutional network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6137" to="6145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Random walks on graphs: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Paul erdos is eighty</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>Combinatorics</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Fast high-dimensional filtering using the permutohedral lattice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="753" to="762" />
			<date type="published" when="2010">2010</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Domain transform for edge-aware image and video processing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2011">2011</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Efficient multi-cue scene segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Scharwächter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">The cityscapes dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scharwächter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on the Future of Datasets in Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06184</idno>
		<title level="m">The apolloscape dataset for autonomous driving</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title level="m" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft coco: Common objects in context, in: European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<title level="m">Semantic understanding of scenes through the ade20k dataset</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>CoRR, abs/1612.03716 5</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Semantic labeling of 3d point clouds for indoor scenes</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<title level="m">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view rgb-d object dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Accurate object localization with shape masks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions, in: Computer Vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009">2009. 2009</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Semantic contours from inverse detectors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Nonparametric scene parsing via label transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2368" to="2382" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Deep multispectral semantic scene understanding of forested environments using multimodal fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="465" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04723</idno>
		<title level="m">Layered interpretation of street view images</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Ladder-style densenets for semantic segmentation of large natural images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K S</forename><surname>Šegvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshop (IC-CVW)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06791</idno>
		<title level="m">Depth-aware cnn for rgb-d segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="541" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">Beyond filters: Compact feature map for portable deep model</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3703" to="3711" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b218">
	<monogr>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06530</idno>
		<title level="m">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Speedup of deep learning ensembles for semantic segmentation using a model compression technique</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holliday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barekatain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laurmaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kandaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prendinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixellevel adversarial and constraint-based adaptation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.111641</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b225">
	<monogr>
		<title level="m" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno>abs/1608.08710</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Fusion of images and point clouds for the semantic segmentation of large-scale 3d scenes based on deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">A multiscale fully convolutional network for semantic labeling of 3d point clouds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yousefhussien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kelbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Ientilucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Salvaggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaichun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Low-latency video semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5997" to="6005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
