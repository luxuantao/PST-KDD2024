<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Efficient Feature Extraction Method with Pseudo-Zernike Moment in RBF Neural Network-Based Human Face Recognition System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Javad</forename><surname>Haddadnia</surname></persName>
							<email>haddadnia@sttu.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Engineering Department</orgName>
								<orgName type="institution">Tarbiat Moallem University of Sabzevar</orgName>
								<address>
									<addrLine>Khorasan 397</addrLine>
									<settlement>Sabzevar</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Majid</forename><surname>Ahmadi</surname></persName>
							<email>ahmadi@uwindsor.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">University of Windsor</orgName>
								<address>
									<postCode>N9B 3P4</postCode>
									<settlement>Windsor</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karim</forename><surname>Faez</surname></persName>
							<email>kfaez@aut.ac.ir</email>
							<affiliation key="aff2">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">Amirkabir University of Technology</orgName>
								<address>
									<postCode>15914</postCode>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Efficient Feature Extraction Method with Pseudo-Zernike Moment in RBF Neural Network-Based Human Face Recognition System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46AA89A21C3738D6D0B3D1E578D4F620</idno>
					<note type="submission">Received 17 April 2002 and in revised form 24 April 2003</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>human face recognition</term>
					<term>face localization</term>
					<term>moment invariant</term>
					<term>pseudo-Zernike moment</term>
					<term>RBF neural network</term>
					<term>learning algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a novel method for the recognition of human faces in digital images using a new feature extraction method that combines the global and local information in frontal view of facial images. Radial basis function (RBF) neural network with a hybrid learning algorithm (HLA) has been used as a classifier. The proposed feature extraction method includes human face localization derived from the shape information. An efficient distance measure as facial candidate threshold (FCT) is defined to distinguish between face and nonface images. Pseudo-Zernike moment invariant (PZMI) with an efficient method for selecting moment order has been used. A newly defined parameter named axis correction ratio (ACR) of images for disregarding irrelevant information of face images is introduced. In this paper, the effect of these parameters in disregarding irrelevant information in recognition rate improvement is studied. Also we evaluate the effect of orders of PZMI in recognition rate of the proposed technique as well as RBF neural network learning speed. Simulation results on the face database of Olivetti Research Laboratory (ORL) indicate that the proposed method for human face recognition yielded a recognition rate of 99.3%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Face recognition has been a very popular research topic in recent years because of wide variety of application domains in both academia and industry. This interest is motivated by applications such as access control systems, model-based video coding, image and film processing, criminal identification and authentication in secure systems like computers or bank teller machines, and so forth <ref type="bibr" target="#b0">[1]</ref>. A complete face recognition system should include three stages. The first stage is detecting the location of the face, which is difficult because of unknown position, orientation, and scaling of the face in an arbitrary image <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. The second stage involves extraction of pertinent features from the localized facial image obtained in the first stage. Finally, the third stage requires classification of facial images based on the derived feature vector obtained in the previous stage.</p><p>In order to design a high recognition rate system, the choice of feature extractor is very crucial and extraction of pertinent features from two-dimensional images of human face plays an important role in any face recognition system. There are various techniques reported in the literature that deal with this problem. A recent survey of the face recognition systems can be found in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. Two main approaches to feature extraction have been extensively used by other researchers <ref type="bibr" target="#b4">[5]</ref>. The first one is based on extracting structural and geometrical facial features that constitute the local structure of facial images, for example, the shapes of the eyes, nose, and mouth <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. The structure-based approaches deal with local information instead of global information, and, therefore, they are not affected by irrelevant information in an image. However, because of the explicitness model of facial features, the structure-based approaches are sensitive to the unpredictability of face appearance and environmental conditions <ref type="bibr" target="#b4">[5]</ref>. The second method is a statistics-based approach that extracts features from the whole image and, therefore uses global information instead of local information. Since the global data of an image are used to determine the feature elements, information that is irrelevant to facial portion, such as hair, shoulders, and background, may create erroneous feature vectors that can affect the recognition results <ref type="bibr" target="#b7">[8]</ref>.</p><p>In recent years, many researchers have noticed this problem and tried to exclude the irrelevant data while performing face recognition. This can be done by eliminating the irrelevant data of a face image with a dark background <ref type="bibr" target="#b8">[9]</ref> and constructing the face database under constrained conditions, such as asking people to wear dark jackets and to sit in front of a dark background <ref type="bibr" target="#b9">[10]</ref>. Turk and Pentland <ref type="bibr" target="#b10">[11]</ref> multiplied the input image by a two-dimensional Gaussian window centered on the face to diminish the effects caused by the nonface portion. Sung and Poggio <ref type="bibr" target="#b11">[12]</ref> tried to eliminate the near-boundary pixels of a normalized face image by using a fixed size mask. In <ref type="bibr" target="#b12">[13]</ref>, Liao et al. proposed a face-only database as the basis for face recognition.</p><p>In this paper, an efficient feature extraction technique is developed, based on the combination of local and global information of face images. At first, face localization based on shape information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> with a new definition for distance measure threshold called facial candidate threshold (FCT) for distinguishing between nonface image and facial image candidate is introduced. We present the effect of varying the FCT on the recognition rate of the proposed technique. A new parameter, called the axis correction ratio (ACR), is defined to eliminate irrelevant data from the face images and to create a subimage for further feature extraction. We have shown how ACR can improve the recognition rate. Once the face localization process is completed, pseudo-Zernike moment invariant (PZMI) with a new method to select moment orders is utilized to obtain the feature vector of the face under recognition. In this paper, PZMI was selected over other types of moments because of its utility in human face recognition approaches in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. The last step in human face recognition requires classification of the facial image into one of the known classes based on the derived feature vector obtained in the previous stage. The radial basis function (RBF) neural network is also used as the classifier <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. The training of the RBF neural network is done, based on the hybrid learning algorithm (HLA) <ref type="bibr" target="#b16">[17]</ref> and we have shown that the proposed feature extraction method with an RBF neural network classifier gives a faster training phase and yields a better recognition rate. The organization of this paper is as follows. Section 2 presents the face localization method. In Section 3, the face feature extraction is presented. Classifier techniques are described in Section 4 and finally, Sections 5 and 6 present the experimental results and conclusions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FACE LOCALIZATION METHOD</head><p>To ensure a robust and accurate feature extraction, the exact location of the face in an image is needed. The ultimate goal of the face localization is finding an object in an image as a face candidate whose shape resembles the shape of a face and, therefore, one of the key problems in building automated systems that perform face recognition task is face localization. Many algorithms have been proposed for face localization and detection, which are based on using shape <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref>, color information <ref type="bibr" target="#b2">[3]</ref>, motion <ref type="bibr" target="#b17">[18]</ref>, and so forth.</p><p>A critical survey on face localization and detection can be found in <ref type="bibr" target="#b4">[5]</ref>. In this paper, we have used a modified version of the shape information technique for the face localization presented in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>. Many researchers have concluded that an ellipse can generally approximate the face of a human. The localization algorithm utilizes the information about the edges of the facial image or the region over which the face is located <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. The advantage of the region-based method is its robustness in the presence of noise and changes in illumination. In the region-based method, the connected components are determined by applying a region growing algorithm <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>, then, for each connected component with a given minimum size, the best-fit ellipse is computed using the properties of the geometric moments. To find a face region, an ellipse model with five parameters is used: X 0 , Y 0 are the centers of the ellipse, θ is the orientation, and α and β are the minor and the major axes of the ellipse, respectively, as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>To calculate these parameters, first we review the geometric moments. The geometric moments of order p + q of a digital image are defined as</p><formula xml:id="formula_0">M pq = x y f (x, y)x p y q , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where p, q = 0, 1, 2, . . . and f (x, y) is the gray-scale value of the digital image at x and y location. The translation invariant central moments are obtained by placing origin at the center of the image</p><formula xml:id="formula_2">µ pq = x y f (x, y) x -x 0 p y -y 0 q , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where x 0 = M 10 /M 00 and y 0 = M 01 /M 00 are the centers of the connected components. Therefore, the center of the ellipse is given by the center of gravity of the connected components. The orientation θ of the ellipse can be calculated by determining the least moment of inertia <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref> </p><formula xml:id="formula_4">θ = 1 2 arctan 2µ 11 µ 20 -µ 02 ,<label>(3)</label></formula><p>where µ pq denotes the central moment of the connected components as described in <ref type="bibr" target="#b1">(2)</ref>. The length of the major and the minor axes of the best-fit ellipse can also be computed by evaluating the moment of inertia. With the least and the greatest moments of inertia of an ellipse defined as</p><formula xml:id="formula_5">I Min = x y x -x 0 cos θ -y -y 0 sin θ 2 , I Max = x y x -x 0 sin θ -y -y 0 cos θ 2 , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>the length of the major and the minor axes are calculated from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref> as</p><formula xml:id="formula_7">α = 1 π I 3 Max /I Min 1/8 , β = 1 π I 3 Min /I Max 1/8 .</formula><p>(</p><formula xml:id="formula_8">)<label>5</label></formula><p>To assess how well the best-fit ellipse approximates the connected components, we define a distance measure between the connected components and the best-fit ellipse as follows:</p><formula xml:id="formula_9">φ i = P inside µ 00 , φ o = P outside µ 00 , (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where the P inside is the number of background points inside the ellipse, P outside is the number of points of the connected components that are outside the ellipse, and µ 00 is the size of the connected components. The connected components are closely approximated by their best-fit ellipses when φ i and φ o are as small as possible. We have named the threshold values for φ i and φ o as FCT. Our experimental study indicates that when FCT is less than 0.1, the connected component is very similar to ellipse; therefore it is a good candidate as a face region. If φ i and φ o are greater than 0.1, there is no face region in the input image, therefore, we reject it as a nonface image. An example of application of this method for locating face region candidates and rejecting nonface images has been presented in Figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FEATURE EXTRACTION TECHNIQUE</head><p>The aim of the feature extractor is to produce a feature vector containing all pertinent information about the face while having a low dimensionality. In order to design a good face recognition system, the choice of feature extractor is very crucial. To design a system with low to moderate complexity, the feature vectors created from feature extraction stage should contain the most pertinent information about the face to be recognized. In the statistics-based feature extraction approaches, global information is used to create a set of feature vector elements to perform recognition. A mixture of irrelevant data, which are usually part of a facial image, may result in an incorrect set of feature vector elements. Therefore, data that are irrelevant to facial portion such as hair, shoulders, and background should be disregarded in the feature extraction phase.</p><p>Face recognition systems should be capable of recognizing face appearances in a changing environment. Therefore we use PZMI to generate the feature vector elements <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Also the feature extractor should create a feature vector with low dimensionality. The low-dimensional feature vector reduces the computational burden of the recognition system; however, if the choice of the feature elements is not properly made, this in turn may affect the classification performance. Also, as the number of feature elements in the feature extraction step decreases, the neural network classifier becomes small with a simple structure. The proposed feature extractor in this paper yields a feature vector with low dimensionality, and, by disregarding irrelevant data from face portion of the image, it improves the recognition rate. The proposed feature extraction is done in two steps. In the first step, after face localization, we create a subimage which contains information needed for the recognition algorithm. In the second step, the feature vector is obtained by calculating the PZMI of the derived subimage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Creating a subimage</head><p>To create a subimage for feature extraction phase, all pertinent information around the face region is enclosed in an ellipse while pixel values outside the ellipse are set to zero. Unfortunately, through creation of the subimage with the best-fit ellipse, as described in Section 2, many unwanted regions of the face image may still appear in this subimage, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. These include hair portion, neck, and part of the background as an example. To overcome this problem, instead of using the best-fit ellipse for creating a subimage, we have defined another ellipse. The proposed ellipse has the same orientation and center as the best-fit ellipse but the lengths of its major and minor axes are calculated from the lengths of the major and minor axes of the best-fit ellipse as follows:</p><formula xml:id="formula_11">A = ρ • α, B = ρ • β, (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where A and B are the lengths of the major and minor axes of the proposed ellipse, and α and β are the lengths of the major and minor axes of the best-fit ellipse that have been defined in <ref type="bibr" target="#b4">(5)</ref>. The coefficient ρ is called ACR and varies from 0 to 1. Figure <ref type="figure">3</ref> shows the effect of changing ACR while Figure <ref type="figure" target="#fig_4">4</ref> shows the corresponding subimages.</p><p>Our experimental results with 400 face images show that the best value for ACR is around 0.87. By using the above procedure, data that are irrelevant to facial portion are disregarded. The feature vector is then generated by computing the PZMI of the subimage obtained in the previous stage. It should be noted that the speed of computing the PZMI is considerably increased due to smaller pixel content of the subimages.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pseudo-Zernike moment invariant</head><p>Statistics-based approaches for feature extraction are very important in pattern recognition for their computational efficiency and their use of global information in an image for extracting features <ref type="bibr" target="#b14">[15]</ref>. The advantages of considering orthogonal moments are that they are shift, rotation, and scale invariants and are very robust in the presence of noise. The invariant properties of moments are utilized as pattern sensitive features in classification and recognition applications <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Pseudo-Zernike polynomials are well known and widely used in the analysis of optical systems. Pseudo-Zernike polynomials are orthogonal sets of complex-valued polynomials defined as (see <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>)</p><formula xml:id="formula_13">V nm (x, y) = R nm (x, y) exp jm tan -1 y x ,<label>(8)</label></formula><p>where x 2 + y 2 ≤ 1, n ≥ 0, |m| ≤ n, and the radial polynomials {R n,m } are defined as</p><formula xml:id="formula_14">R n,m (x, y) = n-|m| s=0 D n,|m|,s x 2 + y 2 (n-s)/2 , (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where</p><formula xml:id="formula_16">D n,|m|,s = (-1) s (2n + 1 -s)! s! n -|m| -s ! n -|m| -s + 1 ! . (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>The PZMI of order n and repetition m can be computed using the scale invariant central moments CM p,q and the radial geometric moments RM p,q as follows <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>:</p><formula xml:id="formula_18">PZMI nm = n + 1 π n-|m| (n-m-s) even, s=0 D n,|m|,s × k a=0 m b=0 k a m b (-j) b CM 2k+m-2a-b, 2a+b + n + 1 π n-|m| (n-m-s) odd, s=0 D n,|m|,s × d a=0 m b=0 d a m b (-j) b RM 2d+m-2a-b, 2a+b , (<label>11</label></formula><formula xml:id="formula_19">)</formula><p>where</p><formula xml:id="formula_20">k = (n -s -m)/2, d = (n -s -m + 1)/2</formula><p>, and also CM p,q and RM p,q are as follows:</p><formula xml:id="formula_21">CM p,q = µ pq M (p+q+2)/2 00 , RM p,q = x y f (x, y) x 2 + y 2 1/2 x p y q M (p+q+2)/2 00 , (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>where x = xx 0 , y = yy 0 , and x 0 , y 0 , µ pq , and M pq are defined in (1) and (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Selecting feature vector elements</head><p>After face localization and subimage creation, we calculate the PZMI inside each subimage as face features. For selecting the best order of the PZMI as face feature elements, we define a feature vector in face recognition application whose elements are based on the PZMI orders as follows:</p><formula xml:id="formula_23">FV j = PZMI km , k = j, j + 1, . . . , N,<label>(13)</label></formula><p>where j varies from 1 to N -1, therefore, FV j is a feature vector which contains all the PZMI from order j to N. Table <ref type="table" target="#tab_0">1</ref> shows samples of feature vector elements for j = 3, 5, 9 when N = 10. Also, Figure <ref type="figure" target="#fig_5">5</ref> shows the number of feature vector elements relative to j value. As Figure <ref type="figure" target="#fig_5">5</ref> shows, when j increases, the number of elements in each feature vector (FV j ) decreases. These results are based on a value of N = 10. Our experimental study indicates that this method of selecting the pseudo-Zernike moment order as the feature elements allows the feature extractor to have a lower-dimensional vector while maintaining a good discrimination capability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CLASSIFIER DESIGN</head><p>Neural network is widely used as a classifier in many face recognition systems. Neural networks have been employed and compared to conventional classifiers for a number of classification problems. The results have shown that the accuracy of the neural network approaches is equivalent to, or slightly better than, other methods. Also, due to the simplicity, generality, and good learning ability of the neural networks, these types of classifiers are found to be more efficient <ref type="bibr" target="#b22">[23]</ref>.</p><p>RBF neural networks have been found to be very attractive for many engineering problems because (1) they are universal approximators, (2) they have a very compact topology, and (3) their learning speed is very fast because of their locally tuned neurons <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. An important property of RBF neural networks is that they form a unifying link between many different research fields such as function approximation, regularization, noisy interpolation, and pattern recognition. Therefore, RBF neural networks serve as an excellent candidate for pattern classification where attempts have been carried out to make the learning process in this type of classification faster than normally required for the multilayer feedforward neural networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>In this paper, an RBF neural network is used as a classifier in a face recognition system where the inputs to the neural network are feature vectors derived from the proposed feature extraction technique described in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">RBF neural network structure</head><p>An RBF neural network structure is shown in Figure <ref type="figure" target="#fig_6">6</ref>. The construction of the RBF neural network involves three different layers with feedforward architecture. The input layer of the neural network is a set of n units, which accept the elements of an n-dimensional input feature vector. The input units are fully connected to the hidden layer with r hidden units. Connections between the input and hidden layers have unit weights and, as a result, do not have to be trained. The goal of the hidden layer is to cluster the data and reduce its dimensionality. In this structure, the hidden units are referred to as the RBF units. The RBF units are also fully connected to the output layer. The output layer supplies the response of the neural network to the activation pattern applied to the input layer. The transformation from the input space to the RBF-unit space is nonlinear (nonlinear activation function), whereas the transformation from the RBF-unit space to the output space is linear (linear activation function). The RBF neural network is a class of neural networks where the activation function of the hidden units is determined by the distance between the input vector and a prototype vector. The activation function of the RBF units is expressed as follows <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>:</p><formula xml:id="formula_24">R i (x) = R i x -c i σ i , i = 1, 2, . . . , r, (<label>14</label></formula><formula xml:id="formula_25">)</formula><p>where x is an n-dimensional input feature vector, c i is an ndimensional vector called the center of the RBF unit, σ i is the width of the RBF unit, and r is the number of the RBF units. Typically, the activation function of the RBF units is chosen as a Gaussian function with mean vector c i and variance vector σ i as follows:</p><formula xml:id="formula_26">R i (x) = exp - x -c i 2 σ 2 i . (<label>15</label></formula><formula xml:id="formula_27">)</formula><p>Note that σ 2 i represents the diagonal entries of the covariance matrix of the Gaussian function. The output units are linear and the response of the jth output unit for input x is</p><formula xml:id="formula_28">y j (x) = b( j) + r i=1 R i (x)w 2 (i, j), (<label>16</label></formula><formula xml:id="formula_29">)</formula><p>where w 2 (i, j) is the connection weight of the ith RBF unit to the jth output node and b( j) is the bias of the jth output.</p><p>The bias is omitted in this network in order to reduce the neural network complexity <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Therefore,</p><formula xml:id="formula_30">y j (x) = r i=1 R i (x) × w 2 (i, j). (<label>17</label></formula><formula xml:id="formula_31">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">RBF neural network classifier design</head><p>To design a classifier based on RBF neural networks, we have set the number of input nodes in the input layer of the neural network equal to the number of feature vector elements. The number of nodes in the output layer is then set to the number of image classes. The number of RBF units as well as their characteristic initialization is carried out using the following steps <ref type="bibr" target="#b16">[17]</ref>.</p><p>Step 1. Initially, the RBF units are set equal to the number of outputs.</p><p>Step 2. For each class k (k = 1, 2, . . . , s), the center of the RBF unit is selected as the mean value of the sample features, belonging to the same class, that is,</p><formula xml:id="formula_32">C k = N k i=1 p k (n, i) N k , k = 1, 2, . . . , s,<label>(18)</label></formula><p>where p k (n, i) is the ith sample with n as the number of features belonging to class k and N k is the number of images in the same class.</p><p>Step 3. For each class k, compute the distance d k from the mean C k to the farthest point p k f belonging to class k:</p><formula xml:id="formula_33">d k = p k f -C k , k = 1, 2, . . . , s. (<label>19</label></formula><formula xml:id="formula_34">)</formula><p>Step 4. For each class k, compute the distance dc(k, j) between the mean of the class and the mean of other classes as follows: k is overlapping with other classes and misclassifications may occur in this case.</p><formula xml:id="formula_35">dc(k, j) = C k -C j , j = 1, 2, . . . , s, j = k. (<label>20</label></formula><p>Step 5. If two classes are overlapped strongly, we first split one of the classes into two to remove the overlap. If the overlap is not removed, the second class is also split. This requires addition of a new RBF unit to the hidden layer.</p><p>Step 6. Repeat Steps 2 to 5 until all the training sample patterns are classified correctly.</p><p>Step 7. The mean values of the classes are selected as the centers of RBF units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hybrid learning algorithm</head><p>The training of the RBF neural networks can be made faster than the methods used to train multilayer neural networks. This is based on the properties of the RBF units, and can lead to a two-stage training procedure. The first stage of the training involves determining output connection weights, which requires solution of a set of linear equations which can be done fast. In the second stage, the parameters governing the basis function (corresponding to the RBF units) are determined using an unsupervised learning method that requires solution of a set of nonlinear equations. The training of the RBF neural networks involves estimating output connection weights, centers, and widths of the RBF units. Dimensionality of the input vector, and the number of classes set the number of input and output units, respectively. In this paper, an HLA, which combines the gradient method and the linear least square (LLS) method, is used for training the neural network <ref type="bibr" target="#b16">[17]</ref>. This is done in two steps. In the first step, the neural network connection weights in the output of the RBF units (w 2 (i, j)) are adjusted under the assumption that the centers and the widths of the RBF units are known a priori. In the second step, the centers and widths (c and σ) of the RBF units are updated as described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Computing connection weights</head><p>Let r and s be the number of inputs and outputs, respectively, and assume that the number of u RBF outputs is generated for all training face patterns. For any input P i (p 1i , p 2i , . . . , p ri ), the jth output y j of the RBF neural network in ( <ref type="formula" target="#formula_24">14</ref>) can be calculated in a more compact form as follows:</p><formula xml:id="formula_36">W 2 × R = Y, (<label>21</label></formula><formula xml:id="formula_37">)</formula><p>where R ∈ u×N is the matrix of the RBF units, W 2 ∈ s×u is the output connection weight matrix, Y ∈ s×N is the output matrix, and N is the total number of sample face patterns. The relationship for error is defined by</p><formula xml:id="formula_38">E = T -Y , (<label>22</label></formula><formula xml:id="formula_39">)</formula><p>where T = (t 1 , t 2 , . . . , t s ) T ∈ s×N is the target matrix consisting of ones and zeros with each column having only one nonzero element and identifies the processing pattern to which the given exemplar belongs.</p><p>Our objective is to find an optimal coefficient matrix W 2 ∈ s×u such that E T E is minimized. This is done by the well-known LLS method <ref type="bibr" target="#b15">[16]</ref> as follows:</p><formula xml:id="formula_40">W 2 × R = Y. (<label>23</label></formula><formula xml:id="formula_41">)</formula><p>The optimal W 2 is given by</p><formula xml:id="formula_42">W 2 = T × R + , (<label>24</label></formula><formula xml:id="formula_43">)</formula><p>where R + is the pseudoinverse of R and is given by</p><formula xml:id="formula_44">R + = R T R -1 R T . (<label>25</label></formula><formula xml:id="formula_45">)</formula><p>We can compute the connection weights using ( <ref type="formula" target="#formula_38">22</ref>) and ( <ref type="formula" target="#formula_40">23</ref>) by knowing matrix R as follows:</p><formula xml:id="formula_46">W 2 = T R T R -1 R T . (<label>26</label></formula><formula xml:id="formula_47">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Defining the center and width of the RBF units</head><p>Here, the center and width of the RBF units (R matrix) are adjusted by taking the negative gradient of the error function, E n , for the nth sample pattern which is given by [25]</p><formula xml:id="formula_48">E n = 1 2 s k=1 t n k -y n k 2 , n = 1, 2, . . . , N, (<label>27</label></formula><formula xml:id="formula_49">)</formula><p>where y n k and t n k represent the kth real output and target output of the nth sample face pattern, respectively.</p><p>For the RBF units with the center C and the width σ, the update value for the center can be derived from ( <ref type="formula" target="#formula_44">25</ref>) by the chain rule as follows:</p><formula xml:id="formula_50">∆C n (i, j) = -ξ ∂E n ∂C n (i, j) = 2ξ s k=1 y n k • w n 2 (k, j) • R n j • P n i -C n (i, j) σ n j 2 (<label>28</label></formula><formula xml:id="formula_51">)</formula><p>and the update value for the width is computed as follows:</p><formula xml:id="formula_52">∆σ n j = -ξ ∂E n ∂σ n j = 2ξ s k=1 y n k • w n 2 (k, j) • R n j • P n i -C n (i, j) σ n j 3 , (<label>29</label></formula><formula xml:id="formula_53">)</formula><p>where i = 1, 2, . . . , r, j = 1, 2, . . . , u, P n i is the ith input variable of the nth sample face pattern, and ξ is the learning rate. ∆C n (i, j) is the update value for the ith variable of the center of the jth RBF unit based on the nth training pattern. ∆σ n j is the update value for the width of the jth RBF unit with respect to the nth training pattern. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head><p>To check the utility of our proposed algorithm, experimental studies are carried out on the ORL database images of Cambridge University. This database contains 400 facial images from 40 individuals in different states, taken between April 1992 and April 1994. The total number of images for each person is 10. None of the 10 images is identical to any other. They vary in position, rotation, scale, and expression. The changes in orientation have been accomplished by each person rotating a maximum of 20 degrees in the same plane, as well as each person changing his/her facial expression in each of the 10 images (e.g., open/close eyes, smiling/not smiling). The changes in scale have been achieved by changing the distance between the person and the video camera. For some individuals, the images were taken at different times and varying facial details (glasses/no glasses). All the images were taken against a dark homogeneous background. Each image was digitized and presented by a 112 × 92 pixel array whose gray levels ranged between 0 and 255. Samples of database used are shown in Figure <ref type="figure" target="#fig_8">7</ref>.</p><p>Experimental studies have been done by dividing database images into training and test sets. A total of 200 images are used to train and another 200 are used to test. Each training set consists of 5 randomly chosen images from the same class in the training stage. There is no overlap between the training and test sets. In the face localization step, the shape information algorithm with FCT has been applied to all images. Subsequently, calculating the PZMI of the subimage, which is created with ACR value, creates the feature vector. The RBF classifier is trained using the HLA method with the training sets, and finally the classifier error rate is computed with respect to the test images. In this study, the classifier error rate is considered as the number of misclassifications in the test phase over the total number of test images. The experimental study conducted in this paper evaluates the effect of the PZMI orders, ACR, FCT, and the presence of noise in images on the recognition rate. Also the utility of the learning algorithm on the recognition rate is studied. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of moment orders</head><p>In this phase of the experiment, simulation has been done, based on the j value defined in <ref type="bibr" target="#b12">(13)</ref>. The ACR has been set equal to one and the RBF neural network classifier has been trained for each j value based on the training images.</p><p>Figure <ref type="figure">8</ref> shows the error rate of the system with respect to j. This figure shows that when j increases, the error rate almost remains unchanged. In contrast, as Figure <ref type="figure" target="#fig_5">5</ref> has shown, when j increases, the number of feature elements of the feature vector in the feature extraction step decreases. This observation is interesting because in spite of the decrease in the number of feature elements, the error rate has remained unchanged. Also, these results show that higher orders of the PZMI contain more and useful information for face recognition process. Figure <ref type="figure">9</ref> shows the misclassified images and their corresponding training sets for the value of j = 9. As indicated in Figure <ref type="figure">9a</ref>, the misclassified image in this set is substantially different from the training set in terms of its facial expression, while the reason for misclassification of the images in Figures <ref type="figure">9b</ref> and<ref type="figure">9c</ref> can be explained with the effect of the irrelevant data in the test images with respect to their training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of ACR when disregarding irrelevant data</head><p>For the purpose of evaluating how the irrelevant data of a facial image such as hair, neck, shoulders, and background will influence the recognition results, we have chosen the PZMI of orders 9 and 10 (set j = 9) for feature extraction. We have also selected FCT = 0.1 for the face localization algorithm and the RBF neural network with the HLA as the classifier. We varied the ACR value and evaluated the recognition rate of the proposed algorithm. Figure <ref type="figure" target="#fig_10">10</ref> shows the effect of ACR values on the error rate.</p><p>As Figure <ref type="figure" target="#fig_10">10</ref> shows, the error rate varies as the ACR values change. At ACR = 1, a recognition rate of 98.7% is obtained (Section 5.1). Now, by changing ACR and calculating the correct recognition rate, it is observed that at ACR = 0.87, a recognition rate of 99.3% can be achieved. This clearly indicates the importance of the ACR in improving the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of FCT when distinguishing between face and nonface regions</head><p>To evaluate the effect of FCT in the face localization step and distinguishing between face and nonface images, we prepared 20 nonface images and applied them to the system. Figure <ref type="figure" target="#fig_2">2</ref> shows a sample of such images with φ i = 0.13 and φ o = 0.191. We varied the FCT value and evaluated the number of nonface images that passed through the system. Experimental results showed that FCT = 0.1 is a good threshold for distinguishing between face and nonface images. Figure <ref type="figure" target="#fig_0">11</ref> shows this result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effect of the HLA method on the RBF classifier</head><p>To investigate the effect of the learning method HLA on the RBF neural network, the ACR has been set equal to one and we have created four categories of feature vectors based on the order (n) of the PZMI. In the first category with n = 1, 2, . . . as feature vector elements. The number of the feature vector elements in this category is 27. In the second category, n = 4, 5, 6, 7 is chosen. All the moments of each order included in this category are summed up to create a feature vector of size 26. In the third category, n = 6, 7, 8 is considered, and the feature vector has 24 elements. Finally, in the last category, n = 9, 10 is considered with 21 feature elements. The neural network classifier was trained in each category based on the training images, and subsequently, the system was tested using the test images. The experimental results are shown in Table <ref type="table" target="#tab_2">2</ref>. This table indicates that in the training phase of the RBF neural network classifier, the number of epochs decreases when the PZMI orders increase. On the other hand, the RBF neural network with the HLA learning method has converged faster in the training phase when higher orders of the PZMI are used in comparison with the lower orders of PZMI. Also, this table indicates that the HLA method in the training phase has a lower root mean squared error (RMSE) with a good discrimination capability.</p><p>To compare the HLA with other learning algorithms, we have developed the k-mean clustering algorithm for training the RBF neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref> and applied it to the database with the same feature extraction technique. Table <ref type="table" target="#tab_3">3</ref> shows the comparison between the two learning methods. It is seen from Table <ref type="table" target="#tab_3">3</ref> that the HLA method converges faster than the k-mean clustering which needs fewer epochs in the training phase. Also, the RMSE during the training phase for the HLA is smaller than that for the k-mean clustering learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Performance evaluation in the presence of noise</head><p>To evaluate the performance of the feature extraction method with ACR parameter, PZMI, and the RBF neural network for human face recognition in the presence of noise, a white Gaussian noise of zero mean and different amplitudes (in gray-level image) has been added to the clean images. The recognition process was then applied to the noisy images. nition is very robust in the presence of noise. In Figure <ref type="figure" target="#fig_13">13</ref>, samples of noisy images have been shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison with other human face recognition systems</head><p>To compare the effectiveness of the proposed method with other algorithms, the PZMI of orders 9 and 10 with 21 feature elements, FCT = 0.1, ACR = 0.87, and the RBF neural network with the HLA learning algorithm have been used. This study compares the proposed technique with the methods that used the same ORL database. These include the shape information neural network (SINN) <ref type="bibr" target="#b14">[15]</ref>, convolution neural network (CNN) <ref type="bibr" target="#b26">[27]</ref>, nearest feature line (NFL) <ref type="bibr" target="#b27">[28]</ref>, and the fractal transformation (FT) <ref type="bibr" target="#b28">[29]</ref>. In this comparison, the training set and the test set were derived in the same way as was suggested in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>: the 10 images from each class of the 40 persons were randomly partitioned into sets, resulting in 200 training images and 200 test images, with no overlap between the two. Also in this study, the error rate was defined, as was used in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, to be the number of misclassified images in the test phase over the total number of test images. To conduct the comparison, an average error rate which has been used in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> was utilized as defined below:</p><formula xml:id="formula_54">E ave = m i=1 N i m mN t , (<label>30</label></formula><formula xml:id="formula_55">)</formula><p>where m is the number of experimental runs, each being performed on a random partitioning of the database into sets, N i m is the number of misclassified images for the ith run, and N t is the number of total test images for each run. Table <ref type="table" target="#tab_4">4</ref> shows the comparison between the different techniques using the same ORL database in terms of E ave . In this table, the CNN error rate was based on the average of three runs as given in <ref type="bibr" target="#b26">[27]</ref>, while for the NFL, the average error rate of four runs was reported in <ref type="bibr" target="#b27">[28]</ref>. Also an average run of one for the FT <ref type="bibr" target="#b28">[29]</ref> and four runs for the SINN <ref type="bibr" target="#b14">[15]</ref> were carried out as suggested in the respective papers. The average error rate of the proposed method for the four runs is 0.682%, which yields the lowest error rate of these techniques on the ORL database.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>This paper presented an efficient method for the recognition of human faces in frontal view of facial images. The proposed technique utilizes a modified feature extraction technique, which is based on a flexible face localization algorithm followed by PZMI. An RBF neural network with the HLA method was used as a classifier in this recognition system. The paper introduces several parameters for efficient and robust feature extraction technique as well as the RBF neural network learning algorithm. These include FCT, ACR, and selection of the PZMI orders and the HLA method. Exhaustive experimentation was carried out to investigate the effect of varying these parameters on the recognition rate. We have shown that high order PZMI contains very useful information about the facial images, and that the HLA method affects the learning speed. We have also indicated the optimum values of the FCT and ACR corresponding to the best recognition results on the ORL database. The robustness of the proposed algorithm in the presence of noise is also investigated. The highest recognition rate of 99.3% with ORL database was obtained using the proposed algorithm. We have implemented and tested some of the existing face recognition techniques on the same ORL database. This comparative study indicates the usefulness and the utility of the proposed technique.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Face model based on ellipse model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>φ i = 0.065 φ o = 0.008 φ i = 0.062 φ o = 0.011 φ i = 0.15 φ o = 0.191</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distinguishing between face and nonface using best-fit ellipse and FCT threshold.</figDesc><graphic coords="4,54.37,222.32,234.09,70.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>ρ 4 Figure 3 :</head><label>43</label><figDesc>Figure 3: Different ellipses with respect to ACR.</figDesc><graphic coords="4,53.67,352.97,234.87,70.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Subimage formation based on different ellipses and ACR values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Number of feature elements (PZMI) with respect to j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: RBF neural network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>) Then, find d min (k, l) = min(dc(k, j)) and check the relationship between d min (k, l), d k , and d l . If d k + d l ≤ d min (k, l), then class k is not overlapping with other classes. Otherwise, class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Samples of facial images in ORL database.</figDesc><graphic coords="7,311.91,73.05,234.92,155.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Error rate with respect to j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Error rate variation with respect to ACR value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 Figure 12 :</head><label>1212</label><figDesc>Figure12: Error rate with respect to noise amplitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>∼ 0.04 n = 6, 7, 8 9 5 ∼ 70 0.06 ∼ 0.04 45 ∼ 60 0.04 ∼ 0.02 n = 9, 10 70 ∼ 50 0.04 ∼ 0.02 30 ∼ 45 0.04 ∼ 0.01 Noise amplitude = 10 Noise amplitude = 20 Noise amplitude = 40 Noise amplitude = 50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Samples of noisy images with different noise value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Feature vector elements based on PZMI.</figDesc><table><row><cell cols="2">j value</cell><cell cols="4">PZMI feature elements n value M value</cell><cell></cell><cell cols="3">Number of feature elements</cell></row><row><cell></cell><cell></cell><cell cols="2">3 0,1,2,3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">4 0,1,2,3,4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">5 0,1,2,3,4,5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell cols="4">6 0,1,2,3,4,5,6 7 0,1,2,3,4,5,6,7</cell><cell></cell><cell></cell><cell>60</cell></row><row><cell></cell><cell></cell><cell cols="4">8 0,1,2,3,4,5,6,7,8</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">9 0,1,2,3,4,5,6,7,8,9</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">10 0,1,2,3,4,5,6,7,8,9,10</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">6 0,1,2,3,4,5,6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">7 0,1,2,3,4,5,6,7</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6</cell><cell cols="4">8 0,1,2,3,4,5,6,7,8</cell><cell></cell><cell></cell><cell>45</cell></row><row><cell></cell><cell></cell><cell cols="4">9 0,1,2,3,4,5,6,7,8,9</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">10 0,1,2,3,4,5,6,7,8,9,10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>9</cell><cell cols="5">9 0,1,2,3,4,5,6,7,8,9 10 0,1,2,3,4,5,6,7,8,9,10</cell><cell></cell><cell>21</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No. of moments</cell><cell>20 40 60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">j value</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>6, all the moments of PZMI are considered</figDesc><table><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Passed nonface image (%)</cell><cell>10 20 30 40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell>0.04</cell><cell>0.06</cell><cell>0.08</cell><cell>0.1</cell><cell>0.12</cell><cell>0.14</cell><cell>0.16</cell><cell>0.18</cell><cell>0.2</cell><cell>0.22</cell><cell>0.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FCT value</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Figure 11: Effect of FCT.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of the HLA method in learning phase.</figDesc><table><row><cell cols="2">Features vectors</cell><cell cols="2">Training phase</cell><cell cols="2">Testing phase</cell></row><row><cell>Category</cell><cell>No. of feature flements</cell><cell>No. of epochs</cell><cell>RMSE</cell><cell>No. of misclassified</cell><cell>Error rate</cell></row><row><cell>n = 1, 2, . . . , 6</cell><cell>2 7</cell><cell>8 0 ∼ 100</cell><cell>0.09 ∼ 0.06</cell><cell>15</cell><cell>7.5%</cell></row><row><cell>n = 4, 5, 6, 7</cell><cell>2 6</cell><cell>6 0 ∼ 80</cell><cell>0.06 ∼ 0.04</cell><cell>9</cell><cell>4.5%</cell></row><row><cell>n = 6, 7, 8</cell><cell>2 4</cell><cell>4 5 ∼ 60</cell><cell>0.04 ∼ 0.02</cell><cell>6</cell><cell>3%</cell></row><row><cell>n = 9, 10</cell><cell>21</cell><cell>30 ∼ 45</cell><cell>0.04 ∼ 0.01</cell><cell>3</cell><cell>1.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison between the two learning techniques.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Error rates of different approaches.</figDesc><table><row><cell>Methods</cell><cell>No. of experimental (m)</cell><cell>E ave %</cell></row><row><cell>CNN [27]</cell><cell>3</cell><cell>3.83</cell></row><row><cell>NFL [28]</cell><cell>4</cell><cell>3.125</cell></row><row><cell>FT [29]</cell><cell>1</cell><cell>1.75</cell></row><row><cell>SINN [15]</cell><cell>4</cell><cell>1.323</cell></row><row><cell>Proposed method</cell><cell>3</cell><cell>0.682</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Javad Haddadnia received his B.S. and M.S. degrees in electrical and electronic engineering with the first rank from Amirkabir University of Technology, Tehran, Iran, in 1993 and 1995, respectively. He received his Ph.D. degree in electrical engineering from Amirkabir University of Technology, Tehran, Iran in 2002. He joined Tarbiat Moallem University of Sabzevar in Iran. His research interests include neural network, digital image processing, computer vision, and face detection and recognition. He has published several papers in these areas. He has served as a Visiting Research Scholar at the University of Windsor, Canada during 2001-2002. He is a member of SPIE, CIPPR, and IEICE.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Natural Sciences and Engineering Research Council (NSERC) of Canada and Micronet for supporting this research and the anonymous reviewers for helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On internal representation in face recognition systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Grudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1161" to="1177" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human face recognition using shape information and pseudo Zernike moments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haddadnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th International Fall Workshop Vision, Modeling and Visualization (VMV &apos;00)</title>
		<meeting>5th International Fall Workshop Vision, Modeling and Visualization (VMV &apos;00)<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face localization and facial feature extraction based on shape and color information</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sobottka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Image Processing (ICIP &apos;96)</title>
		<meeting>IEEE International Conference on Image essing (ICIP &apos;96)<address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-09">September 1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="483" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new face detection method based on shape information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letter</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="463" to="471" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face detection: a survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hjelmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision And Image Understanding</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="236" to="274" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human face recognition and the face image sets topology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bichsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image Understanding</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="261" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparisons between human and computer recognition of faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J B</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd International Conference On Automatic Face and Gesture Recognition (FG &apos;98)</title>
		<meeting>3rd International Conference On Automatic Face and Gesture Recognition (FG &apos;98)<address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-04">April 1998</date>
			<biblScope unit="page" from="408" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why recognition in a statistics-based face recognition system should be based on the pure face portion: a probabilistic decision-based proof</title>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1393" to="1403" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face recognition system using local autocorrelations and multiscale integration</title>
		<author>
			<persName><forename type="first">F</forename><surname>Goudail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kyuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1024" to="1028" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Example-Based Learning for View-Based Human Face Detection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<editor>I. Memo</editor>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">face + hair + shoulders + background = face</title>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on 3D Computer Vision &apos;97</title>
		<meeting>Workshop on 3D Computer Vision &apos;97<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
			<biblScope unit="page" from="91" to="96" />
		</imprint>
		<respStmt>
			<orgName>The Chinese University of Hong Kong</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An efficient method for recognition of human faces using higher orders pseudo Zernike moment invariant</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haddadnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th International Conference On Automatic Face and Gesture Recognition (FG &apos;02)</title>
		<meeting>5th International Conference On Automatic Face and Gesture Recognition (FG &apos;02)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="330" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural network based face recognition with moments invariant</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haddadnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moallem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Image Processing (ICIP &apos;01)</title>
		<meeting>IEEE International Conference on Image essing (ICIP &apos;01)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10">October 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1018" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human face recognition using radial basis function neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haddadnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd International Conference On Human and Computer (HC &apos;00)</title>
		<meeting>3rd International Conference On Human and Computer (HC &apos;00)<address><addrLine>Aizu, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-09">September 2000</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hybrid learning RBF neural network for human face recognition with pseudo Zernike moment invariant</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haddadnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference On Neural Network (IJCNN &apos;02)</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detection and tracking of faces in real environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Herpers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Verghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Recognition, Analysis, and Tracking of Face and Gesture in Real-Time Systems</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-09">September 1999</date>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the accuracy of Zernike moments for image analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pawlak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1358" to="1364" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On image analysis by the methods of moments</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="496" to="513" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pattern recognition with moment invariants: a comparative study and new results</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Belkasim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1117" to="1138" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Orthogonal moment features for use with parametric and non-parametric classifiers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Srinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="399" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Verification of the nonparametric characteristics of backpropagation neural networks for image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="771" to="779" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance evaluation of a sequential minimal radial basis function (RBF) neural network learning algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yingwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="308" to="318" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ANFIS: adaptive-network-based fuzzy inference system</title>
		<author>
			<persName><forename type="first">J.-S</forename><forename type="middle">R</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="665" to="685" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mixture of experts for classification of gender, ethnic origin, and pose of human faces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jonathon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="948" to="960" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face recognition: a convolutional neural network approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face recognition using the nearest feature line method</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="439" to="443" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face recognition by fractal transformations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing (ICASSP &apos;99)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, Signal essing (ICASSP &apos;99)<address><addrLine>Phoenix, Ariz, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-03">March 1999</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3537" to="3540" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
