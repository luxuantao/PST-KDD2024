<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X-GOAL: Multiplex Heterogeneous Graph Prototypical Contrastive Learning Baoyu Jing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-18">18 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
							<email>shengyuf@andrew.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Shengyu Feng</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Language Technology Institute Carnegie Mellon University Yuejia Xiang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">CIKM &apos;22</orgName>
								<address>
									<addrLine>October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">X-GOAL: Multiplex Heterogeneous Graph Prototypical Contrastive Learning Baoyu Jing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-18">18 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3511808.3557490</idno>
					<idno type="arXiv">arXiv:2109.03560v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>• Information systems → Data mining</term>
					<term>• Computing methodologies → Unsupervised learning</term>
					<term>• Networks</term>
					<term>Prototypical Contrastive Learning, Multiplex Heterogeneous Graphs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graphs are powerful representations for relations among objects, which have attracted plenty of attention in both academia and industry. A fundamental challenge for graph learning is how to train an effective Graph Neural Network (GNN) encoder without labels, which are expensive and time consuming to obtain. Contrastive Learning (CL) is one of the most popular paradigms to address this challenge, which trains GNNs by discriminating positive and negative node pairs. Despite the success of recent CL methods, there are still two under-explored problems. Firstly, how to reduce the semantic error introduced by random topology based data augmentations. Traditional CL defines positive and negative node pairs via the node-level topological proximity, which is solely based on the graph topology regardless of the semantic information of node attributes, and thus some semantically similar nodes could be wrongly treated as negative pairs. Secondly, how to effectively model the multiplexity of the real-world graphs, where nodes are connected by various relations and each relation could form a homogeneous graph layer. To solve these problems, we propose a novel multiplex heterogeneous graph prototypical contrastive leaning (X-GOAL) framework to extract node embeddings. X-GOAL is comprised of two components: the GOAL framework, which learns node embeddings for each homogeneous graph layer, and an alignment regularization, which jointly models different layers by aligning layer-specific node embeddings. Specifically, the GOAL framework captures the node-level information by a succinct graph transformation technique, and captures the cluster-level information by pulling nodes within the same semantic cluster closer in the embedding space. The alignment regularization aligns embeddings across layers at both node level and cluster level. We evaluate the proposed X-GOAL on a variety of real-world datasets and downstream tasks to demonstrate the effectiveness of the X-GOAL framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are powerful representations of formalisms and have been widely used to model relations among various objects <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>, such as the citation relation and the same-author relation among papers. One of the primary challenges for graph representation learning is how to effectively encode nodes into informative embeddings such that they can be easily used in downstream tasks for extracting useful knowledge <ref type="bibr" target="#b12">[13]</ref>. Traditional methods, such as Graph Convolutional Network (GCN) <ref type="bibr" target="#b22">[23]</ref>, leverage human labels to train the graph encoders. However, human labeling is usually time-consuming and expensive, and the labels might be unavailable in practice <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73]</ref>. Self-supervised learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b59">60]</ref>, which aims to train graph encoders without external labels, has thus attracted plenty of attention in both academia and industry.</p><p>One of the predominant self-supervised learning paradigms in recent years is Contrastive Learning (CL), which aims to learn an effective Graph Neural Network (GNN) encoder such that positive node pairs will be pulled together and negative node pairs will be pushed apart in the embedding space <ref type="bibr" target="#b59">[60]</ref>. Early methods, such as DeepWalk <ref type="bibr" target="#b41">[42]</ref> and node2vec <ref type="bibr" target="#b11">[12]</ref>, sample positive node pairs based on their local proximity in graphs. Recent methods rely on graph transformation or augmentation <ref type="bibr" target="#b59">[60]</ref> to generate positive pairs and negative pairs, such as random permutation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b52">53]</ref>, structure based augmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b66">67]</ref>, sampling based augmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref> as well as adaptive augmentation <ref type="bibr" target="#b75">[76]</ref>.</p><p>Albeit the success of these methods, they define positive and negative node pairs based upon the node-level information (or local topological proximity) but have not fully explored the cluster-level (or semantic cluster/prototype) information. For example, in an academic graph, two papers about different sub-areas in graph learning (e.g., social network analysis and drug discovery) might not topologically close to each other since they do not have a direct citation relation or same-author relation. Without considering their semantic information such as the keywords and topics, these two papers could be treated as a negative pair by most of the existing methods. Such a practice will inevitably induce semantic errors to node embeddings, which will have a negative impact on the performance of machine learning models on downstream tasks such as classification and clustering. To address this problem, inspired by <ref type="bibr" target="#b26">[27]</ref>, we introduce a graph prototypical contrastive learning (GOAL) framework to simultaneously capture both node-level and cluster-level information. At the node level, GOAL trains an encoder by distinguishing positive and negative node pairs, which are sampled by a succinct graph transformation technique. At the cluster level, GOAL employs a clustering algorithm to obtain the semantic clusters/prototypes and it pulls nodes within the same cluster closer to each other in the embedding space.</p><p>Furthermore, most of the aforementioned methods ignore the multiplexity <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref> of the real-world graphs, where nodes are connected by multiple types of relations and each relation formulates a layer of the multiplex heterogeneous graph. For example, in an academic graph, papers are connected via the same authors or the citation relation; in an entertainment graph, movies are linked through the shared directors or actors/actresses; in a product graph, items have relations such as also-bought and also-view. Different layers could convey different and complementary information. Thus jointly considering them could produce more informative embeddings than separately treating different layers and then applying average pooling over them to obtain the final embeddings <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>. Most of the prior deep learning methods use attention mechanism <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b57">58]</ref> to combine embeddings from different layers. However, attention modules usually require extra tasks or loss functions to train, such as node classification <ref type="bibr" target="#b57">[58]</ref> and concensus loss <ref type="bibr" target="#b37">[38]</ref>. Besides, some attention modules are complex which require significant amount of extra efforts to design and tune, such as the hierarchical structures <ref type="bibr" target="#b57">[58]</ref> and complex within-layer and cross-layer interactions <ref type="bibr" target="#b30">[31]</ref>. Different from the prior methods, we propose an alternative nimble alignment regularization to jointly model and propagate information across different layers by aligning the layerspecific embeddings without extra neural network modules, and the final node embeddings are obtained by simply average pooling over these layer-specific embeddings. The key assumption of the alignment regularization is that layer-specific embeddings of the same node should be close to each other in the embedding space and they should also be semantically similar. We also theoretically prove that the proposed alignment regularization could effectively maximize the mutual information across layers.</p><p>We comprehensively evaluate X-GOAL on a variety of real-world attributed multiplex heterogeneous graphs. The experimental results show that the embeddings learned by GOAL and X-GOAL could outperform state-of-the-art methods of homogeneous graphs and multiplex heterogeneous graphs on various downstream tasks.</p><p>The main contributions are summarized as follows: • Method. We propose a novel X-GOAL framework to learn node embeddings for multiplex heterogeneous graphs, which is comprised of a GOAL framework for each single layer and an alignment regularization to propagate information across different layers. GOAL reduces semantic errors, and the alignment regularization is nimbler than attention modules for combining layer-specific node embeddings. • Theoretical Analysis. We theoretically prove that the proposed alignment regularization can effectively maximize the mutual information across layers. • Empirical Evaluation. We comprehensively evaluate the proposed methods on various real-world datasets and downstream tasks. The experimental results show that GOAL and X-GOAL outperform the state-of-the-art methods for homogeneous and multiplex heterogeneous graphs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>Definition 2.1 (Attributed Multiplex Heterogeneous Graph). An attributed multiplex heterogeneous graph with 𝑉 layers and 𝑁 nodes is denoted as G M = {G 𝑣 } 𝑉 𝑣=1 , where G 𝑣 (A 𝑣 , X) is the 𝑣-th homogeneous graph layer, A 𝑣 ∈ R 𝑁 ×𝑁 and X ∈ R 𝑁 ×𝑑 𝑥 is the adjacency matrix and the attribute matrix, and 𝑑 𝑥 is the dimension of attributes. An illustration is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Problem Statement. The task is to learn an encoder E for G M , which maps the node attribute matrix X ∈ R 𝑁 ×𝑑 𝑥 to node embedding matrix H M ∈ R 𝑁 ×𝑑 without external labels, where 𝑁 is the number of nodes, 𝑑 𝑥 and 𝑑 are the dimension sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We present the X-GOAL framework for multiplex heterogeneous graphs G M , which is comprised of a GOAL framework and an alignment regularization. In Section 3.1, we present the GOAL framework, which simultaneously captures the node-level and the cluster-level information for each layer G = (A, X) of G M . In Section 3.2, we introduce a novel alignment regularization to align node embeddings across layers at both node and cluster level. In section 3.3, we provide theoretical analysis of the alignment regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The GOAL Framework</head><p>The node-level graph topology based transformation techniques might contain semantic errors since they ignore the hidden semantics and will inevitably pair two semantically similar but topologically far nodes as a negative pair. To solve this issue, we introduce a GOAL framework for each homogeneous graph layer 1 G = (A, X) to capture both node-level and cluster-level information. An illustration of GOAL is shown in Figure <ref type="figure" target="#fig_2">2</ref>. Given a homogeneous graph G and an encoder E, GOAL alternatively performs semantic clustering and parameter updating. In the semantic clustering step, a clustering algorithm C is applied over the embeddings H to obtain the hidden semantic clusters. In the parameter updating step, GOAL updates the parameters of E by the loss L given in Equation ( <ref type="formula" target="#formula_3">4</ref>), which pulls topologically similar nodes closer and nodes within the same semantic cluster closer by the node-level loss and the cluster-level loss respectively. A -Node-Level Loss. To capture the node-level information, we propose a graph transformation technique T = {T + , T − }, where T + and T − denote positive and negative transformations, along with a contrastive loss similar to InfoNCE <ref type="bibr" target="#b35">[36]</ref>.</p><p>Given an original homogeneous graph G = (A, X), the positive transformation T + applies the dropout operation <ref type="bibr" target="#b47">[48]</ref> over A and X with a pre-defined probability 𝑝 𝑑𝑟𝑜𝑝 ∈ (0, 1). We choose the dropout operation rather than the masking operation since the dropout re-scales the outputs by We define the node-level contrastive loss as:</p><formula xml:id="formula_0">L N = − 1 𝑁 𝑁 ∑︁ 𝑛=1 log e 𝑐𝑜𝑠 (h 𝑛 ,h + 𝑛 ) e 𝑐𝑜𝑠 (h 𝑛 ,h + 𝑛 ) + e 𝑐𝑜𝑠 (h 𝑛 ,h − 𝑛 )<label>(1)</label></formula><p>where 𝑐𝑜𝑠 (, ) denotes the cosine similarity, h 𝑛 , h + 𝑛 and h − 𝑛 are the 𝑛-th rows of H, H + and H − . B -Cluster-Level Loss. We use a clustering algorithm C to obtain the semantic clusters of nodes {c 𝑘 } 𝐾 𝑘=1 , where c 𝑘 ∈ R 𝑑 is the cluster center, 𝐾 and 𝑑 are the number of clusters and the dimension of embedding space. We capture the cluster-level semantic information to reduce the semantic errors by pulling nodes within the same cluster closer to their assigned cluster center. For clarity, the derivations of the cluster-level loss are provided in Appendix.</p><p>We define the probability of h 𝑛 belongs to the cluster 𝑘 by:</p><formula xml:id="formula_1">𝑝 (𝑘 |h 𝑛 ) = e (c 𝑇 𝑘 •h 𝑛 /𝜏) 𝐾 𝑘 ′ =1 e (c 𝑇 𝑘 ′ •h 𝑛 /𝜏)<label>(2)</label></formula><p>where 𝜏 &gt; 0 is the temperature parameter to re-scale the values. The cluster-level loss is defined as the negative log-likelihood of the assigned cluster 𝑘 𝑛 for h 𝑛 :</p><formula xml:id="formula_2">L C = − 1 𝑁 𝑁 ∑︁ 𝑛=1 log e (c 𝑇 𝑘𝑛 •h 𝑛 /𝜏) 𝐾 𝑘=1 e (c 𝑇 𝑘 •h 𝑛 /𝜏)<label>(3)</label></formula><p>where 𝑘 𝑛 ∈ [1, . . . , 𝐾] is the cluster index assigned to the 𝑛-th node. 1 For clarity, we drop the script 𝑣 of G 𝑣 , A 𝑣 and H 𝑣 for this subsection. C -Overall Loss. Combing the node-level loss in Equation ( <ref type="formula" target="#formula_0">1</ref>) and the cluster-level loss in Equation ( <ref type="formula" target="#formula_2">3</ref>), we have:</p><formula xml:id="formula_3">L = 𝜆 N L N + 𝜆 C L C<label>(4)</label></formula><p>where 𝜆 N and 𝜆 C are tunable hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Alignment Regularization</head><p>Real-world graphs are often multiplex in nature, which can be decomposed into multiple homogeneous graph layers G M = {G 𝑣 } 𝑉 𝑣=1 . The simplest way to extract the embedding of a node x 𝑛 in G M is separately extracting the embedding {h 𝑣 𝑛 } 𝑉 𝑣=1 from different layers and then combing them via average pooling. However, it has been empirically proven that jointly modeling different layers could usually produce better embeddings for downstream tasks <ref type="bibr" target="#b17">[18]</ref>. Most prior studies use attention modules to jointly learn embeddings from different layers, which are clumsy as they usually require extra efforts to design and train <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b57">58]</ref>. Alternatively, we propose a nimble alignment regularization to jointly learn embeddings by aligning the layer-specific {h 𝑣 𝑛 } 𝑉 𝑣=1 without introducing extra neural network modules, and the final node embedding of x 𝑛 is obtained by simply averaging the layer-specific embeddings</p><formula xml:id="formula_4">h M 𝑛 = 1 𝑉 𝑉 𝑣=1 h 𝑣 𝑛 .</formula><p>The underlying assumption of the alignment is that h 𝑣 𝑛 should be close to and reflect the semantics of {h 𝑣 ′ 𝑛 } 𝑉 𝑣 ′ ≠𝑣 . The proposed alignment regularization is comprised of both node-level and cluster-level alignments.</p><p>Given G M = {G 𝑣 } 𝑉 𝑣=1 with encoders {E 𝑣 } 𝑉 𝑣=1 , we first apply GOAL to each layer G 𝑣 and obtain the original and negative node embeddings {H 𝑣 } 𝑉 𝑣=1 and {H 𝑣− } 𝑉 𝑣=1 , as well as the cluster centers {C 𝑣 } 𝑉 𝑣=1 , where C 𝑣 ∈ R 𝐾 𝑣 ×𝑑 is the concatenation of the cluster centers for the 𝑣-th layer, 𝐾 𝑣 is the number of clusters for the 𝑣-the layer. The node-level alignment is applied over {H 𝑣 } 𝑉 𝑣=1 and {H 𝑣− } 𝑉 𝑣=1 . The cluster-level alignment is used on {C 𝑣 } 𝑉 𝑣=1 and {H 𝑣 } 𝑉 𝑣=1 . A -Node-Level Alignment. For a node x 𝑛 , its embedding h 𝑣 𝑛 should be close to embeddings {h 𝑣 ′ 𝑛 } 𝑉 𝑣 ′ ≠𝑣 and far away from the negative embedding h 𝑣− 𝑛 . Analogous to Equation (1), we define the node-level alignment regularization as:</p><formula xml:id="formula_5">R N = − 1 𝑍 𝑁 ∑︁ 𝑛=1 𝑉 ∑︁ 𝑣=1 𝑉 ∑︁ 𝑣 ′ ≠𝑣 log e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣 ′ 𝑛 ) e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣 ′ 𝑛 ) + e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣− 𝑛 )<label>(5)</label></formula><p>where 𝑍 = 𝑁𝑉 (𝑉 − 1) is the normalization factor. B -Cluster-Level Alignment. Similar to the node-level loss in Equation ( <ref type="formula" target="#formula_0">1</ref>), the node-level alignment in Equation ( <ref type="formula" target="#formula_5">5</ref>) could also introduce semantic errors since h 𝑣− 𝑛 might be topologically far from but semantically similar to h 𝑣 𝑛 . To reduce the semantic error, we also align the layer-specific embeddings {h 𝑣 𝑛 } 𝑉 𝑣=1 at the cluster level. Let the 𝑣-th layer be the anchor layer and its semantic cluster centers C 𝑣 ∈ R 𝐾 𝑣 ×𝑑 as the anchor cluster centers. For a node x 𝑛 , we call its layer-specific embedding h 𝑣 𝑛 as the anchor embedding, and its semantic distribution p 𝑣 𝑛 ∈ R 𝐾 𝑣 as the anchor semantics, which is obtained via Equation ( <ref type="formula" target="#formula_1">2</ref> An illustration of the cluster-level alignment is presented in Figure <ref type="figure" target="#fig_3">3</ref>. Given a node x 𝑛 , on the anchor layer 𝑣, we have the anchor cluster centers C 𝑣 , the anchor embedding h 𝑣 𝑛 , and the anchor semantic distribution p 𝑣 𝑛 . Next, we use the embedding h 𝑣 ′ 𝑛 from the layer 𝑣 ′ ≠ 𝑣 to obtain the recovered semantic distribution q 𝑣 ′ 𝑛 based on C 𝑣 via Equation (2). Then we align the semantics of h 𝑣 𝑛 and h 𝑣 ′ 𝑛 by minimizing the KL-divergence of p 𝑣 𝑛 and q 𝑣 ′ 𝑛 :</p><formula xml:id="formula_6">R 𝑣 𝐶 = 1 𝑁 (𝑉 − 1) 𝑁 ∑︁ 𝑛=1 𝑉 ∑︁ 𝑣 ′ ≠𝑣 𝐾𝐿(p 𝑣 𝑛 ||q 𝑣 ′ 𝑛 )<label>(6)</label></formula><p>where p 𝑣 𝑛 is treated as the ground-truth and the gradients are not allowed to pass through p 𝑣 𝑛 during training.</p><p>Finally, we alternatively use all 𝑉 layers as anchor layers and use the averaged KL-divergence as the final semantic regularization:</p><formula xml:id="formula_7">R 𝐶 = 1 𝑉 𝑉 ∑︁ 𝑣=1 R 𝑣 𝐶 (7)</formula><p>C -Overall Loss. By combining the node-level and cluster-level regularization losses, we have:</p><formula xml:id="formula_8">R = 𝜇 N R N + 𝜇 C R C (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where 𝜇 N and 𝜇 C are tunable hyper-parameters.</p><p>The final training objective of the X-GOAL framework is the combination of the contrastive loss L in Equation ( <ref type="formula" target="#formula_3">4</ref>) and the alignment regularization R in Equation ( <ref type="formula" target="#formula_8">8</ref>):</p><formula xml:id="formula_10">L 𝑋 = 𝑉 ∑︁ 𝑣=1 L 𝑣 + R (9)</formula><p>where L 𝑣 is the loss of layer 𝑣</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Theoretical Analysis</head><p>We provide theoretical analysis for the proposed regularization alignments. In Theorem 3.1, we prove that the node-level alignment maximizes the mutual information of embeddings</p><formula xml:id="formula_11">𝐻 𝑣 ∈ {h 𝑣 𝑛 } 𝑁 𝑛=1</formula><p>of the anchor layer 𝑣 and embeddings 𝐻 𝑣 ′ ∈ {h 𝑣 ′ 𝑛 } 𝑁 𝑛=1 of another layer 𝑣 ′ . In Theorem 3.2, we prove that the cluster-level alignment maximizes the mutual information of semantic cluster assignments</p><formula xml:id="formula_12">𝐶 𝑣 ∈ [1, • • • , 𝐾 𝑣 ] for embeddings {h 𝑣 𝑛 } 𝑁 𝑛=1</formula><p>of the anchor layer 𝑣 and embeddings 𝐻 𝑣 ′ ∈ {h 𝑣 ′ 𝑛 } 𝑁 𝑛=1 of the layer 𝑣 ′ . Theorem 3.1 (Maximization of MI of Embeddings from Different Layers). Let 𝐻 𝑣 ∈ {h 𝑣 𝑛 } 𝑁 𝑛=1 and 𝐻 𝑣 ′ ∈ {h 𝑣 ′ 𝑛 } 𝑁 𝑛=1 be the random variables for node embeddings of the 𝑣-th and 𝑣 ′ -th layers, then the node-level alignment maximizes 𝐼 (𝐻 𝑣 ; 𝐻 𝑣 ′ ).</p><p>Proof. According to <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>, the following inequality holds:</p><formula xml:id="formula_13">𝐼 (𝑋 ; 𝑌 ) ≥ E[ 1 𝐾 1 𝐾 1 ∑︁ 𝑖=1 log e 𝑓 (𝑥 𝑖 ,𝑦 𝑖 ) 1 𝐾 2 𝐾 2 𝑗=1 e 𝑓 (𝑥 𝑖 ,𝑦 𝑗 ) ]<label>(10)</label></formula><p>Let</p><formula xml:id="formula_14">𝐾 1 = 1, 𝐾 2 = 2, 𝑓 () = 𝑐𝑜𝑠 (), 𝑥 1 = h 𝑣 𝑛 , 𝑦 1 = h 𝑣 ′ 𝑛 , 𝑦 2 = h −𝑣 𝑛 , then: 𝐼 (𝐻 𝑣 ; 𝐻 𝑣 ′ ) ≥ E[log e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣 ′ 𝑛 ) e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣 ′ 𝑛 ) + e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣− 𝑛 ) ]<label>(11)</label></formula><p>The expectation E is taken over all the 𝑁 nodes, and all the pairs of 𝑉 layers, and thus we have:</p><formula xml:id="formula_15">𝐼 (𝐻 𝑣 ; 𝐻 𝑣 ′ ) ≥ 1 𝑍 𝑁 ∑︁ 𝑛=1 𝑉 ∑︁ 𝑣=1 𝑉 ∑︁ 𝑣 ′ ≠𝑣 log e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣 ′ 𝑛 ) e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣 ′ 𝑛 ) + e 𝑐𝑜𝑠 (h 𝑣 𝑛 ,h 𝑣− 𝑛 )<label>(12)</label></formula><p>where 𝑍 = 𝑁𝑉 (𝑉 − 1) is the normalization factor, and the right side is R N in Equation <ref type="bibr" target="#b6">(7)</ref>. □ Theorem 3.2 (Maximization of MI between Embeddings and Semantic Cluster Assignments). Let 𝐶 𝑣 ∈ [1, • • • , 𝐾 𝑣 ] be the random variable for cluster assignments for {h 𝑣 𝑛 } 𝑁 𝑛=1 of the anchor layer 𝑣, and 𝐻 𝑣 ′ ∈ {h 𝑣 ′ 𝑛 } 𝑁 𝑛=1 be the random variable for node embeddings of the 𝑣 ′ -th layer, then the cluster-level alignment maximizes the mutual information of 𝐶 𝑣 and 𝐻 𝑣 ′ : 𝐼 (𝐶 𝑣 ; 𝐻 𝑣 ′ ). Proof. In the cluster-level alignment, the anchor distribution p 𝑣 𝑛 is regarded as the ground-truth for the 𝑛-th node, and q 𝑣 ′ 𝑛 = 𝑓 (h 𝑣 ′ 𝑛 ) is the recovered distribution from the 𝑣 ′ -th layer, where 𝑓 () is a 𝐾 𝑣 dimensional function defined by Equation (2). Specifically,</p><formula xml:id="formula_16">𝑓 (h 𝑣 ′ 𝑛 ) [𝑘] = 𝑝 (𝑘 |h 𝑣 ′ 𝑛 ) = e (c 𝑇 𝑘 •h 𝑣 ′ 𝑛 /𝜏) 𝐾 𝑣 𝑘 ′ =1 e (c 𝑇 𝑘 ′ •h 𝑣 ′ 𝑛 /𝜏)<label>(13)</label></formula><p>where {c 𝑘 } 𝐾 𝑣 𝑘=1 is the set of cluster centers for the 𝑣-th layer. Since p 𝑣 𝑛 is the ground-truth, and thus its entropy 𝐻 (p 𝑣 𝑛 ) is a constant. As a result, the KL divergence in Equation ( <ref type="formula" target="#formula_6">6</ref>) is equivalent to cross-entropy 𝐻 (p 𝑣 𝑛 , q 𝑣 ′ 𝑛 ) = 𝐾𝐿(p 𝑣 𝑛 ||q 𝑣 ′ 𝑛 ) + 𝐻 (p 𝑣 𝑛 ). Therefore, minimizing the KL-divergence will minimize 𝐻 (p 𝑣 𝑛 , q 𝑣 ′ 𝑛 ). On the other hand, according to <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44]</ref>, we have the following variational lower bound for 𝐼 (𝐶 𝑣 ; 𝐻 𝑣 ′ ):</p><formula xml:id="formula_17">𝐼 (𝐶 𝑣 ; 𝐻 𝑣 ′ ) ≥ E[log e 𝑔 (h 𝑣 ′ 𝑛 ,𝑘) 𝐾 𝑣 𝑘 ′ =1 e 𝑔 (h 𝑣 ′ 𝑛 ,𝑘 ′ ) ]<label>(14)</label></formula><p>where 𝑔() is any function of h 𝑣 ′ 𝑛 and 𝑘. In our case, we let</p><formula xml:id="formula_18">𝑔(h 𝑣 ′ 𝑛 , 𝑘) = 1 𝜏 c 𝑇 𝑘 • h 𝑣 ′ 𝑛 (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>where c 𝑘 is the 𝑘-th semantic cluster center of the 𝑣-th layer, and 𝜏 is the temperature parameter. As a result, we have</p><formula xml:id="formula_20">e 𝑔 (h 𝑣 ′ 𝑛 ,𝑘) 𝐾 𝑣 𝑘 ′ =1 e 𝑔 (h 𝑣 ′ 𝑛 ,𝑘 ′ ) = 𝑓 [h 𝑣 ′ 𝑛 ] [𝑘] = q 𝑣 ′ 𝑛 [𝑘]<label>(16)</label></formula><p>The expectation E is taken over the ground-truth distribution of the cluster assignments for the anchor layer 𝑣:</p><formula xml:id="formula_21">𝑝 𝑔𝑡 (h 𝑣 ′ 𝑛 , 𝑘) = 𝑝 𝑔𝑡 (h 𝑣 ′ 𝑛 )𝑝 𝑔𝑡 (𝑘 |h 𝑣 ′ 𝑛 ) = 1 𝑁 p 𝑣 𝑛 [𝑘]<label>(17)</label></formula><p>where 𝑝 𝑔𝑡 (𝑘 |h 𝑣 ′ 𝑛 ) = p 𝑣 𝑛 [𝑘] is the ground-truth semantic distribution for h 𝑣 ′ 𝑛 on the anchor layer 𝑣, which is different from the recovered distribution 𝑝 (𝑘 |h 𝑣 ′ 𝑛 ) = q 𝑣 ′ 𝑛 [𝑘] shown in Equation <ref type="bibr" target="#b12">(13)</ref>. Therefore, we have</p><formula xml:id="formula_22">𝐼 (𝐶 𝑣 ; 𝐻 𝑣 ′ ) ≥ 1 𝑍 𝑁 ∑︁ 𝑛=1 𝐾 𝑣 ∑︁ 𝑘=1 p 𝑣 𝑛 [𝑘] log q 𝑣 ′ 𝑛 [𝑘] = − 1 𝑍 𝑁 ∑︁ 𝑛=1 𝐻 (p 𝑣 𝑛 , q 𝑣 ′ 𝑛 )<label>(18)</label></formula><p>where 𝑍 = 𝑁 𝐾 𝑣 is the normalization factor.</p><p>Thus, minimizing 𝐻 (p 𝑣 𝑛 , q 𝑣 ′ 𝑛 ) will maximize 𝐼 (𝐶 𝑣 ; 𝐻 𝑣 ′ ). □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Setups</head><p>Datasets. We use publicly available multiplex heterogeneous graph datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>: ACM, IMDB, DBLP and Amazon to evaluate the proposed methods. The statistics is summarized in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Comparison Methods. We compare with methods for (1) attributed graphs, including methods disregarding node attributes: Deep-Walk <ref type="bibr" target="#b41">[42]</ref> and node2vec <ref type="bibr" target="#b11">[12]</ref>, and methods considering attributes: GCN <ref type="bibr" target="#b22">[23]</ref>, GAT <ref type="bibr" target="#b51">[52]</ref>, DGI <ref type="bibr" target="#b52">[53]</ref>, ANRL <ref type="bibr" target="#b70">[71]</ref>, CAN <ref type="bibr" target="#b33">[34]</ref>, DGCN <ref type="bibr" target="#b76">[77]</ref>, HDI <ref type="bibr" target="#b17">[18]</ref>, GCA <ref type="bibr" target="#b75">[76]</ref> and GraphCL <ref type="bibr" target="#b66">[67]</ref>;</p><p>(2) attributed multiplex heterogeneous graphs, including methods disregarding node attributes: CMNA <ref type="bibr" target="#b4">[5]</ref>, MNE <ref type="bibr" target="#b67">[68]</ref>, and methods considering attributes: mGCN <ref type="bibr" target="#b30">[31]</ref>, HAN <ref type="bibr" target="#b57">[58]</ref>, MvAGC <ref type="bibr" target="#b27">[28]</ref>, DMGI, DMGI attn <ref type="bibr" target="#b38">[39]</ref> and HDMI <ref type="bibr" target="#b17">[18]</ref>. Evaluation Metrics. Following <ref type="bibr" target="#b17">[18]</ref>, we first extract embeddings from the trained encoder. Then we train downstream models with the extracted embeddings, and evaluate models' performance on the following tasks: (1) a supervised task: node classification; (2) unsupervised tasks: node clustering and similarity search. For the node classification task, we train a logistic regression model and evaluate its performance with Macro-F1 (MaF1) and Micro-F1 (MiF1). For the node clustering task, we train the K-means algorithm and evaluate it with Normalized Mutual Information (NMI). For the similarity search task, we first calculate the cosine similarity for each pair of nodes, and for each node, we compute the rate of the nodes to have the same label within its 5 most similar nodes (Sim@5). Implementation Details. We use the one layer 1st-order GCN <ref type="bibr" target="#b22">[23]</ref> with tangent activation as the encoder E 𝑣 = tanh(A 𝑣 XW + XW ′ + b). We set dimension 𝑑 = 128 and 𝑝 𝑑𝑟𝑜𝑝 = 0.5. The models are implemented by PyTorch <ref type="bibr" target="#b39">[40]</ref> and trained on NVIDIA Tesla V-100 GPU. During training, we first warm up the encoders by training them with the node-level losses L N and R N . Then we apply the overall loss L X with the learning rate of 0.005 for IMDB and 0.001 for other datasets. We use K-means as the clustering algorithm, and the semantic clustering step is performed every 5 epochs of parameter updating. We adopt early stopping with the patience of 100 to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance</head><p>X-GOAL on Multiplex Heterogeneous Graphs. The overall performance for all of the methods is presented in Tables <ref type="table" target="#tab_3">2-3</ref>, where the upper and middle parts are the methods for homogeneous graphs and multiplex heterogeneous graphs respectively. "OOM" means out-of-memory. Among all the baselines, HDMI has the best overall performance. The proposed X-GOAL further outperforms HDMI. The proposed X-GOAL has 0.023/0.019/0.041/0.021 average improvements over the second best scores on Macro-F1/Micro-F1/NMI/Sim@5. For Macro-F1 and Micro-F1 in Table <ref type="table" target="#tab_2">2</ref>, X-GOAL improves the most on the Amazon dataset (0.050/0.044). For NMI and Sim@5 in Table <ref type="table" target="#tab_3">3</ref>, X-GOAL improves the most on the ACM (0.071) and Amazon (0.050) dataset respectively. The superior overall performance of X-GOAL demonstrate that the proposed approach can effectively extract informative node embeddings for multiplex heterogeneous graph.</p><p>GOAL on Homogeneous Graph Layers. We compare the proposed GOAL framework with recent infomax-based methods (DGI and HDI) and graph augmentation based methods (GraphCL and GCA). The experimental results for each single homogeneous graph layer are presented in Tables <ref type="table">4-5</ref>. It is evident that GOAL significantly outperforms the baseline methods on all single homogeneous graph layers. On average, GOAL has 0.137/0.129/0.151/0.119 improvements on Macro-F1/Micro-F1/NMI/Sim@5. For node classification in Table <ref type="table">4</ref>, GOAL improves the most on the PATAP layer of DBLP: 0.514/0.459 on Macro-F1/Micro-F1. For node clustering and similarity search in Table <ref type="table">5</ref>, GOAL improves the most on the IBI layer of Amazon: 0.391 on NMI and 0.378 on Sim@5. The superior performance of GOAL indicates that the proposed prototypical contrastive learning strategy is better than the infomax-based and graph augmentation based instance-wise contrastive learning strategies. We believe this is because prototypical contrasive learning could effectively reduce the semantic errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Multiplex Heterogeneous Graph Level. In Table <ref type="table" target="#tab_4">6</ref>, we study the impact of the node-level and semantic-level alignments. The results in Table <ref type="table" target="#tab_4">6</ref> indicate that both of the node-level alignment (R 𝑁 ) and the semantic-level alignment (R 𝐶 ) can improve the performance.</p><p>Homogeneous Graph Layer Level. The results for different configurations of GOAL on the PAP layer of ACM are shown in Table <ref type="table" target="#tab_5">7</ref>. First, all of the warm-up, the semantic-level loss L C and the nodelevel loss L N are critical. Second, comparing GOAL (1st-order GCN with tanh activation) with other GCN variants, (1) with the same activation function, the 1st-order GCN perform better than the original GCN; (2) tanh is better than relu. We believe this is because the 1st-order GCN has a better capability for capturing the attribute information, and tanh provides a better normalization for the node embeddings. Finally, for the configurations of graph transformation, if we replace dropout with masking, the performance will drop. This is because dropout re-scales the outputs by 1/(1 − 𝑝 𝑑𝑟𝑜𝑝 ), which improves the performance. Besides, dropout on both attributes and adjacency matrix is important.   ACM is 3, and the results in Figure <ref type="figure" target="#fig_4">4</ref> indicate that over-clustering is beneficial. We believe this is because there are many sub-clusters in the embedding space, which is consistent with the prior findings on image data <ref type="bibr" target="#b26">[27]</ref>.   Multiplex Heterogeneous Graph Level. The visualizations for the combined embeddings are shown in Figure <ref type="figure" target="#fig_7">6</ref>. Embeddings in Figures 6a-6b are the average pooling of the layer-specific embeddings in Figure <ref type="figure" target="#fig_6">5</ref>. Figure <ref type="figure" target="#fig_7">6c</ref> and 6d are X-GOAL w/o cluster-level alignment and the full X-GOAL. Generally, the full X-GOAL best separates different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Number of Clusters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK 5.1 Contrastive Learning for Graphs</head><p>The goal of CL is to pull similar nodes into close positions and push dis-similar nodes far apart in the embedding space. Inspired by word2vec <ref type="bibr" target="#b34">[35]</ref>, early methods, such as DeepWalk <ref type="bibr" target="#b41">[42]</ref> and node2vec <ref type="bibr" target="#b11">[12]</ref> use random walks to sample positive pairs of nodes. LINE <ref type="bibr" target="#b49">[50]</ref> and SDNE <ref type="bibr" target="#b55">[56]</ref> determine the positive node pairs by their first and second-order structural proximity. Recent methods leverage graph transformation to generate pairs. DGI <ref type="bibr" target="#b52">[53]</ref>, GMI <ref type="bibr" target="#b40">[41]</ref>, HDI <ref type="bibr" target="#b17">[18]</ref> and CommDGI <ref type="bibr" target="#b68">[69]</ref> obtain negative samples by randomly shuffling the node attributes. MVGRL <ref type="bibr" target="#b13">[14]</ref> transforms graphs via techniques such as graph diffusion <ref type="bibr" target="#b23">[24]</ref>. The objective of the above methods is to maximize the mutual information of the positive embedding pairs. GraphCL <ref type="bibr" target="#b66">[67]</ref> uses various graph augmentations to obtain positive nodes. GCA <ref type="bibr" target="#b75">[76]</ref> generates positive and negative pairs based on their importance. gCool <ref type="bibr" target="#b24">[25]</ref> introduces graph communal contrastive learning. Ariel <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> proposes a information regularized adversarial graph contrastive learning. These methods use the contrastive losses similar to InfoNCE <ref type="bibr" target="#b35">[36]</ref>.</p><p>For multiplex heterogeneous graphs, MNE <ref type="bibr" target="#b67">[68]</ref>, MVN2VEC <ref type="bibr" target="#b46">[47]</ref> and GATNE <ref type="bibr" target="#b3">[4]</ref> sample node pairs based on random walks. DMGI <ref type="bibr" target="#b38">[39]</ref> and HDMI <ref type="bibr" target="#b17">[18]</ref> use random attribute shuffling to sample negative nodes. HeCo <ref type="bibr" target="#b58">[59]</ref> decides positive and negative pairs based on the connectivity between nodes. Above methods mainly rely on the topological structures to pair nodes, yet do not fully explore the semantic information, which could introduce semantic errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Deep Clustering and Contrastive Learning</head><p>Clustering algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b61">62]</ref> can capture the semantic clusters of instances. DeepCluster <ref type="bibr" target="#b1">[2]</ref> is one of the earliest works which use cluster assignments as "pseudo-labels" to update the parameters of the encoder. DEC <ref type="bibr" target="#b61">[62]</ref> learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Inspired by these works, SwAV <ref type="bibr" target="#b2">[3]</ref> and PCL <ref type="bibr" target="#b26">[27]</ref> combine deep clustering with CL. SwAV compares the cluster assignments rather than the embeddings of two images. PCL is the closest to our work, which alternatively performs clustering to obtain the latent prototypes and train the encoder by contrasting positive and negative pairs of nodes and prototypes. However, PCL has some limitations compared with the proposed X-GOAL: it is designed for single view image data; it heavily relies on data augmentations and momentum contrast <ref type="bibr" target="#b14">[15]</ref>; it has some complex assumptions over cluster distributions and embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multiplex Heterogeneous Graph Neural Networks</head><p>The multiplex heterogeneous graph <ref type="bibr" target="#b3">[4]</ref> considers multiple relations among nodes, and it is also known as multiplex graph <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>, multi-view graph <ref type="bibr" target="#b45">[46]</ref>, multi-layer graph <ref type="bibr" target="#b25">[26]</ref> and multi-dimension graph <ref type="bibr" target="#b29">[30]</ref>. MVE <ref type="bibr" target="#b45">[46]</ref> and HAN <ref type="bibr" target="#b57">[58]</ref> uses attention mechanisms to combine embeddings from different views. mGCN <ref type="bibr" target="#b30">[31]</ref> models both within and across view interactions. VANE <ref type="bibr" target="#b10">[11]</ref> uses adversarial training to improve the comprehensiveness and robustness of the embeddings. Multiplex graph neural networks have been used in many applications <ref type="bibr" target="#b6">[7]</ref>, such as time series <ref type="bibr" target="#b18">[19]</ref>, text summarization <ref type="bibr" target="#b20">[21]</ref>, temporal graphs <ref type="bibr" target="#b9">[10]</ref>, graph alignment <ref type="bibr" target="#b62">[63]</ref>, abstract reasoning <ref type="bibr" target="#b56">[57]</ref>, global poverty <ref type="bibr" target="#b21">[22]</ref> and bipartite graphs <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Deep Graph Clustering</head><p>Graph clustering aims at discovering groups in graphs. SAE <ref type="bibr" target="#b50">[51]</ref> and MGAE <ref type="bibr" target="#b54">[55]</ref> first train a GNN, and then run a clustering algorithm over node embeddings to obtain the clusters. DAEGC <ref type="bibr" target="#b53">[54]</ref> and SDCN <ref type="bibr" target="#b0">[1]</ref> jointly optimize clustering algorithms and the graph reconstruction loss. AGC <ref type="bibr" target="#b69">[70]</ref> adaptively finds the optimal order for graph filters based on the intrinsic clustering scores. M3S <ref type="bibr" target="#b48">[49]</ref> uses clustering to enlarge the labeled data with pseudo labels. SDCN <ref type="bibr" target="#b0">[1]</ref> proposes a structural deep clustering network to integrate the structural information into deep clustering. COIN <ref type="bibr" target="#b19">[20]</ref> co-clusters two types of nodes in bipartite graphs. MvAGC <ref type="bibr" target="#b27">[28]</ref> extends AGC <ref type="bibr" target="#b69">[70]</ref> to multi-view settings. However, MvAGC is not neural network based methods which might not exploit the attribute and non-linearity information. Recent methods combine CL with clustering to further improve the performance. SCAGC <ref type="bibr" target="#b60">[61]</ref> treats nodes within the same cluster as positive pairs. MCGC <ref type="bibr" target="#b36">[37]</ref> combines CL with MvAGC <ref type="bibr" target="#b27">[28]</ref>, which treats each node with its neighbors as positive pairs. Different from SCAGC and MCGC, the proposed GOAL and X-GOAL capture the semantic information by treating a node with its corresponding cluster center as a positive pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we introduce a novel X-GOAL framework for multiplex heterogeneous graphs, which is comprised of a GOAL framework for each homogeneous graph layer and an alignment regularization to jointly model different layers. The GOAL framework captures both node-level and cluster-level information. The alignment regularization is a nimble technique to jointly model and propagate information across different layers, which could maximize the mutual information of different layers. The experimental results on real-world multiplex heterogeneous graphs demonstrate the effectiveness of the proposed X-GOAL framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DERIVATION OF CLUSTER-LEVEL LOSS</head><p>The node-level contrastive loss is usually noisy, which could introduce semantic errors by treating two semantic similar nodes as a negative pair. To tackle this issue, we use a clustering algorithm C (e.g. K-means) to obtain the semantic clusters of nodes, and we use the EM algorithm to update the parameters of E to pull node embeddings closer to their assigned clusters (or prototypes).</p><p>Following <ref type="bibr" target="#b26">[27]</ref>, we maximize the following log likelihood: </p><p>where 𝑄 (𝑘 |h 𝑛 ) = 𝑝 (𝑘 |h 𝑛 , Θ, C) is the auxiliary function.</p><p>In the E-step, we fix Θ and estimate the cluster centers Ĉ and the cluster assignments Q (𝑘 |h 𝑛 ) by running the K-means algorithm over the embeddings of the original graph H = E (G). If a node h 𝑛 belongs to the cluster 𝑘, then its auxiliary function is an indicator function satisfying Q (𝑘 |h 𝑛 ) = 1, and Q (𝑘 ′ |h 𝑛 ) = 0 for ∀𝑘 ′ ≠ 𝑘.</p><p>In the M-step, based on Ĉ and Q (𝑘 |h 𝑛 ) obtained in the E-step, we update Θ by maximizing ELBO: </p><p>where h 𝑛 ∈ R 𝑑 is the embedding of the node x 𝑛 , ĉ𝑘 ∈ R 𝑑 is the vector of the 𝑘-th cluster center, 𝜏 is the temperature parameter. Let's use 𝑘 𝑛 to denote the cluster assignment of h 𝑛 , and normalize the loss by 1  𝑁 , then Equation ( <ref type="formula">22</ref>) can be rewritten as: </p><formula xml:id="formula_25">L C = −<label>1</label></formula><p>The above loss function captures the semantic similarities between nodes by pulling nodes within the same cluster closer to their assigned cluster center.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the multiplex heterogeneous graph G M , which can be decomposed into homogeneous graph layers G 1 and G 2 according to the types of relations. Different colors represent different relations.</figDesc><graphic url="image-1.png" coords="2,349.81,83.68,176.55,130.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 1 −</head><label>1</label><figDesc>𝑝 𝑑𝑟𝑜𝑝 during training, which improves the training results. The negative transformation T − is the random shuffle of the rows for X [53]. The transformed positive and negative graphs are denoted by G + = T + (G) and G − = T − (G), respectively. The node embedding matrices of G, G + and G − are thus H = E (G), H + = E (G + ) and H − = E (G − ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of GOAL. E and C are the encoder and clustering algorithm. G is a homogeneous graph layer and H is the embedding matrix. L is given in Equation (4). The circles and diamonds denote nodes and cluster centers. Blue and orange denote different hidden semantics. The green line is the cluster boundary. "Back Prop." means back propagation. The node-level topology based negative sampling treats the semantic similar node 0 and 2 as a negative pair. The cluster-level loss reduces semantic error by pulling node 0 and 2 closer to their cluster center.</figDesc><graphic url="image-2.png" coords="3,337.20,83.69,201.76,117.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Cluster-level alignment. x 𝑛 is the node attribute. h 𝑣 𝑛 and h 𝑣 ′𝑛 are the layer-specific embeddings. C 𝑣 is the anchor cluster center matrix. p 𝑣 𝑛 and q 𝑣 ′ 𝑛 are the anchor and recovered semantic distributions. R 𝑣 C is given in Equation (6).</figDesc><graphic url="image-3.png" coords="4,85.65,83.69,176.54,93.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4 shows the Macro-F1 and NMI scores on the PSP and PAP layers of ACM w.r.t. the number of clusters 𝐾 ∈ [3, 4, 5, 10, 20, 30, 50]. For PSP and PAP, the best Macro-F1 and NMI scores are obtained when 𝐾 = 30 and 𝐾 = 5. The number of ground-truth classes for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The number of 𝐾 on PSP and PAP of ACM</figDesc><graphic url="image-4.png" coords="7,320.95,493.13,116.01,116.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the embeddings for the PAP and PSP layers of the ACM graph.</figDesc><graphic url="image-10.png" coords="8,55.49,227.18,123.57,92.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of the combined embeddings for the ACM graph.</figDesc><graphic url="image-11.png" coords="8,181.30,227.18,123.57,92.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4. 5</head><label>5</label><figDesc>VisualizationHomogeneous Graph Layer Level. The t-SNE<ref type="bibr" target="#b31">[32]</ref> visualizations of the embeddings for PSP and PAP of ACM are presented in Figure5. L N , L C , R N and R C are the node-level loss, cluster-level loss, node-level alignment and cluster-level alignment. The embeddings extracted by the full GOAL framework (L N + L C ) are better separated than the node-level loss L N only. For GOAL, the numbers of clusters for PSP and PAP are 30 and 5 since they have the best performance as shown in Figure4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>QQQ</head><label></label><figDesc>(𝑘 |h 𝑛 ) log 𝑝 (h 𝑛 , 𝑘 |Θ, Ĉ) (𝑘 |h 𝑛 ) log Q (𝑘 |h 𝑛 )(21)Dropping the second term of the above equation, which is a constant, we will minimize the following loss function:(𝑘 |h 𝑛 ) log 𝑝 (h 𝑛 , 𝑘 |Θ, Ĉ)(22)Assuming a uniform prior distribution over h 𝑛 , we have:𝑝 (h 𝑛 , 𝑘 |Θ, Ĉ) ∝ 𝑝 (𝑘 |h 𝑛 , Θ, Ĉ)(23)We define 𝑝 (𝑘 |h 𝑛 , Θ, Ĉ) by:𝑝 (𝑘 |h 𝑛 , Θ, Ĉ) = e (ĉ 𝑇 𝑘 •h 𝑛 /𝜏) 𝐾 𝑘 ′ =1 e (ĉ 𝑇 𝑘 ′ •h 𝑛 /𝜏)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>𝑘𝑛 •h 𝑛 /𝜏) 𝐾 𝑘=1 e (c 𝑇 𝑘 •h 𝑛 /𝜏)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) based on h 𝑣 𝑛 and C 𝑣 . Our key idea of the cluster-level alignment is to recover the anchor semantics p 𝑣 𝑛 from embeddings {h 𝑣 ′ 𝑛 } 𝑉 𝑣 ′ ≠𝑣 of other layers based on C 𝑣 . Our idea can be justified from two perspectives. Firstly, {h 𝑣 𝑛 } 𝑉 𝑣 𝑛 from the embedding h 𝑣 ′ 𝑛 of another layer 𝑣 ′ ≠ 𝑣, then it indicates that h 𝑣 𝑛 and h 𝑣 ′ 𝑛 share hidden semantics to a certain degree. Secondly, it is impractical to directly align p 𝑣 𝑛 and p 𝑣 ′ 𝑛 , since their dimensions might be different 𝐾 𝑣 ≠ 𝐾 𝑣 ′ , and even if 𝐾 𝑣 = 𝐾 𝑣 ′ , the cluster center vectors C 𝑣 and C 𝑣 ′ are distributed at different positions in the embedding space.</figDesc><table /><note>𝑣=1reflect information of x 𝑛 from different aspects, if we can recover the anchor semantics p</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets</figDesc><table><row><cell>Graphs</cell><cell># Nodes</cell><cell>Layers</cell><cell># Edges</cell><cell># Attributes</cell><cell cols="2"># Labeled Data # Classes</cell></row><row><cell></cell><cell></cell><cell>Paper-Subject-Paper (PSP)</cell><cell>2,210,761</cell><cell>1,830</cell><cell></cell></row><row><cell>ACM</cell><cell>3,025</cell><cell></cell><cell></cell><cell></cell><cell>600</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>Paper-Author-Paper (PAP)</cell><cell>29,281</cell><cell>(Paper Abstract)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Movie-Actor-Movie (MAM)</cell><cell>66,428</cell><cell>1,007</cell><cell></cell></row><row><cell>IMDB</cell><cell>3,550</cell><cell></cell><cell></cell><cell></cell><cell>300</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>Movie-Director-Movie (MDM)</cell><cell>13,788</cell><cell>(Movie plot)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Paper-Author-Paper (PAP)</cell><cell>144,783</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2,000</cell><cell></cell></row><row><cell>DBLP</cell><cell>7,907</cell><cell>Paper-Paper-Paper (PPP)</cell><cell>90,145</cell><cell></cell><cell>80</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Paper Abstract)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Paper-Author-Term-Author-Paper (PATAP) 57,137,515</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Item-AlsoView-Item (IVI)</cell><cell>266,237</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2,000</cell><cell></cell></row><row><cell>Amazon</cell><cell>7,621</cell><cell>Item-AlsoBought-Item (IBI)</cell><cell>1,104,257</cell><cell></cell><cell>80</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Item description)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Item-BoughtTogether-Item (IOI)</cell><cell>16,305</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall performance of X-GOAL on the supervised task: node classification.</figDesc><table><row><cell>Dataset</cell><cell>ACM</cell><cell></cell><cell>IMDB</cell><cell></cell><cell>DBLP</cell><cell></cell><cell cols="2">Amazon</cell></row><row><cell>Metric</cell><cell cols="8">Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1</cell></row><row><cell>DeepWalk</cell><cell>0.739</cell><cell>0.748</cell><cell>0.532</cell><cell>0.550</cell><cell>0.533</cell><cell>0.537</cell><cell>0.663</cell><cell>0.671</cell></row><row><cell>node2vec</cell><cell>0.741</cell><cell>0.749</cell><cell>0.533</cell><cell>0.550</cell><cell>0.543</cell><cell>0.547</cell><cell>0.662</cell><cell>0.669</cell></row><row><cell>GCN/GAT</cell><cell>0.869</cell><cell>0.870</cell><cell>0.603</cell><cell>0.611</cell><cell>0.734</cell><cell>0.717</cell><cell>0.646</cell><cell>0.649</cell></row><row><cell>DGI</cell><cell>0.881</cell><cell>0.881</cell><cell>0.598</cell><cell>0.606</cell><cell>0.723</cell><cell>0.720</cell><cell>0.403</cell><cell>0.418</cell></row><row><cell>ANRL</cell><cell>0.819</cell><cell>0.820</cell><cell>0.573</cell><cell>0.576</cell><cell>0.770</cell><cell>0.699</cell><cell>0.692</cell><cell>0.690</cell></row><row><cell>CAN</cell><cell>0.590</cell><cell>0.636</cell><cell>0.577</cell><cell>0.588</cell><cell>0.702</cell><cell>0.694</cell><cell>0.498</cell><cell>0.499</cell></row><row><cell>DGCN</cell><cell>0.888</cell><cell>0.888</cell><cell>0.582</cell><cell>0.592</cell><cell>0.707</cell><cell>0.698</cell><cell>0.478</cell><cell>0.509</cell></row><row><cell>GraphCL</cell><cell>0.884</cell><cell>0.883</cell><cell>0.619</cell><cell>0.623</cell><cell>0.814</cell><cell>0.806</cell><cell>0.461</cell><cell>0.472</cell></row><row><cell>GCA</cell><cell>0.798</cell><cell>0.797</cell><cell>0.523</cell><cell>0.533</cell><cell>OOM</cell><cell>OOM</cell><cell>0.408</cell><cell>0.398</cell></row><row><cell>HDI</cell><cell>0.901</cell><cell>0.900</cell><cell>0.634</cell><cell>0.638</cell><cell>0.814</cell><cell>0.800</cell><cell>0.804</cell><cell>0.806</cell></row><row><cell>CMNA</cell><cell>0.782</cell><cell>0.788</cell><cell>0.549</cell><cell>0.566</cell><cell>0.566</cell><cell>0.561</cell><cell>0.657</cell><cell>0.665</cell></row><row><cell>MNE</cell><cell>0.792</cell><cell>0.797</cell><cell>0.552</cell><cell>0.574</cell><cell>0.566</cell><cell>0.562</cell><cell>0.556</cell><cell>0.567</cell></row><row><cell>mGCN</cell><cell>0.858</cell><cell>0.860</cell><cell>0.623</cell><cell>0.630</cell><cell>0.725</cell><cell>0.713</cell><cell>0.660</cell><cell>0.661</cell></row><row><cell>HAN</cell><cell>0.878</cell><cell>0.879</cell><cell>0.599</cell><cell>0.607</cell><cell>0.716</cell><cell>0.708</cell><cell>0.501</cell><cell>0.509</cell></row><row><cell>DMGI</cell><cell>0.898</cell><cell>0.898</cell><cell>0.648</cell><cell>0.648</cell><cell>0.771</cell><cell>0.766</cell><cell>0.746</cell><cell>0.748</cell></row><row><cell>DMGI attn</cell><cell>0.887</cell><cell>0.887</cell><cell>0.602</cell><cell>0.606</cell><cell>0.778</cell><cell>0.770</cell><cell>0.758</cell><cell>0.758</cell></row><row><cell>MvAGC</cell><cell>0.778</cell><cell>0.791</cell><cell>0.598</cell><cell>0.615</cell><cell>0.509</cell><cell>0.542</cell><cell>0.395</cell><cell>0.414</cell></row><row><cell>HDMI</cell><cell>0.901</cell><cell>0.901</cell><cell>0.650</cell><cell>0.658</cell><cell>0.820</cell><cell>0.811</cell><cell>0.808</cell><cell>0.812</cell></row><row><cell>X-GOAL</cell><cell>0.922</cell><cell>0.921</cell><cell>0.661</cell><cell>0.663</cell><cell>0.830</cell><cell>0.819</cell><cell>0.858</cell><cell>0.857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overall performance of X-GOAL on the unsupervised tasks: node clustering and similarity search.</figDesc><table><row><cell>Dataset</cell><cell cols="2">ACM</cell><cell cols="2">IMDB</cell><cell cols="2">DBLP</cell><cell cols="2">Amazon</cell></row><row><cell>Metric</cell><cell cols="2">NMI Sim@5</cell><cell cols="2">NMI Sim@5</cell><cell cols="2">NMI Sim@5</cell><cell cols="2">NMI Sim@5</cell></row><row><cell cols="2">DeepWalk 0.310</cell><cell>0.710</cell><cell>0.117</cell><cell>0.490</cell><cell>0.348</cell><cell>0.629</cell><cell>0.083</cell><cell>0.726</cell></row><row><cell>node2vec</cell><cell>0.309</cell><cell>0.710</cell><cell>0.123</cell><cell>0.487</cell><cell>0.382</cell><cell>0.629</cell><cell>0.074</cell><cell>0.738</cell></row><row><cell cols="2">GCN/GAT 0.671</cell><cell>0.867</cell><cell>0.176</cell><cell>0.565</cell><cell>0.465</cell><cell>0.724</cell><cell>0.287</cell><cell>0.624</cell></row><row><cell>DGI</cell><cell>0.640</cell><cell>0.889</cell><cell>0.182</cell><cell>0.578</cell><cell>0.551</cell><cell>0.786</cell><cell>0.007</cell><cell>0.558</cell></row><row><cell>ANRL</cell><cell>0.515</cell><cell>0.814</cell><cell>0.163</cell><cell>0.527</cell><cell>0.332</cell><cell>0.720</cell><cell>0.166</cell><cell>0.763</cell></row><row><cell>CAN</cell><cell>0.504</cell><cell>0.836</cell><cell>0.074</cell><cell>0.544</cell><cell>0.323</cell><cell>0.792</cell><cell>0.001</cell><cell>0.537</cell></row><row><cell>DGCN</cell><cell>0.691</cell><cell>0.690</cell><cell>0.143</cell><cell>0.179</cell><cell>0.462</cell><cell>0.491</cell><cell>0.143</cell><cell>0.194</cell></row><row><cell>GraphCL</cell><cell>0.673</cell><cell>0.890</cell><cell>0.149</cell><cell>0.565</cell><cell>0.545</cell><cell>0.803</cell><cell>0.002</cell><cell>0.360</cell></row><row><cell>GCA</cell><cell>0.443</cell><cell>0.791</cell><cell>0.007</cell><cell>0.496</cell><cell>OOM</cell><cell>OOM</cell><cell>0.002</cell><cell>0.478</cell></row><row><cell>HDI</cell><cell>0.650</cell><cell>0.900</cell><cell>0.194</cell><cell>0.605</cell><cell>0.570</cell><cell>0.799</cell><cell>0.487</cell><cell>0.856</cell></row><row><cell>CMNA</cell><cell>0.498</cell><cell>0.363</cell><cell>0.152</cell><cell>0.069</cell><cell>0.420</cell><cell>0.511</cell><cell>0.070</cell><cell>0.435</cell></row><row><cell>MNE</cell><cell>0.545</cell><cell>0.791</cell><cell>0.013</cell><cell>0.482</cell><cell>0.136</cell><cell>0.711</cell><cell>0.001</cell><cell>0.395</cell></row><row><cell>mGCN</cell><cell>0.668</cell><cell>0.873</cell><cell>0.183</cell><cell>0.550</cell><cell>0.468</cell><cell>0.726</cell><cell>0.301</cell><cell>0.630</cell></row><row><cell>HAN</cell><cell>0.658</cell><cell>0.872</cell><cell>0.164</cell><cell>0.561</cell><cell>0.472</cell><cell>0.779</cell><cell>0.029</cell><cell>0.495</cell></row><row><cell>DMGI</cell><cell>0.687</cell><cell>0.898</cell><cell>0.196</cell><cell>0.605</cell><cell>0.409</cell><cell>0.766</cell><cell>0.425</cell><cell>0.816</cell></row><row><cell>DMGI attn</cell><cell>0.702</cell><cell>0.901</cell><cell>0.185</cell><cell>0.586</cell><cell>0.554</cell><cell>0.798</cell><cell>0.412</cell><cell>0.825</cell></row><row><cell>MvAGC</cell><cell>0.665</cell><cell>0.824</cell><cell>0.219</cell><cell>0.525</cell><cell>0.281</cell><cell>0.437</cell><cell>0.082</cell><cell>0.237</cell></row><row><cell>HDMI</cell><cell>0.695</cell><cell>0.898</cell><cell>0.198</cell><cell>0.607</cell><cell>0.582</cell><cell>0.809</cell><cell>0.500</cell><cell>0.857</cell></row><row><cell>X-GOAL</cell><cell>0.773</cell><cell>0.924</cell><cell>0.221</cell><cell>0.613</cell><cell>0.615</cell><cell>0.809</cell><cell>0.556</cell><cell>0.907</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of X-GOAL at the multiplex heterogeneous graph level.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell>ACM</cell><cell></cell><cell></cell><cell cols="2">IMDB</cell><cell></cell><cell></cell><cell cols="2">DBLP</cell><cell></cell><cell></cell><cell cols="2">Amazon</cell><cell></cell></row><row><cell>Metric</cell><cell>MaF1</cell><cell>MiF1</cell><cell>NMI</cell><cell>Sim@5</cell><cell>MaF1</cell><cell>MiF1</cell><cell>NMI</cell><cell>Sim@5</cell><cell>MaF1</cell><cell>MiF1</cell><cell>NMI</cell><cell>Sim@5</cell><cell>MaF1</cell><cell>MaF1</cell><cell>MiF1</cell><cell>Sim@5</cell></row><row><cell>X-GOAL</cell><cell>0.922</cell><cell>0.921</cell><cell>0.773</cell><cell>0.924</cell><cell>0.661</cell><cell>0.663</cell><cell>0.221</cell><cell>0.613</cell><cell>0.830</cell><cell>0.819</cell><cell>0.615</cell><cell>0.809</cell><cell>0.858</cell><cell>0.857</cell><cell>0.556</cell><cell>0.907</cell></row><row><cell>w/o R 𝐶</cell><cell>0.919</cell><cell>0.917</cell><cell>0.770</cell><cell>0.922</cell><cell>0.658</cell><cell>0.661</cell><cell>0.211</cell><cell>0.606</cell><cell>0.817</cell><cell>0.807</cell><cell>0.611</cell><cell>0.804</cell><cell>0.856</cell><cell>0.856</cell><cell>0.555</cell><cell>0.906</cell></row><row><cell>w/o R 𝑁 , R 𝐶</cell><cell>0.893</cell><cell>0.893</cell><cell>0.724</cell><cell>0.912</cell><cell>0.651</cell><cell>0.658</cell><cell>0.194</cell><cell>0.606</cell><cell>0.803</cell><cell>0.791</cell><cell>0.590</cell><cell>0.801</cell><cell>0.835</cell><cell>0.834</cell><cell>0.506</cell><cell>0.904</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of GOAL on the PAP layer of ACM.</figDesc><table><row><cell></cell><cell>MaF1 MiF1</cell><cell cols="2">NMI Sim@5</cell></row><row><cell>GOAL</cell><cell cols="2">0.908 0.908 0.735</cell><cell>0.917</cell></row><row><cell>w/o warm-up</cell><cell cols="2">0.863 0.865 0.721</cell><cell>0.903</cell></row><row><cell>w/o L C</cell><cell cols="2">0.865 0.867 0.693</cell><cell>0.899</cell></row><row><cell>w/o L N</cell><cell cols="2">0.878 0.880 0.678</cell><cell>0.881</cell></row><row><cell>1st-ord. GCN (relu)</cell><cell cols="2">0.865 0.866 0.559</cell><cell>0.859</cell></row><row><cell>GCN (tanh)</cell><cell cols="2">0.881 0.881 0.486</cell><cell>0.886</cell></row><row><cell>GCN (relu)</cell><cell cols="2">0.831 0.831 0.410</cell><cell>0.837</cell></row><row><cell>dropout → masking</cell><cell cols="2">0.888 0.890 0.716</cell><cell>0.903</cell></row><row><cell>w/o attribute drop</cell><cell cols="2">0.843 0.845 0.568</cell><cell>0.869</cell></row><row><cell cols="3">w/o adj. matrix drop 0.888 0.888 0.715</cell><cell>0.903</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>𝑛 , 𝑘 |Θ, C)<ref type="bibr" target="#b18">(19)</ref> where h 𝑛 is the 𝑛-th row of h, Θ and C are the parameters of E and K-means algorithm C, 𝑘 ∈ [1, • • • , 𝐾] is the cluster index, and 𝐾 is the number of clusters. Directly optimizing this objective is impracticable since the cluster index is a latent variable.The Evidence Lower Bound (ELBO) of Equation (19) is given by: 𝑛 , 𝑘 |Θ, C) 𝑄 (𝑘 |h 𝑛 )</figDesc><table><row><cell></cell><cell>𝑁</cell><cell>𝐾</cell></row><row><cell cols="3">∑︁ 𝑛=1 𝑝 (h ELBO = log ∑︁ 𝑘=1 𝑁 ∑︁ 𝐾 ∑︁ 𝑄 (𝑘 |h 𝑛 ) log 𝑝 (h</cell></row><row><cell>𝑛=1</cell><cell>𝑘=1</cell></row></table><note>𝑁 ∑︁ 𝑛=1 log 𝑝 (h 𝑛 |Θ, C) =</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>BJ and HT are partially supported by NSF (1947135, 2134079 and  1939725), and NIFA (2020-67021-32799).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structural deep clustering network</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning for attributed multiplex heterogeneous network</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-network embedding for multi-network alignment</title>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingping</forename><surname>Bi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="273" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Neiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10862</idno>
		<title level="m">Hypergraph Pre-training with Graph Neural Networks</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New Frontiers of Multi-Network Mining: Recent Developments and Future Trend</title>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4038" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial graph contrastive learning with information regularization</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
				<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1362" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ARIEL: Adversarial Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2208.06956</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2208.06956" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Meta-Learned Metrics over Multi-Evolution Temporal Graphs</title>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liri</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vetle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>Torvik</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A View-Adversarial Framework for Multi-View Network Embedding</title>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD</title>
				<meeting>the 22nd ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05582</idno>
		<title level="m">Contrastive Multi-View Representation Learning on Graphs</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sub-Graph Contrast for Scalable Self-Supervised Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Yizhu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hdmi: High-order deep multiplex infomax</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2414" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Network of Tensor Time Series</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3449969</idno>
		<ptr target="https://doi.org/10.1145/3442381.3449969" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00006</idno>
		<title level="m">COIN: Co-Cluster Infomax for Bipartite Graphs</title>
				<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiplex Graph Neural Network for Extractive Text Summarization</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="133" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-gcn: Graph convolutional networks for multi-view networks, with applications to global poverty</title>
		<author>
			<persName><forename type="first">Raza</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">E</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Blumenstock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="606" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph Communal Contrastive Learning</title>
		<author>
			<persName><forename type="first">Bolian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
				<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-layered network embedding</title>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="684" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph filter-based multi-view attributed graph clustering</title>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00111</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multidimensional network embedding with hierarchical structure</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-dimensional graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chara</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="657" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Formal limitations on the measurement of mutual information</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics. PMLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="875" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Co-embedding attributed networks</title>
		<author>
			<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-view Contrastive Graph Clustering</title>
		<author>
			<persName><forename type="first">Erlin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep multiplex graph infomax: Attentive multiplex network embedding using global information</title>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page">105861</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised Attributed Multiplex Network Embedding</title>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5371" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD</title>
				<meeting>the 20th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking softmax with cross-entropy</title>
		<author>
			<persName><forename type="first">Zhenyue</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10688</idno>
	</analytic>
	<monogr>
		<title level="m">Neural network classifier as mutual information estimator</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An attention-based collaboration framework for multi-view network representation learning</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangqiu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06597</idno>
		<title level="m">mvn2vec: Preservation and collaboration in multi-view network embedding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labeled nodes</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5892" to="5899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
				<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Deep graph infomax. ICLR</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Attributed graph clustering: A deep attentional embedding approach</title>
		<author>
			<persName><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06532</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Abstract Diagrammatic Reasoning with Multiplex Graph Networks</title>
		<author>
			<persName><forename type="first">Duo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateja</forename><surname>Jamnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TheWebConf</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Self-supervised heterogeneous graph neural network with co-contrastive learning</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1726" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07342</idno>
		<title level="m">Selfsupervised on Graphs: Contrastive, Generative, or Predictive</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-supervised Contrastive Attributed Graph Clustering</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanxue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Contrastive Multi-View Multiplex Network Embedding with Applications to Robust Network Alignment</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1913" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiplex Bipartite Network Embedding using Dual Hypergraph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1649" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dynamic Knowledge Alignment</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bright: A bridging algorithm for network alignment</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3907" to="3917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scalable Multiplex Network Embedding</title>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3082" to="3088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">CommDGI: community detection oriented deep graph infomax</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1843" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Attributed graph clustering via adaptive graph convolution</title>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01210</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">ANRL: Attributed Network Representation Learning via Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinggang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3155" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13798</idno>
		<title level="m">Tackling oversmoothing of gnns with contrastive learning</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09401</idno>
		<title level="m">Heterogeneous Contrastive Learning</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A data-driven graph generative model for temporal interaction networks</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="401" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Misc-GAN: A multi-scale generative model for graphs</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiejun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for graph-based semi-supervised classification</title>
		<author>
			<persName><forename type="first">Chenyi</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
