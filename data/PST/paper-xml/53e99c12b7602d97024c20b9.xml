<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Scale Semi-supervised Linear SVMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
							<email>vikass@cs.uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Sathiya</forename><surname>Keerthi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Yahoo! Research Media Studios North Burbank</orgName>
								<address>
									<postCode>91504</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Scale Semi-supervised Linear SVMs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">85114FF03886D0EE08E94ED48A86E5CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology-Classifier design and evaluation Algorithms</term>
					<term>Performance</term>
					<term>Experimentation Support Vector Machines</term>
					<term>Unlabeled data</term>
					<term>Global optimization</term>
					<term>Text Categorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large scale learning is often realistic only in a semi-supervised setting where a small set of labeled examples is available together with a large collection of unlabeled data. In many information retrieval and data mining applications, linear classifiers are strongly preferred because of their ease of implementation, interpretability and empirical performance. In this work, we present a family of semi-supervised linear support vector classifiers that are designed to handle partially-labeled sparse datasets with possibly very large number of examples and features. At their core, our algorithms employ recently developed modified finite Newton techniques. Our contributions in this paper are as follows: (a) We provide an implementation of Transductive SVM (TSVM) that is significantly more efficient and scalable than currently used dual techniques, for linear classification problems involving large, sparse datasets. (b) We propose a variant of TSVM that involves multiple switching of labels. Experimental results show that this variant provides an order of magnitude further improvement in training efficiency. (c) We present a new algorithm for semi-supervised learning based on a Deterministic Annealing (DA) approach. This algorithm alleviates the problem of local minimum in the TSVM optimization procedure while also being computationally attractive. We conduct an empirical study on several document classification tasks which confirms the value of our methods in large scale semisupervised settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Consider the following situation: In a single web-crawl, search engines like Yahoo! and Google index billions of documents. Only a very small fraction of these documents can possibly be hand-labeled by human editorial teams and assembled into topic directories. In information retrieval relevance feedback, a user labels a small number of documents returned by an initial query as being relevant or not. The remaining documents form a massive collection of unlabeled data. Despite its natural and pervasive need, solutions to the problem of utilizing unlabeled data with labeled examples have only recently emerged in machine learning literature. Whereas the abundance of unlabeled data is frequently acknowledged as a motivation in most papers, the true potential of semi-supervised learning in large scale settings is yet to be systematically explored. This appears to be partly due to the lack of scalable tools to handle large volumes of data.</p><p>In this paper, we propose extensions of linear Support Vector Machines (SVMs) for semi-supervised classification. Linear techniques are often the method of choice in many applications due to their simplicity and interpretability. When data appears in a rich high-dimensional representation, linear functions often provide a sufficiently complex hypothesis space for learning high-quality classifiers. This has been established, for example, for document classification with Linear SVMs in numerous studies.</p><p>Our methods are motivated by the intuition of margin maximization for semi-supervised SVMs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4]</ref>. The key idea is to bias the classification hyperplane to pass through a low data density region keeping points in each data cluster on the same side of the hyperplane while respecting labels. This algorithm uses an extended SVM objective function with a non-convex loss term over the unlabeled examples to implement the cluster assumption in semi-supervised learning <ref type="foot" target="#foot_0">1</ref> . This idea is of historical importance as one of the first concrete proposals for learning from unlabeled data; its popular implementation in <ref type="bibr" target="#b5">[5]</ref> is considered state-of-the-art in text categorization, even in the face of increasing recent competition.</p><p>We highlight the main contributions of this paper. <ref type="bibr" target="#b1">(1)</ref> We outline an implementation for a variant of Transductive SVM <ref type="bibr" target="#b5">[5]</ref> designed for linear semi-supervised classification on large, sparse datasets. As compared to currently used dual techniques (e.g in the SVM light implementation of TSVM), our method effectively exploits data sparsity and linearity of the problem to provide superior scalability. Additionally, we propose a multiple switching heuristic that further improves TSVM training by an order of magnitude. These speed enhancements turn TSVM into a feasible tool for large scale applications.</p><p>(2) We propose a novel algorithm for semi-supervised SVMs inspired from Deterministic Annealing (DA) techniques. This approach generates a family of objective functions whose non-convexity is controlled by an annealing parameter. The global minimizer is parametrically tracked in this family. This approach alleviates the problem of local minima in the TSVM optimization procedure which results in better solutions on some problems. A computationally attractive training algorithm is presented that involves a sequence of alternating convex optimizations.</p><p>(3) We conduct an experimental study on many document classification tasks with several thousands of examples and features. This study clearly shows the utility of our tools for large scale problems.</p><p>The modified finite Newton algorithm (abbreviated l2-SVM-MFN) of Keerthi and Decoste <ref type="bibr" target="#b8">[8]</ref> for fast training of linear SVMs is a key subroutine for our algorithms.</p><p>This paper is arranged as follows. In section 2 we describe the l2-SVM-MFN algorithm and present its semi-supervised extensions in section 3. Experimental results are reported in section 4. Section 5 contains some concluding comments. A more detailed version of this paper is available at <ref type="bibr" target="#b11">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODIFIED FINITE NEWTON LINEAR L2-SVM</head><p>The modified finite Newton l2-SVM method <ref type="bibr" target="#b8">[8]</ref> (l2-SVM-MFN) is a recently developed training algorithm for Linear SVMs that is ideally suited to sparse datasets with large number of examples and possibly large number of features.</p><p>Given a binary classification problem with l labeled examples {xi, yi} l i=1 where the input patterns xi ∈ R d (e.g documents) and the labels yi ∈ {+1, -1}, l2-SVM-MFN provides an efficient primal solution to the following SVM optimization problem:</p><formula xml:id="formula_0">w = argmin w∈R d 1 2 l X i=1 l2(yiw T xi) + λ 2 w 2 (1)</formula><p>where l2 is the l2-SVM loss given by l2(z) = max(0, 1z) <ref type="foot" target="#foot_1">2</ref> , λ is a real-valued regularization parameter and the final classifier is given by sign(w T x). This objective function differs from the standard SVM problem in some respects. First, instead of using the hinge loss as the data fitting term, the square of the hinge loss (or the so-called quadratic soft margin loss function) is used. This makes the objective function continuously differentiable, allowing easier applicability of gradient techniques. Secondly, the bias term ("b") is also regularized. In the problem formulation of Eqn. 1, it is implicitly assumed that an additional component in the weight vector and a constant feature in the example vectors have been added to indirectly incorporate the bias. This formulation combines the simplicity of a least squares aspect with algorithmic advantages associated with SVMs. We also note that all the discussion in this paper can be applied to other loss functions such as Huber's Loss and rounded Hinge loss using the modifications outlined in <ref type="bibr" target="#b8">[8]</ref>.</p><p>We consider a version of l2-SVM-MFN where a weighted quadratic soft margin loss function is used.</p><formula xml:id="formula_1">min w f (w) = 1 2 X i∈j(w) cil2(yiw T xi) + λ 2 w 2<label>(2)</label></formula><p>Here we have rewritten Eqn. 1 in terms of the support vector set j(w) = {i : yi (w T xi) &lt; 1}. Additionally, the loss associated with the i th example has a cost ci. f (w) refers to the objective function being minimized, evaluated at a candidate solution w. Note that if the index set j(w) were independent of w and ran over all data points, this would simply be the objective function for weighted linear regularized least squares (RLS).</p><p>Following <ref type="bibr" target="#b8">[8]</ref>, we observe that f is a strictly convex, piecewise quadratic, continuously differentiable function having a unique minimizer. The gradient of f at w is given by:</p><formula xml:id="formula_2">∇ f (w) = λ w + X T j(w) C j(w) ˆXj(w) w -Y j(w)</formula><p>where X j(w) is a matrix whose rows are the feature vectors of training points corresponding to the index set j(w), Y j(w) is a column vector containing labels for these points, and C j(w) is a diagonal matrix that contains the costs ci for these points along its diagonal. l2-SVM-MFN is a primal algorithm that uses the Newton's Method for unconstrained minimization of a convex function. The classical Newton's method is based on a second order approximation of the objective function, and involves updates of the following kind:</p><formula xml:id="formula_3">w k+1 = w k + δ k n k (3)</formula><p>where the step size δ k ∈ R, and the Newton direction n k ∈ R d is given by:</p><formula xml:id="formula_4">n k = -[∇ 2 f (w k )] -1 ∇ f (w k ).</formula><p>Here, ∇ f (w k ) is the gradient vector and ∇ 2 f (w k ) is the Hessian matrix of f at w k . However, the Hessian does not exist everywhere, since f is not twice differentiable at those weight vectors w where w T xi = yi for some index i. 2 Thus a generalized definition of the Hessian matrix is used. The modified finite Newton procedure <ref type="bibr" target="#b8">[8]</ref> proceeds as follows. The step wk = w k + n k in the Newton direction can be seen to be given by solving the following linear system associated with a weighted linear regularized least squares problem over the data subset defined by the indices j(w k ):</p><formula xml:id="formula_5">h λI + X T j(w k ) C j(w k ) X j(w k ) i wk = X T j(w k ) C j(w k ) Y j(w k ) (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where I is the identity matrix. Once wk is obtained, w k+1 is obtained from Eqn. 3 by setting w k+1 = w k + δ k ( wkw k ) after performing an exact line search for δ k , i.e by exactly solving a one-dimensional minimization problem:</p><formula xml:id="formula_7">δ k = argmin δ≥0 φ(δ) = f " w k + δ( wk -w k ) "<label>(5)</label></formula><p>The modified finite Newton procedure has the property of finite convergence to the optimal solution. The key features that bring scalability and numerical robustness to l2-SVM-MFN are: (a) Solving the regularized least squares system of Eqn. 4 by a numerically well-behaved Conjugate Gradient scheme referred to as CGLS, which is designed for large, sparse data matrices X. The benefit of the least squares aspect of the loss function comes in here to provide access to a powerful set of tools in numerical computation. (b) Due to the one-sided nature of margin loss functions, these systems are required to be solved over only restricted index sets j(w) which can be much smaller than the whole dataset. This also allows additional heuristics to be developed such as terminating CGLS early when working with a crude starting guess like 0, and allowing the following line search step to yield a point where the index set j(w) is small. Subsequent optimization steps then work on smaller subsets of the data Below, we briefly discuss the CGLS and Line search procedures. We refer the reader to <ref type="bibr" target="#b8">[8]</ref> for full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CGLS</head><p>CGLS is a special conjugate-gradient solver that is designed to solve, in a numerically robust way, large, sparse, weighted regularized least squares problems such as the one in Eqn. 4. Starting with a guess solution, several specialized conjugate-gradient iterations are applied to get wk that solves Eqn. 4. The major expense in each iteration consists of two operations of the form X j(w k ) p and X T j(w k ) q. If there are n0 non-zero elements in the data matrix, these involve O(n0) cost. It is worth noting that, as a subroutine of l2-SVM-MFN, CGLS is typically called on a small subset, X j(w k ) of the full data set. To compute the exact solution of Eqn. 4, r iterations are needed, where r is the rank of X j(w k ) . But, in practice, such an exact solution is unnecessary. CGLS uses an effective stopping criterion for early termination. The total cost of CGLS is O(t cgls n0) where t cgls is the number of iterations, which depends on the practical rank of X j(w k ) and is typically found to be very small relative to the dimensions of X j(w k ) (number of examples and features). The memory requirements of CGLS are also minimal: only five vectors need to be maintained, including the outputs over the currently active set of data points.</p><p>Finally, an important feature of CGLS is worth emphasizing. Suppose the solution w of a regularized least squares problem is available, i.e the linear system in Eqn. 4 has been solved using CGLS. If there is a need to solve a perturbed linear system, it is greatly advantageous in many settings to start the CG iterations for the new system with w as the initial guess. This is called seeding. If the starting residual is small, CGLS can converge much faster than with a guess of 0 vector. The utility of this feature depends on the nature and degree of perturbation. In l2-SVM-MFN, the candidate solution w k obtained after line search in iteration k is seeded for the CGLS computation of wk . Also, in tuning λ over a range of values, it is valuable to seed the solution for a particular λ onto the next value. For the semi-supervised SVM implementations with l2-SVM-MFN, we will seed solutions across linear systems with slightly perturbed label vectors, data matrices and costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Line Search</head><p>Given the vectors w k , wk in some iteration of l2-SVM-MFN, the line search step requires us to solve Eqn. 5. The one-dimensional function φ(δ) is the restriction of the objective function f on the ray from w k onto wk . Hence, like f , φ(δ) is also a continuously differentiable, strictly convex, piecewise quadratic function with a unique minimizer. φ is a continuous piecewise linear function whose root, δ k , can be easily found by sorting the break points where its slope changes and then performing a sequential search on that sorted list. The cost of this operation is negligible compared to the cost of the CGLS iterations. l2-SVM-MFN alternates between calls to CGLS and line searches. Its computational complexity is O(t mf n tcgls n0) where t mf n is the number of outer iterations of CGLS calls and line search, and tcgls is the average number of CGLS iterations. These depend on the data set and the tolerance desired in the stopping criterion, but are typically very small. For example, on a text classification experiment involving 198788 examples and 252472 features, t mf n = 11 and tcgls = 102. Therefore, the complexity of l2-SVM-MFN is effectively linear in the number of entries in the data matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SEMI-SUPERVISED LINEAR SVMS</head><p>We now assume we have l labeled examples {xi, yi} l i=1 and u unlabeled examples {x j } u j=1 with xi, x j ∈ R d and yi ∈ {-1, +1}. Our goal is to construct a linear classifier sign(w T x) that utilizes unlabeled data, typically in situations where l u. We present semi-supervised algorithms that provide l2-SVM-MFN the capability of dealing with unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transductive SVM</head><p>Transductive SVM appends an additional term in the SVM objective function whose role is to drive the classification hyperplane towards low data density regions. Variations of this idea have appeared in the literature <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr">6]</ref>. Since <ref type="bibr" target="#b5">[5]</ref> appears to be the most natural extension of standard SVMs among these methods, and is popularly used in Text classification applications, we will focus on developing its large scale implementation.</p><p>The following optimization problem is setup for standard TSVM<ref type="foot" target="#foot_2">3</ref> :</p><formula xml:id="formula_8">min w,{y j } u j=1 λ 2 w 2 + 1 2l l X i=1 l(yi w T xi) + λ 2u u X j=1 l(y j w T x j ) subject to: 1 u u X j=1 max[0, sign(w T x j )] = r</formula><p>where the hinge loss function, l(z) = l1(z) = max(0, 1-z) is normally used. The labels on the unlabeled data, y 1 . . . y u , are {+1, -1}-valued variables in the optimization problem.</p><p>In other words, TSVM seeks a hyperplane w and a labeling of the unlabeled examples, so that the SVM objective function is minimized, subject to the constraint that a fraction r of the unlabeled data be classified positive. SVM margin maximization in the presence of unlabeled examples can be interpreted as an implementation of the cluster assumption.</p><p>In the optimization problem above, λ is a user-provided parameter that provides control over the influence of unlabeled data. For example, if the data has distinct clusters with a large margin, but the cluster assumption does not hold, then λ can be set to 0 and the standard SVM is retrieved. If there is enough labeled data, λ, λ can be tuned by cross-validation. An initial estimate of r can be made from the fraction of labeled examples that belong to the positive class and subsequent fine tuning can be done based on validation performance. This optimization is implemented in <ref type="bibr" target="#b5">[5]</ref> by first using an inductive SVM to label the unlabeled data and then iteratively switching labels and retraining SVMs to improve the objective function. The TSVM algorithm wraps around an SVM training procedure. The original (and widely popular) implementation of TSVM uses the SVM light software. There, the training of SVMs in the inner loops of TSVM uses dual decomposition techniques. As shown by experiments in <ref type="bibr" target="#b8">[8]</ref>, in sparse, linear settings one can obtain significant speed improvements with l2-SVM-MFN over SVM light . Thus, by implementing TSVM with l2-SVM-MFN, we expect similar improvements for semi-supervised learning on large, sparse datasets. Note that l2-SVM-MFN can also be used to speedup other TSVM formulations e.g <ref type="bibr" target="#b4">[4]</ref> in such cases. The l2-SVM-MFN retraining steps in the inner loop of TSVM are typically executed extremely fast by using seeding techniques. Additionally, we also propose a version of TSVM where more than one pair of labels may be switched in each iteration. These speed-enhancement details are discussed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Implementing TSVM Using l2-SVM-MFN</head><p>To develop the TSVM implementation with l2-SVM-MFN, we consider the TSVM objective function but with the L2-SVM loss function, l = l2. Note that this objective function above can also be equivalently written in terms of the following loss over each unlabeled example x:</p><formula xml:id="formula_9">min[l2(w T x), l2(-w T x)] = max[0, 1 -|w T x|] 2</formula><p>Here, we pick the value of the label variable y that minimizes the loss on the unlabeled example x, and rewrite in terms of the absolute value of the output of the classifier on x. This loss function is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We note in passing that, l1 and l2 loss terms over unlabeled examples are very similar on the interval [-1, +1]. The non-convexity of this loss function implies that the TSVM training procedure is susceptible to local optima issues. In the next subsection, we will outline a deterministic annealing procedure that can overcome this problem.</p><p>The TSVM algorithm with l2-SVM-MFN closely follows the presentation in <ref type="bibr" target="#b5">[5]</ref>. A classifier is obtained by first running l2-SVM-MFN on just the labeled examples. Temporary labels are assigned to the unlabeled data by thresholding the soft outputs of this classifier so that the fraction of the total number of unlabeled examples that are temporarily labeled positive equals the parameter r. Then starting from a small value of λ , the unlabeled data is gradually brought in by increasing λ by a certain factor in the outer loop. This gradual increase of the influence of the unlabeled data is a way to protect TSVM from being immediately trapped in a local minimum. An inner loop identifies pairs of unlabeled examples with positive and negative temporary labels such that switching these labels would decrease the objective function. l2-SVM-MFN is then retrained with the switched labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Multiple Switching</head><p>The TSVM algorithm presented in <ref type="bibr" target="#b5">[5]</ref> involves switching a single pair of labels at a time. We propose a variant where upto S pairs are switched such that the objective function improves. Here, S is a user controlled parameter. Setting S = 1 recovers the original TSVM algorithm, whereas setting S = u/2 switches as many pairs as possible in the inner loop of TSVM. The implementation is conveniently done as follows:</p><p>1. Identify unlabeled examples with active indices and currently positive labels. Sort corresponding outputs in ascending order. Let the sorted list be L + .</p><p>2. Identify unlabeled examples with active indices and currently negative labels. Sort corresponding outputs in descending order. Let the sorted list be L -.</p><p>3. Pick pairs of elements, one from each list, from the top of these lists until either a pair is found such that the output from L + is greater than the output from L -, or if S pairs have been picked.</p><p>4. Switch the current labels of these pairs.</p><p>Using arguments similar to Theorem 2 in <ref type="bibr" target="#b5">[5]</ref> we can show that Transductive l2-SVM-MFN with multiple-pair switching converges in a finite number of steps.</p><p>We are unaware of any prior work that suggests and evaluates this simple multiple-pair switching heuristic. Our experimental results in section 4 establish that this heuristic is remarkably effective in speeding up TSVM training while maintaining generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Seeding</head><p>The effectiveness of l2-SVM-MFN on large sparse datasets combined with the efficiency gained from seeding w in the re-training steps (after switching labels or after increasing λ ) make this algorithm quite attractive. The complexity of Transductive L2-TSVM-MFN is O(n switches tmfn tcgls n0), where n switches is the number of label switches. Typically, n switches is expected to strongly depend on the data set and also on the number of labeled examples. Since it is difficult to apriori estimate the number of switches, this is an issue that is best understood from empirical observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deterministic Annealing</head><p>The transductive SVM loss function over the unlabeled examples can be seen from Fig. <ref type="figure" target="#fig_0">1</ref> to be non-convex. This makes the TSVM optimization procedure susceptible to local minimum issues causing a loss in its performance in many situations, e.g as recorded in <ref type="bibr" target="#b3">[3]</ref>. We now present a new algorithm based on deterministic annealing that can potentially overcome this problem while also being computation-ally very attractive for large scale applications. Deterministic Annealing <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b9">9]</ref> (DA) is an established tool for combinatorial optimization that approaches the problem from information theoretic principles. The discrete variables in the optimization problem are relaxed to continuous probability variables and a non-negative temperature parameter T is used to track the global optimum.</p><p>We begin by re-writing the TSVM objective function as follows:</p><formula xml:id="formula_10">w = argmin w,{μ j } u j=1 λ 2 w 2 + 1 2l l X i=1 l2(w T xi) + λ 2u u X j=1 " μjl2(w T x j ) + (1 -μj )l2(-w T x j ) "</formula><p>Here, we introduce binary valued variables μj = (1 + yj)/2.</p><p>Let pj ∈ [0, 1] denote the belief probability that the unlabeled example x j belongs to the positive class. The Ising model 4 motivates the following objective function, where we relax the binary variables μj to probability variables pj , and include entropy terms for the distributions defined by pj:</p><formula xml:id="formula_11">w T = argmin w,{p j } u j=1 λ 2 w 2 + 1 2l l X i=1 l2(yiw T xi) + λ 2u u X j=1 " pjl2(w T x j ) + (1 -pj)l2(-w T x j ) " + T 2u u X j=1 [pj log pj + (1 -pj) log (1 -pj)]<label>(6)</label></formula><p>Here, the "temperature" T parameterizes a family of objective functions. The objective function for a fixed T is minimized under the following class balancing constraint:</p><formula xml:id="formula_12">1 u u X j=1 pj = r (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where r is the fraction of the number of unlabeled examples belonging to the positive class. As in TSVM, r is treated as a user-provided parameter. It may also be estimated from the labeled examples. The solution to the optimization problem above is tracked as the temperature parameter T is lowered to 0. We monitor the value of the objective function in the optimization path and return the solution corresponding to the minimum value achieved.</p><p>To develop an intuition for the working on this method, we consider the loss term in the objective function associated with an unlabeled example as a function of the output of the classifier. Fig. <ref type="figure" target="#fig_1">2</ref> plots this loss term for various values of T . As the temperature is decreased, the loss function deforms from a squared-loss shape where a global optimum is easier to achieve, to the TSVM loss function in Fig. <ref type="figure" target="#fig_0">1</ref>. At high temperatures a global optimum is easier to obtain. The minimizer is then slowly tracked as the temperature is lowered towards zero. 4 A multiclass extension would use the Potts glass model. There, one would have to append the entropy of the distribution over multiple classes to a multi-class objective function. The optimization is done in stages, starting with high values of T and then gradually decreasing T towards 0. For each T , the problem in Eqns. 6,7 is optimized by alternating the minimization over w and p = [p1 . . . pu] respectively. Fixing p, the optimization over w is done by l2-SVM-MFN with seeding. Fixing w, the optimization over p can also be done easily as described below. Both these problems involve convex optimization and can be done exactly and efficiently. We now provide some details.</p><p>where gj = λ [l2(w T x j ) -l2(-w T x j )]. Substituting this expression in the balance constraint in Eqn. 7, we get a one-dimensional non-linear equation in 2ν:</p><formula xml:id="formula_14">1 u u X j=1 1 1 + e g i -2ν T = r</formula><p>The root is computed by using a hybrid combination of Newton-Raphson iterations and the bisection method together with a carefully set initial value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Stopping Criteria</head><p>For a fixed T , the alternate minimization of w and p proceeds until some stopping criterion is satisfied. A natural criterion is the mean Kullback-Liebler divergence (relative entropy) KL(p, q) between current values of pi and the values, say qi, at the end of last iteration. Thus the stopping criterion for fixed T is:</p><formula xml:id="formula_15">KL(p, q) = u X j=1 pj log pj qj + (1 -pj) log 1 -pj 1 -qj &lt; u</formula><p>A good value for is 10 -6 . The temperature may be decreased in the outer loop until the total entropy falls below a threshold, which we take to be = 10 -6 as above, i.e.,</p><formula xml:id="formula_16">H(p) = - u X j=l (pj log pj + (1 -pj) log (1 -pj)) &lt; u</formula><p>The TSVM objective function,</p><formula xml:id="formula_17">λ 2 w 2 + 1 2l l X i=1 l2(yi (w T xi) + λ 2u u X j=1 max h 0, 1 -|w T x j | i 2</formula><p>is monitored as the optimization proceeds. The weight vector corresponding to the minimum transductive cost in the optimization path is returned as the solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EMPIRICAL STUDY</head><p>Semi-supervised learning experiments were conducted to test these algorithms on six text binary classification problems. These are listed in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The aut-avn and real-sim binary classification datasets come from a collection of UseNet articles<ref type="foot" target="#foot_3">5</ref> from four discussion groups, for simulated auto racing, simulated aviation, real autos, and real aviation. The ccat and gcat data sets pose the problem of separating corporate and government related articles respectively; these are the top-level categories in the RCV1 training data set <ref type="bibr" target="#b7">[7]</ref>. These data sets create an interesting situation where semi-supervised learning is required to learn different low density separators respecting different classification tasks in the same input space. The 33-36 data set is a subset of a multiclass Yahoo shopping data set. Finally, the pcmac data set is a small subset of the 20-newsgroups data popularly used in semi-supervised learning literature (e.g in <ref type="bibr" target="#b3">[3]</ref>). The results below are averaged over 10 random stratified splits of training (labeled and unlabeled) and test sets. The performance of SVM, DA and TSVM is studied as a function of the amount of labeled data in the training set. Since the two classes are fairly well represented in these datasets, we report error rates, but expect our conclusions to also hold for other performance measures such as F-measure. We use a default value of λ = 1 for all datasets except 6 for aut-avn and ccat where λ = 10. The default value of λ = 0.001 was used for all datasets.</p><p>For datasets and software implementation, we point the reader to <ref type="bibr">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimization of Objective Function</head><p>We first examine the effectiveness of TSVM and DA in optimizing the TSVM objective function. In Table <ref type="table" target="#tab_2">2</ref>, we report the minimum value of the objective function achieved by TSVM and DA with respect to varying number of labels. As compared to TSVM, we see that DA performs significantly better optimization on aut-avn, ccat, and pcmac datasets and slightly better optimization on the other datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Performance</head><p>In Table <ref type="table" target="#tab_4">3</ref>  (1) Comparing the performance of SVM against the semisupervised algorithms, the benefit of unlabeled data for boost- 6 This produced better results for both TSVM and DA. A careful optimization of λ was not attempted. ing generalization performance is evident on all datasets. This is true even for moderate number of labels, though it is particularly striking towards the lower end.</p><p>(2) On aut-avn and pcmac, DA outperforms TSVM significantly. On ccat, DA performs a much better optimization (Table <ref type="table" target="#tab_2">2</ref>) but this does not translate into major error rate improvements. DA and TSVM are very closely matched on gcat and 33-36. On real-sim, TSVM and DA perform very similar optimization of the transduction objective function (Table <ref type="table" target="#tab_2">2</ref>), but appear to return very different solutions. The TSVM solution returns lower error rates as compared to DA on this dataset.</p><p>(3) On all datasets we found that multiple switching (see rows corresponding to TSVM (S=max) in Table <ref type="table" target="#tab_4">3</ref>) returned nearly identical performance as single switching. Since it saves significant computation time, our study establishes multiple switching as a valuable heuristic for training TSVM.</p><p>(4) These observations are also true for in-sample transductive performance. Both TSVM and DA are found to provide high quality extension to unseen test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Timings</head><p>In Figure <ref type="figure" target="#fig_2">3</ref>, we plot the average computation time for DA and TSVM with single and maximum switching. We make the following observations. The single switch TSVM is an order of magnitude slower than the maximum switching variant. DA is significantly faster than single switch TSVM but slower than TSVM with maximum switching.  In Table <ref type="table" target="#tab_6">4</ref>, we compare our TSVM and DA implementations with SVM light at its default optimization settings on the first split with fewest labeled examples. These comparisons clearly demonstrate massive speedups with our methods. Note that the results presented in this section were obtained with a MATLAB implementation. We expect significantly faster computation times with a C or a fortran implementation, especially with parallel computation of matrix vector products 7 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of Annealing</head><p>To confirm the necessity of an annealing component (tracking the minimizer with respect to T ) in the optimization, we compare DA with the alternating w,p optimization procedure where the temperature parameter is held fixed at T = 0.1 and T = 0.001. In Figure <ref type="figure" target="#fig_3">4</ref> we plot the error rates achieved with and without annealing. We see that annealing tends to provide higher quality solutions as compared to fixed temperature optimization.</p><p>It is important to note that the gradual increase of λ to the user-set value in TSVM is also a mechanism to avoid 7 preliminary experiments with a C++ implementation (to be made available at <ref type="bibr">[12]</ref>) suggested about 5-fold further improvements in speed.  local optima. The non-convex part of the TSVM objective function is gradually increased to a desired value. In this sense, λ simultaneously plays the role of an annealing parameter and also provides control over the strength of the cluster assumption. This dual role has the advantage that a suitable λ can be chosen by monitoring performance on a validation set as the algorithm proceeds. In DA, however, we directly apply a framework for global optimization, and decouple annealing from the implementation of the cluster assumption. As our experiments show, this can lead to significantly better solutions on many problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper we have proposed a family of primal SVM algorithms for large scale semi-supervised learning based on the finite Newton technique. Our methods significantly enhance the training speed of TSVM over existing methods such as SVM light and also include a new effective technique based on deterministic annealing. The new TSVM method with multiple switching is the fastest of all the algorithms considered, and also returns good generalization performance. The DA method is relatively slower but often gives the best accuracy. These algorithms can be very valuable in applied scenarios where sparse classification problems arise frequently, labeled data is scarce and plenty of unlabeled data is easily available. Even in situations where a good number of labeled examples are available, utilizing unlabeled data to obtain a semi-supervised solution using these algorithms can be worthwhile. For one thing, the semisupervised solutions never lag behind purely supervised solutions in terms of performance. The presence of a mix of labeled and unlabeled data can provide added benefits such as reducing performance variability and stabilizing the linear classifier weights. Our algorithms can be extended to the non-linear setting <ref type="bibr" target="#b10">[10]</ref>, and may also be developed to handle clustering and one-class classification problems. These are subjects for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1: l2 loss function for TSVM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DA loss function parameterized by T.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Computation time with respect to number of labels for DA and Transductive l2-SVM-MFN with single and multiple switches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Error Rates achieved by DA and fixed temperature optimization with respect to number of labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Two-class datasets. d : data dimensionality, n0 : average sparsity, l + u :</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>number of labeled and unlabeled examples, t : number of test examples, r : positive class ratio.</head><label></label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>d</cell><cell>n0</cell><cell>l + u</cell><cell>t</cell><cell>r</cell></row><row><cell cols="6">aut-avn 20707 51.32 35588 35587 0.65</cell></row><row><cell cols="6">real-sim 20958 51.32 36155 36154 0.31</cell></row><row><cell>ccat</cell><cell cols="3">47236 75.93 17332</cell><cell>5787</cell><cell>0.46</cell></row><row><cell>gcat</cell><cell cols="3">47236 75.93 17332</cell><cell>5787</cell><cell>0.30</cell></row><row><cell>33-36</cell><cell cols="5">59072 26.56 41346 41346 0.49</cell></row><row><cell>pcmac</cell><cell>7511</cell><cell>54.58</cell><cell>1460</cell><cell>486</cell><cell>0.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : TSVM,DA: Minimum value of objective function achieved with respect to number of labels. Statistically significantly better minimizations are shown in bold.</head><label>2</label><figDesc></figDesc><table><row><cell>aut-avn</cell><cell>l=45</cell><cell>89</cell><cell>178</cell><cell>356</cell><cell>712</cell><cell>1424</cell></row><row><cell>TSVM</cell><cell>0.692</cell><cell>0.692</cell><cell>0.698</cell><cell>0.709</cell><cell>0.715</cell><cell>0.722</cell></row><row><cell>DA</cell><cell cols="6">0.669 0.663 0.670 0.679 0.687 0.696</cell></row><row><cell>real-sim</cell><cell>l=46</cell><cell>91</cell><cell>181</cell><cell>362</cell><cell>724</cell><cell>1447</cell></row><row><cell>TSVM</cell><cell>0.168</cell><cell>0.181</cell><cell>0.194</cell><cell>0.212</cell><cell>0.235</cell><cell>0.257</cell></row><row><cell>DA</cell><cell>0.166</cell><cell>0.176</cell><cell>0.190</cell><cell>0.210</cell><cell>0.234</cell><cell>0.256</cell></row><row><cell>ccat</cell><cell>l=44</cell><cell>87</cell><cell>174</cell><cell>348</cell><cell>695</cell><cell>1389</cell></row><row><cell>TSVM</cell><cell>0.583</cell><cell>0.602</cell><cell>0.637</cell><cell>0.663</cell><cell>0.682</cell><cell>0.704</cell></row><row><cell>DA</cell><cell>0.575</cell><cell>0.594</cell><cell cols="4">0.603 0.619 0.639 0.662</cell></row><row><cell>gcat</cell><cell>l=44</cell><cell>87</cell><cell>174</cell><cell>348</cell><cell>695</cell><cell>1389</cell></row><row><cell>TSVM</cell><cell>0.135</cell><cell>0.140</cell><cell>0.147</cell><cell>0.159</cell><cell>0.177</cell><cell>0.193</cell></row><row><cell>DA</cell><cell>0.134</cell><cell>0.138</cell><cell>0.146</cell><cell>0.158</cell><cell>0.176</cell><cell>0.192</cell></row><row><cell>33-36</cell><cell>l=52</cell><cell>104</cell><cell>207</cell><cell>414</cell><cell>827</cell><cell>1654</cell></row><row><cell>TSVM</cell><cell>0.198</cell><cell>0.210</cell><cell>0.233</cell><cell>0.256</cell><cell>0.286</cell><cell>0.316</cell></row><row><cell>DA</cell><cell>0.196</cell><cell>0.208</cell><cell>0.230</cell><cell>0.255</cell><cell>0.285</cell><cell>0.315</cell></row><row><cell>pcmac</cell><cell>l=37</cell><cell>73</cell><cell>110</cell><cell>146</cell><cell>183</cell><cell>220</cell></row><row><cell>TSVM</cell><cell>0.136</cell><cell>0.137</cell><cell>0.138</cell><cell>0.139</cell><cell>0.141</cell><cell>0.142</cell></row><row><cell>DA</cell><cell cols="6">0.122 0.125 0.128 0.131 0.134 0.136</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : SVM, TSVM, DA: test error rate compari- son. TSVM (S=max) are results for multiple (max- imum possible) switching TSVM. Bold numbers in- dicate a significant performance difference between TSVM and DA.</head><label>3</label><figDesc></figDesc><table><row><cell>aut-avn</cell><cell>l= 45</cell><cell>89</cell><cell>178</cell><cell>356</cell><cell>712</cell><cell>1424</cell></row><row><cell>SVM</cell><cell>31.8</cell><cell>24.0</cell><cell>15.6</cell><cell>10.3</cell><cell>7.6</cell><cell>5.8</cell></row><row><cell>DA</cell><cell>7.0</cell><cell>4.6</cell><cell>4.5</cell><cell>3.9</cell><cell>3.7</cell><cell>3.6</cell></row><row><cell>TSVM</cell><cell>6.2</cell><cell>5.6</cell><cell>5.2</cell><cell>4.7</cell><cell>4.3</cell><cell>3.9</cell></row><row><cell>TSVM (S=max)</cell><cell>6.1</cell><cell>5.6</cell><cell>5.2</cell><cell>4.7</cell><cell>4.3</cell><cell>3.9</cell></row><row><cell>real-sim</cell><cell>l= 46</cell><cell>91</cell><cell>181</cell><cell>362</cell><cell>724</cell><cell>1447</cell></row><row><cell>SVM</cell><cell>28.7</cell><cell>24.9</cell><cell>18.2</cell><cell>12.8</cell><cell>9.8</cell><cell>7.5</cell></row><row><cell>DA</cell><cell>15.4</cell><cell>13.5</cell><cell>11.6</cell><cell>10.0</cell><cell>8.4</cell><cell>7.2</cell></row><row><cell>TSVM</cell><cell>16.3</cell><cell>12.5</cell><cell>10.2</cell><cell>9.0</cell><cell>7.9</cell><cell>6.9</cell></row><row><cell>TSVM (S=max)</cell><cell>15.9</cell><cell cols="3">12.4 10.2 8.9</cell><cell>7.9</cell><cell>6.9</cell></row><row><cell>ccat</cell><cell>l=44</cell><cell>87</cell><cell>174</cell><cell>348</cell><cell>695</cell><cell>1389</cell></row><row><cell>SVM</cell><cell>23.9</cell><cell>18.6</cell><cell>14.6</cell><cell cols="2">11.9 10.0</cell><cell>8.7</cell></row><row><cell>DA</cell><cell>18.1</cell><cell>13.7</cell><cell>11.9</cell><cell>10.3</cell><cell>9.2</cell><cell>8.5</cell></row><row><cell>TSVM</cell><cell>20.3</cell><cell>13.9</cell><cell>11.7</cell><cell>10.2</cell><cell>9.1</cell><cell>8.3</cell></row><row><cell>TSVM (S=max)</cell><cell>20.2</cell><cell>14.0</cell><cell>11.7</cell><cell>10.2</cell><cell>9.1</cell><cell>8.3</cell></row><row><cell>gcat</cell><cell>l=44</cell><cell>87</cell><cell>174</cell><cell>348</cell><cell>695</cell><cell>1389</cell></row><row><cell>SVM</cell><cell>26.1</cell><cell>18.1</cell><cell>11.8</cell><cell>8.3</cell><cell>6.6</cell><cell>5.6</cell></row><row><cell>DA</cell><cell>6.0</cell><cell>5.9</cell><cell>5.8</cell><cell>5.7</cell><cell>5.4</cell><cell>5.2</cell></row><row><cell>TSVM</cell><cell>6.1</cell><cell>6.0</cell><cell>5.8</cell><cell>5.7</cell><cell>5.4</cell><cell>5.1</cell></row><row><cell>TSVM (S=max)</cell><cell>6.2</cell><cell>6.0</cell><cell>5.8</cell><cell>5.8</cell><cell>5.4</cell><cell>5.1</cell></row><row><cell>33-36</cell><cell>l= 52</cell><cell>104</cell><cell>207</cell><cell>414</cell><cell>827</cell><cell>1654</cell></row><row><cell>SVM</cell><cell>26.0</cell><cell>20.3</cell><cell>15.5</cell><cell cols="2">12.7 10.7</cell><cell>8.9</cell></row><row><cell>DA</cell><cell>21.5</cell><cell>17.4</cell><cell>13.3</cell><cell>11.4</cell><cell>9.8</cell><cell>8.6</cell></row><row><cell>TSVM</cell><cell>21.3</cell><cell>16.6</cell><cell>12.8</cell><cell>11.2</cell><cell>9.7</cell><cell>8.5</cell></row><row><cell>TSVM (S=max)</cell><cell>21.4</cell><cell>16.6</cell><cell>12.8</cell><cell>11.2</cell><cell>9.7</cell><cell>8.5</cell></row><row><cell>pcmac</cell><cell>l= 37</cell><cell>73</cell><cell>110</cell><cell>146</cell><cell>183</cell><cell>220</cell></row><row><cell>SVM</cell><cell>18.0</cell><cell>12.1</cell><cell>9.7</cell><cell>8.0</cell><cell>7.4</cell><cell>6.7</cell></row><row><cell>DA</cell><cell>5.2</cell><cell>5.0</cell><cell>4.6</cell><cell>4.7</cell><cell>4.4</cell><cell>4.3</cell></row><row><cell>TSVM</cell><cell>7.4</cell><cell>6.9</cell><cell>6.0</cell><cell>5.7</cell><cell>5.1</cell><cell>4.7</cell></row><row><cell>TSVM (S=max)</cell><cell>7.3</cell><cell>6.8</cell><cell>6.0</cell><cell>5.7</cell><cell>5.1</cell><cell>4.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 : Speed comparisons (in seconds) with SVM light . S=1 and S=max denotes our single and multiple maximum implementations.</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">Dataset SVM light S=1 S=max</cell><cell>DA</cell></row><row><cell>aut-avn</cell><cell>101759</cell><cell>5849</cell><cell>390</cell><cell>1446</cell></row><row><cell>real-sim</cell><cell>498313</cell><cell>6244</cell><cell>373</cell><cell>1129</cell></row><row><cell>ccat</cell><cell>13540</cell><cell>2352</cell><cell>390</cell><cell>1185</cell></row><row><cell>gcat</cell><cell>243840</cell><cell>1267</cell><cell>358</cell><cell>159</cell></row><row><cell>33-36</cell><cell>48390</cell><cell>7406</cell><cell>309</cell><cell>393</cell></row><row><cell>pcmac</cell><cell>167</cell><cell>4</cell><cell>2</cell><cell>12</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The assumption that points in a cluster should have similar labels. The role of unlabeled data is to identify clusters and high density regions in the input space.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In the neighborhood of such a w, the index i leaves or enters j(w). However, at w, yiw T xi = 1. So f is continuously differentiable inspite of these index jumps.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The bias term is typically excluded from the regularizer, but this factor is not expected to make any significant difference.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Available at: http://www.cs.umass.edu/∼mccallum/data/ sraa.tar.gz</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Optimizing w</head><p>We describe the steps to efficiently implement the l2-SVM-MFN loop for optimizing w keeping p fixed. The call to l2-SVM-MFN is made on the data X = ˆXT X T X T ˜T whose first l rows are formed by the labeled examples, and the next 2u rows are formed by the unlabeled examples appearing as two repeated blocks. The associated label vector and cost matrix are given by</p><p>Even though each unlabeled data contributes two terms to the objective function, effectively only one term contributes to the complexity. This is because matrix-vector products, which form the dominant expense in l2-SVM-MFN, are performed only on unique rows of a matrix. The output may be duplicated for duplicate rows. Infact, we can re-write the CGLS calls in l2-SVM-MFN so that the unlabeled examples appear only once in the data matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Optimizing p</head><p>For the latter problem of optimizing p for a fixed w, we construct the Lagrangian:</p><p>Solving ∂L/∂pj = 0, we get:</p><p>1 + e g j -2ν T <ref type="bibr" target="#b9">(9)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semi-Supervised Support Vector Machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demirez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Bilbro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Van Den</surname></persName>
		</author>
		<title level="m">Optimization by Mean Field Annealing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification by Low Density Separation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI &amp; Statistics</title>
		<imprint>
			<date type="published" when="2005-01">January 2005</date>
			<pubPlace>Barbados</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Large Scale Transductive SVMs</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<title level="m">Transductive Inference for Text Classification using Support Vector Machines, ICML</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-Supervised Support Vector Machines for Unlabeled Data Classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Methods and Software</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RCV1: A New Benchmark Collection for Text Categorization Research</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="341" to="361" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new method for mapping optimization problems onto neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Soderberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deterministic Annealing for Semi-supervised Kernel Machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large Scale Semi-supervised Linear SVMs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Yahoo research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
