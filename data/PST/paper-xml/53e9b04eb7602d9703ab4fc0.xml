<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Happened to Remote Usability Testing? An Empirical Study of Three Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Morten</forename><forename type="middle">Sieker</forename><surname>Andreasen</surname></persName>
							<email>morten@sieker.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Systematic A/S</orgName>
								<address>
									<addrLine>Søren Frichs Vej 39</addrLine>
									<postCode>DK-8000</postCode>
									<settlement>Århus C</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henrik</forename><forename type="middle">Villemann</forename><surname>Nielsen</surname></persName>
							<email>henrik@villemann.net</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<postCode>DK-9220</postCode>
									<settlement>Aalborg East</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><forename type="middle">Ormholt</forename><surname>Schrøder</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Danske Bank Group</orgName>
								<address>
									<addrLine>Edwin Rahrs Vej 40</addrLine>
									<postCode>DK-8220</postCode>
									<settlement>Brabrand</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Stage</surname></persName>
							<email>jans@cs.aau.dk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<postCode>DK-9220</postCode>
									<settlement>Aalborg East</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What Happened to Remote Usability Testing? An Empirical Study of Three Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7782234EAD7E2FEB748846947D7041D7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Remote testing</term>
					<term>usability testing</term>
					<term>empirical study H.5.2 [Information Interfaces and Presentation]: User Interfaces -Graphical user interfaces (GUI)</term>
					<term>Theory and methods. D.2.2 [Software Engineering]: Design Tools and Techniques -User interfaces</term>
					<term>Object-oriented design methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The idea of conducting usability tests remotely emerged ten years ago. Since then, it has been studied empirically, and some software organizations employ remote methods. Yet there are still few comparisons involving more than one remote method. This paper presents results from a systematic empirical comparison of three methods for remote usability testing and a conventional laboratorybased think-aloud method. The three remote methods are a remote synchronous condition, where testing is conducted in real time but the test monitor is separated spatially from the test subjects, and two remote asynchronous conditions, where the test monitor and the test subjects are separated both spatially and temporally. The results show that the remote synchronous method is virtually equivalent to the conventional method. Thereby, it has the potential to conveniently involve broader user groups in usability testing and support new development approaches. The asynchronous methods are considerably more timeconsuming for the test subjects and identify fewer usability problems, yet they may still be worthwhile.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Remote usability testing denotes a situation where "the evaluators are separated in space and/or time from users" <ref type="bibr" target="#b7">[8]</ref>. The first methods for remote usability testing emerged about ten years ago. At that time, some empirical studies were conducted <ref type="bibr" target="#b16">[17]</ref>.</p><p>Today, we are employing new development approaches that radically increase the potential of remote usability testing. Development of Open Source Software (OSS) is very relevant for remote usability testing. The usability problems in many OSS user interfaces are well documented <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Moreover, a recent study of OSS developers showed that there is a genuine interest from OSS developers in improving the usability of OSS <ref type="bibr" target="#b1">[2]</ref>. OSS development is characterized by distributed collaboration between contributors to a specific project. A project can have hundreds of contributors spread worldwide <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. This makes it hard to employ conventional usability testing methods.</p><p>Outsourcing and global software development are other contemporary development approaches where remote usability testing is very relevant. With both approaches, developers, evaluators and users are distributed across organizations and time zones. These circumstances make conventional usability testing considerably more complex and challenging <ref type="bibr" target="#b24">[25]</ref>.</p><p>Changes in software development such as these new approaches imply that remote usability testing is becoming increasingly important as an alternative to conventional usability testing. Separating evaluators and users in time and space makes it convenient to involve broader user groups in usability testing across organizational and geographical boundaries. In order to realize this potential, we need systematic comparisons of remote usability testing methods. This will help researchers and practitioners understand options for and tradeoffs between the methods. This paper provides results from a systematic empirical comparison of methods for remote usability testing. In the following section, we describe previous research on remote usability testing. This survey forms the basis for selecting the three remote methods that we have compared. Next, we present the experimental method of our empirical study. Then we present the results of the study. This is followed by a discussion of implications of the results of the study. Finally, we provide the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synchronous methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Asynchronous methods Usability evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Usability inspection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self administered web study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self reporting of critical incident</head><p>Logged use pattern Text communication <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> [22]</p><p>[28] <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref> Questionnaire or multiple choice <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1]</ref> [22] <ref type="bibr" target="#b26">[27]</ref> [17] <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32]</ref> Post-test interview <ref type="bibr" target="#b27">[28]</ref> Workflow logging <ref type="bibr" target="#b16">[17]</ref> [22] <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32]</ref> Screen shot (still image) <ref type="bibr" target="#b14">[15]</ref> [22]</p><p>Live observation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5</ref>] Audio communication <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11]</ref> [9] <ref type="bibr" target="#b16">[17]</ref> Video capture of screen <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11]</ref> [9] <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref> Video capture of face <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref> Table <ref type="table">1</ref>. Remote usability testing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>We have conducted a systematic survey of literature on methods for remote usability testing. We based the survey on a definition of remote usability testing as a situation where "the evaluators are separated in space and/or time from users" <ref type="bibr" target="#b7">[8]</ref>. This definition emphasizes two general types of methods; synchronous and asynchronous. With a synchronous method, the evaluator is separated from the user spatially, but not temporally. When conducting an asynchronous test, the evaluator is separated from the user both temporally and spatially.</p><p>Based on the literature about remote usability testing, we identified five different methods. Two of them are synchronous and three are asynchronous. We have related the literature on remote usability testing to these five methods, see Table <ref type="table">1</ref>.</p><p>Most synchronous methods conduct remote usability testing by simulating a conventional laboratory-based think-aloud test. This is achieved by using video and audio connections combined with remote desktop sharing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>. The advantages found include cost efficiency, a more diverse pool of suitable test users, and identification of the same number of problems as a conventional usability test; and in some cases even more. The disadvantages are problems in building up trust between test monitor and user, a longer setup time, and severe difficulties in order to re-establish the test setup if there is a malfunction in the hardware or software <ref type="bibr" target="#b10">[11]</ref>.</p><p>The other group of synchronous usability testing methods is based on inspection. Two references describe experiments where synchronous remote usability inspection was performed either as collaboration between usability experts that inspects a system together <ref type="bibr" target="#b8">[9]</ref> or as a walkthrough conducted by geographically dispersed members of a development team <ref type="bibr" target="#b21">[22]</ref>.</p><p>These results illustrate that there is a substantial body of literature that provides guidance on synchronous methods.</p><p>Most of the references concentrate on giving pros and cons as well as practical advice with a specific method. In many cases, there is only little description of the experimental method and the data analysis of the underlying study, and often the empirical data are not provided. However, there are some notable exceptions where papers report from empirical comparisons <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>, usually between a single remote method and the conventional method. These references will be discussed in detail in the Discussion section below.</p><p>The literature on asynchronous testing methods is more limited. Two references discuss strengths and weaknesses of asynchronous remote testing based on case studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>. Another reference reports from a self-administered web study, where the user filled out a questionnaire during the test. This is compared to a conventional usability test in a laboratory. This study revealed a number of disadvantages with remote testing as the frequency of completion amongst the users was low, it was very time consuming, and it provided less qualitative information <ref type="bibr" target="#b26">[27]</ref>.</p><p>A different approach is to make the users themselves identify and report the critical incidents they experience. In a study of this method, the users were taught how to do this with minimal training. The study showed that the users only missed few of the critical issues found through conventional testing. The method also proved to be both cost and labor efficient as much of the work was moved from the evaluators to the users <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Thus with this method, the users are performing much of the evaluation work themselves with only a minimal involvement of expert evaluators.</p><p>Finally, it has been studied how automatic logging of use patterns employed by the users can help identify usability problems <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>. This study found the same disadvantages as Olmsted et al. <ref type="bibr" target="#b26">[27]</ref> when analyzing logged use patterns.</p><p>The lack of accurate qualitative data made the analysis difficult and it proved to be a lot less efficient compared to conventional usability testing methods <ref type="bibr" target="#b34">[35]</ref>.</p><p>This survey illustrates that the empirical studies of remote usability testing methods are generally characterized by studying only a single remote method or comparing one such method to the conventional method with laboratorybased think-aloud testing (sometimes also referred to as local usability testing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHOD</head><p>We have conducted a systematic experimental comparison of three methods for remote usability testing and a conventional laboratory-based think-aloud method. The three remote methods were selected from the five methods discussed above. The conventional method was used as a benchmark. Thus we compared the following four methods:</p><p>• Laboratory testing (LAB) In the rest of this section, we describe how we studied and compared these four methods. First, we describe the factors that were common for all four conditions. Second, we describe the specific aspects of each condition.</p><p>Participants: A total of 24 test subjects, 14 male and 10 female, participated as users/experts in the four different conditions. They were all students at Aalborg University and aged between 19 and 30 (mean=25.  One of the authors of this paper served as test monitor (moderator) in all twelve synchronous tests (LAB and RS). Three of the authors served as evaluators by carrying out the data analysis that is described below.</p><p>System: We tested the email client Mozilla Thunderbird 1.5.</p><p>We wanted to test a system within a domain that was familiar to the test subjects. During the screening and selection of test subjects we made sure that none of them had experience with Thunderbird; but they had all used an e-mail client like Outlook or Netscape mail, so they were familiar with the basic concepts of an e-mail application.</p><p>Tasks: We developed nine tasks that the test subjects should complete during the tests. They are shown in Table <ref type="table">3</ref>. These tasks were used in all four conditions.</p><formula xml:id="formula_0">Task # Description 1 Create a new email account (data provided) 2</formula><p>Check the number of new emails in the inbox of this account 3</p><p>Create a folder with a name (provided) and make a mail filter that automatically moves emails that has the folder name in the subject line into this folder 4</p><p>Run the mail filter just made on the emails that were in the inbox and determine the number of emails in the folder 5</p><p>Create a contact (data provided) 6</p><p>Create a contact based on an email received from a person (name provided) 7</p><p>Activate the spam filter (settings provided) 8</p><p>Find suspicious emails in the inbox, mark them as spam and check if they were automatically deleted 9</p><p>Find an email in the inbox (specified by subject line contents), mark it with a label (provided) and note what happened Table <ref type="table">3</ref>. The tasks used in the usability tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laboratory Testing (LAB)</head><p>Setting: The LAB tests were based on the conventional think-aloud protocol <ref type="bibr" target="#b29">[30]</ref> and were conducted in a state-ofthe-art usability laboratory. The test subject and the test monitor performed the tests in a designated test room fitted with video cameras and microphones (Figure <ref type="figure" target="#fig_0">1</ref>, room A).</p><p>The test monitor and test subject were both seated in front of the PC. The role of the test monitor was primarily to ensure that the test subjects were thinking aloud while performing tasks, but he would also ask test subjects to proceed to the next task if they got stuck. Procedure: The test monitor introduced the test subjects to 'thinking aloud'. Then the test subjects were asked to solve the nine tasks. We did not specify a time limit, but encouraged the test subjects to try to solve all tasks without help from the test monitor. After the test session, the test subject was debriefed and interviewed about the test method.</p><p>Data collection: We recorded both audio and video feeds.</p><p>The video feed consisted of the test subject's desktop and a small video image of the test subject's face in the bottom right hand corner of the screen. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remote Synchronous Testing (RS)</head><p>Setting: The RS tests were based on the literature described in Related Work, where a simulated laboratory test using web-cams, remote desktop connections and audio was found to be the most frequently used and effective setup.</p><p>We performed these tests in the usability laboratory, with a setup that simulated such a remote testing environment. The test monitor and test subject were in separate rooms (Figure <ref type="figure" target="#fig_1">2</ref>, room A (test monitor) and room B (test subject)), and they could only communicate through web-cams and audio connection (Figure <ref type="figure" target="#fig_3">3</ref>). We chose VNC (Virtual Network Computing) and Microsoft Netmeeting as the testing platform since this software allowed shared desktop and communication via web-cams. We selected Skype for the audio communication, since the audio quality in Netmeeting was not satisfactory.</p><p>Procedure: The procedure in the RS tests was the same as in the LAB tests.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remote Asynchronous Testing (AE and AU)</head><p>Setting: The method used in both of the asynchronous tests was inspired by particularly one remote method where the users themselves report the 'critical incidents' they experience <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. With this method, the test subjects not only perform the tests but also identify usability problems in the software that is tested. This has the advantage of relieving the evaluators of a considerable workload. In these tests, we wanted to examine if users without any formal usability knowledge were able to generate results that were useful for a usability evaluation.</p><p>To better understand whether this was possible, we conducted the tests in two conditions with two different groups of test subjects: usability experts (AE) and ordinary users (AU). Both conditions were conducted in a remote location at the test subjects' own computers at a time that was convenient for them. Their task solving process was guided by an online questionnaire that the tests subjects would read and answer.</p><p>Procedure: We made an installation manual to Thunderbird and included that as the first page of the online questionnaire. This was done in order to minimize the contact between test subjects and evaluators before and during the tests. The tasks were also included as an integrated part of the online questionnaire. The test subject would first install the system. Then they would work through the online questionnaire task by task, and for each task they could report any identified usability problems into the questionnaire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data collection:</head><p>The data collection for the two asynchronous conditions was done solely through the online questionnaire that was constructed in UCCASS <ref type="bibr">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>The data analysis was conducted by three of the authors of this paper. The analysis procedure was designed carefully to maximize inter-evaluator reliability and to minimize the subjective bias of the evaluators <ref type="bibr" target="#b20">[21]</ref>, given that we were only three usability evaluators to condense the problem lists from the empirical data.</p><p>The data analysis was not started until all tests in all four conditions had been conducted and all data was collected.</p><p>The tests produced 24 sets of data for analysis, i.e. twelve video recordings from the synchronous usability tests and twelve questionnaire responses from the asynchronous usability tests. The 24 sets of data were given a random identifier to avoid that an evaluator would know which data set he was analyzing. Then each evaluator randomly drew the order in which he would analyze the 24 sets of data.</p><p>The three evaluators conducted the whole data analysis independently of each other. Each evaluator analyzed one data set at a time. In each set of data, he identified usability problems and numbered them with a unique identifier to make it possible to trace each problem back to the original occurrence. Each problem was also categorized as critical, serious or cosmetic. The evaluators also made their own categorization of the usability problems from the asynchronous tests, independently of the categorizations made by the test subjects. Half of the data sets were video recordings that were analyzed systematically to identify usability problems experienced by the test subject, while the other half were questionnaire responses that were processed to compile a list of the usability problems reported by the test subject. Through this analysis each evaluator generated his own list of usability problem for the tested software. This process took approximately 42 hours per evaluator, a total of 126 person-hours.</p><p>When the three evaluators had completed their own problem lists, they negotiated a joint list of all identified usability problems for the tested system. This negotiation was conducted until consensus was reached. In the joint problem list, the categorization of each usability problem was determined by using a 'worst case' schema between the individual categorizations of the three evaluators. Thus a problem was categorized as critical if just one evaluator had categorized it as such. This process took approximately 30 person-hours.</p><p>In order to examine the reliability of the joint problem list, we calculated the evaluator effect. The evaluator effect denotes the fact that different evaluators find different usability problems. The measure determines to what extent the evaluators have found the same problems in their individual analysis <ref type="bibr" target="#b17">[18]</ref>. In Equation 1, P i and P j are the problems detected by evaluator i and j and n is the number of evaluators. We calculated the evaluator effect using this any-two agreement equation in order to see how often evaluators agreed on identified usability problems. The average percentage of any-two agreement in our data analysis was 66.9 percent, see Table <ref type="table" target="#tab_5">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head><p>In this section, the results of our empirical study are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Completion</head><p>We have determined the number of tasks completed by each test subject. The 24 test subjects completed an average of 8.1 out of 9 tasks. The AE and AU conditions had a higher number of solved tasks than the LAB and RS conditions, see Table <ref type="table" target="#tab_8">7</ref>. However, an ANOVA test gave no significant difference in the number of tasks completed in the four tests (F <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>  Only task 2 and 9 were completed by all test subjects. Task 6 was most difficult as five test subjects did not complete it. Task 7 and 8 together caused difficulties for six test subjects. The remaining tasks were completed by all except two or three test subjects. Task 1, 3 and 4 were difficult for several test subjects, but they were able to complete them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Completion Time</head><p>We have measured the time each test subject used to complete the tasks. The results in Table <ref type="table" target="#tab_8">7</ref> reflect that in the two asynchronous conditions, the test subjects spent more time on the tasks. This is especially characteristic for the test subjects in the AU condition who had a considerably longer task completion time (M=1:03:48, SD=0:48:37) compared to the test subjects in the LAB (M=00:22:10, SD=00:05:20) and the RS condition (M=00:22:30, SD=00:03:31). An ANOVA test gave a significant difference in task completion time (F <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>=3.514, p=0.034). Yet, we found no pairwise differences between the conditions using a Tukey's post hoc test, at a five percent significance level.</p><p>A reason for this marked difference may be that the test subjects in the asynchronous conditions took breaks. The online questionnaire used in these conditions only recorded the starting and ending time. Therefore, we do not know if the test subjects had any breaks during the test sessions, and therefore we do not know the exact time spent on the test, which is an essential element when comparing AE, AU and LAB evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Usability Problems Identified</head><p>The number of usability problems identified in the tests show considerable differences, see  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAB vs RS</head><p>In the RS tests, the evaluators identified 38 of the 46 overall usability problems. This is slightly better but comparable to the number of problems identified in the LAB tests. According to a Fisher's exact test (see Table <ref type="table" target="#tab_10">8</ref>) there is no significant difference in the number of problems identified in the two conditions (p=0.6073).</p><p>A similar result was found with the critical problems. The LAB and RS tests both identified 22 of the 24 critical problems, where 4 of the 22 problems were found in only one of the two evaluations. Thus, 20 of the critical problems identified in the two conditions were the same.</p><p>In the identification of serious and cosmetic problems the LAB and RS tests gave almost equal results. The RS tests identified 8 of 10 serious problems and 8 of 12 cosmetic problems, which was slightly better than the LAB tests. In the identification of critical (p=1.000), serious (p=0.3498), and cosmetic (p=1.000) problems no significant difference was found through a Fisher's exact test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAB vs AE</head><p>The AE tests identified 21 of the 46 problems, compared to the LAB evaluation that identified 35. A Fisher's exact test shows a significant difference (p=0.0051) in the number of problems identified in the two conditions, see Table <ref type="table" target="#tab_10">8</ref>.</p><p>In the identification of critical problems, the difference between the LAB tests and the AE tests was not as distinct, since the AE tests identified 15 of 24 critical problems against the LAB tests' identification of 22 of the 24 critical problems. This shows that even though the AE tests did not find as many problems as the LAB tests, the majority of the problems identified were critical. However, a Fisher's exact test still gives a significant difference (p=0.0363) in the number of critical problems.</p><p>The AE tests identified 3 out of 10 serious and 3 out of 12 cosmetic problems. Compared to the LAB condition Fisher's exact test does not give a significant difference in the number of serious (p=0.6499) or cosmetic (p=0.0995) problems for the AE condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAB vs AU</head><p>The AU tests identified 13 of the 46 overall problems. A comparison of this result to the LAB condition through a Fisher's exact test gives an extremely significant difference (p&lt;0.0001) as shown in Table <ref type="table" target="#tab_10">8</ref>.</p><p>In the identification of critical problems, the difference between the two conditions is also significant (p=0.0078), since the AU tests only identified 11 of the 24 critical problems, where the LAB tests identified 22. The majority (84,6%) of the problems identified in the AU tests were critical.</p><p>The difference in the number of serious problems identified was not significant according to the Fisher's exact test (p=0.3498). The AU tests did not find any cosmetic problems, while the LAB tests identified 8 of the 12 overall cosmetic problems. With a Fisher exact test this gives a significant difference between the two tests methods (p=0.0013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AE vs AU</head><p>A key aim was to compare the expert and user based evaluations. This is interesting because these two conditions are identical, except for the competence of the test subjects. Table <ref type="table" target="#tab_11">9</ref> shows the results of a Fisher's exact test used to compare the critical, serious and cosmetic problems identified in the two asynchronous conditions. This shows that there is no significant difference in the number of problems identified in these two conditions, despite the differences in the competence of the test subjects. </p><formula xml:id="formula_1">p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Problems Identified in a Test Session</head><p>The number of usability problems identified in each test session also varies between the four conditions. Table <ref type="table" target="#tab_12">10</ref> shows the average number of usability problems identified in each of the six test sessions that were conducted in each condition. The average number is almost the same in the RS and LAB conditions, and the Tukey comparison did not yield a significant difference in the number of problems identified (p&gt;0.05). However, we found a very significant difference when comparing the average number of problems identified in the LAB tests with the average number of problems identified by the test subjects in the AE and AU conditions (p≤0.001). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unique Problems</head><p>Different usability testing methods may reveal unique usability problems. We define unique problems in a manner that is inspired by the identification of action areas in Karat et al. <ref type="bibr" target="#b19">[20]</ref>. In our data, we analyzed the problems that were identified in one test session only, and the problems that were identified by only one evaluation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problems Identified in One Test Session Only</head><p>One type of unique problems is those identified only in one test session. As shown in the Sum column of Table <ref type="table" target="#tab_13">11</ref>, none of the 24 critical problems, 2 of the 10 serious problems, and 6 of the 12 cosmetic problems were identified in one test session only. This emphasizes the validity of the critical problems. On the other hand, it also</p><p>shows that 50% of all the cosmetic problems were only identified in one test session. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problems Identified by One Test Method Only</head><p>Another type of unique problems is those identified by only one test method. In the Sum column of Table <ref type="table" target="#tab_12">10</ref> we see that 1 of the 24 critical problems, 5 of the 10 serious problems, and 6 of the 12 cosmetic problems were identified by only one of the four methods. Thus 23 of the total of 24 critical problems were identified with more than one test method. Furthermore, 50% of the serious and cosmetic problems were only identified with one test method.</p><p>Table <ref type="table" target="#tab_6">6</ref> showed that the AE and AU tests identified only 63% and 46% of all critical problems, whereas the LAB and RS tests both identified 92%. This is a clear weakness for the asynchronous methods. On the other hand, Table <ref type="table" target="#tab_13">11</ref> shows that the asynchronous methods did not identify unique critical problems. This shows that the problems identified with the AE or AU method were also identified with at least one of the other methods. In comparison, we found that the RS tests identified 1 critical, 3 serious, and 2 cosmetic problems that were not identified in any of the other tests. This is the largest amount of unique problems identified by any of the four methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluators vs Self-Reporting Test Subjects</head><p>The basic idea of the two asynchronous methods is that the test subjects themselves identify and categorize usability problems. With respect to identification, we have already shown that the asynchronous methods (AE and AU) did not identify as many problems as the LAB and RS methods. If we look at the individual evaluator and test subject, we see the same difference. Based on the synchronous tests, each evaluator identified on average 4.17 problems per test session. Each test subject in the asynchronous tests, who reported his/her own problems, identified on average 1.92 problems. Thus the evaluators identified more than twice as many problems from the synchronous tests as the test subjects in the asynchronous conditions. Moreover, the most remarkable result is that the experts did not identify more problems than the ordinary users.</p><p>With respect to categorization, the difference was even more outstanding. The test subjects in the two asynchronous conditions were also supposed to categorize the problems they identified. In doing so, they should use the guidelines in Table <ref type="table" target="#tab_3">4</ref>. When we started analyzing the data, it quickly became evident that the categorizations made by the test subjects were not useful at all. The test subjects categorized almost all problems as cosmetic. Only 3 problems in total were deemed by them to be critical. A possible explanation may be that these test subjects solved most of the tasks (see Table <ref type="table" target="#tab_8">7</ref>). This may have affected their categorization, as they felt successful after managing to solve the tasks even when encountering problems. This is supported by Hartson et al. <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>In our survey of related work, we found prior research on remote usability testing that compare and assess remote methods. In this section, we compare that to our results.</p><p>An early study concluded that remote usability testing in the RS condition is feasible compared to the conventional laboratory-based method (the LAB condition) <ref type="bibr" target="#b16">[17]</ref>. The number of usability problems identified in the two conditions was very similar. These results were promising for the remote method, although the study did not allow for definite conclusions. They also assessed the AU method and concluded that the low cost for the evaluators made this method feasible. This latter conclusion was not based on a comparison. The results provided in this study are clearly in line with our findings.</p><p>A more recent study aimed to define effective tools and methods for remote usability testing <ref type="bibr" target="#b33">[34]</ref>. Their study compared the LAB and RS conditions. They found that the remote test subjects took longer to complete most tasks which differ from our results. The remote users also made more errors. They also compared usability problems found in the two conditions. They made no statistical test, but the numbers of problems found are quite similar which is in line with our conclusion.</p><p>There is also a systematic overview of remote usability testing methods <ref type="bibr" target="#b22">[23]</ref>. This is accompanied with results from three case studies. The first case study was of an email application, and it compared the LAB and RS conditions. They found only one significant difference between the two conditions, and they conclude that this is likely to be caused by a lack of balance across conditions. The second case study was with the same conditions, and it only gave a significant difference on task completion time. This is similar to results that others have obtained <ref type="bibr" target="#b33">[34]</ref>. There is also a third case study, but it does not involve any comparisons. It is concluded that the LAB and RS methods produce equivalent results on usability problems. Again, this is entirely in line with our findings.</p><p>Finally, there is a recent study with a comparison of the LAB and RS conditions and with a setup that is close to ours <ref type="bibr" target="#b6">[7]</ref>. They found no significant differences between the two conditions in terms of the number, types and severities of usability problems. They note that this is consistent with others <ref type="bibr" target="#b16">[17]</ref> and clearly in line with our results.</p><p>The amount of comparisons involving asynchronous methods is much more limited. The original presentations of the idea of having users reporting critical incidents provide some positive experiences, but the studies are only informal <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. A recent paper is based on a systematic study involving two cases where the LAB method is compared to a method that is close to the AU extended with an interview <ref type="bibr" target="#b27">[28]</ref>. Based on the two case studies, they conclude that the results of the two conditions are comparable on some quantitative measures. Yet in one of the case studies the number of usability problems identified is more than four times higher for the LAB condition than the AU condition. This result is close to our findings.</p><p>A key difference between the LAB method and the three remote methods is that the test monitor is absent from the test situation, either spatially or both temporally and spatially. In our study, we did not experience that the 'distance' between the test monitor and test subject influenced the performance negatively. This is contrary to the suggestion of several others <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref>. The equivalent results of the LAB and RS conditions indicate that the physical presence of the test monitor is not important. In fact, several of our test subjects in the RS condition, who had tried the LAB condition before, expressed that the RS method was less stressful. One such test subject said: "I liked this test method better than the traditional method where the test leader looks over your shoulder." Others supported this by saying that the video image of the test monitor was a positive element, since it was nice to be able to see the attitude of the moderator.</p><p>This observation is supported by another study <ref type="bibr" target="#b6">[7]</ref>. This study compared the comfort level experienced in the LAB and RS conditions. It was concluded that the majority of participants felt that the RS condition was more convenient and would prefer this to participating in a LAB test, and nobody said the opposite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>Remote usability testing is becoming increasingly important. This paper has presented results from a systematic experimental comparison of three methods for remote usability testing and a conventional laboratorybased think-aloud test. The results show that the remote synchronous method is virtually equivalent to the conventional method. The two methods identified almost the same number of usability problems, and test subjects spent the same time completing the tasks. The former conclusion is clearly in line with most of the prior research. The results on the latter are more varied. These conclusions show that remote usability testing has the potential to cross organizational and geographical boundaries and support new approaches to software development such as outsourcing and global and open source software development.</p><p>The results on asynchronous methods are not as clear and positive. The asynchronous methods intend to move the majority of effort from expert evaluators to ordinary users. Our findings confirm that the asynchronous methods are more time-consuming for the users and identify fewer usability problems. Moreover, the tests subjects could not provide a usable categorization of the usability problems. Despite the disappointing results, these methods may still be worthwhile to use because they relieve the expert evaluators from a considerable amount of work, and enable collection of use data from a large number of participants.</p><p>The aim of this study is to increase our understanding of the tradeoffs between different types of methods for conducting usability testing. In order to move further in that direction, it would be useful with more studies of and experiments with the asynchronous methods. In addition, it would be interesting to perform comparative studies of remote usability testing methods outside the controlled environment of a usability laboratory. Finally, it would be relevant to conduct follow-up experiments with more test subjects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The setting for the LAB test.</figDesc><graphic coords="4,66.96,128.94,215.64,132.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The setting for the RS test.</figDesc><graphic coords="4,66.96,428.10,215.64,132.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Data collection: We recorded the audio and video that the test monitor experienced through VNC, web-cam and Skype. The video feed consisted of the view of the test subject's desktop as provided by Netmeeting and a small video image from the test subject's web-cam in the lower right hand corner. During the test, the test subject had a web-cam image of the test monitor in the lower right hand corner of the screen, but this was not visible in the recorded video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A test subject in the RS test.</figDesc><graphic coords="4,329.76,267.42,215.76,161.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 . Number of test subjects in the four conditions. The number in parenthesis denotes the average age.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 . Guidelines used for categorizing the severity of usability problems.</head><label>4</label><figDesc>UnitCommand Climate Assessment and Survey System). It gathered the input from the test subjects and stored it in a MySQL database. The questionnaire enabled the test subjects to categorize the identified problems as 'small', 'medium' and 'large'. These categories were correlated to the commonly used classifications 'cosmetic', 'serious', and 'critical'. The test subjects were presented with a table specifying how to classify a specific usability problem, see Table4. Furthermore, they were asked to log the location in the program where they encountered a problem and describe how it influenced the completion of the task.</figDesc><table><row><cell>Problem</cell><cell>Delay in task</cell><cell cols="2">Irritation Expectation to</cell></row><row><cell>Severity</cell><cell>completion</cell><cell></cell><cell>system behaviour</cell></row><row><cell>Small</cell><cell>Less than 30</cell><cell>Slight</cell><cell>Minor difference</cell></row><row><cell></cell><cell>seconds delay</cell><cell>irritation</cell><cell>in expected action</cell></row><row><cell cols="2">Medium More than 30</cell><cell>Average</cell><cell>Significant</cell></row><row><cell></cell><cell>seconds delay</cell><cell>irritation</cell><cell>difference in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>expected action</cell></row><row><cell>Large</cell><cell>Could not com-</cell><cell>High</cell><cell>Critical difference</cell></row><row><cell></cell><cell>plete the task</cell><cell>irritation</cell><cell>in expected action</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell>E1 E2</cell><cell>E1 E3</cell><cell>E2 E3</cell><cell>Avg.</cell></row><row><cell>Problems</cell><cell>29</cell><cell>30</cell><cell>28</cell><cell>29</cell></row><row><cell>agreed on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of</cell><cell>42</cell><cell>45</cell><cell>43</cell><cell>43.3</cell></row><row><cell>problems</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Any-two</cell><cell>69.0%</cell><cell>66.7%</cell><cell>65.9%</cell><cell>66.9%</cell></row><row><cell>agreement</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 . Calculation of the evaluator effect between E1, E2 and E3 using the any-two agreement formula (Equation 1).</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>LAB</cell><cell></cell><cell>RS</cell><cell></cell><cell>AE</cell><cell></cell><cell>AU</cell><cell></cell></row><row><cell></cell><cell>N=6</cell><cell></cell><cell>N=6</cell><cell></cell><cell>N=6</cell><cell></cell><cell>N=6</cell><cell></cell></row><row><cell>Task completion time:</cell><cell cols="2">22:10 (05:20)</cell><cell cols="2">22:30 (03:31)</cell><cell cols="2">45:29 (18:51)</cell><cell cols="2">1:03:48 (48:37)</cell></row><row><cell>Average (SD)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Usability problems</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>%</cell></row><row><cell>Critical (24)</cell><cell>22</cell><cell>92%</cell><cell>22</cell><cell>92%</cell><cell>15</cell><cell>63%</cell><cell>11</cell><cell>46%</cell></row><row><cell>Serious (10)</cell><cell>5</cell><cell>50%</cell><cell>8</cell><cell>80%</cell><cell>3</cell><cell>30%</cell><cell>2</cell><cell>20%</cell></row><row><cell>Cosmetic (12)</cell><cell>8</cell><cell>67%</cell><cell>8</cell><cell>67%</cell><cell>3</cell><cell>25%</cell><cell>0</cell><cell>0%</cell></row><row><cell>Total (46)</cell><cell>35</cell><cell>76%</cell><cell>38</cell><cell>83%</cell><cell>21</cell><cell>46%</cell><cell>13</cell><cell>28%</cell></row></table><note><p><p><p>Hertzum and Jakobsen found that the average agreement between any two evaluators in twelve studies varied from 5 percent to 65 percent (avg. 22.4%, SD=19.8)</p><ref type="bibr" target="#b17">[18]</ref></p>. Compared to these figures we achieved a very high any-two agreement measure which indicates a high level of reliability of the joint problem list.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 . Task completion time (average and standard deviation) and number of identified usability problems (absolute and percentage of total number).</head><label>6</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>=0.68, p=0.575).</figDesc><table><row><cell></cell><cell>Mean</cell><cell>SD</cell></row><row><cell></cell><cell>value</cell><cell></cell></row><row><cell>LAB</cell><cell>8.0</cell><cell>1.1</cell></row><row><cell>RS</cell><cell>7.5</cell><cell>2.1</cell></row><row><cell>AE</cell><cell>8.5</cell><cell>0.8</cell></row><row><cell>AU</cell><cell>8.3</cell><cell>0.8</cell></row><row><cell>Total</cell><cell>8.1</cell><cell>1.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 . Number of tasks completed.</head><label>7</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>The 24 usability test sessions resulted in a total of 46 usability problems. Based on the guidelines in Table4, we categorized 24 of them as critical, 10 as serious, and 12 as cosmetic. Below, we provide further results on the number of usability problems identified in each condition.</figDesc><table><row><cell>LAB</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">From the LAB tests, the evaluators identified 35 of the 46</cell></row><row><cell cols="5">usability problems. 22 of these problems were critical, 5</cell></row><row><cell cols="3">were serious, and 8 were cosmetic.</cell><cell></cell></row><row><cell></cell><cell>LAB</cell><cell>RS</cell><cell>AE</cell><cell>AU</cell></row><row><cell>LAB</cell><cell></cell><cell>(p=0.6073)</cell><cell>p=0.0051 *</cell><cell>p&lt;0.0001 ***</cell></row><row><cell>RS</cell><cell>(p=0.6073)</cell><cell></cell><cell>p=0.0004 ***</cell><cell>p&lt;0.0001 ***</cell></row><row><cell>AE</cell><cell>* p=0.0051</cell><cell>*** p=0.0004</cell><cell></cell><cell>(p=0.1300)</cell></row><row><cell>AU</cell><cell>*** p&lt;0.0001</cell><cell>*** p&lt;0.0001</cell><cell>(p=0.1300)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 . Fisher's exact test for the total number of usability problems identified in the four conditions. (p)=not significant, *=significant, **=very significant and ***=extremely significant.</head><label>8</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 . Fisher's exact test for the number of usability problems in each category identified in the AU and AE conditions. p=significance level.</head><label>9</label><figDesc></figDesc><table><row><cell>Overall</cell><cell>0.1300</cell></row><row><cell>Critical</cell><cell>0.7702</cell></row><row><cell>Serious</cell><cell>1.0000</cell></row><row><cell>Cosmetic</cell><cell>0.2174</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 . Average number of usability problems identified in a test session. The Tukey test compares the three remote conditions with the LAB condition.</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>SD</cell><cell>Tukey comparison</cell></row><row><cell>LAB</cell><cell>15.33</cell><cell>4.41</cell><cell></cell></row><row><cell>RS</cell><cell>16.67</cell><cell>2.42</cell><cell>p&gt;0.05</cell></row><row><cell>AE</cell><cell>4.67</cell><cell>2.66</cell><cell>p≤0.001</cell></row><row><cell>AU</cell><cell>3.17</cell><cell>1.72</cell><cell>p≤0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 . Identification of unique problems. The number in bold is unique problems identified in one test session only. The number in parenthesis is the unique problems identified by this one method only.</head><label>11</label><figDesc></figDesc><table><row><cell></cell><cell>LAB</cell><cell>RS</cell><cell>AE</cell><cell>AU</cell><cell>Sum</cell></row><row><cell></cell><cell>N=6</cell><cell>N=6</cell><cell>N=6</cell><cell>N=6</cell><cell>N=24</cell></row><row><cell>Critical (24)</cell><cell>0 (0)</cell><cell>0 (1)</cell><cell>0 (0)</cell><cell>0 (0)</cell><cell>0 (1)</cell></row><row><cell>Serious (10)</cell><cell>1 (1)</cell><cell>0 (3)</cell><cell>0 (0)</cell><cell>1 (1)</cell><cell>2 (5)</cell></row><row><cell>Cosmetic (12)</cell><cell>2 (2)</cell><cell>2 (2)</cell><cell>2 (2)</cell><cell>0 (0)</cell><cell>6 (6)</cell></row><row><cell>Total (46)</cell><cell>3 (3)</cell><cell>2 (6)</cell><cell>2 (2)</cell><cell>1 (1)</cell><cell>8 (12)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The research behind this paper was partly financed by the Danish Research Councils (grant number 2106-04-0022). We are very grateful to the 24 test subjects that helped us in the usability tests. We would also like to thank Mikael Skov for his help and advice regarding statistical calculations and the procedure for data analysis. Finally, we want to thank the anonymous reviewers for their comments and advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ames</surname></persName>
		</author>
		<ptr target="http://www.ocf.berkeley.edu/˜morganya/research/dmp/report.html" />
		<title level="m">Final Report on Remote Usability Studies</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Usability in open source software development: Opinions and practice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Andreasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Schrøder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Technology and Control</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="303" to="312" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Experience Remote Usability Testing, Part 1</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bartek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheatham</surname></persName>
		</author>
		<ptr target="http://www-106.ibm.com/developerworks/library/wa-rmusts1/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Bartek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheatham</surname></persName>
		</author>
		<ptr target="http://www-106.ibm.com/developerworks/web/library/wa-rmusts2.html" />
		<title level="m">Experience Remote Usability Testing, Part 2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Experiences in Remote Rsability Evaluations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bartek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheatham</surname></persName>
		</author>
		<ptr target="http://www-3.ibm.com/ibm/easy/eouext.nsf/Publish/50?OpenDocument&amp;/Publish/1116/$File/paper1116.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Professional usability in open source projects: Gnome, openoffice.org, netbeans</title>
		<author>
			<persName><forename type="first">C</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller-Prove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mzourek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI 2004</title>
		<meeting>CHI 2004</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1083" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of synchronous remote and local usability studies for an</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Brush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ames</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI 2007 Proceedings • Usability Evaluation</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">April 28-May 3, 2007. 2004</date>
			<biblScope unit="page" from="1179" to="1182" />
		</imprint>
	</monogr>
	<note>Proceedings of CHI 2004</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Remote usability evaluation: Can users report their own critical incidents?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Hartson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andhix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI 1998</title>
		<meeting>CHI 1998</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="253" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A repeatable collaboration process for usability testing</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>De Vreede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fruhling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrapani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HICSS 2005</title>
		<meeting>HICSS 2005</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Who is an open source software developer?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Dempsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="67" to="72" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Siegel</surname></persName>
		</author>
		<title level="m">Remote possibilities?: International usability testing at a distance. interactions</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving the Usability of Open Source Software: Usability Testing of staroffice calc</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trombley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sinha</surname></persName>
		</author>
		<ptr target="http://www.sims.berkeley.edu/˜sinha/opensource.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Getting to know you: Open source development meets usability</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frishberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dirks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nickell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI 2002</title>
		<meeting>CHI 2002</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="932" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Remote Online Usability Testing: Why, How, and When to Use it</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Phillips</surname></persName>
		</author>
		<ptr target="http://www.boxesandarrows.com/view/" />
	</analytic>
	<monogr>
		<title level="m">remote online usability testing why how and when to use it</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Hammontree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote usability testing. Interactions</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="25" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Remote evaluation for post-deployment usability improvement</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Hartson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AVI 1998</title>
		<meeting>AVI 1998</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Remote evaluation: The network as an extension of the usability laboratory</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Hartson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kelso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Neale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI 1996</title>
		<meeting>CHI 1996</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The evaluator effect: A chilling fact about usability evaluation methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hertzum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="183" to="204" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Houck-Whitaker</surname></persName>
		</author>
		<ptr target="http://boltpeters.com/articles/versus.html" />
		<title level="m">Remote Testing versus Lab Testing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparison of empirical testing and walkthrough methods in user interface evaluation</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Karat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fiegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI 1992</title>
		<meeting>CHI 1992</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="397" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Does time heal: A longitudinal study of usability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kjeldskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Skov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OZCHI 2005</title>
		<meeting>OZCHI 2005</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Methodology for remote usability activities: A case study</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S H</forename><surname>Krauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="582" to="593" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Remote usability evaluation: Overview and case studies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mcfadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Elie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Essence of Distributed Work: The Case of the linux Kernel</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sproull</surname></persName>
		</author>
		<ptr target="http://www.firstmonday.org/issues/issue511/moon/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Location, location, location: Challenges of outsourced usability evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kjeldskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goschnick</surname></persName>
		</author>
		<idno>no. 2004/2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Improving the Interplay between Usability Evaluation and User Interface Design</title>
		<meeting>the Workshop on Improving the Interplay between Usability Evaluation and User Interface Design</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="12" to="15" />
		</imprint>
		<respStmt>
			<orgName>Aalborg University, Department of Computer Science, HCI-Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Usability and open source software</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Twidale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Waikato</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Working Paper Series</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In-person usability study compared with self-administered web (remote-different time-place) study: Does mode of study produce similar results?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Olmsted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UPA 2005</title>
		<meeting>UPA 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Remote usability evaluation with disabled people</title>
		<author>
			<persName><forename type="first">H</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI 2006</title>
		<meeting>CHI 2006</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1133" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="middle">E</forename><surname>Raymond</surname></persName>
		</author>
		<title level="m">The Revenge of the Hackers. O&apos;Reilly and Associates</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Handbook of Usability Testing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Remote moderated usability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Safire</surname></persName>
		</author>
		<ptr target="http://www.upassoc.org/usabilityresources/conference/2004/imsafire.html" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaption of traditional usability testing methods for remote testing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Scholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HICCS &apos;01</title>
		<meeting>HICCS &apos;01</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supporting problem identification in usability evaluations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Skov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OzCHI 2005</title>
		<meeting>OzCHI 2005</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Here, there, anywhere: Remote usability testing that works</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Rozanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Haake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CITC5</title>
		<meeting>CITC5</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="132" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Usability remote evaluation for www</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A</forename><surname>Winckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M D S</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>De Lima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI 2000</title>
		<meeting>CHI 2000</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="131" to="132" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
