<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">It&apos;s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
							<email>timo.schick@sulzer.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Sulzer GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hinrich</forename><surname>Sch Ütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">It&apos;s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020)  achieve remarkable few-shot performance on challenging natural language understanding benchmarks. In this work, we show that performance similar to GPT-3 can be obtained with language models whose parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain some form of task description, combined with gradient-based optimization; additionally exploiting unlabeled data gives further improvements. Based on our findings, we identify several key factors required for successful natural language understanding with small language models. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretraining ever-larger language models (LMs) on massive plain text corpora has led to significant improvements on a wide range of NLP tasks <ref type="bibr" target="#b25">(Radford et al., 2018;</ref><ref type="bibr" target="#b7">Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr">Raffel et al., 2019, inter alia)</ref>. A standard approach is to replace the pretrained model's output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as "the correct answer is "), allowing pretrained LMs to solve them without any or with only very few labeled examples <ref type="bibr" target="#b26">(Radford et al., 2019;</ref><ref type="bibr" target="#b30">Schick and Schütze, 2020a)</ref>.</p><p>Very recently, <ref type="bibr" target="#b1">Brown et al. (2020)</ref> introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks 1 Our implementation is publicly available at https:// github.com/timoschick/pet. as language modeling problems, GPT-3 achieves near state-of-the-art results for some tasks in the SuperGLUE benchmark <ref type="bibr">(Wang et al., 2019)</ref> given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed. While being straightforward to use, this method has two major drawbacks:</p><p>• It requires a gigantic LM to work well, making it unusable in many real-world scenarios.</p><p>• It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens.</p><p>An alternative to priming is pattern-exploiting training (PET) <ref type="bibr" target="#b30">(Schick and Schütze, 2020a)</ref>, which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way.</p><p>In this work, we modify PET to also work for tasks that require predicting more than one token. We then show that in combination with ALBERT <ref type="bibr" target="#b15">(Lan et al., 2020)</ref>, PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure <ref type="figure" target="#fig_0">1</ref>). Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET's strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Enabling LMs to perform zero-shot learning by providing task descriptions was proposed by <ref type="bibr" target="#b26">Radford et al. (2019)</ref> and has been applied to text classification <ref type="bibr" target="#b24">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type="bibr" target="#b5">(Davison et al., 2019)</ref> and argumentative relation classification <ref type="bibr" target="#b20">(Opitz, 2019)</ref>. It is also commonly used for probing the knowledge contained within LMs <ref type="bibr" target="#b33">(Trinh and Le, 2018;</ref><ref type="bibr" target="#b22">Petroni et al., 2019;</ref><ref type="bibr" target="#b32">Talmor et al., 2019;</ref><ref type="bibr" target="#b31">Schick and Schütze, 2020b;</ref><ref type="bibr">Ettinger, 2020, i.a.)</ref>.</p><p>As finding ways to reformulate tasks as cloze questions that are understood well by LMs is difficult <ref type="bibr" target="#b12">(Jiang et al., 2019)</ref>, <ref type="bibr" target="#b30">Schick and Schütze (2020a)</ref> propose PET, a method that uses knowledge distillation <ref type="bibr" target="#b11">(Hinton et al., 2015)</ref> to easily combine several reformulations. In contrast to PET, which uses gradient-based optimization, <ref type="bibr" target="#b26">Radford et al. (2019)</ref> and <ref type="bibr" target="#b1">Brown et al. (2020)</ref> investigate priming, where examples are given as context but no parameter updates are performed.</p><p>Our modified version of PET uses masked language models <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> to assign probabilities to sequences of text; this is similar to using them in a generative fashion <ref type="bibr" target="#b35">(Wang and Cho, 2019)</ref> and has previously been investigated by <ref type="bibr" target="#b29">Salazar et al. (2020)</ref> and <ref type="bibr" target="#b10">Ghazvininejad et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pattern-Exploiting Training</head><p>Let M be a masked language model (MLM), T its vocabulary and ∈ T the mask token. For some</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P (x)</head><p>Oil prices rise ? , Oil prices fall back .</p><p>x 2 x 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes</head><p>No entailment not entailment y v(y) q p (y | x)</p><p>Figure <ref type="figure">2</ref>: Application of a PVP p = (P, v) for recognizing textual entailment: An input x = (x 1 , x 2 ) is converted into a cloze question P (x); q p (y | x) for each y is derived from the probability of v(y) being a plausible choice for the masked position.</p><p>z ∈ T * containing at least k masks and t ∈ T , we denote with q k M (t | z) the probability that M assigns to t at the kth masked position in z; the model's logits before applying softmax are denoted with s k M (t | z). We consider the task of mapping inputs x ∈ X to outputs y ∈ Y , for which PET requires a set of pattern-verbalizer pairs (PVPs). Each PVP p = (P, v) consists of • a pattern P : X → T * that maps inputs to cloze questions containing a single mask;</p><p>• a verbalizer v : Y → T that maps each output to a single token representing its task-specific meaning in the pattern.</p><p>As illustrated in Figure <ref type="figure">2</ref>, the core idea of PET is to derive the probability of y being the correct output for x from the probability of v(y) being the "correct" token at the masked position in P (x). Based on this intuition, a conditional probability distribution q p of Y given X is defined as</p><formula xml:id="formula_0">q p (y | x) = exp s p (y | x) y ∈Y exp s p (y | x)<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">s p (y | x) = s 1 M (v(y) | P (x))</formula><p>is the raw score of v(y) at the masked position in P (x).</p><p>For a given task, identifying PVPs that perform well is challenging in the absence of a large development set. Therefore, PET enables a combination of multiple PVPs P = {p 1 , . . . , p n } as follows:</p><p>1. For each PVP p, a MLM is finetuned on training examples (x, y) by minimizing the cross entropy between y and q p (y | x). In practice, Schick and Schütze (2020a) train three MLMs per pattern as performance can vary substantially between runs.</p><p>2. The ensemble of finetuned MLMs is used to annotate a set of unlabeled examples; each unlabeled example x ∈ X is annotated with soft labels based on the probability distribution</p><formula xml:id="formula_2">q P (y | x) ∝ exp p∈P w p • s p (y | x) (2)</formula><p>similar to Eq. 1 where w p is a weighting term that is proportional to the accuracy achieved with p on the training set before training.</p><p>3. The resulting soft-labeled dataset is used to train a regular sequence classifier by minimizing cross entropy between its output and q P .</p><p>As steps (2) and (3) above closely resemble knowledge distillation <ref type="bibr" target="#b11">(Hinton et al., 2015)</ref>, we also refer to them simply as distillation.</p><p>To give MLMs trained on different patterns further opportunity to learn from one another, Schick and Schütze (2020a) also propose iPET, an iterative variant of PET in which several generations of models are trained on datasets of increasing size that are labeled by previous generations. This is achieved as follows: First, an ensemble of MLMs is trained as in regular PET. For each model M i , a random subset of other models is used to generate a new training set T i by assigning labels to those unlabeled examples for which the selected subset of models is most confident in its prediction. Each M i is then retrained on T i ; this process is repeated several times, each time increasing the number of examples in T i by a constant factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PET with Multiple Masks</head><p>An important limitation of PET is that the verbalizer v must map each possible output to a single token, which is impractical or outright impossible for many tasks. We thus generalize verbalizers to functions v : Y → T * ; this requires some modifications to inference and training. <ref type="foot" target="#foot_0">2</ref> We further generalize PET in that we do not assume the output space to be identical for each input: for each x ∈ X, we denote with Y x ⊆ Y the set of possible outputs given x as input. Given a PVP p = (P, v), we define l(x) = max y∈Yx |v(y)| to be the maximum number of tokens required to express any output in Y x and P k (x) to be P (x) with the mask token replaced by k masks.</p><p>Inference For x ∈ X, y ∈ Y x and |v(y)| = k, we redefine q p (y | x) in an autoregressive fashion: Starting from P k (x), we perform k consecutive predictions, where we always select the next token to predict based on the MLM's confidence. That is, we set q p (y | x) = q(v(y) | P k (x)) where</p><formula xml:id="formula_3">q(t 1 ... t k |z) = 1 if k = 0 q j M (t j |z) • q(t |z ) if k ≥ 1 (3) with j = arg max k i=1 q j M (t j | z), z is z except z j = t j and t = t 1 ... t j−1 t j+1 ... t k .</formula><p>Note that unlike in original PET (Eq. 1), q p is not a probability distribution as its values do not sum to one.</p><p>Training Computing q p (y | x) as in Eq. 3 for each training example (x, y) would be prohibitively expensive. To enable computation of all required probabilities in a single forward pass, we approximate q p (y | x) by (i) always inserting the maximum number of mask tokens required to express any output and (ii) for each y ∈ Y x , predicting all tokens in v(y ) = t 1 . . . t k in parallel, where we simply ignore the model's predictions for all l(x) − k superfluous mask tokens:</p><formula xml:id="formula_4">qp (y | x) = k i=1 q i M (t i | P l(x) (x))<label>(4)</label></formula><p>As qp is not a probability distribution over Y x , cross entropy is not an ideal training objective as it can also be minimized by reducing the probability assigned to sequences z / ∈ v(Y x ) that are not part of the output space, despite this having no effect on the model's prediction. We instead opt for multiclass hinge loss <ref type="bibr" target="#b37">(Weston and Watkins, 1999;</ref><ref type="bibr" target="#b8">Dogan et al., 2016)</ref> and minimize:</p><formula xml:id="formula_5">y ∈Yx max 0; 1− log qp (y|x)+ log qp (y |x) (5)</formula><p>That is, we require the difference between the log probability of y and the log probability of any output y ∈ Y x \ {y} to be at least 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We use the SuperGLUE benchmark <ref type="bibr">(Wang et al., 2019)</ref> to compare PET and GPT-3. We cannot evaluate PET using the exact same training data as GPT-3 because for most tasks, GPT-3 uses a different set of training examples for each test example and for the other tasks, training sets were not available upon request; however, the exact choice of examples has little impact on GPT-3's performance. <ref type="foot" target="#foot_1">3</ref> We thus create new training sets by randomly selecting 32 examples for each task using a fixed random seed. Following previous work <ref type="bibr" target="#b27">(Raffel et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020)</ref>, we select only positive examples for WSC; for both MultiRC and ReCoRD, we follow <ref type="bibr" target="#b1">Brown et al. (2020)</ref> and select a total of 32 questions.</p><p>We additionally create sets of up to 20,000 unlabeled examples for each task; this is done by removing all labels from the original training sets. As the training sets for RTE and CB are very small, we additionally select random unlabeled examples from MNLI <ref type="bibr" target="#b38">(Williams et al., 2018)</ref> for both tasks. We refer to the resulting sets of training examples and unlabeled examples as FewGLUE.<ref type="foot" target="#foot_2">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks</head><p>Below, we describe each of the SuperGLUE tasks and our corresponding PVPs. We use a vertical bar (|) to mark boundaries between text segments.</p><p>BoolQ <ref type="bibr" target="#b3">(Clark et al., 2019)</ref> is a QA task where each example consists of a passage p and a yes/no question q. We use the following patterns:</p><p>• p. Question: q? Answer: .</p><p>• p. Based on the previous passage, q? .</p><p>• Based on the following passage, q? . p</p><p>We define two verbalizers mapping questions containing a true statement to yes/true and others to no/false, respectively, for a total of 6 PVPs.</p><p>CB <ref type="bibr" target="#b6">(De Marneffe et al., 2019)</ref> and RTE <ref type="bibr" target="#b4">(Dagan et al., 2006)</ref> are textual entailment tasks like MNLI, so we use PVPs similar to <ref type="bibr" target="#b30">Schick and Schütze (2020a)</ref>. For a premise p and hypothesis h, we use</p><formula xml:id="formula_6">h? | , p , "h"? | , "p" , h? | . p , "h"? | . "p"</formula><p>and a verbalizer that maps entailment to yes, disagreement to no and neutral to maybe.</p><p>Given a premise p, the task in COPA <ref type="bibr" target="#b28">(Roemmele et al., 2011)</ref> is to determine the cause or effect of the premise given two options c 1 and c 2 . For determining the effect, we use the following patterns:</p><p>"c 1 " or "c 2 "? p, so . , c 1 or c 2 ? p, so .</p><p>For determining the cause, we use the same patterns but replace so with because. The verbalizer for c 1 and c 2 is the identity function.</p><p>For WiC (Pilehvar and Camacho-Collados, 2019), given a word w and two sentences s 1 and s 2 in which it occurs, the task is to decide if w is used with the same sense in both sentences. We use:</p><p>• "s 1 " / "s 2 ". Similar sense of "w"? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>s 1 s 2 Does w have the same meaning in both sentences?</p><p>• w. Sense (1) (a) "s 1 " ( ) "s 2 "</p><p>For the first two patterns, we use yes as verbalization for words used in the same sense and no for other words; for the third pattern, we use b and 2.</p><p>For WSC <ref type="bibr" target="#b16">(Levesque et al., 2011)</ref>, each example consists of a sentence s with a marked pronoun p and noun n, and the task is to determine whether p refers to n. We follow previous work <ref type="bibr" target="#b27">(Raffel et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020)</ref> and treat WSC as a generative task. We highlight p in s by putting it in asterisks and use the following patterns:</p><p>• s The pronoun ' * p * ' refers to .</p><p>• s In the previous sentence, the pronoun ' * p * ' refers to .</p><p>• s In the passage above, what does the pronoun ' * p * ' refer to? Answer: .</p><p>We use the identity function as verbalizer for n. Note that WSC is different from other tasks in that it requires free-form completion. This in turn requires some modifications during training and inference that are discussed in Appendix A.</p><p>MultiRC <ref type="bibr" target="#b13">(Khashabi et al., 2018</ref>) is a QA task. Given a passage p, a question q and an answer candidate a, the task is to decide whether a is a correct answer for q. We use the same verbalizer as for BoolQ and similar patterns:</p><p>• p. Question: q? Is it a? .</p><p>• p. Question: q? Is the correct answer "a"? .</p><p>• p. Based on the previous passage, q? Is "a" a correct answer? .</p><p>For ReCoRD <ref type="bibr">(Zhang et al., 2018)</ref>, given a passage p and a cloze question q, the task is to decide which of a given set of answer candidates is the correct replacement for the placeholder in the cloze question. As this task is already presented in the form of a cloze question, there is little room for designing PVPs, so we only use a trivial one: the concatenation of p and q as pattern and the identity function as verbalizer. With only one PVP, there is no need to perform knowledge distillation so we directly use the resulting model as our final classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>As underlying LM for PET we choose ALBERTxxlarge-v2 <ref type="bibr" target="#b15">(Lan et al., 2020)</ref>, the best-performing MLM on SuperGLUE when training is performed on the regular, full size training sets. 5 We run PET on the FewGLUE training sets for all SuperGLUE tasks using the exact same setup as <ref type="bibr" target="#b30">Schick and Schütze (2020a)</ref>. For COPA, WSC and ReCoRD, we use our proposed modification of PET to support verbalizers mapping labels to multiple tokens; for all other tasks, we use regular PET. We train iPET on all tasks except COPA and WSC, as their unlabeled sets contain well below 1000 examples, as well as ReCoRD, for which iPET makes no sense as we only use a single PVP. For these three tasks, we simply reuse the results of regular PET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Our main results are shown in Table <ref type="table" target="#tab_0">1</ref>. As can be seen, ALBERT with PET performs similar to the largest GPT-3 model, which is larger by a factor of 785. On average, PET performs 18 points better 5 Results for various models can be found at https:// super.gluebenchmark.com/leaderboard. compared to GPT-3 Med, a model of similar size. iPET brings further improvements for 3 out of 5 tasks, most notably for CB, but results in a slight performance drop for MultiRC. Similar to GPT-3, we find that PET does not perform above random chance for WiC, which is difficult to reformulate as a language modeling task. ReCoRD is the only task for which GPT-3 consistently performs better than both PET and iPET. Despite PET's strong performance, it still clearly performs worse than a state-of-the-art model trained on the regular, full size SuperGLUE training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We investigate the importance of several factors for good few-shot performance: the choice of patterns and verbalizers, the usage of both unlabeled and labeled data, and properties of the underlying language model. We also look into our proposed modification for PET to work with multiple masks and compare it to various baselines. Finally, we measure how choosing different sets of training examples affects performance. Our analysis focuses on PET as GPT-3 is not publicly available.<ref type="foot" target="#foot_3">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Patterns</head><p>One factor that has gained little attention in previous work is the way tasks are reformulated as cloze questions. These reformulations can be arbitrarily complex; for example, the pattern used by GPT-3 for WSC contains an introductory section that spans almost 30 words; it is unclear if and how this formulation has been optimized. To investigate the importance of patterns and verbalizers, we compare three different sets of PVPs: our initial set of PVPs as defined in Section 4.1 (denoted p ours ), the single PVP used by GPT-3 (p GPT-3 ), and the combination of both (p comb ). We train ALBERT using PET with all three sets of patterns; results for selected SuperGLUE tasks are shown in Table <ref type="table">2</ref> (top). As can be seen, the PVP used by GPT-3 outperforms our PVPs on RTE whereas our initial set of patterns performs much better on MultiRC. These large differences in performance highlight the importance of finding good ways to express tasks as cloze questions. As it is extremely difficult to ascertain which patterns perform well without trying them on a large set of examples, a key challenge for few-shot approaches is to be able to compensate for PVPs that the used LM fails to understand well. As seen in the performance of the model trained with p comb , PET is able to do so: not only does combining all PVPs compensate for the worse performance of p ours on RTE and of p GPT-3 on MultiRC, it even further improves average performance across the three tasks considered compared to the best-performing set of patterns. This clearly demonstrates the potential of carefully engineering a set of suitable patterns as opposed to just choosing a single formulation without means of evaluating its actual effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unlabeled Data Usage</head><p>Unlike GPT-3, PET requires unlabeled data to distill the knowledge of all models based on individual PVPs into a single classifier; for iPET, unlabeled data is additionally used to generate training sets for future generations. While the benefit of iPET is already analyzed in Table <ref type="table" target="#tab_0">1</ref>, we now investigate the importance of unlabeled data for regular PET. To this end, we compare the performance of the final classifier in PET to that of directly using the ensemble of models corresponding to individual PVPs. While using this ensemble entirely removes the need for unlabeled data, the ensemble for k PVPs is larger than the distilled model by a factor of 3 • k as we follow the default setting of PET and train three models per PVP. However, even for a large number of PVPs the ensemble is smaller than GPT-3 by two orders of magnitude.</p><p>Results without distillation can be seen in Table 2 (bottom). Averaged across the three tasks, the ensemble performs even better than the distilled classifier. This shows that if the goal is only to achieve good performance, then unlabeled data is not necessary; however, it is required to obtain a single, lightweight model as final classifier.</p><p>Of course, there are further ways to leverage unlabeled data such as keeping an auxiliary language modeling objective during finetuning <ref type="bibr" target="#b2">(Chronopoulou et al., 2019)</ref>. While we leave investigating the impact of additionally using such methods to future work, we note that they can easily be applied to PET while there is no straightforward way to combine them with priming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Labeled Data Usage</head><p>We next investigate the effect of how labeled data is used, which is one of the key differences between priming and PET. We first compare PET with regular supervised training (i.e., without using any patterns), and with a fully unsupervised model (Table <ref type="table" target="#tab_2">3</ref>). Given 32 examples, PET clearly outperforms both baselines, which is in line with findings by <ref type="bibr" target="#b30">Schick and Schütze (2020a)</ref>.</p><p>We next compare PET directly to priming. However, we cannot do so using ALBERT as it, like most pretrained MLMs, is only able to process sequences of up to 512 tokens, which is not enough for a set of 32 examples. As it theoretically supports sequences of arbitrary length, we instead use XLNet <ref type="bibr" target="#b40">(Yang et al., 2019)</ref> for this compari- son. As shown in Table <ref type="table" target="#tab_2">3</ref>, XLNet in general performs worse than ALBERT. More importantly, XL-Net with PET performs much better than priming (and priming does not even perform above random chance for RTE). We were not able to obtain results with priming on MultiRC because the 32 examples in FewGLUE would require more than 10,000 tokens, so processing them with a standard Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> is infeasible due to the quadratic complexity of self-attention. This highlights another important issue with priming: It does not scale well to more than a few examples; even GPT-3 is only able to process sequences of up to 2,048 tokens. While there are some variants of the Transformer that can deal with much longer contexts (e.g., <ref type="bibr" target="#b14">Kitaev et al., 2020;</ref><ref type="bibr" target="#b0">Beltagy et al., 2020)</ref>, it has yet to be investigated to what extent such models are able to actually make good use of priming examples over such a long context span. We further investigate the effectiveness of priming by looking at results obtained with GPT-3 more closely. To analyze how well GPT-3 is able to make use of examples given as context, Figure <ref type="figure" target="#fig_1">3</ref> shows the performance difference between priming with 32 examples and priming with just a single example for each task and model size. <ref type="foot" target="#foot_4">7</ref> As can be seen, priming with 32 examples only slightly improves performance for most tasks and model sizes. For some tasks, adding more examples even leads to The bottom row of Figure <ref type="figure" target="#fig_1">3</ref> shows the performance difference between ALBERT trained with PET (without distillation) and a fully unsupervised ALBERT model on all tasks. While results are not directly comparable due to different underlying models and PVPs, using PET results in much stronger performance improvements compared to priming and does not worsen results for any of the eight tasks.</p><formula xml:id="formula_7">B o o lQ C B A c c C B F 1 C O P A R T E W iC W S C M u lt iR C E M M u lt iR C F 1 a R e C o R D A c c R e C o R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Model Type</head><p>We next look into the relevance of the underlying LM for PET's performance. To this end, we compare ALBERT with RoBERTa large <ref type="bibr">(Liu et al., 2019)</ref> and GPT-2 medium <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>. As GPT-2 is a unidirectional model similar to GPT-3, it can only process patterns where the mask token is the very last token. We therefore use p GPT-3 for CB and RTE; for MultiRC, we stick with our original set of patterns as they already fulfill this requirement. We also do not perform distillation and instead report the ensemble's performance as there is no established way of equipping GPT-2 with a sequence classification head.</p><p>Results for training all three LMs with PET in Table <ref type="table" target="#tab_3">4</ref> show that using ALBERT as underlying LM is crucial for PET's strong performance; exchanging ALBERT with RoBERTa results in an average performance drop of 8 points. However, RoBERTa still clearly outperforms GPT-3 13B, which is larger by two orders of magnitude. Importantly, PET with GPT-2 performs much worse than with both other models. As anticipated by previous work <ref type="bibr" target="#b1">(Brown et al., 2020)</ref>, a key reason for this drop in performance could be that like GPT-3, GPT-2 is a unidirectional LM, making tasks that require comparing two sequences of text a much greater challenge. However, it is important to note that there are also other substantial differences between GPT-2 and the other two models, most notably the dataset used for pretraining. Regardless of whether its unidirectionality is the reason for GPT-2's bad performance, bidirectionality of the underlying LM is important for PET as it removes the need for the mask token to be at the very end and thus allows for more flexibility in the creation of patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">PET with Multiple Masks</head><p>We modified PET to work for outputs that require more than a single token. To investigate the impact of this modification, we look at the three tasks for which this is required: COPA, WSC and ReCoRD. We compare our decoding strategy of predicting tokens in order of the probability assigned to them, to which we refer as max-first, with two alternatives: decoding left-to-right (ltr) as is common for many autoregressive language models, and decoding all tokens simultaneously (parallel) as is done during training. Additionally, we compare PET with untrained ALBERT to measure the effectiveness of our proposed training loss.</p><p>Results are shown in Table <ref type="table">5</ref>. PET clearly outperforms untrained ALBERT for the three tasks. Not performing distillation hurts performance for COPA, but leads to slight improvements on WSC. Our decoding strategy is clearly superior to parallel decoding except for WSC, for which most predictions consist only of one or two tokens, and performs slightly better than left-to-right decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Training Examples</head><p>Recall that we conduct our experiments with training examples from FewGLUE, a randomly selected subset of the original SuperGLUE training examples. We used a fixed random seed s 0 to generate FewGLUE. Let Σ i be the randomly selected subset of SuperGLUE for random seed s i , so Σ 0 = FewGLUE. In this subsection, we create two additional subsets of SuperGLUE, Σ 1 and Σ 2 , based on different seeds. This allows us to investigate  <ref type="table" target="#tab_5">6</ref> show that for all tasks, changing the set of training examples can result in significant performance differences for PET. This highlights the importance of using the same set of examples when comparing different few-shot approaches, which is why we make the particular set of examples in FewGLUE publicly available. However, we note that the average performance of PET is similar to that of GPT-3 for all seeds.</p><p>While our results may seem contrary to the insight that for GPT-3, the exact choice of examples does not play a major role, we suspect this to be due to the fact that priming benefits much less from training examples than PET (cf. Section 5.3); accordingly, the influence of the exact set of training examples on the model's performance is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that it is possible to achieve fewshot performance similar to GPT-3 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This is achieved using PET, a method that reformulates tasks as cloze questions and trains an ensemble of models for different reformulations. We have proposed a simple yet effective modification to PET that enables us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of PET combined with pretrained ALBERT: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates, and the underlying LM itself. To enable comparisons with our work, we make our dataset of few-shot training examples publicly available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>Our implementation extends the original implementation of PET by Schick and Schütze (2020a) which, in turn, is based on the Transformers library <ref type="bibr" target="#b39">(Wolf et al., 2019)</ref> and PyTorch <ref type="bibr" target="#b21">(Paszke et al., 2017)</ref>. Unless explicitly stated differently, we use the exact same set of hyperparameters as Schick and Schütze (2020a) with the only difference that for iPET, we only train 3 generations of models to speed up training. Below, we list task-specific implementation details for all tasks in SuperGLUE.</p><p>COPA For COPA, we randomly switch the two options c 1 and c 2 during training with a probability of 50% to make the input more diverse; for inference, we always keep the original order. For distilling the final PET model, we obtain logits for unlabeled examples x from individual PVPs p as s p (y | x) = log q p (y | x); we use the input format proposed by <ref type="bibr">Liu et al. (2019)</ref>.</p><p>WiC Similar to COPA, we randomly switch the input sentences s 1 and s 2 during training. Given a word w and two sentences s 1 and s 2 , we use the sequence w: s 1 | s 2 as input for the final sequence classification model, where | marks the boundary between two text segments.</p><p>WSC Unlike other SuperGLUE tasks, the WSC formulation of <ref type="bibr" target="#b27">Raffel et al. (2019)</ref> and <ref type="bibr" target="#b1">Brown et al. (2020)</ref> requires free-form completion, meaning that for each sentence s and pronoun p, we only have a single correct choice n that the model needs to predict, but we do not provide any alternatives.</p><p>During training, we thus use regular cross entropy loss between n and qp (n | s, p) as defined in Eq. 4. However, in many cases this would allow the LM to easily identify the correct target based on the number of masks provided, so we modify each target by randomly adding up to three additional mask tokens, for which we require the model to predict a special &lt;pad&gt; token. For inference, we always just add a single mask token to ensure consistent results across multiple evaluations and perform greedy decoding as described in Section 3.</p><p>We then follow <ref type="bibr" target="#b27">Raffel et al. (2019)</ref> to map the output produced by the LM to a label y ∈ {true, false}.</p><p>For distillation, given an unlabeled example x we set s p (y | x) = 1 if the model's output for x was mapped to y and s p (y | x) = 0 otherwise. We provide inputs to the final PET model in the format s | n where | is the boundary between two text segments and mark p in s with asterisks.</p><p>MultiRC Deviating from the hyperparameters used by <ref type="bibr" target="#b30">Schick and Schütze (2019)</ref>, we use a maximum sequence length of 512 tokens for MultiRC both during training and inference because we found many passages to be much longer than 256 tokens. Input for the final sequence classification model is of the form p | q | a where p is the passage, q is the question, a is the answer candidate and we use | to mark boundaries between text segments.</p><p>ReCoRD For ReCoRD, we again use a maximum sequence length of 512 because many passages require more than 256 tokens. For some questions q, the ReCoRD training set contains a huge number of answer candidates. To facilitate training, we split each example into multiple examples as follows: let C be the set of answer candidates with C + ⊂ C being the set of correct answers. We create a training example for each c ∈ C + by randomly selecting up to 9 negative examples from C \ C + for a total of 10 answer candidates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of ALBERT with PET/iPET and GPT-3 on SuperGLUE for 32 training examples. AL-BERT with PET/iPET outperforms GPT-3 even though it has three orders of magnitude fewer parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Accuracy differences between priming with 32 examples and one-shot priming for all GPT-3 models as well as between ALBERT with PET (without distillation) and unsupervised ALBERT (bottom row)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on SuperGLUE for GPT-3 primed with 32 randomly selected examples and for PET / iPET with ALBERT-xxlarge-v2 after training on FewGLUE. State-of-the-art results when using the regular, full size training sets for all tasks<ref type="bibr" target="#b27">(Raffel et al., 2019)</ref> are shown in italics.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Params BoolQ</cell><cell>CB</cell><cell cols="4">COPA RTE WiC WSC MultiRC</cell><cell>ReCoRD Avg</cell></row><row><cell></cell><cell>Model</cell><cell>(M)</cell><cell>Acc.</cell><cell>Acc. / F1</cell><cell>Acc.</cell><cell cols="2">Acc. Acc. Acc.</cell><cell>EM / F1a</cell><cell>Acc. / F1</cell><cell>-</cell></row><row><cell></cell><cell>GPT-3 Small</cell><cell>125</cell><cell>43.1</cell><cell>42.9 / 26.1</cell><cell>67.0</cell><cell>52.3 49.8</cell><cell>58.7</cell><cell cols="2">6.1 / 45.0 69.8 / 70.7 50.1</cell></row><row><cell></cell><cell>GPT-3 Med</cell><cell>350</cell><cell>60.6</cell><cell>58.9 / 40.4</cell><cell>64.0</cell><cell>48.4 55.0</cell><cell cols="3">60.6 11.8 / 55.9 77.2 / 77.9 56.2</cell></row><row><cell></cell><cell>GPT-3 Large</cell><cell>760</cell><cell>62.0</cell><cell>53.6 / 32.6</cell><cell>72.0</cell><cell>46.9 53.0</cell><cell cols="3">54.8 16.8 / 64.2 81.3 / 82.1 56.8</cell></row><row><cell></cell><cell>GPT-3 XL</cell><cell>1,300</cell><cell>64.1</cell><cell>69.6 / 48.3</cell><cell>77.0</cell><cell>50.9 53.0</cell><cell cols="3">49.0 20.8 / 65.4 83.1 / 84.0 60.0</cell></row><row><cell>dev</cell><cell>GPT-3 2.7B GPT-3 6.7B</cell><cell>2,700 6,700</cell><cell>70.3 70.0</cell><cell>67.9 / 45.7 60.7 / 44.6</cell><cell>83.0 83.0</cell><cell>56.3 51.6 49.5 53.1</cell><cell cols="3">62.5 24.7 / 69.5 86.6 / 87.5 64.3 67.3 23.8 / 66.4 87.9 / 88.8 63.6</cell></row><row><cell></cell><cell>GPT-3 13B</cell><cell>13,000</cell><cell>70.2</cell><cell>66.1 / 46.0</cell><cell>86.0</cell><cell>60.6 51.1</cell><cell cols="3">75.0 25.0 / 69.3 88.9 / 89.8 66.9</cell></row><row><cell></cell><cell>GPT-3</cell><cell>175,000</cell><cell>77.5</cell><cell>82.1 / 57.2</cell><cell>92.0</cell><cell>72.9 55.3</cell><cell cols="3">75.0 32.5 / 74.8 89.0 / 90.1 73.2</cell></row><row><cell></cell><cell>PET</cell><cell>223</cell><cell>79.4</cell><cell>85.1 / 59.4</cell><cell>95.0</cell><cell>69.8 52.4</cell><cell cols="3">80.1 37.9 / 77.3 86.0 / 86.5 74.1</cell></row><row><cell></cell><cell>iPET</cell><cell>223</cell><cell>80.6</cell><cell>92.9 / 92.4</cell><cell>95.0</cell><cell>74.0 52.2</cell><cell cols="3">80.1 33.0 / 74.0 86.0 / 86.5 76.8</cell></row><row><cell></cell><cell>GPT-3</cell><cell>175,000</cell><cell>76.4</cell><cell>75.6 / 52.0</cell><cell>92.0</cell><cell>69.0 49.4</cell><cell cols="3">80.1 30.5 / 75.4 90.2 / 91.1 71.8</cell></row><row><cell>test</cell><cell>PET iPET</cell><cell>223 223</cell><cell>79.1 81.2</cell><cell>87.2 / 60.2 88.8 / 79.9</cell><cell>90.8 90.8</cell><cell>67.2 50.7 70.8 49.3</cell><cell cols="3">88.4 36.4 / 76.6 85.4 / 85.9 74.0 88.4 31.7 / 74.1 85.4 / 85.9 75.4</cell></row><row><cell></cell><cell>SotA</cell><cell>11,000</cell><cell>91.2</cell><cell>93.9 / 96.8</cell><cell>94.8</cell><cell>92.5 76.9</cell><cell cols="3">93.8 88.1 / 63.3 94.1 / 93.4 89.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on selected tasks for various ways of using the labeled examples available in FewGLUE</figDesc><table><row><cell></cell><cell>CB</cell><cell cols="2">RTE MultiRC Avg</cell></row><row><cell>Model</cell><cell cols="3">Acc. / F1 Acc. EM / F1a</cell><cell>-</cell></row><row><cell>PET</cell><cell cols="3">85.1 / 59.4 69.8 37.9 / 77.3 66.6</cell></row><row><cell>unsupervised</cell><cell cols="2">33.5 / 23.1 55.0</cell><cell>3.9 / 60.3 38.5</cell></row><row><cell>supervised</cell><cell cols="2">60.7 / 42.5 50.2</cell><cell>4.3 / 49.8 43.0</cell></row><row><cell>PET (XLNet)</cell><cell cols="3">88.7 / 83.0 60.4 21.4 / 66.6 63.4</cell></row><row><cell cols="3">Priming (XLNet) 56.3 / 37.7 49.5</cell><cell>-/ -</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on selected tasks for PET without knowledge distillation combined with various LMs using p GPT-3 for CB/RTE and p ours for MultiRC worse performance, especially for smaller models. For ReCoRD, even the largest model's performance slightly drops when adding more examples.</figDesc><table><row><cell></cell><cell>CB</cell><cell cols="2">RTE MultiRC Avg</cell></row><row><cell>Model</cell><cell cols="2">Params Acc. / F1 Acc. EM / F1a</cell><cell>-</cell></row><row><cell cols="4">ALBERT 223M 87.5 / 78.7 74.7 38.9 / 76.2 71.8</cell></row><row><cell cols="4">RoBERTa 355M 85.7 / 77.5 62.8 23.3 / 70.0 63.7</cell></row><row><cell>GPT-2</cell><cell cols="3">345M 73.2 / 73.7 47.7 12.4 / 57.4 52.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results on selected tasks for GPT-3 and for PET using training sets Σ 0 , Σ 1 , Σ 2 how different sets of training examples affect performance. To this end, we run PET for CB, RTE and MultiRC using all three Σ i . To measure only the effect of varying the training set while ignoring unlabeled examples, we do not use distillation.Results in Table</figDesc><table><row><cell></cell><cell>CB</cell><cell cols="2">RTE MultiRC Avg</cell></row><row><cell>Model</cell><cell cols="2">Acc. / F1 Acc. EM / F1a</cell><cell>-</cell></row><row><cell>GPT-3</cell><cell cols="3">82.1 / 57.2 72.9 32.5 / 74.8 65.4</cell></row><row><cell>PET -dist (Σ0)</cell><cell cols="3">83.9 / 76.2 66.4 38.9 / 76.2 68.0</cell></row><row><cell>PET -dist (Σ1)</cell><cell cols="3">82.1 / 57.4 61.4 39.2 / 77.9 63.2</cell></row><row><cell>PET -dist (Σ2)</cell><cell cols="3">87.5 / 84.0 61.4 34.7 / 76.3 67.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 5753-5763. Curran Associates, Inc.</figDesc><table><row><cell>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng</cell></row><row><cell>Gao, Kevin Duh, and Benjamin Van Durme. 2018.</cell></row><row><cell>ReCoRD: Bridging the gap between human and ma-</cell></row><row><cell>chine commonsense reading comprehension. Com-</cell></row><row><cell>puting Research Repository, arXiv:1810.12885.</cell></row></table><note>E</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">While our approach can easily be adapted to models capable of doing conditional generation with bidirectional context (e.g.,<ref type="bibr" target="#b17">Lewis et al., 2020;</ref><ref type="bibr" target="#b27">Raffel et al., 2019)</ref>, we stick with MLMs as they are more lightweight and we found them to perform better on simple cloze tasks in preliminary experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Based on personal correspondence with the authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">The FewGLUE dataset is publicly available at https: //github.com/timoschick/fewglue.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">We requested access to OpenAI's GPT-3 API, but as of this writing, it has not been granted.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">We do not compare priming to zero-shot performance as for unknown reasons, zero-shot GPT-3 performs well below random guessing for some tasks (e.g., 0.0% accuracy for WiC).To not overestimate the benefit of priming, we therefore show gains from providing 32 examples compared to just one.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are fewshot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach for transfer learning from pretrained language models</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2089" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The CommitmentBank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Sinn und Bedeutung 23</title>
				<meeting>Sinn und Bedeutung 23</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified view on multi-class support vector classification</title>
		<author>
			<persName><forename type="first">Ürün</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Glasmachers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00298</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">3448</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1633</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12543</idno>
		<title level="m">How can we know what language models know? Computing Research Repository</title>
				<imprint>
			<date type="published" when="2019-06">Jun Araki, and Graham Neubig. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<idno type="arXiv">arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Argumentative relation classification as plausibility ranking</title>
		<author>
			<persName><forename type="first">Juri</forename><surname>Opitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preliminary proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019): Long Papers</title>
				<meeting><address><addrLine>Erlangen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>German Society for Computational Linguistics &amp; Language Technology</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">WiC: The word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Zero-shot text classification with generative language models</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10165</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 AAAI Spring Symposium Series</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Masked language model scoring</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toan</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.240</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2699" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting cloze questions for few shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07676</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence. Timo Schick and Hinrich Schütze</title>
				<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence. Timo Schick and Hinrich Schütze</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2020a</date>
		</imprint>
	</monogr>
	<note>Learning semantic representations for novel words: Leveraging both form and context</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13283</idno>
		<title level="m">oLMpics -on what language model pre-training captures. Computing Research Repository</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BERT has a mouth, and it must speak: BERT as a Markov random field language model</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-2304</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</title>
				<meeting>the Workshop on Methods for Optimizing and Evaluating Neural Language Generation<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="30" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>arXiv preprint 1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Support vector machines for multi-class pattern recognition</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transformers: State-ofthe-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rmi</forename><surname>Louf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Morgan Funtowicz, and Jamie Brew</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
