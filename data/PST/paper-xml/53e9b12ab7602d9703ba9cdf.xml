<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Power Allocation in Server Farms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anshul</forename><surname>Gandhi</surname></persName>
							<email>anshulg@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mor</forename><surname>Harchol-Balter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
							<email>rajarshi@us.ibm.com</email>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Lefurgy</surname></persName>
							<email>lefurgy@us.ibm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research Hawthorne</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IBM Research Austin</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Power Allocation in Server Farms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DDC16FD43FF554DC4DB4CA44E4FC73A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.4 [Performance of Systems]: Modeling techniques Theory</term>
					<term>Experimentation</term>
					<term>Measurement</term>
					<term>Performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Server farms today consume more than 1.5% of the total electricity in the U.S. at a cost of nearly $4.5 billion. Given the rising cost of energy, many industries are now seeking solutions for how to best make use of their available power. An important question which arises in this context is how to distribute available power among servers in a server farm so as to get maximum performance.</p><p>By giving more power to a server, one can get higher server frequency (speed). Hence it is commonly believed that, for a given power budget, performance can be maximized by operating servers at their highest power levels. However, it is also conceivable that one might prefer to run servers at their lowest power levels, which allows more servers to be turned on for a given power budget. To fully understand the effect of power allocation on performance in a server farm with a fixed power budget, we introduce a queueing theoretic model, which allows us to predict the optimal power allocation in a variety of scenarios. Results are verified via extensive experiments on an IBM BladeCenter.</p><p>We find that the optimal power allocation varies for different scenarios. In particular, it is not always optimal to run servers at their maximum power levels. There are scenarios where it might be optimal to run servers at their lowest power levels or at some intermediate power levels. Our analysis shows that the optimal power allocation is non-obvious and depends on many factors such as the power-to-frequency relationship in the processors, the arrival rate of jobs, the maximum server frequency, the lowest attainable server frequency and the server farm configuration. Furthermore, our theoretical model allows us to explore more general settings than we can implement, including arbitrarily large server farms and different power-to-frequency curves. Importantly, we show that the optimal power allocation can significantly</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Servers today consume ten times more power than they did ten years ago <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. Recent articles estimate that a 300W high performance server requires more than $330 of energy cost per year <ref type="bibr" target="#b23">[24]</ref>. Given the large number of servers in use today, the worldwide expenditure on enterprise power and cooling of these servers is estimated to be in excess of $30 billion <ref type="bibr" target="#b20">[21]</ref>.</p><p>Power consumption is particularly pronounced in CPU intensive server farms composed of tens to thousands of servers, all sharing workload and power supply. We consider server farms where each incoming job can be routed to any server, i.e., it has no affinity for a particular server.</p><p>Server farms usually have a fixed peak power budget. This is because large power consumers operating server farms are often billed by power suppliers, in part, based on their peak power requirements. The peak power budget of a server farm also determines its cooling and power delivery infrastructure costs. Hence, companies are interested in maximizing the performance at a server farm given a fixed peak power budget <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>The power allocation problem we consider is: how to distribute available power among servers in a server farm so as to minimize mean response time. Every server running a given workload has a minimum level of power consumption, b, needed to operate the processor at the lowest allowable frequency and a maximum level of power consumption, c, needed to operate the processor at the highest allowable frequency. By varying the power allocated to a server within the range of b to c Watts, one can proportionately vary the server frequency (See Fig. <ref type="figure" target="#fig_3">2</ref>). Hence, one might expect that running servers at their highest power levels of c Watts, which we refer to as PowMax, is the optimal power allocation scheme to minimize response time. Since we are constrained by a power budget, there are only a limited number of servers that we can operate at the highest power level. The rest of the servers remain turned off. Thus PowMax corresponds to having few fast servers. In sharp contrast is PowMin, which we define as operating servers at their lowest power levels of b Watts. Since we spend less power on each server, PowMin corresponds to having many slow servers. Of course there might be scenarios where we neither operate our servers at the highest power levels nor at the lowest power levels, but we operate them at some intermediate power levels. We refer to such power allocation schemes as PowMed.</p><p>Understanding power allocation in a server farm is intrinsically difficult for many reasons: First, there is no single allocation scheme which is optimal in all scenarios. For example, it is commonly believed that PowMax is the optimal power allocation scheme <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b6">7]</ref>. However, as we show later, PowMin and PowMed can sometimes outperform PowMax by almost a factor of 1.5. Second, it turns out that the optimal power allocation depends on a very long list of external factors, such as the outside arrival rate, whether an open or closed workload configuration is used, the powerto-frequency relationship (how power translates to server frequency) inherent in the technology, the minimum power consumption of a server (b Watts), the maximum power that a server can use (c Watts), and many other factors. It is simply impossible to examine all these factors via experiments.</p><p>To fully understand the effect of power allocation on mean response time in a server farm with a fixed power budget, we introduce a queueing theoretic model, which allows us to predict the optimal power allocation in a variety of scenarios. We then verify our results via extensive experiments on an IBM BladeCenter.</p><p>Prior work in power management has been motivated by the idea of managing power at the global data center level <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref> rather than at the more localized single-server level. While power management in server farms often deals with various issues such as reducing cooling costs, minimizing idle power wastage and minimizing average power consumption, we are more interested in the problem of allocating peak power among servers in a server farm to maximize performance. Notable prior work dealing with peak power allocation in a server farm includes Raghavendra et al. <ref type="bibr" target="#b20">[21]</ref>, Femal et al. <ref type="bibr" target="#b9">[10]</ref> and Chase et al. <ref type="bibr" target="#b4">[5]</ref> among others. Raghavendra et al. <ref type="bibr" target="#b20">[21]</ref> present a power management solution that coordinates different individual approaches to simultaneously minimize average power, peak power and idle power wastage. Femal et al. <ref type="bibr" target="#b9">[10]</ref> allocate peak power so as to maximize throughput in a data center while simultaneously attempting to satisfy certain operating constraints such as loadbalancing the available power among the servers. Chase et al. <ref type="bibr" target="#b4">[5]</ref> present an auction-based architecture for improving the energy efficiency of data centers while achieving some quality-of-service specifications. We differ from the above work in that we specifically deal with minimizing mean response time for a given peak power budget and understanding all the factors that affect it.</p><p>Our contributions As we have stated, the optimal power allocation scheme depends on many factors. Perhaps the most important of these is the specific relationship between the power allocated to a server and its frequency (speed), henceforth referred to as the power-to-frequency relationship. There are several mechanisms within processors that control the powerto-frequency relationship. These can be categorized into DFS (Dynamic Frequency Scaling), DVFS (Dynamic Volt-age and Frequency Scaling) and DVFS+DFS. Section 2 discusses these mechanisms in more detail. The functional form of the power-to-frequency relationship for a server depends on many factors such as the workload used, maximum server power, maximum server frequency and the voltage and frequency scaling mechanism used (DFS, DVFS or DVFS+DFS). Unfortunately, the functional form of the server level power-to-frequency relationship is only recently beginning to be studied (See the 2008 papers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>) and is still not well understood. Our first contribution is the investigation of how power allocation affects server frequency in a single server using DFS, DVFS, and DVFS+DFS for various workloads. In particular, in Section 3 we derive a functional form for the power-to-frequency relationship based on our measured values (See Figs. 2(a) and (b)).</p><p>Our second contribution is the development of a queueing theoretic model which predicts the mean response time for a server farm as a function of many factors including the power-to-frequency relationship, arrival rate, peak power budget, etc. The queueing model also allows us to determine the optimal power allocation for every possible configuration of the above factors (See Section 4).</p><p>Our third contribution is the experimental implementation of our schemes, PowMax, PowMin, and PowMed, on an IBM BladeCenter, and the measurement of their response time for various workloads and voltage and frequency scaling mechanisms (See Sections 5 and 6). Importantly, our experiments show that using the optimal power allocation scheme can significantly reduce mean response time, sometimes by as much as a factor of 5. To be more concrete, we show a subset of our results in Fig. <ref type="figure" target="#fig_1">1</ref>, which assumes a CPU bound workload in an open loop setting. Fig. <ref type="figure" target="#fig_1">1</ref>(a) depicts one possible scenario using DFS where PowMax is optimal. By contrast, Fig. <ref type="figure" target="#fig_1">1</ref>(b) depicts a scenario using DVFS where PowMin is optimal for high arrival rates. Lastly, Fig. <ref type="figure" target="#fig_1">1(c</ref>) depicts a scenario using DVFS+DFS where PowMed is optimal for high arrival rates.</p><p>We experiment with different workloads. While Section 5 presents experimental results for a CPU bound workload, LINPACK, Section 6 repeats all the experiments under the STREAM memory bound workload, the WebBench web workload, and other workloads. In all cases, experimental results are in excellent agreement with our theoretical predictions. Section 7 summarizes our work. Finally, Section 8 discusses future applications of our model to more complex situations such as workloads with varying arrival rates, servers with idle (low power) states and power management at the subsystem level, such as the storage subsystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EXPERIMENTAL FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experimental setup</head><p>Our experimental setup consists of a server farm with up to fourteen IBM BladeCenter HS21 blade servers featuring two 3.0 GHz dual-core Intel Woodcrest Xeon processors and 1 GB memory per blade, all residing in a single chassis. We installed and configured Apache as an application server on each of the blade servers to process transactional requests. To generate HTTP requests for the Apache web servers, we employ an additional blade server on the same chassis as the workload generator to reduce the effects of network latency. The workload generator uses the web server performance benchmarking tool httperf <ref type="bibr" target="#b18">[19]</ref> in the open server  farm configuration and wbox <ref type="bibr" target="#b22">[23]</ref> in the closed server farm configuration. We modified and extended httperf and wbox to allow for multiple servers and to specify the routing probability among the servers. We measure and allocate power to the servers using IBM's Amester software. Amester, along with additional scripts, collects all relevant data for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Voltage and frequency scaling mechanisms</head><p>Processors today are commonly equipped with mechanisms to reduce power consumption at the expense of reduced server frequency. Common examples of these mechanisms are Intel's SpeedStep Technology and AMD's Cool 'n' Quiet Technology. The power-to-frequency relationship in such servers depends on the specific voltage and frequency scaling mechanism used. Most mechanisms fall under the following three categories: Dynamic Frequency Scaling (DFS) a.k.a. Clock Throttling or T-states is a technique to manage power by running the processor at a less-than-maximum clock frequency. Under DFS, the Intel's 3.0 GHz Woodcrest Xeon processors we use allow for 8 operating points which correspond to effective frequencies of 12.5%, 25%, 37.5%, 50%, 62.5%, 75%, 87.5%, and 100% of the maximum server frequency.</p><p>Dynamic Voltage and Frequency Scaling (DVFS) a.k.a. P-states is a more efficient power-savings mechanism that reduces server frequency by reducing the processor voltage and frequency. Under DVFS, our processors allow for 4 operating points which correspond to effective frequencies of 66.6%, 77.7%, 88.9%, and 100% of the maximum server frequency.</p><p>DVFS+DFS attempts to leverage both DVFS, and DFS by applying DFS on the lowest performance state available in DVFS. Under DVFS+DFS, our processors allow for 11 operating points which correspond to effective frequencies of 8.3%, 16.5%, 25%, 33.3%, 41.6%, 50.0%, 58.3%, 66.6%, 77.7%, 88.9%, and 100% of the maximum server frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Power consumption within a single server</head><p>When allocating power to a server, there is a minimum level of power consumption (b) needed to operate the processor at the lowest allowable frequency and a maximum level of power consumption (c) needed to operate the processor at the highest allowable frequency (Of course the specific values of b and c depend on the application that the server is running). Formally, we define the following notation: Baseline power: b Watts The minimum power consumed by a fully-utilized server over the allowable range of processor frequency. Speed at baseline: s b Hertz The speed (or frequency) of a fully utilized server running at b Watts. Maximum power: c Watts The maximum power consumed by a fully-utilized server over the allowable range of processor frequency. Speed at maximum power: sc Hertz The speed (or frequency) of a fully utilized server running at c Watts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">POWER-TO-FREQUENCY</head><p>An integral part of determining the optimal power allocation is to understand the power-to-frequency relationship. This relationship differs for DFS, DVFS, and DVFS+DFS, and also differs based on the workload in use. Unfortunately, the functional form of the power-to-frequency relationship is not well studied in the literature. The servers we use support all three voltage and frequency scaling mechanisms and therefore can be used to study the power-to-frequency relationships. In this section, we present our measurements depicted in Figs. <ref type="figure" target="#fig_3">2(a</ref>) and (b) showing the functional relationship between power allocated to a server and its frequency for the LINPACK <ref type="bibr" target="#b12">[13]</ref> workload. We generalize the functional form of the power-to-frequency relationship to other workloads in Section 6 (See Figs. <ref type="figure">8</ref> and<ref type="figure" target="#fig_1">10</ref>). Throughout we assume a homogeneous server farm.</p><p>We use the tools developed in <ref type="bibr" target="#b16">[17]</ref> to limit the maximum power allocated to each server. Limiting the maximum power allocated to a server is usually referred to as capping the power allocated to a server. We run LINPACK jobs back-to-back to ensure that the server is always occupied by the workload, and that the server is running at the specified power cap value. Hence, the power values we observe will be the peak power values for the specified workload. Recall from Section 2 that the voltage and frequency scaling mechanisms have certain discrete performance points (in terms of frequency) at which the server can operate. At each of these performance points, the server consumes a certain amount of power for a given workload. By quickly dithering between available performance states, we can ensure that the server never consumes more than the set power cap value. In this way, we also get the best performance from the server for the given power cap value. Note that when we say power, we mean the system-level power which includes the power consumed by the processor and all other components within the server.</p><p>Fig. <ref type="figure" target="#fig_3">2</ref>(a) illustrates the power-to-frequency curves obtained for LINPACK using DFS and DVFS. From the figure, we see that the power-to-frequency relationship for both DFS and DVFS is almost linear. It may seem surprising that the power-to-frequency relationship for DVFS looks like a linear plot. This is opposite to what is widely suggested in the literature for the processor power-to-frequency relationship, which is cubic <ref type="bibr" target="#b10">[11]</ref>. The reason why the server powerto-frequency relationship is linear can be explained by two interrelated factors. First, manufacturers usually settle on a limited number of allowed voltage levels (or performance states), which results in a less-than-ideal relationship between power and frequency in practice. Second, DVFS is not applied on many components at the system level. For example, power consumption in memory remains proportional to the number of references to memory per unit time, which is only linearly related to the frequency of the processor. Thus, the power-to-frequency curve for both DFS and DVFS can be approximated as a linear function. Using the terminology introduced in Section 2, we approximate the server speed (or frequency), s GHz, as a function of the power allocated to it, P Watts, as:</p><formula xml:id="formula_0">s = s b + α(P -b).</formula><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>where the coefficient α (units of GHz per Watt) is the slope of the power-to-frequency curve. Specifically, our experiments show that α = 0.008 GHz/W for DVFS and α = 0.03 GHz/W for DFS. Also, from our experiments, we find that b = 180W and c = 240W for both DVFS and DFS. However, s b = 2.5 GHz for DVFS and s b = 1.2 GHz for DFS. The maximum speed in both cases is sc = 3 GHz, which is simply the maximum speed of the processor we use. Note that the specific values of these parameters change depending on the workload in use. Section 6 discusses our measured parameter values for different workloads.</p><p>For DVFS+DFS, we expect the power-to-frequency relationship to be piecewise linear since it is a combination of DVFS and DFS. Experimentally, we see from Fig. <ref type="figure" target="#fig_3">2(b</ref>) that the power-to-frequency relationship is in fact piecewise linear (3 GHz -2.5 GHz and then 2.5 GHz -1.2 GHz).</p><p>Though we could use a piecewise linear fit for DVFS+DFS, we choose to approximate it using a cubic curve for the following reasons:</p><p>1. Using a cubic fit demonstrates how we can extend our results to non-linear power-to-frequency relationships.</p><p>2. As previously mentioned, several papers consider the power-to-frequency relationship to be cubic, especially for the processor. By using a cubic model for DVFS+DFS, we wish to analyze the optimal power allocation policy for those settings.</p><p>Approximating DVFS+DFS using a cubic fit gives the following relationship between the speed of a server and the power allocated to it:</p><formula xml:id="formula_2">s = s b + α 3 √ P -b.</formula><p>(2) Specifically, our experiments show that α = 0.39 GHz/ 3 √ W . Also, from our experiments, we found that b = 150W, c = 250W, s b = 1.2 GHz and sc = 3 GHz for DVFS+DFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THEORETICAL RESULTS</head><p>The optimal power allocation depends on a large number of factors including the power-to-frequency relationship just discussed in Fig. <ref type="figure" target="#fig_3">2</ref>, the arrival rate, the minimum and maximum power consumption levels (b and c respectively), whether the server farm has an open loop configuration or a closed loop configuration, etc.</p><p>In order to investigate the effects of all these factors on the optimal power allocation, we develop a queueing theoretic model which predicts the mean response time as a function of all the above factors (See Section 4.1). We then produce theorems that determine the optimal power allocation for every possible configuration of the above factors, including open loop configuration (See Theorems 1 and 2 in Section 4.2) and closed loop configuration (See Theorems 3, 4 and 5 in Section 4.3).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Queueing model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) illustrates our measurements for DFS and DVFS. In both these mechanisms, we see that the server frequency is linearly related to the power allocated to the server. Fig.(b) illustrates our measurements for DVFS+DFS, where the power-to-frequency curve is better approximated by a cubic relationship.</head><p>i, receives a fraction qi of the total workload coming in to the server farm. Corresponding to any vector of power allocation (P1, . . . , P k ), there exists an optimal workload allocation vector (q * 1 , . . . , q * k ). We derive the optimal workload allocation for each power allocation and use that vector, (q * 1 , . . . , q * k ), both in theory and in the actual experiments. The details of how we obtain the optimal (q * 1 , . . . , q * k ) are deferred to <ref type="bibr" target="#b11">[12]</ref>.</p><p>Our model assumes that the jobs at a server are scheduled using the Processor-Sharing (PS) scheduling discipline. Under PS, when there are n jobs at a server, they each receive 1/n th of the server capacity. PS is identical to Round-Robin with quantums (as in Linux), when the quantum size approaches zero. A job's response time, T , is the time from when the job arrives until it has completed service, including waiting time. We aim to minimize mean response time, E[T ].</p><p>We will analyze our server farm model under both an open loop configuration (See Section 4.2) and a closed loop configuration (See Section 4. <ref type="figure" target="#fig_2">3</ref>). An open loop configuration is one in which jobs arrive from outside the system and leave the system after they complete service. We assume that the arrival process is Poisson with average rate λ jobs/sec. Sometimes it will be convenient to, instead, express λ in units of GHz. This conversion is easily achievable since an average job has size E[S] gigacycles. In the theorems presented in the paper, λ is in the units of GHz. However, in the proofs in <ref type="bibr" target="#b11">[12]</ref>, when convenient for queueing analysis, we switch to jobs/sec. Likewise, while it is common for us to express the speed of the server, s, in GHz, we sometimes switch to jobs/sec in <ref type="bibr" target="#b11">[12]</ref> when convenient. A closed loop configuration is one in which there are always a fixed number of users N (also referred to as the multi-programming level) who each submit one job to the server. Once a user's job is completed, he immediately creates another job, keeping the number of jobs constant at N . In all of the theorems that follow, we find the optimal power allocation, (P * 1 , P * 2 , . . . , P * k ), for a k-server farm, which minimizes the mean response time, E[T ], given the fixed peak power budget P = P k i=1 P * i . While deriving the op-timal power allocation is non-trivial, computing E[T ] for a given allocation is easy. Hence we omit showing the mean response time in each case and refer the reader to <ref type="bibr" target="#b11">[12]</ref>. Due to lack of space, we defer all proofs to <ref type="bibr" target="#b11">[12]</ref>. However, we present the intuition behind the theorems in each case. Recall from Section 2 that each fully utilized server has a minimum power consumption of b Watts and maximum power consumption of c Watts.</p><p>To illustrate our results clearly, we shall assume throughout this section that the power budget P is such that Pow-Max allows us to run n servers (each at power c) and PowMin allows us to run m servers (each at power b). This is equivalent to saying:</p><formula xml:id="formula_3">P = m • b = n • c (3)</formula><p>where m and n are less than or equal to k. Obviously, m ≥ n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Theorems for open loop configurations</head><p>Theorem 1 derives the optimal power allocation in an open loop configuration for a linear power-to-frequency relationship, as is the case for DFS and DVFS. In such cases, the server frequency varies with the power allocated to it as si = s b + α(Pib). The theorem says that if the speed at baseline, s b , is sufficiently low, then PowMax is optimal. By contrast, if s b is high, then PowMin is optimal for high arrival rates and PowMax is optimal for low arrival rates. If s * i is the speed of server i when run at power P * i , then the stability condition requires that λ &lt;</p><formula xml:id="formula_4">P k i=1 s * i . Theorem 1.</formula><p>Given an open k-server farm configuration with a linear power-to-frequency relationship (given by Eq. ( <ref type="formula" target="#formula_1">1</ref>)) and power budget P, the following power allocation minimizes E[T ]:</p><formula xml:id="formula_5">If s b b ≤ α: P * 1,2,..,n = c, P * n+1,n+2,..,k = 0 If s b b &gt; α: j P * 1,2,..,n = c, P * n+1,n+2,..,k = 0 if λ ≤ λ low P * 1,2,..,m = b, P * m+1,m+2,..,k = 0 if λ &gt; λ low where λ low = α • P.</formula><p>Corollary 1. For DFS, PowMax is optimal. For DVFS, PowMax is optimal at low arrival rates and PowMin is optimal at high arrival rates.</p><p>Intuition For a linear power-to-frequency relationship, we have from Eq. ( <ref type="formula" target="#formula_1">1</ref>) that the speed of a server, si, varies with the power allocated to it, Pi, as si = s b + α(Pib). From this equation, it follows that the frequency per Watt for a single server, s i P i , can be written as:</p><formula xml:id="formula_6">si Pi = s b -αb Pi + α.</formula><p>Hence, maximizing the frequency per Watt depends on whether s b ≤ αb or s b &gt; αb. If s b ≤ αb, maximizing s i P i is equivalent to maximizing Pi, which is achieved by PowMax. Alternatively, if s b &gt; αb, we want to minimize Pi, which is achieved by PowMin. However, the above argument still does not take into account the mean arrival rate, λ. If λ is sufficiently low, there are very few jobs in the server farm, hence, few fast servers, or PowMax, is optimal. The corollary follows by simply plugging in the values of s b , α and b for DFS and DVFS from Section 3.</p><p>Theorem 2 derives the optimal power allocation for nonlinear power-to-frequency relationships, such as the cubic relationship in the case of DVFS+DFS. In such cases, the server frequency varies with the power allocated to it as si = s b + α <ref type="foot" target="#foot_1">3</ref>√ Pib. The theorem says that if the arrival rate is sufficiently low, then PowMax is optimal. However, if the arrival rate is high, PowMed is optimal. Although Theorem 2 specifies a cubic power-to-frequency relationship, we conjecture that similar results hold for more general powerto-frequency curves where server frequency varies as the n-th root of the power allocated to the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2. Given an open k-server farm configuration with a cubic power-to-frequency relationship (given by Eq. (2)) and power budget P, the following power allocation minimizes E[T ]:</head><p>P * 1,2,..,n = c, P * n+1,n+2,..,k = 0 if λ ≤ λ low P * 1,2,..,l = P l , P * l+1,l+2,..,k = 0 if λ &gt; λ low .</p><p>where</p><formula xml:id="formula_7">λ low = nlα l-n " 3 √ c -b -3 q P l -b « and l = $ P b+ " α P 3λ " 3 2 % .</formula><p>Corollary 2. For DVFS+DFS, PowMax is optimal at low arrival rates and PowMed is optimal at high arrival rates.</p><p>Intuition When the arrival rate is sufficiently low, there are very few jobs in the system, hence, PowMax is optimal. However, for higher arrival rates, we allocate to each server the amount of power that maximizes its frequency per Watt ratio. For the cubic power-to-frequency relationship, which has a downwards concave curve (See Fig. <ref type="figure" target="#fig_3">2(b</ref>)), we find that the optimal power allocation value for each server (derived via calculus) lies between the maximum c and the minimum b. Hence, PowMed is optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Theorems for closed loop configurations</head><p>We now move on to closed loop configurations, where the number of jobs in the system, N , is always constant. We will rely on asymptotic operational laws (See <ref type="bibr" target="#b13">[14]</ref>) which approximate the performance of the system for very high N and very low N (see <ref type="bibr" target="#b11">[12]</ref> for details).</p><p>Theorem 3 says that for a closed server farm configuration with sufficiently low value of N , PowMax is optimal. Theorem 3. Given a closed k-server farm configuration with a linear or cubic power-to-frequency relationship (given by Eqs. ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula">2</ref>)), the following power allocation minimizes E[T ] for low N , based on the asymptotic approximations in <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_8">P * 1,2,..,n = c, P * n+1,n+2,..,k = 0</formula><p>Corollary 3. For a closed-loop server farm configuration with low N , PowMax is optimal for DFS, DVFS, and DVFS+DFS.</p><p>Intuition When N is sufficiently low, there are very few jobs in the system. Hence few fast servers are optimal since there aren't enough jobs to utilize the servers, leaving servers idle. Thus PowMax is optimal. This is similar to the case of low arrival rate that we considered for an open loop server farm configuration in Theorems 1 and 2.</p><p>When N is high, the optimal power allocation is nontrivial. From here on, we assume N is large enough to keep all servers busy, so that asymptotic bounds apply (See <ref type="bibr" target="#b13">[14]</ref>).</p><p>In our experiments, we find that N &gt; 10k suffices.</p><p>Theorem 4 says that for high N , if the speed at baseline, s b , is sufficiently low, then PowMax is optimal. By contrast, if s b is high, then PowMin is optimal.</p><p>Theorem 4. Given a closed k-server farm configuration with a linear power-to-frequency relationship (given by Eq. ( <ref type="formula" target="#formula_1">1</ref>)), the following power allocation minimizes E[T ] for high N , based on the asymptotic approximations in <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_9">If s b b &lt; α: P * 1,2,..,n = c, P * n+1,n+2,..,k = 0 If s b b ≥ α: P * 1,2,..,m = b, P * m+1,m+2,..,k = 0 Corollary 4.</formula><p>For DFS, PowMax is optimal for high N . For DVFS, PowMin is optimal for high N .</p><p>Intuition For a closed queueing system with zero think time, the mean response time is inversely proportional to the throughput of the system. Hence, to minimize the mean response time, we must maximize the throughput of the kserver farm with power budget P. When N is high, under a server farm configuration, all servers are busy. Hence the throughput is the sum of the server speeds. It can be easily shown that the throughput of the system under PowMin, s b •m, exceeds the throughput of the system under PowMax, sc • n, when s b ≥ αb. Hence the result.</p><p>Theorem 5 deals with the case of high N for a non-linear power-to-frequency relationship. The theorem says that if the speed at baseline, s b , is sufficiently low, then PowMax is optimal. By contrast, if s b is high, then PowMed is optimal. server at b units of power, the increase in throughput of the system is s b GHz. For low values of s b , this increase is small. Hence, for low values of s b , we wish to turn on as few servers as possible. Thus, PowMax is optimal. However, when s b is high, it pays to turn servers on. Once a server is on, the initial steep increase in frequency per Watt afforded by a cubic power-to-frequency relationship advocates running the server at more than the minimal power b. The exact optimal PowMed power value (b + x in the Theorem) is close to the knee of the cubic power-to-frequency curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5. Given a closed k-server farm configuration with a cubic power-to-frequency relationship (given by Eq. (2)), the following power allocation minimizes E[T ] for high N , based on the asymptotic approximations in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head><p>In this section we test our theoretical results from Section 4 on an IBM BladeCenter using the experimental setup discussed in Section 2. We shall first present our experimental results for the open server farm configuration and then move on to the closed server farm configuration. For the experiments in this section, we use the Intel LINPACK <ref type="bibr" target="#b12">[13]</ref> workload, which is CPU bound. We defer experimental results for other workloads to Section 6.</p><p>As noted in Section 3, the baseline power level and the maximum power level for both DFS and DVFS are b = 180W and c = 240W respectively. For DVFS+DFS, b = 150W and c = 250W. In each of our experiments, we try to fix the power budget, P, to be an integer multiple of b and c, as in Eq. (3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Open server farm configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5: Open loop experimental results for mean response time as a function of the arrival rate using DVFS+DFS for LINPACK. At lower arrival rates, PowMax outperforms PowMed by up to 12%, while at higher arrival rates, PowMed outperforms PowMax by up to 20%. Note that PowMin is worse than both PowMed and PowMax throughout the range of arrival rates.</head><p>PowMin is huge; ranging from a factor of 3 at low arrival rates (load, ρ ≈ 0.2) to as much as a factor of 5 at high arrival rates (load, ρ ≈ 0.7). This is because the power-tofrequency relationship for DFS is steep (See Fig. <ref type="figure" target="#fig_3">2(a)</ref>), hence running servers at maximum power levels affords a huge gain in server frequency. Arrival rates higher than 0.22 jobs/sec cause our systems to overload under PowMin because s b is very low for DFS. Hence, we only go as high as 0.22 jobs/sec.  PowMin (represented by the solid line) denotes running 4 servers at b = 180W and turning off all other servers. We see that when the arrival rate is low, PowMax produces lower mean response times than PowMin. In particular, when the arrival rate is 0.5 jobs/sec, PowMax affords a 22% improvement in mean response time over PowMin. However, at higher arrival rates, PowMin outperforms PowMax, as predicted by Theorem 1. In particular, when the arrival rate is 1 job/sec, PowMin affords a 14% improvement in mean response time over PowMax. Under DVFS, we can afford arrival rates up to 1 job/sec before overloading the system. To summarize, under DVFS, we see that PowMin can be preferable to PowMax. This is due to the flatness of the power-to-frequency curve for DVFS (See Fig. <ref type="figure" target="#fig_3">2</ref>(a)), and agrees perfectly with Theorem 1. Fig. <ref type="figure">5</ref> plots the mean response time as a function of the arrival rate for DVFS+DFS with a power budget of P = 1000W. In this case, PowMax (represented by the dashed line) denotes running 4 servers at c = 250W and turning off all other servers. PowMed (represented by the solid line) denotes running 5 servers at b+c 2 = 200W and turning off all other servers. We see that when the arrival rate is low, PowMax produces lower mean response times than PowMed. However, at higher arrival rates, PowMed outperforms Pow-Max, exactly as predicted by Theorem 2. For the sake of completion, we also plot PowMin (dotted line in Fig. <ref type="figure">5</ref>). Note that PowMin is worse than both PowMed and Pow-Max throughout the range of arrival rates. Note that we use the value of b+c 2 = 200W as the optimal power allocated to each server in PowMed for our experiments as this value is close to the theoretical optimum predicted by Theorem 2 (which is around 192W for the range of arrival rates we use) and also helps to keep the power budget at 1000W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Closed server farm configuration</head><p>We now turn to our experimental results for closed server farm configurations. Fig. <ref type="figure" target="#fig_7">6</ref>(a) plots the mean response time as a function of the multi-programming level (MPL = N ) for DFS with a power budget of P = 720W. In this case,   denotes running 4 servers at b = 180W and turning off all other servers. We see that when N is high, PowMin produces lower mean response times than PowMax. This is in agreement with the predictions of Theorem 4. In particular, when N = 100, PowMin affords a 30% improvement in mean response time over PowMax. However, when N is low, Pow-Max produces slightly lower response times than PowMin. This is in agreement with Theorem 3. Fig. <ref type="figure" target="#fig_9">7</ref> plots the mean response time as a function of the multi-programming level for DVFS+DFS with a power budget of P = 1000W. In this case, PowMax (represented by the dashed line) denotes running 4 servers at c = 250W and turning off all other servers. PowMed (represented by the solid line) denotes running 5 servers at b+c 2 = 200W and turning off all other servers. PowMin (represented by the dotted line) denotes running 6 servers at 170W. We see that when N is high, PowMed produces lower mean response times than PowMax. This is in agreement with the predictions of Theorem 5. In particular, when N = 100, PowMed affords a 40% improvement in mean response time over PowMax. However, when N is low, PowMed produces only slightly lower response times than PowMax. Note that throughout the range of N , PowMin is outperformed by both PowMax and PowMed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">OTHER WORKLOADS</head><p>Thus far we have presented experimental results for a CPU bound workload LINPACK. In this section we present experimental results for other workloads. Our experimental results agree with our theoretical predictions even in the case of non-CPU bound workloads.</p><p>We fully discuss experimental results for two workloads, DAXPY and STREAM, in this section and summarize our results for other workloads at the end of the section. Due to lack of space, we only show open loop configuration results. DAXPY DAXPY <ref type="bibr" target="#b21">[22]</ref> is a CPU bound workload which we have sized to be L1 cache resident. This means DAXPY uses a lot of processor and L1 cache but rarely uses the server memory and disk subsystems. Hence, the power-to-frequency relationship for DAXPY is similar to that of CPU bound LINPACK except that DAXPY's peak power consumption tends to be lower than that of LINPACK, since DAXPY does not use a lot of memory or disk.</p><p>Figs. <ref type="bibr">8(a)</ref> and (b) present our results for the power-tofrequency relationship for DAXPY. The functional form of the power-to-frequency relationship under DFS and DVFS in Fig. <ref type="figure">8</ref>(a) is clearly linear. However, the power-to-frequency relationship under DVFS+DFS in Fig. <ref type="figure">8(b</ref>) is better approximated by a cubic relationship. These trends are similar to the power-to-frequency relationship for LINPACK seen in Fig. <ref type="figure" target="#fig_3">2</ref>.</p><p>Figs. 9(a), (b) and (c) present our power allocation results for DAXPY under DFS, DVFS, and DVFS+DFS respectively. For DFS, in Fig. <ref type="figure">9</ref>(a), PowMax outperforms PowMin throughout the range of arrival rates, by as much as a factor of 5. This is in agreement with Theorem 1. Note that we use 165W as the power allocated to each server under PowMin to keep the power budget same for PowMin and PowMax. For DVFS, in Fig. <ref type="figure">9</ref> √ W for DAXPY as compared to α = 0.39 GHz/ 3 √ W for LINPACK. This is in agreement with the predictions of Theorem 2 for high values of α . Intuitively, for a cubic power-to-frequency relationship, we have from Eq. (2): s = s b + α 3 √ Pb. As α increases, we get more server frequency for every Watt of power added to the server. Thus, at high α , we allocate as much power as possible to every server, implying PowMax. STREAM STREAM <ref type="bibr" target="#b17">[18]</ref> is a memory bound workload which does not use a lot of processor cycles. Hence, the power consumption at a given server frequency for STREAM is usually lower than CPU bound LINPACK and DAXPY.</p><p>Figs. <ref type="figure" target="#fig_1">10(a</ref>) and (b) present our results for the powerto-frequency relationship for STREAM. Surprisingly, the functional form of the power-to-frequency relationship under DFS, DVFS, and DVFS+DFS is closer to a cubic relationship than to a linear one. In particular, the gain in server frequency per Watt at higher power allocations is much lower than the gain in frequency per Watt at lower power allocations. We argue this observation as follows: At extremely low server frequencies, the bottleneck for STREAM's performance is the CPU. Thus, every extra Watt of power added to the system would be used up by the CPU to improve its frequency. However, at higher server frequencies, the bottleneck for STREAM's performance is the memory subsystem since STREAM is memory bound. Thus, every extra Watt of power added to the system would mainly be used up by the memory subsystem and the improvement in processor frequency would be minimal.</p><p>Figs. 11(a), (b) and (c) present our power allocation results for STREAM under DFS, DVFS, and DVFS+DFS respectively. Due to the downwards concave nature of the power-to-frequency curves for STREAM studied in Fig. <ref type="figure" target="#fig_1">10</ref>, Theorem 2 says that PowMax should be optimal at low arrival rates and PowMed should be optimal at high arrival rates. However, for the values of α in Fig. <ref type="figure" target="#fig_1">10</ref>, we find that the threshold point, λ low , below which PowMax is optimal, is quite high. Hence, PowMax is optimal in Fig. <ref type="figure" target="#fig_1">11(c</ref>). In Figs. <ref type="figure" target="#fig_1">11(a</ref>) and (b), PowMax and PowMed produce similar response times. GZIP and BZIP2 GZIP and BZIP2 are common software applications used for data compression in Unix systems. These CPU bound compression applications use sophisticated algorithms to reduce the size of a given file. We use GZIP and BZIP2 to compress a file of uncompressed size 150 MB. For GZIP, we find that PowMax is optimal for all of DFS, DVFS, and DVFS+DFS. These results are similar to the results for DAXPY. For BZIP2, the results are similar to those of LINPACK. In particular, at low arrival rates, PowMax is optimal. For high arrival rates, PowMax is optimal for DFS, PowMin is optimal for DVFS and PowMed is optimal for DVFS+DFS. WebBench WebBench <ref type="bibr" target="#b14">[15]</ref> is a benchmark program used to measure web server performance by sending multiple file requests to a server. For WebBench, we find the power-to-frequency relationship for DFS, DVFS, and DVFS+DFS to be cubic. This is similar to the power-to-frequency relationships observed for STREAM since WebBench is more memory and disk intensive. As theory predicts (See Theorem 2), we find PowMax to be optimal at low arrival rates and PowMed to be optimal at high arrival rates for DFS, DVFS, and DVFS+DFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">SUMMARY</head><p>In this paper, we consider the problem of allocating an available power budget among servers in a server farm to minimize mean response time. The amount of power allocated to a server determines its speed in accordance to some power-to-frequency relationship. Hence, we begin by measuring the power-to-frequency relationship within a single server. We experimentally find that the power-to-frequency relationship within a server for a given workload can be either linear or cubic. Interestingly, we see that the relationship is linear for DFS and DVFS when the workload is CPU bound, but cubic when it is more memory bound. By contrast, the relationship for DVFS+DFS is always cubic in our experiments.</p><p>Given the power-to-frequency relationship, we can view the problem of finding the optimal power allocation in terms of determining the optimal frequencies of servers in the server farm to minimize mean response time. However, there are several factors apart from the server frequencies that affect the mean response time for a server farm. These include the arrival rate, the maximum speed of a server, the total power budget, whether the server farm has an open or closed configuration, etc. To fully understand the effects of these factors on mean response time, we develop a queueing theo-  retic model (See Section 4.1) that allows us to predict mean response time as a function of the above factors. We then produce theorems (See Sections 4.2 and 4.3) that determine the optimal power allocation for every possible configuration of the above factors.</p><p>To verify our theoretical predictions, we conduct extensive experiments on an IBM BladeCenter for a range of workloads using DFS, DVFS, and DVFS+DFS (See Section 5 and 6). In every case, we find that the experimental results are in excellent agreement with our theoretical predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">DISCUSSION AND FUTURE WORK</head><p>There are many extensions to this work that we are exploring, but are beyond the scope of this paper.</p><p>First of all, the arrival rate into our server farm may vary dynamically over time. In order to adjust to a dynamically varying arrival rate, we may need to adjust the power allocation accordingly. The theorems in this paper already tell us the optimal power allocation for any given arrival rate. We are now working on incorporating the effects of switching costs into our model.</p><p>Second, while we have considered turning servers on or off, today's technology <ref type="bibr">[2,</ref><ref type="bibr" target="#b5">6]</ref> allows for servers which are sleeping (HALT state or deep C states). These sleeping servers consume less power than servers that are on, and can more quickly be moved into the on state than servers that are turned off. We are looking at ways to extend our theorems to allow for servers with sleep states.</p><p>Third, while this paper deals with power management at the server level (measuring and allocating power to the server as a whole), our techniques can be extended to deal with individual subsystems within a server, such as power allocation within the storage subsystem. We are looking at extending our implementation to individual components within a server.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>PowMax is best. (b) PowMin is best (c) PowMed is best at high arrival rates. at high arrival rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Subset of our results, showing that no single power allocation scheme is optimal. Fig.(a) depicts a scenario using DFS where PowMax is optimal. Fig.(b) depicts a scenario using DVFS where PowMin is optimal at high arrival rates whereas PowMax is optimal at low arrival rates. Fig.(c) depicts a scenario using DVFS+DFS wherePowMed is optimal at high arrival rates whereas PowMax is optimal at low arrival rates.</figDesc><graphic coords="3,388.92,53.82,162.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of our k-server farm model</figDesc><graphic coords="4,328.32,408.06,216.00,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Power-to-frequency curves for DFS, DVFS, and DVFS+DFS for the CPU bound LINPACK workload. Fig.(a) illustrates our measurements for DFS and DVFS. In both these mechanisms, we see that the server frequency is linearly related to the power allocated to the server. Fig.(b) illustrates our measurements for DVFS+DFS, where the power-to-frequency curve is better approximated by a cubic relationship.</figDesc><graphic coords="5,311.04,54.66,216.00,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 √Figure 4 :</head><label>34</label><figDesc>Figure 4: Open loop experimental results for mean response time as a function of the arrival rate using DFS and DVFS for LINPACK. In Fig.(a) PowMax outperforms PowMin for all arrival rates under DFS, by as much as a factor of 5. By contrast in Fig.(b), for DVFS, at lower arrival rates, PowMax outperforms PowMin by up to 22%, while at higher arrival rates, PowMin outperforms PowMax by up to 14%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 (</head><label>4</label><figDesc>Fig. 4(a) plots the mean response time as a function of the arrival rate for DFS with a power budget of P = 720W. In this case, PowMax (represented by the dashed line) denotes running 3 servers at c = 240W and turning off all other servers. PowMin (represented by the solid line) denotes running 4 servers at b = 180W and turning off all other servers. Clearly, PowMax outperforms PowMin throughout the range of arrival rates. This is in agreement with the predictions of Theorem 1. Note from Fig. 4(a) that the improvement in mean response time afforded by PowMax over</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 (</head><label>4</label><figDesc>b) plots the mean response time as a function of the arrival rate for DVFS with a power budget of P = 720W. PowMax (represented by the dashed line) again denotes running 3 servers at c = 240W and turning off all other servers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Closed loop experimental results for mean response time as a function of number of jobs (N ) in the system using DFS and DVFS for LINPACK. In Fig.(a), for DFS, PowMax outperforms PowMin for all values of N , by almost a factor of 2 throughout. By contrast in Fig.(b), for DVFS, at lower values of N , PowMax is slightly better than PowMin, while at higher values of N , PowMin outperforms PowMax by almost 30%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Closed loop experimental results for mean response time as a function of number of jobs (N ) in the system using DVFS+DFS for LINPACK. At lower values of N , PowMed is slightly better than PowMax, while at higher values of N , PowMed outperforms PowMax, by as much as 40%. Note that PowMin is worse than both PowMed and PowMax for all values of N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Power-to-frequency curves for DFS, DVFS, and DVFS+DFS for the CPU bound DAXPY workload. Fig.(a) illustrates our measurements for DFS and DVFS. In both these mechanisms, we see that the server frequency is linearly related to the power allocated to the server. Fig.(b) illustrates our measurements for DVFS+DFS, where the power-to-frequency curve is better approximated by a cubic relationship.</figDesc><graphic coords="9,82.68,53.84,216.00,156.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(b), PowMax outperforms PowMin throughout the range of arrival rates, by around 30%. This is in contrast to LINPACK, where PowMin outperforms PowMax at high arrival rates. The reason why PowMax outperforms PowMin for DAXPY is the lower value of s b = 2.2 GHz for DAXPY as compared to s b = 2.5 GHz for LINPACK. Since s b b = 0.0137 &lt; α = 0.014 for DAXPY under DVFS, Theorem 1 rightly predicts PowMax to be optimal. Finally, in Fig. 9(c) for DVFS+DFS, PowMax outperforms both PowMed and PowMin throughout the range of arrival rates. Again, this is in contrast to LINPACK, where PowMed outperforms PowMax at high arrival rates. The reason why PowMax outperforms PowMed for DAXPY is the higher value of α = 0.46 GHz/ 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Power-to-frequency curves for DFS, DVFS, and DVFS+DFS for the memory bound STREAM workload. Fig.(a) illustrates our measurements for DFS and DVFS, while Fig.(b) illustrates our measurements for DVFS+DFS. In all the three mechanisms, the power-to-frequency curves are downwards concave, depicting a cubic relationship between power allocated to a server and its frequency.</figDesc><graphic coords="11,311.04,55.06,216.00,157.88" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>s b ).Corollary 5. For DVFS+DFS, for high N , PowMed is optimal if s b is high, else PowMax is optimal.Intuition As in the case of Theorem</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p><ref type="bibr" target="#b3">4</ref>, we wish to maximize the throughput of the system. When we turn on a new</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Research supported by NSF SMA/PDOS Grant CCR-0615262 and a 2009 IBM Faculty Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Lesswatts</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://www.lesswatts.org/projects/applications-power-management/race-to-idle.php" />
		<title level="m">Race to idle</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Epa report on server and data center energy efficiency</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>U.S. Environmental Protection Agency</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">National Electrical Contractors Association. Data centers -meeting today&apos;s demand</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Managing energy and server resources in hosting centers</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prachi</forename><forename type="middle">N</forename><surname>Thakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><forename type="middle">M</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the Eighteenth ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="103" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Intel</forename><surname>Corp</surname></persName>
		</author>
		<ptr target="http://download.intel.com/design/mobile/datashts/32012001.pdf" />
		<title level="m">Intel Core2 Duo Mobile Processor Datasheet: Table 20</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Energy conservation policies for web servers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elnozahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kistler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajamony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USITS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Power provisioning for a warehouse-sized computer</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolf-Dietrich</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><forename type="middle">Andre</forename><surname>Barroso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A performance-conserving approach for reducing peak power consumption in server systems</title>
		<author>
			<persName><forename type="first">Wes</forename><surname>Felter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthick</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Rusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS &apos;05: Proceedings of the 19th annual International Conference on Supercomputing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting Data Center Performance Through Non-Uniform Power Allocation</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Femal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Freeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAC &apos;05: Proceedings of the Second International Conference on Automatic Computing</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="250" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">System Power Management Support in the IBM POWER6 Microprocessor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Rawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="733" to="746" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Optimal power allocation in server farms</title>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Lefurgy</surname></persName>
		</author>
		<idno>CMU-CS-09-113</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Intel</forename><surname>Corp</surname></persName>
		</author>
		<ptr target="http://www.intel.com/cd/software/products/asmo-na/eng/266857.htm" />
		<title level="m">Intel Math Kernel Library 10.0 -LINPACK</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Art of Computer Systems Performance Analysis: techniques for experimental design, measurement, simulation, and modeling</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Wiley</publisher>
			<biblScope unit="page" from="563" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Web bench</title>
		<author>
			<persName><forename type="first">Radim</forename><surname>Kolar</surname></persName>
		</author>
		<ptr target="http://home.tiscali.cz:8080/cz210552/webbench.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Queueing Systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kleinrock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Wiley-Interscience</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Power capping: a prelude to power shifting. Cluster Computing</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Lefurgy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Ware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stream: Sustainable memory bandwidth in high performance computers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mccalpin</surname></persName>
		</author>
		<ptr target="http://www.cs.virginia.edu/stream/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">httperf-A Tool for Measuring Web Server Performance</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mosberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigmetrics: Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="31" to="37" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DMA-Aware Memory Energy Management</title>
		<author>
			<persName><forename type="first">W</forename><surname>Vivek Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA &apos;06: The 12th International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2006-02">Feb. 2006</date>
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Power&quot; Struggles: Coordinated Multi-Level Power Management for the Data Center</title>
		<author>
			<persName><forename type="first">Ramya</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanish</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikui</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS XIII: Proceedings of the 13th international conference on Architectural support for programming languages and operating systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Online power and performance estimation for dynamic power management</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Rawson</surname></persName>
		</author>
		<idno>RC-24007</idno>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Sanfilippo</surname></persName>
		</author>
		<ptr target="http://www.hping.org/wbox/" />
		<title level="m">WBox HTTP testing tool (Version 4)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cluster-level Feedback Power Control for Performance Optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Symposium on High-Performance Computer Architecture (HPCA 2008)</title>
		<imprint>
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feedback Control Algorithms for Power Management of Servers</title>
		<author>
			<persName><forename type="first">Zhikui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanish</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Workshop on Feedback Control Implementation and Design in Computing Systems and Networks (FeBid)</title>
		<meeting><address><addrLine>Annapolis, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
