<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Emotion Modeling with Learnable Graphs and Graph Inception Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amir</forename><surname>Shirian</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Tanaya</forename><surname>Guha</surname></persName>
						</author>
						<title level="a" type="main">Dynamic Emotion Modeling with Learnable Graphs and Graph Inception Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph learning</term>
					<term>graph neural network</term>
					<term>inception network</term>
					<term>emotion recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human emotion is expressed, perceived and captured using a variety of dynamic data modalities, such as speech (verbal), videos (facial expressions) and motion sensors (body gestures). We propose a generalized approach to emotion recognition that can adapt across modalities by modeling dynamic data as structured graphs. The motivation behind the graph approach is to build compact models without compromising on performance. To alleviate the problem of optimal graph construction, we cast this as a joint graph learning and classification task. To this end, we present the Learnable Graph Inception Network (L-GrIN) that jointly learns to recognize emotion and to identify the underlying graph structure in the dynamic data. Our architecture comprises multiple novel components: a new graph convolution operation, a graph inception layer, learnable adjacency, and a learnable pooling function that yields a graph-level embedding. We evaluate the proposed architecture on five benchmark emotion recognition databases spanning three different modalities (video, audio, motion capture), where each database captures one of the following emotional cues: facial expressions, speech and body gestures. We achieve state-of-the-art performance on all five databases outperforming several competitive baselines and relevant existing methods. Our graph architecture shows superior performance with significantly fewer parameters (compared to convolutional or recurrent neural networks) promising its applicability to resource-constrained devices. Our code is available at /github.com/AmirSh15/graph_emotion_recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human emotion is expressed, perceived and captured using a variety of dynamic data modalities, such as speech (verbal), videos (facial expressions) and motion capture (body gestures). Modeling and analysis of these cues are critical for many human-centric systems with applications ranging from driver's safety to mental healthcare to human-robot conversational systems. In recent years, significant progress has been made towards the recognition and analysis of emotion using dynamic facial expressions <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, speech <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and body gestures <ref type="bibr" target="#b4">[5]</ref>. Since human emotion is inherently multimodal, research efforts that combine information from multiple modalities are also on the rise <ref type="bibr" target="#b5">[6]</ref>. Besides expressed emotion, work has also been done to analyze emotion evoked by natural images <ref type="bibr" target="#b6">[7]</ref>, videos <ref type="bibr" target="#b7">[8]</ref> and music <ref type="bibr" target="#b8">[9]</ref>.</p><p>A. Shirian and T. Guha are with the Department of Computer Science, University of Warwick, Coventry, UK.</p><p>S. Tripathi is with Intel Labs, San Diego, US.  In the literature of dynamic emotion recognition, recurrent models, such as Long Short Term Memory networks (LSTM) are common <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>. These networks often have complex architecture with millions of trainable parameters requiring large amounts of training data. This makes many emotion recognition models incompatible for use in resource-constrained devices. A compact, efficient and scalable way to represent data is in the form of graphs. We thus adopt a graph approach to building a compact model for dynamic emotion recognition. Furthermore, existing emotion recognition models assume a prior knowledge of the input modality. Since emotion can be sensed through a variety of modalities, a generalized model that can handle disparate modalities efficiently is important. We show that our modality-agnostic graph approach is able to achieve state-of-the-art accuracy across various modalities with significantly fewer trainable parameters.</p><p>Traditionally, sequences are modeled using Recurrent Neural Networks (RNNs). However, recent literature has successfully used the idea of defining a sequence over a graph <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Considering a video frame sequence as a 'structured' graph, <ref type="bibr">Mao et al.</ref> showed that graph models can outperform RNNs <ref type="bibr" target="#b10">[11]</ref>. Motivated by these recent successes and in the pursuit of a compact model, we propose to adopt a graph approach to model emotion dynamics. Subsequently, we cast emotion recognition as a joint graph learning and classification problem (see Fig. <ref type="figure" target="#fig_1">1</ref> for an overview). In our arXiv:2008.02661v2 [cs.CV] 8 Feb 2021 approach, each dynamic data sample is represented as a graph, where each node corresponds to a short temporal segment in the data. Each node is associated with the features extracted from the short temporal segment (frame) as its node attributes. This frame-to-node graph construction approach focuses on modeling the temporal dynamics in data; note that spatiotemporal structure (e.g., facial keypoints structure) within the graph resists the idea of a generic, modality-agnostic model and also increases model size significantly. Our graph structure (and hence the model) does not change with the choice of modality or node attributes. Modeling as a graph offers compactness and convenience to handle missing data (particularly common in mocap).</p><p>The graph structure i.e., the edge weights connecting the nodes is not naturally defined here. When a graph structure is not known apriori, a common practice is to manually construct the graph. This, however, results into sub-optimal graphs. We thus propose to learn the graph structure itself during the training stage. This is a generalized formulation, where the temporal dependencies between the nodes are automatically discovered. The only assumption we make is that the graph structure remains the same for all samples in a given database. To this end, we propose a novel Graph Convolution Network (GCN) architecture, the Learnable Graph Inception Network (L-GrIN), with several novel components: a new definition of graph convolution that uses a non-linear layer-wise projection technique, introduction of an inception module in graph domain, learnable graph structure and a learnable graph-to-vector pooling function. Our architecture produces superior results on five benchmark emotion recognition databases spanning three different modalities (video, audio, mocap). Each database captures one of the following emotional cues: facial expressions, speech and body gestures. In summary, the main contributions of this paper are as follows:</p><p>• A generalized, modality-agnostic graph approach to classify dynamic signals that combines graph learning with graph classification. • A novel graph architecture, termed L-GrIN, with a new graph convolution layer, a graph inception module, learnable graph structure and learnable graph-to-vector pooling.</p><p>• State-of-the-art performance on dynamic emotion recognition tasks spanning three sensory modalities (video, audio, motion sensors) on five benchmark databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review the related work in the areas of GCNs and emotion recognition using various modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph neural network.</head><p>Deep learning on graph data has emerged as a major topic in the past few years. This is because graphs provide a natural and convenient way to deal with large data. Among the different graph neural networks, GCNs have received the most attention <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b11">[12]</ref>. GCNs have been successfully applied to various image and video-based tasks, such as face clustering <ref type="bibr" target="#b15">[16]</ref>, object detection <ref type="bibr" target="#b16">[17]</ref>, and video representation learning <ref type="bibr" target="#b10">[11]</ref>. GCNs have been used to address skeleton-based action recognition recorded using motion capture <ref type="bibr" target="#b12">[13]</ref>. The application of graph networks has also started emerging in automatic speech recognition <ref type="bibr" target="#b17">[18]</ref>.</p><p>GCNs can be broadly classified into two categories: spatial and spectral. Spatial GCNs imitate the convolution operation of the Convolutional Neural Networks (CNN) by aggregating the information from neighboring nodes <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>. The problem of different graph nodes having different number of neighbours is usually addressed by using a fixed size neighborhood <ref type="bibr" target="#b18">[19]</ref> or by converting graph structures to a regular grid and subsequently applying traditional CNNs <ref type="bibr" target="#b19">[20]</ref>. A recent work proposed to develop the graph structure considering the Weisfeiler-Lehman graph isomorphism test <ref type="bibr" target="#b20">[21]</ref>, and achieved state-of-the-art performance in node classification task in citation networks. On the other hand, spectral GCNs formulate the convolution operation as a frequency domain filtering operation following the theory of signal processing <ref type="bibr" target="#b21">[22]</ref>, where convolution filters are seen as a set of learnable parameters. The ChebNet <ref type="bibr" target="#b22">[23]</ref> is proposed to reduce the computational cost of spectral GCNs that redefined the convolution filter in terms of Chebyshev polynomials bypassing the need for eigen decomposition of the graph Laplacian. In a follow-up work <ref type="bibr" target="#b14">[15]</ref>, a first order approximation of the Chebyshev polynomials was introduced. This further simplified the spectral GCN computation as the convolution operation reduces to a linear projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Emotion recognition.</head><p>Facial emotion recognition.: Recognizing facial expressions is the most common way of analyzing emotion. The majority of work rely on identifying an individual's facial expression from images or videos (fewer work on videos), and associating them to one of the basic emotion classes. Recent efforts in image-based recognition are focused on using CNNs and its variants <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and on using adversarial learning <ref type="bibr" target="#b25">[26]</ref>. A few works have proposed to use attention networks to account for the context <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. RNNs and 3D CNNs have been used for video-based emotion recognition due to their ability to capture the temporal information <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Another line of work focuses on the dynamics of landmark points in faces extracted from videos. In this context, a deep temporal appearance geometry network has been proposed <ref type="bibr" target="#b31">[32]</ref> that uses the landmark point geometry and a CNN for emotion recognition. Another recent work constructed a trajectory matrix from the landmark points and used them as inputs to a CNN <ref type="bibr" target="#b32">[33]</ref>.</p><p>Speech emotion recognition: Speech emotion recognition, especially using categorical labels, has been studied widely in the past years. Many speech emotion recognition systems still rely on low-level acoustic, prosodic and lexical features, that are then fed to deep models for classification. Other approaches use spectrograms (usually used as inputs to CNN models) <ref type="bibr" target="#b33">[34]</ref> and even raw speech <ref type="bibr" target="#b34">[35]</ref>. Recurrent models are prevalent due to their ability to capture the temporal dynamics of emotion <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b34">[35]</ref>. A 3D RNN model has been recently proposed for end-to-end modeling <ref type="bibr" target="#b36">[37]</ref>. Attentionbased techniques have been widely explored <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, while transformer-based architectures are gaining momentum in this field <ref type="bibr" target="#b39">[40]</ref>. Body emotion recognition.: Body expressions are relatively less studied in emotion recognition. The existing literature is focused on using motion information in terms of lowlevel descriptors, such as joint angles, 3D positions, distance between joints, velocity and acceleration <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. A trajectory learning approach <ref type="bibr" target="#b4">[5]</ref> proposed to identify 'neutral' motion from input data, and used the deviation of a given input from the neutral motion as a feature for classifying emotions. Another recent work combined deep features with psychological attributes to detect emotion from 3D body pose using a random Forest classifier <ref type="bibr" target="#b40">[41]</ref>. Gait information has also been considered for recognizing emotion, where a spatial GCN is used to detect the emotional state <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>In this section, we describe our deep graph approach to emotion recognition. First, we construct a graph from dynamic input data following a generalized frame-to-node approach. Next, we propose a novel architecture that jointly performs graph learning and graph classification. This is achieved by optimizing over a new loss function that combines classification loss and a graph structure loss. The proposed architecture, L-GRIN, is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Below, we describe each component of this network in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph construction</head><p>Given a dynamic input sequence, our first task is to construct an undirected graph G = (V, E) that can efficiently capture the emotion-related dynamics in the data, where V is the set of nodes with cardinality |V| = M and E is the set of all edges between the connected nodes. A representative description of G is typically given by an adjacency matrix A ∈ R M ×M which is symmetric for an undirected graph.</p><p>Our graph construction approach follows a frame-to-node transformation, where M frames in the data form the M graph nodes {v i } M i=1 ∈ V (see Fig. <ref type="figure">3</ref>). A frame refers to a small temporal segment of the data, e.g., an audio segment of length 40ms. In order to encode the temporal information, a frame (node) should be connected with weights to a series of past and future nodes. An element (A) ij ∈ A contains the weight corresponding to the edge e ij ∈ E connecting v i and v j . Note that the graph structure is not naturally defined here, i.e., the elements in A are unknown. A common way to define the elements in A is through constructing a distance function manually <ref type="bibr" target="#b12">[13]</ref>. However, this may result into a suboptimal graph representation. Hence, we propose to learn the elements in A by jointly optimizing a structural loss combined with a classification loss. This loss function will be discussed in Section III-B.</p><p>In order to capture the emotion content at node level, we rely on modality-specific features or even, raw data. Each node v i is thus associated with a node feature vector n i ∈ R P . A feature matrix N ∈ R M ×P consisting all the node feature vectors is defined as</p><formula xml:id="formula_0">N = [n 1 , n 2 , • • • , n M ] T .</formula><p>Features for individual modalities is discussed in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learnable graph inception network</head><p>Given a set of (dynamic inputs transformed to) graphs {G 1 , ..., G N } and their true labels {y 1 , ..., y N }, our task is to develop a deep graph architecture that is able to recognize the emotional content in the data. Since the graph structure is not naturally defined here, we also learn an optimal A from the training data with the underlying assumption that each graph has different node features but the same edge weights. We formulate this as a joint graph learning and graph classification problem.</p><p>A common GCN architecture takes the node feature matrix N ∈ R M ×P and the graph adjacency matrix A as inputs and produces a node-level representation matrix Z ∈ R M ×Q , where Q is the dimension of the output feature vector at each node. A GCN layer H (k+1) can be defined as a non-linear function of its previous layer as follows</p><formula xml:id="formula_1">H (k+1) = σ(AH (k) W (k) )<label>(1)</label></formula><p>where W (k) is the weight matrix for the k th layer of the neural network, σ is a non-linear activation function, such as a ReLU, and k is the layer number (k = 0, • • • K). Note that</p><formula xml:id="formula_2">H (0) = N and H (K) = Z.</formula><p>An effective improvement on this propagation rule has been recently proposed <ref type="bibr" target="#b14">[15]</ref>.</p><formula xml:id="formula_3">H (k+1) = σ(D − 1 2 (A + I)D − 1 2 H (k) W (k) ) (2)</formula><p>where D is the degree matrix of A, and I is an M × M identity matrix. Note that the terms within the parenthesis in Eq. ( <ref type="formula">2</ref>) is simply a linear projection, and can be re-written as</p><formula xml:id="formula_4">H (k+1) = σ( ÂH (k) W (k) )<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">Â = D − 1 2 (A + I)D − 1 2</formula><p>. We present a new GCN architecture, called L-GrIN (see Fig. <ref type="figure" target="#fig_2">2</ref>), for joint graph learning and classification. It has the following four new components:</p><p>• Non-linear spectral graph convolution (G * conv). Motivated by a recent work on spatial graph neural network <ref type="bibr" target="#b42">[43]</ref>, we replace the linear projection in (3) by a multi-layer perceptron (MLP) layer, and replace Â by a learnable A. Thus, instead of the linear layer in (3), we define a new spectral graph convolution layer G * (•) as follows:</p><formula xml:id="formula_6">G * (H (k) ) = σ MLP (k) ReLU(A)H (k)<label>(4)</label></formula><p>where MLP(.) has two hidden layers with η neurons each, A is the learnable adjacency matrix and σ is a nonlinear activation function. A is learned through a joint optimization process described later in this section. The ReLU(•) in Eq. ( <ref type="formula" target="#formula_6">4</ref>) ensures the non-negativity of A. We refer to the convolution operation defined above as G * conv in the rest of the paper.</p><p>• Graph inception. We extend the idea of inception layer in traditional CNNs <ref type="bibr" target="#b43">[44]</ref> to the graph domain, and introduce a graph inception module in our architecture (see Fig. <ref type="figure" target="#fig_2">2</ref>). Our graph inception layer consists of two graph convolution layers and one maxpool layer operating on directly connected (1-hop) neighbours only.</p><p>Given an input H (k) , the proposed graph inception layer is defined as follows:</p><formula xml:id="formula_7">H (k+1) = G * 1 (H (k) ) | G * 2 (H (k) ) | maxpool(H (k) )<label>(5)</label></formula><p>where | denotes concatenation of the node features, and G * 1 and G * 2 are two G * conv layers (see Eq. ( <ref type="formula" target="#formula_6">4</ref>)) with different size of their MLP layers (η = 128 for G * 1 and η = 64 for G * 2 ). Hence, for an input of H (k) ∈ R M ×P , the inception layer produces an output</p><formula xml:id="formula_8">H (k+1) ∈ R M ×(128+64+P ) .</formula><p>The motivation behind the inception layer is to be able to capture the emotion dynamics at multiple temporal scales. The two G * conv layers that yield embeddings of different dimensions can be interpreted as a multiscale analysis on graphs in spectral domain. Like a traditional inception layer in CNN, our graph inception layer also combines features from multiple scales allowing the network to have both width and depth. Our graph inception layer has fewer parameters (compared to inception networks in CNNs) enabling us to feed the input directly to the inception layer.</p><p>The maxpool function in Eq. ( <ref type="formula" target="#formula_7">5</ref>) operates on every node separately. For each node v i , we only consider its directly connected neighbors (1-hop), and maxpool over the embeddings along feature dimension. Note that as we start with a <ref type="figure">3</ref>. Graph construction: Given a dynamic input sequence of M segments, a fully-connected graph with M nodes is constructed without making any assumption. The edge weights are learned during the training phase. Each node is associated with a node attribute vector n i . fully-connected graph, initially this operation is the same as maxpooling over all nodes, but this changes quickly as we start learning the graph structure.</p><formula xml:id="formula_9">v M v 1 v 2 v j v j+1 v j-1 n 1 n j n j-1 t n j+1 n M n 2 M Fig.</formula><p>• Learnable pooling for graph-level representation. Our objective is to classify entire graphs, as opposed to the more common task of classifying each node. Hence, we seek a graph-level representation h G ∈ R Q as the output of our network. This can be obtained by pooling the node-level representations H (k) at the K-th layer before passing them to the classification layer (see Fig. <ref type="figure" target="#fig_2">2</ref>). Common choices for pooling functions in graph domain are mean, max and sum pooling. Max and mean pooling often can not preserve the underlying information about the graph structure while sum pooling is shown to be a better alternative <ref type="bibr" target="#b42">[43]</ref>. However, all these pooling functions treat every neighboring node with equal importance, which may not be optimal. To this end, we propose to learn a pooling function Ψ that combines the node embeddings from the K-th layer to produce an embedding for the entire graph. Additionally, we also use maxpool and meanpool and combine all the graph-level embeddings together. The pooling layer is thus defined as follows:</p><formula xml:id="formula_10">h G = maxpool(H (K) ) | Ψ(H (K) ) | meanpool(H (K) ) (6) Ψ(H (K) ) = H (K) p</formula><p>where p has learnable weights to combine node-level embeddings to obtain a graph-level embedding.</p><p>• Learnable adjacency (A). Recall that in our task the graph structure is not known. Although we can define such structure manually, results are sub-optimal. An effective approach would be to learn the graph structure (adjacency matrix) itself by jointly optimizing over a classification loss and graph learning loss. We assume that all videos have the same underlying graph structure containing the same number of nodes and edges. This largely simplifies our task of graph structure learning. The overall loss L for joint graph learning and classification is composed of two components: (i) L GC : a graph classification loss, and (ii) L GL : a graph learning loss. The classification loss L GC is defined as the cross-entropy loss:</p><formula xml:id="formula_11">L GC = − N n=1 y n log ŷn (7)</formula><p>where ŷn is the predicted label for the n th sample. The graph learning loss, L GL , is designed to facilitate learning the pooling vector p and the adjacency matrix A. This is defined as follows:</p><formula xml:id="formula_12">L GL = λ 1 e T (A d A)e + λ 2 A 2 F graph structure loss + λ 3 p 2 2 learnable pooling (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>where denotes element-wise multiplication, e is an all-ones vector, • F denotes Frobenious norm, λ 1 , λ 2 , and λ 3 control the relative weights of the three terms, and A d is a structure matrix defined as follows:</p><formula xml:id="formula_14">(A d ) ij = (i − j) 2<label>(9)</label></formula><p>The structure matrix A d forces the nodes that are temporally close to each other to have stronger relationship, i.e. higher weights in the A. The larger the squared distance between two nodes v i and v j (frames), the smaller will be the weights in (A) ij . The ReLU operation (see Eq. ( <ref type="formula" target="#formula_6">4</ref>)) ensures the nonnegativity of the elements in A. The overall optimization is thus as follows: min</p><formula xml:id="formula_15">A,p,Θ (1:k) L = min A,p,Θ (1:k) L GC + L GL</formula><p>where, Θ denotes all other learnable network parameters across all graph convolution layers including its constituent MLP layers. Every term in the overall loss function L is differentiable, thereby allowing an end-to-end optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We now present extensive experimental results and analysis to evaluate the performance of the proposed architecture for facial, speech and body emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Facial emotion recognition</head><p>Video databases: We use three large video emotion recognition databases for our experiments. The databases are chosen based on their popularity in emotion recognition literature. The RML database <ref type="bibr" target="#b44">[45]</ref> contains 720 videos of 6 basic emotions: anger, disgust, fear, joy, sadness, surprise collected when the subjects speak. The subjects are from various ethnic groups and speak different languages. The eNTERFACE <ref type="bibr" target="#b45">[46]</ref> is contains 1170 videos of 42 subjects with six basic emotion classes as RML. These emotions are the reactions after listening to six different short stories, where each person reads out 5 phrases based on their emotional reaction. The RAVDESS database <ref type="bibr" target="#b46">[47]</ref> contains 4904 videos labeled with 8 classes: anger, calmness, disgust, fear, joy, neutral, sadness and surprise. This is the largest video emotion database currently available.  Node features: The databases we use provide only raw video clips. We choose to use facial landmark points extracted from the video frames as node attributes. This is because landmark points are known to effectively capture the facial dynamics <ref type="bibr" target="#b51">[52]</ref>. We extract 68 landmark points at every video frame using a state-of-the-art landmark detection method <ref type="bibr" target="#b52">[53]</ref>, resulting into node feature vectors of dimension P = 136.</p><p>Implementation details: We use a 10-fold cross-validation for all three databases, and report the average recognition accuracy in Table <ref type="table" target="#tab_0">I</ref>. We fix the length of each input video to 90 frames yielding a graph with M = 90 nodes. The shorter videos are simply padded by duplicating frames from the beginning of the video (cyclic padding). Our network weights are initialized following the Xavier initialization. We set λ 1 = λ 2 = 0.1 and λ 3 = 1 × 10 −4 (see Eq. 8). We used Adam optimizer with a learning rate of 0.01 and decay rate of 0.5 after each 50 epochs for all experiments. To initialize the learnable adjacency matrix A, we generate a random matrix whose elements are drawn from a Normal distribution with zero mean and unit variance. We used Pytorch for implementing our model and the baselines, and an NVIDIA RTX-2080Ti GPU for all experiments.</p><p>Baselines, state-of-the-art: We compare our model against two competitive and relevant baselines as follows: BLSTM. The first baseline is a Bidirectional LSTM (BLSTM), an extension of the traditional LSTMs <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. LSTM and its variants have been successfully used in sentiment analysis in language and speech. This BLSTM comprises 1-layered bidirectional cells with embedding size 300 followed by a fully connected layer. GCN <ref type="bibr" target="#b14">[15]</ref>. A natural baseline to compare with our model is a spectral GCN in its standard form (as in Eq. ( <ref type="formula" target="#formula_4">3</ref>)). The original network <ref type="bibr" target="#b14">[15]</ref> is designed for node classification and only yields node-level embeddings. To obtain a graph-level embedding, we used max and mean pooling at the end of convolution layers. The GCN uses a binary adjacency matrix constructed following the method used in graph-based action recognition <ref type="bibr" target="#b12">[13]</ref>.</p><p>In addition to the baselines, we compare with two state-ofthe-art graph classification architectures: PATCHY-SAN <ref type="bibr" target="#b18">[19]</ref> is a recent architecture that learns CNNs for arbitrary graphs. This architecture is originally developed for graph classification. PATCHY-Diff <ref type="bibr" target="#b47">[48]</ref> is referred to an architecture where PATCHY-SAN is used in combination with the differentiable pooling layer between graph convolution layers proposed recently <ref type="bibr" target="#b47">[48]</ref>. SENet <ref type="bibr" target="#b24">[25]</ref>, Squeeze and Excitation net is a state-of-theart CNN architecture recently proposed for facial emotion recognition in videos.</p><p>Comparisons are also made with other existing works on the respective databases: AudioVisual Emotion Fusion (AVEF) <ref type="bibr" target="#b5">[6]</ref>, Kernel Crossmodal Factor Analysis (KCFA) <ref type="bibr" target="#b48">[49]</ref>, Optimized Kernel-Laplacian (OKL) <ref type="bibr" target="#b49">[50]</ref> and Temporal Joint Embeddings (TJE) <ref type="bibr" target="#b50">[51]</ref>.</p><p>Results: Table <ref type="table" target="#tab_0">I</ref> compares the performance of L-GrIN with all the methods mentioned above. Clearly, the proposed model outperforms all the existing methods by a significant margin, including the graph-based state-of-the-art architectures, such as PATCHY-SAN and PATCHY-Diff. Our model performs better than BLSTM -a class of models most commonly used in video-based emotion recognition. SENet is a very recent CNN architecture developed for emotion recognition, which also trails our model in terms of performance. When compared to the GCN baseline <ref type="bibr" target="#b14">[15]</ref>, L-GrIN improves the recognition accuracy by more than 10% on RML and eNTERFACE, and more than 5% on RAVDESS.</p><p>Also note that KCFA, OKL and TJE use both audio and visual information for recognition. Our model, even though uses only visual information, shows significant improvement over the audiovisual methods.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows the learned adjacency matrix for the RAVDESS database. The learned graph structure shows higher values closer to the diagonal i.e., the weights shared among the neighboring nodes. This indicates higher temporal dependencies locally and weaker dependency as we go further from a node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speech emotion recognition</head><p>Databases: We use the popular IEMOCAP database <ref type="bibr" target="#b56">[57]</ref> for evaluating the performance of our model on speech emo- Node features: We extract a set of low-level descriptors (LLDs) from the raw speech utterances as proposed for In-terspeech2009 emotion challenge <ref type="bibr" target="#b57">[58]</ref> using the OpenSMILE toolkit <ref type="bibr" target="#b58">[59]</ref>. The feature set includes Mel-Frequency Cepstral Coefficients (MFCCs), zero-crossing rate, voice probability, fundamental frequency (F0) and frame energy. For each sample, we use a sliding window of length 25ms with a stride length of 10ms to extract the LLDs locally. Each feature is then smoothed using a moving average filter, and the smoothed version is used to compute their respective first order delta coefficients. In addition, motivated by a recent work on speech emotion recognition <ref type="bibr" target="#b59">[60]</ref>, we also add spontaneity as a binary feature. The spontaneity information comes with the database. Altogether this produces node feature vectors of dimension P = 35.</p><p>Implementation details: Each audio sample produces a graph of M = 120 nodes, where each node corresponds to a (overlapping) speech segment of length 25ms. Cyclic padding is used to make the samples of equal length, as before. We perform a 5-fold cross-validation and report the average unweighted accuracy in Table <ref type="table" target="#tab_1">II</ref>. The unweighted accuracy, a standard evaluation strategy for IEMOCAP, does not take into account the class imbalances. It simply computes the total number of correct classifications across all classes. All other parameters and settings remain the same as before to show the generalizability of our model.</p><p>Baselines, state-of-the-art: Our model is compared with two baselines (BLSTM and GCN), two state-of-the-art graphbased architectures (PATCHY-SAN and PATCHY-Diff) as before. In addition, we also compare our model with four state-of-art methods in speech emotion recognition: CNN <ref type="bibr" target="#b34">[35]</ref>, CNN-LSTM <ref type="bibr" target="#b34">[35]</ref>, representation learning <ref type="bibr" target="#b55">[56]</ref> and LSTM with Connectionist Temporal Modeling (LSTM-CTC) <ref type="bibr" target="#b3">[4]</ref>.</p><p>Results: Table <ref type="table" target="#tab_1">II</ref> shows that our model performs better than the baselines and state-of-the-art methods on IEMOCAP. Our  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy (%) Parameters</head><p>Databases: We use the MPI emotional body expression database <ref type="bibr" target="#b60">[61]</ref> for our experiments. This database contains 1447 body motion samples of actors narrating coherent stories labeled with 11 emotions: amusement, anger, disgust, fear, joy, neutral, pride, relief, sadness, shame, and surprise. During their performance, a mocap system (device model: Xsens MVN) recorded the human motion using miniature inertial sensors. The system recorded dynamic 3D postures from 22 joints with a sampling rate of 120Hz.</p><p>Node features: For this database, we use the raw information provided by the mocap system. Each node contains the 3D positions and orientations (measure in terms of the Euler angles, pitch, yaw and roll) at a given time-step. These measurements come with the database. The feature consists of Euler angles from 22 joints and additional location information of the reference point. We use all the information (without any preprocessing) as node features, resulting into a vector of dimension P = 72.</p><p>Implementation details: Each input sample produces a graph of M = 120 nodes, where each node corresponds to a temporal segment of 120 th of a second. Cyclic padding is used as before. We perform a 5-fold cross-validation and report the  average accuracy in Table <ref type="table" target="#tab_2">III</ref>. All other network parameters remain the same as before.</p><p>Baselines, state-of-the-art: Our model is compared with the baselines (BLSTM and GCN), the state-of-the-art graphbased architectures (PATCHY-SAN and PATCHY-Diff), and a recent work on this database, i.e., trajectory learning <ref type="bibr" target="#b4">[5]</ref>. The trajectory learning system <ref type="bibr" target="#b4">[5]</ref> models neural motion and analyzes the spectral difference between an expressive motion and a neutral motion in order to recognize the body expressions.</p><p>Results: Table <ref type="table" target="#tab_2">III</ref> shows that L-GrIN outperforms the baselines and state-of-the-art methods on the MPI body expression database. Graph-based methods continue to perform well, indicating the effectiveness of graph-based methods for such tasks. Fig. <ref type="figure" target="#fig_3">4</ref> shows the learned adjacency A for the MPI database. As before, the learned graph structure exhibit higher temporal dependencies among the neighboring nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network analysis</head><p>Network size: Tables I, II and III list the number of learnable network parameters for the baselines, state-of-theart graph-based architectures and the proposed L-GrIN. As mentioned earlier, a graph network largely reduces the number of learnable parameters as compared to the BLSTM or CNN architectures such as SENet (see Table <ref type="table" target="#tab_0">I</ref>) without compromising the recognition accuracy. Our model has more parameters than the baseline GCN due to the inception layers and other learnable parameters, but also improves the recognition accuracy significantly. PATCHY-SAN and PATCHY-Diff have smaller network size compared to L-GRIN, but both trail L-GrIN in terms of performance on all databases. In case of facial emotion recognition, we discount the model size of the landmark detector in the comparison as it is common to all except SENet. For speech and body emotion recognition, no additional network was required as we used hand-crafted features and raw data. Learnable vs. fixed pooling: Recall that to obtain a graphlevel embedding from node-level embeddings, L-GrIN learns a pooling function (see Fig. <ref type="figure" target="#fig_2">2</ref>). To show if learnable pooling indeed improves the recognition performance, we compare its performance with various fixed pooling strategies: max pooling, mean pooling and sort pooling (sortpool) <ref type="bibr" target="#b61">[62]</ref>. Table <ref type="table" target="#tab_3">IV</ref> presents the comparisons on the RML database in terms of facial emotion recognition accuracy, which clearly shows the advantage of learnable pooling over fixed pooling strategies. Similar trend is observed for other databases.</p><p>Learnable vs. manually constructed adjacency: An adjacency matrix represents the pairwise relationship between the graph nodes. When this information is not available naturally, a common practice is to manually construct an adjacency matrix. We argued earlier that this may result in sub-optimal graph structures which in turn affects the classification performance. We now compare the performance of leranable adjacency with two fixed adjacency matrices: (i) Binary adjacency: a natural choice is a binary adjacency matrix as used for graph-based action recognition <ref type="bibr" target="#b12">[13]</ref>. This is defined as (A b ) ij = 1 if |i − j| = 1 and 0 otherwise, i.e., a node (frame) is connected only to its subsequent and preceding node in the temporal direction. (ii) Weighted adjacency: Another adjacency matrix is formed by using the squared 2 distance between two node attributes as their edge weight. This is defined as</p><formula xml:id="formula_16">(A w ) ij = n i − n j 2 2 .</formula><p>Table V compares the performance of the proposed learnable adjacency with the two fixed adjacency matrices described above on the RML, IEMOCAP and the MPI databases. We chose one database from every modality. For this set of experiments we used only maxpooling to obtain the graphlevel embeddings for fair comparison. Clearly, the learnable adjacency matrix shows consistent improvement in accuracy across all databases for a relatively small increase in model complexity (only 6% additional parameters). The results show that a learnable adjacency has better at generalizing across databases and modalities.</p><p>Ablation study: We performed exhaustive ablation experiments to investigate the contribution of each component we proposed to build L-GrIN.   component brings significant improvement (row 2 to row 5) over the performance of standard GCN <ref type="bibr" target="#b14">[15]</ref> which has 76.57% recognition accuracy (the top row in Table <ref type="table" target="#tab_5">VI</ref>).</p><p>The introduction of the graph inception layer increases the recognition rate by 11%; when combined with our new graph convolution layer G * conv (Eq. ( <ref type="formula" target="#formula_6">4</ref>)), the accuracy increases to 90.65%. Adding the learnable graph structure (learned A) and learnable pooling bring the accuracy up to 94.11% both contributing to the accuracy. Removing either of the leanrable components reduces the accuracy by 2.61%. The ablation results show that each of the proposed components in our architecture is important, and contributes positively towards its superior performance. Similar ablation trend was observed for other databases.</p><p>Inception layer settings: We also investigate the effects of the graph inception layer hyperparameters: (i) the parameter η corresponding to the size of the graph convolution filters G * 1 and G * 2 in Eq. ( <ref type="formula" target="#formula_7">5</ref>), and (ii) the number of graph inception layers in L-GrIN. First, we vary the filter dimensions (can be interpreted as scales) in the two inception layers and note how this correspond to the model's performance. Results for or increase filter sizes within the layers. We notice a small drop in performance with larger filter sizes and with higher number of inception layers. This could be possibly due to oversmoothing and over-mixing of the node features. However, the over-smoothing effect is not as prominent as in many node classification tasks. Analysis of the control weights: We also examine the impact of the weights controlling the various components of the loss function in Eq. ( <ref type="formula" target="#formula_12">8</ref>), i.e., λ 1 , λ 2 and λ 3 . Fig. <ref type="figure" target="#fig_5">6</ref> shows that highest performance is achieved for λ 1 = λ 2 = 0.1 and λ 2 = 0.0001 (marked red in the plots) on the RML database. We use these λ values in our experiments.</p><p>Cross-corpus performance: Methods exhibiting superior performance on one corpus, often fall short when tested on another corpus having different statistical distributions. We investigated the ability of our model to generalize across databases by evaluating its cross-corpus performance. To this end, we trained L-GrIN on one database, followed by finetuning a fully-connected layer on the target database, without changing the graph structure (or other parameters) learned from the training database.</p><p>Results in Table <ref type="table" target="#tab_8">VIII</ref> shows that our model can generalize well producing consistent results under cross-corpus evaluation. Our cross-corpus results higher accuracy compared to the same-corpus GCN accuracy. Cross-corpus results are comparable with the same-corpus performance of PATCHY-SAN. This shows the strength of the proposed architecture. It is worth noticing that the RML database (when used for training) does not have neutral and calmness emotion classes, but our model still recognizes those emotions on RAVDESS with 67.2% and 73.4% accuracy.</p><p>Network visualization: To get an insight into the learning process of our model, we visualized how it attends to different nodes. The video data are the most suitable for the visualization. We use our trained model, and then feed-forward each test video sample through the network, and identify the node (each node corresponds to a video frame) that responded most strongly towards the maxpooling layer. This yields a salient node corresponding to each input. We present the corresponding video frames -one example per emotion class for RML, eNTERFACE and RAVDESS databases in Fig. <ref type="figure">7</ref>. The results show that the proposed model is able to learn the salient information from the input graphs such that it is representative of each emotion.</p><p>V. CONCLUSION We proposed a novel, generalized graph architecture that can recognize emotion in a variety of dynamic input sequence. Our proposed architecture, L-GrIN, learns to detect emotion while jointly learning the underlying graph structure (adjacency matrix) and a pooling function to yield graph-level representation from node-level embeddings. We proposed a new spectral graph convolution operation and introduced the idea of inception in the graph domain. The advantage of our model lies in its state-of-the-art performance spanning three different modalities (video, audio and motion capture), with significantly fewer parameters compared to the CNNs and RNNs. This indicates that our model is suitable for applications in resource-constrained devices, such as smartphones.</p><p>We used both modality-specific features and even raw data as node features in this work. Our approach is not tied to any particular feature. In fact, our model can be trained end-to-end by combining it with modality-specific networks (e.g., a CNN) for feature extraction. The architecture we developed, although focuses on emotion recognition, is fairly generic. It will be applicable to a variety of classification tasks involving dynamic data, such as pose estimation, action recognition and visual speech recognition. Since our model makes no assumption about the graph structure, this is also applicable to common unstructured graphs. Future work will be directed towards building multimodal graph architectures taking advantage of the modality-agnostic architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A generalized graph approach to modeling emotion dynamics. Data samples are transformed to a learnable graph structure, where each node corresponds to a short temporal segment or frame. A novel graph architecture (L-GRIN) produces an embedding for the entire graph facilitating emotion recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our proposed architecture, L-GrIN, consists of two graph inception layers (with a new spectral graph convolution layer) and a pooling layer (two fixed pooling layers and a learnable pooling layer). The inception layers produce node-level representations that are pooled to obtain a graph-level representation by the pooling layer. L-GRIN also learns the underlying graph structure (adjacency matrix) by jointly optimizing a classification loss and a graph structure loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Learned adjacency matrices for facial and body emotion recognition showing strong temporal dependency between neighboring segments. Darker values indicate higher weights.</figDesc><graphic url="image-8.png" coords="5,441.95,310.54,97.87,107.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Motion capture recording set-up for the MPI database showing an actor posing for (left to right) T pose (reference), neutral and pride pose.</figDesc><graphic url="image-10.png" coords="7,51.94,49.83,240.70,106.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Effect of the weight parameters in the loss function; experiments on the RML database.</figDesc><graphic url="image-14.png" coords="8,353.99,352.83,173.18,87.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I FACIAL</head><label>I</label><figDesc>EMOTION RECOGNITION RESULTS ON THREE VIDEO DATABASES.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Accuracy (%)</cell><cell></cell><cell>Params</cell></row><row><cell></cell><cell>RML</cell><cell cols="2">eNTERFACE RAVDESS</cell><cell></cell></row><row><cell>*BLSTM</cell><cell>60.00</cell><cell>58.67</cell><cell>56.14</cell><cell>∼ 1M</cell></row><row><cell>*GCN [15]</cell><cell>76.57</cell><cell>69.81</cell><cell>69.34</cell><cell>∼ 102K</cell></row><row><cell>*PATCHY-SAN [19]</cell><cell>80.00</cell><cell>67.49</cell><cell>73.52</cell><cell>∼ 52K</cell></row><row><cell>*PATCHY-Diff [48]</cell><cell>85.59</cell><cell>76.96</cell><cell>79.83</cell><cell>∼ 71K</cell></row><row><cell>SENet [25]</cell><cell>71.20</cell><cell>79.22</cell><cell>71.06</cell><cell>∼ 26M</cell></row><row><cell>AVEF [6]</cell><cell>82.48</cell><cell>85.69</cell><cell>-</cell><cell>-</cell></row><row><cell>KCFA [49]</cell><cell>82.22</cell><cell>76.00</cell><cell>-</cell><cell>-</cell></row><row><cell>OKL [50]</cell><cell>90.83</cell><cell>86.67</cell><cell>-</cell><cell>-</cell></row><row><cell>TJE [51]</cell><cell>-</cell><cell>-</cell><cell>72.30</cell><cell>-</cell></row><row><cell>*L-GrIN</cell><cell>94.11</cell><cell>87.49</cell><cell>85.65</cell><cell>∼ 120K</cell></row></table><note>*use same node features RAVDESS (90 × 90)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II SPEECH</head><label>II</label><figDesc>EMOTION RECOGNITION RESULTS ON IEMOCAP DATABASE.</figDesc><table><row><cell>Model</cell><cell>Accuracy (%)</cell><cell>Params</cell></row><row><cell>*  BLSTM (baseline)</cell><cell>58.04</cell><cell>∼ 0.8M</cell></row><row><cell>*  GCN (baseline)</cell><cell>56.14</cell><cell>∼ 78K</cell></row><row><cell>*  PATCHY-SAN [19]</cell><cell>60.34</cell><cell>∼ 60K</cell></row><row><cell>*  PATCHY-Diff [48]</cell><cell>63.23</cell><cell>∼ 68K</cell></row><row><cell>CNN [35]</cell><cell>58.52</cell><cell>∼ 0.45M</cell></row><row><cell>CNN-LSTM [35]</cell><cell>59.23</cell><cell>∼ 0.6M</cell></row><row><cell>Rep learning [56]</cell><cell>50.40</cell><cell>-</cell></row><row><cell>LSTM-CTC [4]</cell><cell>64.20</cell><cell>∼ 0.4M</cell></row><row><cell>*  L-GrIN</cell><cell>65.50</cell><cell>∼ 92K</cell></row><row><cell>*  use same node features</cell><cell></cell><cell></cell></row><row><cell cols="3">tion recognition. This database contains a total of 12 hours</cell></row><row><cell cols="3">of data recorded in 5 sessions, where each session contains</cell></row><row><cell cols="3">utterances from two speakers. The final database contains a</cell></row><row><cell cols="3">total of 5531 utterances: 1103 angry, 1708 neutral, 1636 happy</cell></row><row><cell>and 1084 sad.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III BODY</head><label>III</label><figDesc>EMOTION RECOGNITION RESULTS ON THE MPI DATABASE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>BETWEEN LEARNABLE AND FIXED POOLING STRATEGIES ON THE RML DATABASE. ALL EXPERIMENTS IN THIS TABLE USE THE SAME (BINARY) ADJACENCY MATRIX FOR FAIR COMPARISON.</figDesc><table><row><cell>Pooling</cell><cell>Accuracy (%)</cell></row><row><cell>Maxpool</cell><cell>89.76</cell></row><row><cell>Meanpool</cell><cell>90.23</cell></row><row><cell>Sortpool [62]</cell><cell>83.66</cell></row><row><cell>Learnable pool</cell><cell>91.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON</head><label>V</label><figDesc>BETWEEN LEARNABLE AND MANUALLY CONSTRUCTED GRAPH STRUCTURES. FOR FAIR COMPARISON, ALL EXPERIMENTS USE MAXPOOL TO CONVERT NODE EMBEDDINGS TO GRAPH EMBEDDINGS.</figDesc><table><row><cell></cell><cell></cell><cell>Accuracy (%)</cell><cell></cell><cell></cell><cell>Params</cell><cell></cell></row><row><cell></cell><cell>RML</cell><cell>IEMOCAP</cell><cell>MPI</cell><cell>RML</cell><cell>IEMOCAP</cell><cell>MPI</cell></row><row><cell>Binary</cell><cell>89.5</cell><cell>61.4</cell><cell>53.6</cell><cell>113K</cell><cell>78K</cell><cell>96K</cell></row><row><cell>Weighted</cell><cell>62.4</cell><cell>54.3</cell><cell>49.0</cell><cell>113K</cell><cell>78K</cell><cell>96K</cell></row><row><cell>Learnable</cell><cell>91.5</cell><cell>65.5</cell><cell>58.9</cell><cell>120K</cell><cell>92K</cell><cell>110K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDY ON THE RML DATABASE. EACH NEW COMPONENT IN L-GRIN CONTRIBUTES TOWARDS ITS PERFORMANCE.</figDesc><table><row><cell cols="3">G  *  conv Inception Learned A</cell><cell cols="2">Learned p Accuracy (%)</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.57</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.12</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>87.58</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>79.78</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>82.86</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>84.21</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>90.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>91.50</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>91.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>94.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VI presents the ablation results on the RML database. We observe that each new</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII ANALYZING</head><label>VII</label><figDesc>INCEPTION LAYER SETTINGS ON THE RML DATABASE.</figDesc><table><row><cell cols="2">Effect of filter size (η)</cell></row><row><cell>Size of the two filters</cell><cell>Accuracy (%)</cell></row><row><cell>(16, 32)</cell><cell>90.82</cell></row><row><cell>(32, 64)</cell><cell>92.47</cell></row><row><cell>(64, 128)</cell><cell>94.11</cell></row><row><cell>(128, 256)</cell><cell>93.13</cell></row><row><cell cols="2">Effect of number of inception layers</cell></row><row><cell>Number of layers</cell><cell>Accuracy (%)</cell></row><row><cell>1</cell><cell>91.77</cell></row><row><cell>2</cell><cell>94.11</cell></row><row><cell>3</cell><cell>90.78</cell></row><row><cell>Accuracy (%)</cell><cell></cell></row><row><cell cols="2">𝝀 1 𝝀 2</cell></row><row><cell>Accuracy (%)</cell><cell></cell></row><row><cell>𝝀 3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII CROSS</head><label>VIII</label><figDesc>-CORPUS PERFORMANCE OF OUR MODEL (L-GRIN) FOR FACIAL EMOTION RECOGNITION. RML database is presented in Table VII; similar trends have been observed for other databases. Results in Table VII show that we achieve the best performance for the combination of (64, 128), which is used in our model. Next, we vary the number of inception layers in the model, each with (64, 128) filter combination (see Table VII. We observe that reducing or increasing the number of inception layers from 2 results in a drop in performance. We chose to use two inception layers in the proposed model. It is obvious that the model size increases significantly as we add more inception layers</figDesc><table><row><cell>Trained on</cell><cell>Evaluated on</cell><cell>Accuracy (%)</cell></row><row><cell>RAVDESS</cell><cell>RML</cell><cell>81.94</cell></row><row><cell></cell><cell>eNTERFACE</cell><cell>75.80</cell></row><row><cell>RML</cell><cell>RAVDESS</cell><cell>75.42</cell></row><row><cell></cell><cell>eNTERFACE</cell><cell>61.71</cell></row><row><cell>eNTERFACE</cell><cell>RML</cell><cell>79.86</cell></row><row><cell></cell><cell>RAVDESS</cell><cell>77.51</cell></row></table><note>the</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal 2d+ 3d facial expression recognition with deep fusion convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2816" to="2831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A deep spatial and temporal aggregation framework for video-based facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">815</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised speech emotion recognition with ladder networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2697" to="2709" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards temporal modelling of categorical speech emotion recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="932" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an efficient body expression recognition based on the synthesis of a neutral movement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Crenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Konik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bouakaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio-visual emotion fusion (avef): A deep efficient weighted approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Košir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring discriminative representations for image emotion recognition with cnns</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="515" to="523" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognition of emotions in user-generated videos with kernelized features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2824" to="2835" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning affective correspondence between music and image</title>
		<author>
			<persName><forename type="first">G</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Dhekane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3975" to="3979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual scene-aware hybrid neural network architecture for video-based facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical video frame sequence representation with deep convolutional graph network</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
				<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linkage based face clustering via graph convolution network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1117" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph-based semisupervised learning for acoustic modeling in automatic speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1946" to="1956" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph wavelet neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A compact embedding for facial expression similarity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5683" to="5692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Occluded facial expression recognition enhanced through privileged information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="566" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2893" to="2901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feratt: Facial expression recognition with attention net</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Marrero Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Guerrero Pena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Emotion-aware human attention prediction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Cordel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4026" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning the dynamic appearance and shape of facial action units</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using cnn-rnn and c3d hybrid networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="445" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-cue fusion for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">309</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using cnn</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="801" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Direct modelling of speech emotion from raw speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jurdak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="3920" to="3924" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic speech emotion recognition using recurrent neural networks with local attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirsamadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2227" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auditoryinspired end-to-end speech emotion recognition using 3d convolutional recurrent neural networks based on spectral-temporal representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia &amp; Expo (ICME)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo (ICME</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mutual correlation attentive factors in dyadic fusion networks for speech emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-attention for speech emotion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tarantino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2578" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identifying emotions from walking using affective and deep features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1906">1906.11884, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">STEP: spatial temporal graph convolutional networks for emotion perception from gaits</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1910">1910.12906, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recognizing human emotional state from audiovisual signals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="936" to="946" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The enterface&apos;05 audiovisual emotion database</title>
		<author>
			<persName><forename type="first">O</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering Workshops (ICDEW)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="8" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">e0196391</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Kernel cross-modal factor analysis for information fusion with application to bimodal emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="597" to="607" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A combined rule-based &amp; machine learning audio-visual emotion recognition approach</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multimodal and temporal perception of audio-visual cues for emotion recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ghaleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Asteriadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing &amp; Intelligent Interaction (ACII)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic facial analysis: From bayesian filtering to recurrent neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1548" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lstm-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Representation learning for speech emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laksana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3603" to="3607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language resources and evaluation</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The interspeech 2009 emotion challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Openear-introducing the munich open-source emotion and affect recognition toolkit</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interactions</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning spontaneity to improve emotion recognition in speech</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="946" to="950" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The mpi emotional body expressions database for narrative scenarios</title>
		<author>
			<persName><forename type="first">E</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bulthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
