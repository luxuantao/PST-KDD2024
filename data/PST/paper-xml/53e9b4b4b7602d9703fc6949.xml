<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaser</forename><surname>Yacoob</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742-3275</postCode>
									<settlement>College Park</settlement>
									<country>M D</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742-3275</postCode>
									<settlement>College Park</settlement>
									<country>M D</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">068C9A9D2A9DC218F30E8A28489A0ABB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face expression recognition</term>
					<term>non-rigis motion analysis</term>
					<term>optical flow</term>
					<term>tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An approach to the analysis and representation of facial dynamics for recognition of facial expressions from image sequences is presented. The algorithms utilize optical flow computation to identify the direction of rigid and nonrigid motions that are caused by human facial expressions. A mid-level symbolic representation motivated by psychological considerations is developed. Recognition of six facial expressions, as well as eye blinking, is demonstrated on a large set of image sequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the influence of the motion and deformation of facial features on the recognition of facial expressions. Bassili 121 showed that motion in the image of a face would allow expressions to be identified even with minimal information about the spatial arrangement of features. His subjects viewed image sequences in which only white dots on the darkened surface of the face displaying the expression were visible (see Fig. <ref type="figure">2</ref>). Notice that the face features, texture and complexion were unavailable to the subjects. The figure also shows the principal facial motions that provide cues to the recognition of facial expressions. Bassili's experimental results indicate that facial expressions were more accurately recognized from dynamic images than from a single static "mug-shot" image. The problem of recognizing facial expressions has recently attracted attention in the computer vision community <ref type="bibr">[3]</ref>, 191, 1111,   1121, 1131, [141, [151. The work of [91, [121, [141 is most closely related to ours since they use optical flow computation for recognizing and analyzing facial expressions. Mase approached facial expression recognition from both the top-down and bottom-up directions. In both cases, the focus was on computing the motions of facial muscles rather than the motions of facial features. Four facial expressions were studied: surprise, anger, happiness, and disgust.</p><p>Essa and Pentland [9] and Essa [lo] recently proposed a physically based approach for modeling and analyzing facial expressions. They proposed extending the FACS model to the temporal dimension (thus calling it FACS+) to allow combined spatial and temporal modeling of facial expressions. They assumed that a mesh is originally overlayed on the face, then tracked all its vertices based on the optical flow field throughout the sequence. The emphasis is on accuracy of capturing facial changes, which is most essential to synthesis. Recognition results were reported in 1101 on six subjects displaying four expressions and eyebrow raising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PRQPQsED ARCHITE~TURE</head><p>Before proceeding, we introduce some terminology that is needed in the paper. Face region motion refers to changes in images of facial features caused by facial actions corresponding to physical feature deformations on the 3D surface of the face. The goal is to 0162.8828 /96$05.00 01996 IEEE develop computational methods that use face region motions as cues for action recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Happiness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surprise</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sadness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anger</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fear</head><p>Disgust Fig. <ref type="figure">2</ref>. The cues for facial expression as suggested by <ref type="bibr">Bassili [2]</ref> The following is the framework within which our approach for</p><p>The face is viewed from a near frontal view throughout the sequence.</p><p>The overall rigid motion of the head is small between consecutive frames.</p><p>The nonrigid motions resulting from face deformations are spatially bounded, in practice, by an n x n window between any two consecutive frames. We consider only the six universal expressions-happiness, sadness, anger, fear, disgust and s u r p r i s e a n d blinking. Fig. <ref type="figure" target="#fig_1">3</ref> describes the flow of computation of our facial expression analysis and recognition of facial expressions is developed:</p><formula xml:id="formula_0">0 * system.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRACKING FACE REGIONS</head><p>The approach we use for optical flow computation is a correlation approach recently proposed by <ref type="bibr">Abdel-Mottaleb et al. [l]</ref>. Our choice stems from its reliability when only small subpixel motions occur, and employing minimal assumptions on the structure of the image and the motions occuring.</p><p>Locating the facial features in an image has been extensively addressed in the past five years [5], [16], <ref type="bibr">[18]</ref>. An extensive review of this subject appears in <ref type="bibr">[6]</ref>. In our work we simply assume that rectangles enclosing the facial features have been initially placed before the onset of tracking.  Since our approach is based on a statistical characterization of the motion patterns in specified regions of the face, and not in tracking individual point motions, we develop a region tracker for rectangles enclosing the face features. Each rectangle encloses one feature of interest, so the flow computation within the region is not "contaminated by the motions of other facial features. To simplify the modeling of the eyebrows, we define the rectangles to include the eyes, and then subtract the rectangle of the eye from the combined rectangle.</p><p>The tracking algorithm integrates spatial and temporal information at each frame. The former is based on the gradient magnitudme of the intensity image and the latter is based on the optical flow field. The spatial tracking of the face regions is based on computing two sets of parameters at the points with high gradient values within the rectangle that encloses each feature. We assume that the gradient image is relatively stable between any two images (a reasonable assumption considering that the face is sampled at 3OHz and the lighting is stable). The following are computed from frame i after placing the rectangles from frame i -1 over the image in frame i:</p><p>The centroid (C:, Ci) of the points having a high gradient value within each rectangle in frame i.</p><p>The window w = (wx;!l,l -2, w;,, -2, wx:n,, + 2, w;,, + 2) which encloses those high gradient values and leaves a buffer, two pixels deep, that allows the detection of window expansion during subsequent iterations.</p><p>The centroids location determines the translation of the rectangle from the previous frame. The window W determines the scaling of the rectangle.</p><p>In order to enhance the tracking the statistics of the motion directions within a rectangle are used to verify translation of rectangles upward and downward (by measuring significant similar opt:ical flow) and verify scaling of the rectangles (by measuring motions that imply scaling).</p><p>1.n spite of the simplicity of the tracking algorithms, it was quite robust, routinely tracking face features for hundreds of frames with no manual intervention. In more recent work 141, Black and Yacoob report a new robust approach for tracking facial features while undergoing significant rigid and nonrigid motions. Their approach models the motion of the head as a plane moving in 3D, while allowing each feature to undergo extended affine deformations. Our dictionary of facial feature actions borrows from the facial cues of universal expression descriptions proposed by <ref type="bibr">Ekman and Friesen [7]</ref>, and from the motion patterns of expression proposed by <ref type="bibr">Bassili [2]</ref>. As a result, we arrive at a dictionary that is a motionbased feature descviption ojfacial actions.</p><p>The dictionary is divided into components, basic actions of these components, and motion cues. The components are defined qualitatively and relative to the rectangles surrounding the face regions, the basic actions are determined by the component's visible deformations, and the cues are used to recognize the basic actions using optical flow within these regions. Table <ref type="table">1</ref> shows the components, basic actions, and cues that model the mouth. Similar models were created for the eyes and the eyebrows. Basic actions cues are computed from the optical flow field as follows. The flow magnitudes are first thresholded to reduce the effect of small motions due to noise. The motion vectors are then requantized into eight principal directions.</p><p>The optical flow vectors are filtered using both spatial and temporal procedures that improve their coherence and continuity, respectively. The spatial procedure examines the neighborhood of each point and performs a voting among all neighbors and enforces coherence of its direction label. The temporal procedure follows the spatial procedure; it uses a fixed temporal window to determine the plurality's flow direction, again changing the flow direction at the center of a temporal window if it disagrees with the plurality.</p><p>Statistical analyses of the resulting flow directions within each face region window provide indicators about the general motion patterns that the face features undergo. The statistical analyses differ from one feature to another, based on an allowable set of motions. The largest set of motions is associated with the mouth since it has the most degrees of freedom at the anatomic and musculature levels. we use it as an example to illustrate the procedure.</p><p>We measure the motion of the mouth by considering a set of vertical and horizontal partitions of its surrounding rectangle. The horizontal partitions are used to capture vertical motions of the mouth. These generally correspond to independent motions of the lips. The two types of vertical partitions are designed to capture several mouth motions. Single vertical partitions capture mouth horizontal expansions and contractions when the mouth is not completely horizontal. The two vertical partitions are designed to capture the motion of the corners of the mouth.</p><p>The partitions use free-sliding dividers. For each possible partition, P, and for every side of the divider of P we define the following parameters: m+he total number of points on this side of the divider.</p><p>* c4--the number of points having a motion vector direction q(q = 1,2, ..., 8) on this side of the divider. p 4 = cq/m--the percentage of points with motion vectors in direction q. p4 indicates the degree of clustering of the motion across the divider, while cq indicates the "strength," in count, of the motion in direction 7. The confidence measure of the motion's homogeneity and strength that a divider creates in a direction q is given by (applicable to any type of divider):</p><p>(1) Within each type of partition, partitions are ranked according to the values H q .</p><p>These measurements are used to construct the mid-level representation of a region motion. The highest ranking partition in each type is used as a pointer into the dictionary of motions (see Table <ref type="table">l</ref>), to determine the action that may have occurred at the feature. The set of all detected facial actions is used in the following section for recognizing facial expressions. Hq = , q . p q 5 RECOGNlZlNG FAClAL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Temporal Considerations for Recognizing</head><p>In the following, we assume that the face's initial expression is neutral.' We divide every facial expression into three temporal 1. The assumption of a neutral expression is not limiting. In an online environment (or when extended viewing is available), it is reasonable to assume that the portion of time a person expresses emotion is short, so the system could examine the relatively long periods of no motion and designate them as neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expressions</head><p>segments: the beginning, the apex, and the ending. Fig. <ref type="figure" target="#fig_3">4</ref> shows the temporal segments of a smile model. Since the outward-upward motion of the mouth corners are the principal cues for a smile motion, these are used as the criteria for temporal classification also. Notice that Fig. <ref type="figure" target="#fig_3">4</ref> indicates that the detection of mouth corner motions might not occur at the same frames at either the beginning or ending of actions, but it is required that at least one corner starts moving to label a frame with a "beginning of a smile" label, while the motions must terminate before a frame is labeled as an "apex" or an "ending."</p><p>We have designed a rule based system that combines some of the expression descriptions from 171 and [2]. Table <ref type="table">2</ref> shows the rules used in identifying the onsets of the "beginning" and the "ending" of each facial expression. These rules are applied to the mid-level representation to create a complete temporal map describing the evolving facial expression. This is demonstrated by an example of detecting a happiness expression. The system identifies the first frame, fi, with a "raising mouth corners" action, and verifies that the frames following fi show a region or basic action that is consistent with this action (in this case, it can be one of the following: right or mouth corner raised, or mouth expansion with/without some opening). It then locates the first frame f2 where the motions within the mouth region stop occurring (verified with later frames, as before). Then, it identifies the first frame, f3 , in which an action "lowering mouth corners" is detected and verifies it as before. Finally, it identifies the first frame, f4 , where the motion is stopped and verifies it. The temporal labeling of the smile expression will have the frames (fi ... f2l), (f2 ... f3l), and (f3 ... f4) as the "beginning," "apex," and "ending" of a smile, respectively.</p><p>Due to errors in the optical flow, some temporal cues need "assistance" from the spatial cues such as the size and shape changes in the rectangles tracking the facial features. Such changes are currently computed only for the mouth since the Feature location (abstracted) A accuracy of measuring size changes of other face features was not sufficient at the spatial resolution employed. For example, an "anger" expression is characterized by inward lowering motion of the eyebrows and by compaction of the mouth. The compaction may be hard to detect from the optical flow due to noise, aperture, or tracking inaccuracies. We measure the aspect ratios of the window surrounding the mouth during the hypothesized start of an expression, and verify that some compaction in the window size occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Resolving conflicts between expressions</head><p>The system is designed to identify (i.e., to detect an occurrence of an expression) and recognize facial expressions from long video clips (in this case, clips including three to six expressions varied in dui-ation and with neutral expressions in-between). Fig. <ref type="figure" target="#fig_5">5</ref> illustrates the high level architecture of the system. We simplified the behavior model of our subjects by asking them to display one expression at a time, and include an arbitrary duration of a neutral state between expressions. Since the six expression classifiers operate in parallel on the whole sequence the system may create conflicting hypotheses as to the occurrence of facial expressions.</p><p>Conflicts may arise when an ending of one expression is confused as the beginning of another expression (quite a likely event in (a long sequence containing several expressions). For example, the "anger" recognition module may consider the lowering of the eyebrows during the ending of a "surprise" expression as the beginning of an "anger" expression. To resolve such conflicts, we employ a memory-based process that gives preference to the expression that started earlier, and we also assume that initially (i.e., the first frame) a neutral expression is displayed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>Our experimental subjects were asked to display expressions wiihout additional directions (in fact, we asked the subjects to  choose any subset of expressions and display them in any order and as naturally as they possibly could). As , a result, we acquired a variety of presumably similar facial expressions; some were consistent with Ekman and Friesen's dictionary 171 for static images and Bassili's 121 dictionary for motion images, while others varied considerably. This variance can be attributed to the real variance in dynamics and intensities of expressions of individuals as well as to the artificial environment in which the subjects had to develop facial expressions that indicate emotions they were not feeling at the time Our database of image sequences includes 32 different faces  <ref type="figure" target="#fig_7">6</ref>). For each face several expressions were recorded each lasting between 15-120 frames of 120 x 160 pixels (at 30 frames per second), some expressions recurring. We requested each subject to display the expressions of emotion in front of the video camera while minimizing his/her head motion. Nevertheless, subjects inevitably moved their head during a facial expression.</p><p>In our sample of 46 image sequences of 32 subjects, we had a total of 116 expressions and 106 blinkings (65 percent recognition rate for the latter). The recognition is relative to a ground truth recognition that we established by viewing the video clips and visually identifying the expressions based on the psychology lit-    <ref type="table" target="#tab_2">3</ref>). This ground truth recognition also identified the 'beginning' and 'ending' of expressions, which were found to vary slightly from the automatic system's detected 'beginnings' and 'endings' (measureable optical flow turned out to occur earlier than our visual recognition of such motionv-possibly due to different levels of thresholds). Table <ref type="table" target="#tab_2">3</ref> shows the details of our results in the form of a confusion matrix (the left column indicates the automatic system recognition and the upper line indicates the ground truth). Occurrences of fear and sadness are less frequent than happiness, surprise and anger. Notice that a 'neutral' category was added (as a "reject" category). Some occurrences of expressions are classified as 'neutral' (i.e., rejected) due to reasons listed in the table.</p><p>Some confusion of expressions occurred between the following pairs: fear and surprise, anger and disgust, and sadness and surprise. These distinctions rely on subtle coarse shape and motion information that were not always accurately measured.  With the exception of the computation of optical flow, the algorithms operate at near frame-rates. Since the correlation-based optical flow performs an exhaustive search, its computational cost is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUMMARY AND CONCLUSIONS</head><p>An approach to analyzing and classifying facial expressions from optical flow was proposed. This approach is based on qualitative tracking of principal regions of the face and flow computation at high intensity gradient points. A mid-level representation is computed from the spatial and the temporal motion fields. The representation is motivated by the psychology literature 121, <ref type="bibr">[7]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Six universal (i.e., pancultural) expressions expressed by six faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The flow of the facial analysis algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>an upper part of window downward motion of an upper part of window horizontal shrinking of an upper part of window horizontal expansion of an upper part of window upward motion of a lower part of window downward motion of a lower part of window horizontal shrinking of a lower pari of window horizontal expansion of a lower part of window upward motion of a left part of window downward motion of a left Dart of window upward motion of a right part of window downward motion of a right part of window upward motion throughout window downward motion throughout window overall shrinkage in mouth's size overall exDansion in mouth's size O M P ~T ~N G LOCAL OTION REP R ESE NTATlO NS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The temporal model of the "smile" expression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Beginning lowering mouth corners, raising mid mouth and raising inner parts of brows lowering mouth corners, lowering mid mouth and lowering inner parts of brows slight expansion and lowering of mouth and raising inner parts of brows slight contraction and raising of mouth and lowering inner parts of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The high level flow of the facial analysis system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>32 I o I o l o I o I o I o I Key: S = Subtle expression, RM = Rigid Motion, A P = Aperture dominated motion, FE = Following closely an expression erature descriptions (the top horizontal line of Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Twelve faces (out of about 32) used in experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 Fig. 7 .</head><label>77</label><figDesc>Fig. 7. Four frames analyzed by the facial expression system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7</head><label>7</label><figDesc>Fig.7shows the detection of the beginning of a 'fear' expression; the main cues are the inward raising of the eyebrows and the opening of the mouth. The detection of the inward motion of the eyebrows takes place by analysis of the optical flow field in the rectangles.With the exception of the computation of optical flow, the algorithms operate at near frame-rates. Since the correlation-based optical flow performs an exhaustive search, its computational cost is high.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 CONFUSION</head><label>3</label><figDesc>MATRIX FOR FACIAL EXPRESSION RECOGNITION RESULTS</figDesc><table><row><cell>Visual Automatic</cell><cell>Smile</cell><cell>Anger</cell><cell>Surprise Disgust Fear Sadness Neutral</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The support of the Defense Advanced Research Projects Agency (ARPA Order No. 6989) and the US. Army Topographic Engineering Center under Contract DACA76-92-C-0009 is gratefully acknowledged.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We carried out experiments on over 30 subjects in a laboratory [I71 Handbook of Research on Face Processing, A.W. Young and E D . Further study of the system's components will be carried out as well as expanding its capability to deal with nonemotion facial messages. Specifically, the richness of facial expression requires developing more sophisticated representations and capabilities at all levels. At the lowest level, the optical flow and tracking have to be improved to process real video clips that involve rigid, articulated and nonrigid motions of the subject (see 141). At the midlevel, the representation of actions, spatially and temporally, can be enhanced to capture more of facial behaviors. The incorporation of shape in addition to motion analysis may be useful in refining the representation. At the highest level, there is a need to develop more complex models that are able to capture the diversity of facial actions both from a single subject and across subjects. and pp, <ref type="bibr">104-109, 1989,</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Binocular Motion Stereo Using MAP Estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattevn Recognition</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="321" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotion Recognition: The Role of Facial Movement and the Relative Importance of Upper and Lower Areas of the Face</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Bassili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality and Social PsychoIogy</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note>2,049-2,059</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Example Based Image Analysis and Synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">M.I.T. A.I. Memo</title>
		<imprint>
			<biblScope unit="volume">1431</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking and Recognizing Rigid and Non-Rigid Facial Motions Using Local Parametric Models of Image Motions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Computer Vision</title>
		<meeting>Int&apos;l Conf. Computer Vision<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human and Machine Recognition of Faces: A Survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brunelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Sirohey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Barnes ; Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<idno>CAR-TR-731</idno>
	</analytic>
	<monogr>
		<title level="m">Unmasking the Face</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1975">1993. Aug. 1994. 1975</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="42" to="43" />
		</imprint>
		<respStmt>
			<orgName>Center for Automation Research, Univ. of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Face Recognition: Features Versus Templates</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Facial Action Coding System</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Consulting Psychologists Press</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Vision System for Observing and Extracting Facial Action Parameters</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><surname>Prai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Ieee Cvpr</surname></persName>
		</author>
		<author>
			<persName><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">M.I.T. Media Laboratory</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Analysis, Interpretation, and Synthesis of Facial Expressions. Perceptual Computing Group Report No. 303</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Motion Estimation in Model-Based Facial Image Coding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roivainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forcheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Iritelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3" to="474" />
			<date type="published" when="1991">1993. 1991</date>
		</imprint>
	</monogr>
	<note>IEICE Trans.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human Emotion Recognition from Motion Using a Radial Basis Function Netbvork Architecture</title>
		<author>
			<persName><forename type="first">K</forename><surname>Matsuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsuji ; Puoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eccv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">lEEE Workshop Motion of Non-Rigid and Articuh:&lt;.i Objects</title>
		<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11">Nov. 1994</date>
			<biblScope unit="page" from="43" to="49" />
		</imprint>
	</monogr>
	<note>Recognition of Human Facial Expressions Without Feature Extraction</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis and Synthesis of Facial lmage Sequences Using Physical and Anatomical Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">l E E E Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="569" to="579" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Labeling of Human Face Components from Range Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image Understanding</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="520" />
			<date type="published" when="1994">1994. 1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
