<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asymptotically Stable Adaptive-Optimal Control Algorithm With Saturating Actuators and Relaxed Persistence of Excitation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Kyriakos</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
							<email>kyriakos@ece.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Systems and Computation</orgName>
								<orgName type="institution">University of California at Santa Barbara</orgName>
								<address>
									<postCode>93106-9560</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcio</forename><forename type="middle">Fantini</forename><surname>Miranda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Systems and Computation</orgName>
								<orgName type="institution">University of California at Santa Barbara</orgName>
								<address>
									<postCode>93106-9560</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Colégio Técnico</orgName>
								<orgName type="institution" key="instit2">Universidade Federal de Minas Gerais</orgName>
								<address>
									<postCode>31270-901</postCode>
									<settlement>Belo Horizonte</settlement>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">João</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
							<email>hespanha@ece.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Systems and Computation</orgName>
								<orgName type="institution">University of California at Santa Barbara</orgName>
								<address>
									<postCode>93106-9560</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Asymptotically Stable Adaptive-Optimal Control Algorithm With Saturating Actuators and Relaxed Persistence of Excitation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">85DDB81FDBBAFA1BFC40A9EDE073B643</idno>
					<idno type="DOI">10.1109/TNNLS.2015.2487972</idno>
					<note type="submission">received February 5, 2014; revised March 12, 2015, August 22, 2015, and September 30, 2015; accepted October 3, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Approximate dynamic programming (ADP)</term>
					<term>optimal control</term>
					<term>reinforcement learning</term>
					<term>saturating actuators</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a control algorithm based on adaptive dynamic programming to solve the infinite-horizon optimal control problem for known deterministic nonlinear systems with saturating actuators and nonquadratic cost functionals. The algorithm is based on an actor/critic framework, where a critic neural network (NN) is used to learn the optimal cost, and an actor NN is used to learn the optimal control policy. The adaptive control nature of the algorithm requires a persistence of excitation condition to be a priori validated, but this can be relaxed using previously stored data concurrently with current data in the update of the critic NN. A robustifying control term is added to the controller to eliminate the effect of residual errors, leading to the asymptotically stability of the closed-loop system. Simulation results show the effectiveness of the proposed approach for a controlled Van der Pol oscillator and also for a power system plant.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>therein-can only guarantee the uniform ultimate boundedness of the closed-loop system, i.e., a milder form of stability <ref type="bibr" target="#b10">[11]</ref> in the sense of Lyapunov, and require an a priori knowledge of a persistence of excitation (PE) condition. The need for adaptive controllers with the ability to learn optimal solutions, while still guaranteeing asymptotic stability, motivates our research. The algorithm proposed uses the structure of a reinforcement learning algorithm, called policy iteration (PI), which is inspired by behavioral psychology <ref type="bibr" target="#b11">[12]</ref>. This two-step algorithm has an actor/critic structure that involves two neural networks (NNs). One, the critic NN, is trained to become an approximation of the value function solution at the policy evaluation step, while the second one is trained to approximate an optimal policy at the policy improving step.</p><p>In industrial applications, physical inputs to devices (such as voltages, currents, flows, and torques) <ref type="bibr" target="#b11">[12]</ref> are subject to saturations, which must be considered in the optimal control problem. To the best of our knowledge, there are no asymptotically stable online solutions to the continuous-time HJB equation with saturations, since they add additional nonlinearities to the HJB equation and make the problem more difficult. This challenge is also addressed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Lyshevski <ref type="bibr" target="#b12">[13]</ref> proposed a nonquadratic functional that involves bounded control inputs but does not provide solutions to the HJB equation, and based on that, Abu-Khalaf and Lewis <ref type="bibr" target="#b13">[14]</ref> used NNs to solve the HJB equation offline. Novel iterative two-stage dual heuristic programming (DHP) to solve the optimal control problem for a class of switched discrete-time systems subject to actuators saturation has been introduced in <ref type="bibr" target="#b14">[15]</ref>. Kiumarsi and Lewis <ref type="bibr" target="#b15">[16]</ref> proposed a partially model-free adaptive-optimal control solution to the deterministic nonlinear discrete-time tracking control problem in the presence of input constraints. The work of Yang and Jagannathan <ref type="bibr" target="#b16">[17]</ref> develops a learning framework for computing the HJB solution of discrete-time systems under bounded disturbances, but Yang and Jagannathan only proved uniform ultimate boundedness. Jiang and Jiang <ref type="bibr" target="#b17">[18]</ref> derived a gain condition for global asymptotic stability that allows the presence of dynamic uncertainties with an unmeasured state and unknown system order/dynamics. However, the PI algorithm is not performed in a synchronous manner, i.e., here, the policy evaluation and policy improvement steps take place simultaneously. Recently, Song et al. <ref type="bibr" target="#b18">[19]</ref> proposed an approximate dynamic programming method to solve the optimal control problem for complex-valued systems. Several adaptive/critic designs have been applied in real-time applications, such as <ref type="bibr" target="#b19">[20]</ref>, where <ref type="bibr">Liang et al.</ref> proposed an intelligent adaptive/critic controller to provide a nonlinear optimal control to a smart grid operation in an environment with high shortterm uncertainty and variability.</p><p>Temporal-difference (TD) algorithms <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref> incrementally update the policies based on each individual experience. The combination of experience replay <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and a fitted Q iteration <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, with online TD methods, have been shown in the past to speed up learning in lowdimensional control problems. But again, due to the large state space, value function approximation is a necessity, violating the assumptions for guaranteed convergence and, thus, leaving room for asymptotic performance gains as well. Fairbank and Alonso <ref type="bibr" target="#b25">[26]</ref> and Si et al. <ref type="bibr" target="#b26">[27]</ref> showed that the DHP algorithms can eventually find an optimal solution without the explicit need for stochastic exploration, but the value learning algorithms [i.e., TD and TD(0)] could not. In particular, the work of Werbos <ref type="bibr" target="#b27">[28]</ref> states that, for any given plant, the data from optimal and suboptimal action strategies can be used without difficulty in system identification, which means that DHP has less excitation problems. There is an extensive research work on the Markov decision processes (MDPs) and linear discrete-time systems that is not readily applied to control systems, does not consider input constraints, and does not have proper convergence proofs. In this direction, the effort of residual gradients <ref type="bibr" target="#b28">[29]</ref> or Galerkinized methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, where the critic NN weights do not converge to the optimal values in the stochastic case <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, but converge only in the deterministic case. The aforementioned limitations of Galerkinized methods have been overcome in <ref type="bibr" target="#b33">[34]</ref>, where <ref type="bibr">Maei et al. solved</ref> this problem under the standard technical assumptions. Fairbank et al. <ref type="bibr" target="#b34">[35]</ref> proposed a variant of DHP that has a guaranteed convergence, under certain smoothness conditions and a greedy policy, when using a general smooth nonlinear function approximator for the critic.</p><p>In MDPs, it is known that the conventional TD methods do not use trajectory data efficiently, since after a gradient update, the state transitions and rewards are ignored. The leastsquares temporal-difference (LSTD) learning algorithm <ref type="bibr" target="#b35">[36]</ref>, on the other side, is known for its efficient use of sample experiences compared with pure TD. In spite of the fact that it is initially appealing to attempt to use LSTD in the evaluation step of a PI algorithm, this combination can be problematic. LSTD is more accurate than TD(0), but more algorithmically complex. This can be easily seen when the value function approximator involves a very large number of basis sets. The LSTD algorithm is very similar to heuristic dynamic programming (HDP) <ref type="bibr" target="#b36">[37]</ref>, but it involves extra advantages that include convergence with arbitrary initial conditions and recursive formulations.</p><p>Wei et al. <ref type="bibr" target="#b37">[38]</ref> proposed a generalized value iteration algorithm for discrete-time systems to overcome the disadvantage of the traditional value iteration algorithms by allowing an arbitrary positive semi-definite function as initialization. Our work in this paper is focused on a Galerkinized asymptotically stable algorithm for known deterministic nonlinear continuoustime systems. Wei et al. <ref type="bibr" target="#b38">[39]</ref> proposed a novel generalized PI algorithm, which was a general idea of an interacting policy and value iteration algorithms, developed for discretetime nonlinear systems. In <ref type="bibr" target="#b39">[40]</ref>, an iterative θ -ADP algorithm was developed, where the iterative control law can stabilize the nonlinear system offline and online. A model-free optimal control scheme for a class of linear discrete-time systems with multiple delays in state, control, and output vectors is proposed in <ref type="bibr" target="#b40">[41]</ref>. In <ref type="bibr" target="#b41">[42]</ref>, an online adaptive policy learning algorithm based on ADP is proposed for learning, in real time, the solution to the Hamilton-Jacobi-Isaacs equation. A new optimal control scheme based on an iterative ADP algorithm is presented in <ref type="bibr" target="#b42">[43]</ref>, which makes the value function converge iteratively to the greatest lower bound of all the value function indices within an error, in finite time. A new fuzzy ADP scheme is presented in <ref type="bibr" target="#b43">[44]</ref> to design an optimal coordination control framework for the consensus problem of a multiagent differential game. The work of Wei and Liu <ref type="bibr" target="#b44">[45]</ref> proposes stable iterative ADP with guaranteed convergence and stability performance to achieve the optimal temperature control of water gas shift reaction. Optimal battery control problems for home energy management systems were solved by ADP in <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b46">[47]</ref>.</p><p>There is a lot of research work on approximate dynamic programming for linear systems <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b47">[48]</ref>, where the PE condition used also in adaptive control <ref type="bibr" target="#b48">[49]</ref> needs to be satisfied in order to guarantee the convergence of the critic NN. For nonlinear systems, there is no clear development. Recent adaptive-optimal control algorithms with approximate dynamic programming <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b49">[50]</ref> require a PE condition that is essentially analogous to space exploration in reinforcement learning <ref type="bibr" target="#b11">[12]</ref>. This condition is restrictive and often difficult to guarantee in practice. Hence, convergence cannot be guaranteed. The study of <ref type="bibr" target="#b50">[51]</ref> from the adaptive control side, and the studies of <ref type="bibr" target="#b51">[52]</ref> and <ref type="bibr" target="#b52">[53]</ref> from the reinforcement learning side propose some frameworks that rely on concurrently using current and recorded data for adaptation to obviate the difficulty of guaranteeing convergence with PE. Recently, Modares et al. <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> used concurrent learning in optimal-adaptive control, but they only proved a milder form of stability, namely, the uniform ultimate boundedness of the closed-loop signals, using an approach that is based on integral reinforcement learning and NN identifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contributions</head><p>The contributions of this paper lie in the development of an adaptive learning algorithm to solve an infinite-horizon optimal control problem for known deterministic nonlinear systems, while considering symmetric input constraints. The algorithm proposed is an appropriate combination of adaptive control, optimization, and reinforcement learning. A novelty of our approach lies in the use of concurrent information to relax the PE condition. By concurrent, it is meant that current and stored data are used for the adaptation process, which facilitates the convergence of the algorithm. In fact, we prove the asymptotic stability of the closed-loop system, which includes the online updates of the critic and actor NNs using state measurements.</p><p>This paper is structured as follows. In Section II, we formulate the optimal control problem with saturated inputs. In Section III, the approximate solution for the HJB equation is presented. In Section IV, the proof of asymptotic stability of the closed loop is presented. In Section V, the simulation results for a power system plant and a controlled Van der Pol oscillator are given. Finally, Section VI concludes and discusses about future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Notation</head><p>The notation used in this paper is standard. R + is the set of positive real numbers, and Z + is the set of positive integer numbers. The superscript is used to denote the optimal solution of an optimization, λ min (A) is the minimum eigenvalue of a matrix A, and 1 m is the column vector with m ones. The gradient of a scalar-valued function with respect to a vector-valued variable x is defined as a column vector, and is denoted by ∇ := ∂/∂ x. A function α : R + → R is said to belong to class K(α ∈ K) functions if it is continuous, strictly increasing, and α(0) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM FORMULATION</head><p>Consider the nonlinear continuous-time system given by ẋ(t) = f (x(t)) + g(x(t))u(t); x(0) := x 0 , t ≥ 0 <ref type="bibr" target="#b0">(1)</ref> where x(t) ∈ R n is the state vector, u(t) ∈ U ⊆ R m is the control input, and f (x(t)) ∈ R n and g(x(t)) ∈ R n×m are the known functions. We assume that x(t) is available for full state feedback.</p><p>It is desired to minimize the following infinite-horizon cost functional:</p><formula xml:id="formula_0">V (x(0)) = ∞ 0 r (x(τ ), u(τ )) dτ ∀x(0)<label>(2)</label></formula><p>with</p><formula xml:id="formula_1">r (x, u) = Q(x) + R s (u) ∀x, u<label>(3)</label></formula><p>with functions Q(x) positive definite and R s (u) nonnegative, ∀u ∈ U . Since this paper is concerned with providing an asymptotically stable framework for solving the HJB equation, we provide the following definition. Definition 1 <ref type="bibr" target="#b10">[11]</ref>: Given an autonomous, time-invariant nonlinear system of the form ẋ(t) = f (x(t)) with x(t) ∈ R n , f : R n → R n and f continuous and an equilibrium point given as x e , i.e., f (x e ) = 0. Let ψ(t; 0, x ) denote the unique solution x(t) to ẋ(t) = f (x(t)) that corresponds to x(0) = x. Then, the equilibrium point x e is said to be asymptotically stable, if ∀ ∈ R + there exists a δ ∈ R + , such that</p><formula xml:id="formula_2">x ∈ B(x e , δ) ⇒ ψ(t; 0, x ) ∈ B(x e , ) ∀t ≥ 0 lim t →∞ ψ(t; 0, x) = x e</formula><p>where B( x, ) denotes the open ball that is centered at x of radius , i.e., the set {x ∈ R n : x -x &lt; }.</p><p>The optimal control problem is to find an admissible control u (t), such that the equilibrium point of the closed-loop system (1) is asymptotically stable on R n in the sense of Definition 1 and the value V is finite. To force bounded inputs, e.g., |u i | ≤ ū, ∀i ∈ {1, . . . , m}, we follow the approach in <ref type="bibr" target="#b12">[13]</ref> and use a nonquadratic penalty function of the form:</p><formula xml:id="formula_3">R s (u) = 2 m i=1 u i 0 (θ -1 (v i )) T i dv i ∀u</formula><p>with a weighting factor i ∈ R + , i = {1, . . . , m}, and with abuse of notation, we can write the componentwise operations in a compact form as</p><formula xml:id="formula_4">R s (u) = 2 u 0 θ -1 (v) T Rdv ∀u</formula><p>where R is a diagonal positive definite matrix consisting</p><formula xml:id="formula_5">of i &gt; 0, i = {1, . . . , m} terms, v ∈ R m , and θ(•) is a continuous, one-to-one real-analytic integrable function of class C μ , μ ≥ 1, used to map R onto the interval (-ū, ū), satisfying θ(0) = 0. Also note that R s (u) is positive definite, because θ -1 (v) is monotonic odd. For example, one could select R s (u) = 2 u 0 (θ -1 (v)) T Rdv := 2 u 0 ū tanh -1 (v/ ū) T Rdv &gt; 0 ∀u.<label>(4)</label></formula><p>The optimal value function is defined as</p><formula xml:id="formula_6">V (x(t)) = min u∈U ∞ t r (x, u)dτ ∀x, t ≥ 0 (5)</formula><p>subject to the state dynamics in <ref type="bibr" target="#b0">(1)</ref>. The Hamiltonian of (1), associated with the cost functions ( <ref type="formula" target="#formula_0">2</ref>) and (3), can be written as</p><formula xml:id="formula_7">H (x, u, ∇V (x)) = ∇V (x) T f (x) + g(x)u + Q(x) + R s (u) ∀x, u.<label>(6)</label></formula><p>The constrained optimal control K * (x) [i.e., u(t) := K * (x)] for the system (1), with the cost functions (3)-( <ref type="formula">5</ref>), can be obtained using the stationarity condition in the Hamiltonian of ( <ref type="formula" target="#formula_7">6</ref>) as</p><formula xml:id="formula_8">K * (x) = arg min u H (x, u, ∇V (x)) ⇒ ∇V (x) T g(x) + 2Rθ -T ( K * (x)) = 0 ⇒ K (x) = -θ 1 2 R -1 g T (x)∇V (x) T ∀x. (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>The corresponding optimal cost and optimal control satisfy the following HJB equation:</p><formula xml:id="formula_10">H (x, K (x), ∇V (x)) := ∇V (x) T ( f (x) + g(x) K (x)) + Q(x) + R s ( K (x)) = 0 ∀x.<label>(8)</label></formula><p>The next result provides a sufficient condition for the existence of the optimal control solution.</p><p>Theorem 1: Suppose that there exists a positive definite and radially unbounded smooth function V ∈ C 1 that satisfies V (0) = 0 and</p><formula xml:id="formula_11">H (x, K (x), ∇V (x)) = 0 ∀x (9)</formula><p>with H (•) given by ( <ref type="formula" target="#formula_10">8</ref>) and K (x) given by</p><formula xml:id="formula_12">K (x) = -θ 1 2 R -1 g T (x)∇V (x) T (10)</formula><p>and that the closed-loop system (1) and ( <ref type="formula">10</ref>) have a locally Lipschitz right-hand side with state x ∈ R n , i.e., x → f (x) + g(x) K (x). The origin is a globally asymptotically stable equilibrium point of the closed-loop system (1) with control <ref type="bibr" target="#b9">(10)</ref>, and the control policy (10) minimizes the cost <ref type="bibr" target="#b4">(5)</ref>.</p><p>Proof of Theorem 1: Because of ( <ref type="formula" target="#formula_7">6</ref>) and ( <ref type="formula">9</ref>) (since we have assumed that V is a positive definite and radially unbounded function that solves the HJB), the time derivative of V along the closed-loop solutions satisfies</p><formula xml:id="formula_13">V = ∇V (x) T ( f (x) + g(x) K (x)) = -R s ( K (x)) -Q(x) ≤ -Q(x)</formula><p>where we used (9) with K (x) given by <ref type="bibr" target="#b9">(10)</ref>. Using V as a Lyapunov function, we conclude that the origin is a globally asymptotically stable equilibrium point of (1). Now, since V is smooth and V (0) = 0, as t → ∞, one has</p><formula xml:id="formula_14">V (x(0)) + ∞ 0 ∇V (x) T ( f (x) + g(x) K (x)) = 0. (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>Since the function V (x) is smooth, converge to zero as t → ∞ (due to asymptotic stability) and V (0) = 0, using <ref type="bibr" target="#b10">(11)</ref>, we can write (2) as</p><formula xml:id="formula_16">V (x(0); u) = ∞ 0 R s (u) + Q(x) dt + V (x(0)) + ∞ 0 ∇V (x) T ( f (x) + g(x) K (x))dt ∀u.</formula><p>By subtracting zero (using the HJB equation), we have</p><formula xml:id="formula_17">V (x(0); u) = ∞ 0 ((R s (u) -R s ( K (x))) + ∇V (x) T g(x) × (u -K (x)))dt + V (x(0)) ∀u. (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>By noting that ∇V (x) T g(x) = -2Rθ -T ( K (x)) in ( <ref type="formula" target="#formula_17">12</ref>) and after completing the squares, we have</p><formula xml:id="formula_19">V (x(0); u) = ∞ 0 2 u 0 (θ -1 (v)) T dv - K (x) 0 (θ -1 (v)) T dv -Rθ -T (u)(u -K (x)) dt + V (x(0)) ∀u.</formula><p>We can complete the squares, and hence, we have</p><formula xml:id="formula_20">V (x(0); u) = ∞ 0 R s (u -K (x))dt + V (x(0)) ∀u.</formula><p>Now, by setting u := K (x), one can show that</p><formula xml:id="formula_21">V (x(0)) ≤ V (x(0); u)</formula><p>from which the result follows.</p><p>Section III provides approximate techniques to converge to the solution of the HJB equation <ref type="bibr" target="#b7">(8)</ref>. Given admissible policies μ (0) and i = 1 3:</p><formula xml:id="formula_22">while V μ (i) -V μ (i-1) ≥ ac , ∀x do 4:</formula><p>Solve for the value V (i) (x) using Bellman's equation</p><formula xml:id="formula_23">Q(x)+∇V μ (i) T ( f (x) + g(x)μ (i) )+ R s (μ (i) ) = 0, V μ (i) (0) = 0, 5:</formula><p>Update the control policy μ (i+1) using</p><formula xml:id="formula_24">μ (i+1) = -θ 1 2 R -1 g T (x)∇V μ (i) T 6: i := i + 1 7:</formula><p>end while 8: end procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROXIMATE SOLUTION</head><p>The structure used for our approximate solution is motivated by the PI algorithm (Algorithm 1), where ac is a small number used to terminate the algorithm when two consecutive value functions differ by less than ac . In the linear case, this algorithm reduces to Kleinman's algorithm <ref type="bibr" target="#b55">[56]</ref>.</p><p>Section III-A lays the foundation for updating the two steps 4 and 5 in PI simultaneously using the data collected along the closed-loop trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Critic Neural Network and Recorded Past Data</head><p>The first step to solve the HJB equation ( <ref type="formula" target="#formula_10">8</ref>) is to locally approximate the value function V (x) in ( <ref type="formula">5</ref>) with a critic NN, within a set ⊆ R n that contains the origin, as follows:</p><formula xml:id="formula_25">V (x) = W T φ(x) + (x) ∀x<label>(13)</label></formula><p>where</p><formula xml:id="formula_26">W ∈ R N is an ideal weight vector satisfying W ≤ W m ; φ(x) : → R N and φ(x) = [ϕ 1 (x)ϕ 2 (x) . . . ϕ N (x)</formula><p>] T are the NN activation functions, such that ϕ i (0) = 0 and ∇ϕ i (0) = 0, ∀i = 1, . . . , N; N is the number of neurons in the hidden layer; and (x) is the NN approximation error.</p><p>One should pick the NN activation functions ϕ i (x), ∀i ∈ {1, 2, . . . , N}, as quadratic, radial basis, or sigmoid functions, so that they define a complete independent basis set for V . In this case, V and its derivatives</p><formula xml:id="formula_27">∇V (x) = ∂ ∂ x φ(x) T W + ∂ ∂ x (x) =: ∇φ(x) T W + ∇ (x) ∀x ∈<label>(14)</label></formula><p>can be uniformly approximated on any given compact set .</p><p>According to the Weierstrass higher order approximation theorem <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b56">[57]</ref>, as the number of basis sets N increases, the approximation error on a compact set goes to zero, i.e., (x) → 0 as N → ∞. We shall require a form of uniformity in this approximation result that is common in neuroadaptive control and other approximation techniques <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b56">[57]</ref>. This assumption also involves the approximate HJB defined as</p><formula xml:id="formula_28">H (x, K (x), W T ∇φ) := W ∇φ( f (x) + g(x) K (x)) + Q(x) + R s ( K (x)) = H ∀x<label>(15)</label></formula><p>which is obtained using ( <ref type="formula" target="#formula_27">14</ref>) in <ref type="bibr" target="#b7">(8)</ref>, and that leads to the residual error</p><formula xml:id="formula_29">H := H (•) -H (•) = -∇ T ( f (x) + g(x) K (x)) ∀x (16)</formula><p>where, for brevity, we have omitted the arguments of H and H . Assumption 1 (Critic Uniform Approximation): The critic activation functions φ, the value function approximation error , their derivatives, and the Hamiltonian residual error H are all uniformly bounded on a set ⊆ R n , in the sense that there exist finite constants φ m , φ dm , m , dm ,</p><formula xml:id="formula_30">Hm ∈ R + , such that |φ(x)| φ m , |∇φ(x)| φ dm , | (x)| ≤ m , |∇ (x)|</formula><p>dm , and | H (x)| ≤ Hm , ∀x ∈ . In order to get small, we also assume that we have a large number of basis sets, i.e., N → ∞.</p><p>Since the ideal weights W for the (approximate) value function V (x) that appears in ( <ref type="formula" target="#formula_25">13</ref>) are unknown, one must consider the critic weight estimates Ŵ ∈ R N , associated with the approximate value function</p><formula xml:id="formula_31">V (x) = Ŵ T φ(x) ∀x. (<label>17</label></formula><formula xml:id="formula_32">)</formula><p>Our objective is to find an update law for the weight estimates Ŵ , so that they converge to the ideal values W , and thus provide a good estimate</p><formula xml:id="formula_33">Ĥ (x, u, Ŵ T ∇φ) := Ŵ T ∇φ( f (x) + g(x)u) + Q(x) + R s (u) ∀x, u<label>(18)</label></formula><p>for the (approximate) Hamiltonian.</p><p>Definition 2 <ref type="bibr" target="#b48">[49]</ref>:</p><formula xml:id="formula_34">A vector signal (t) is exciting over the interval [t, t + T PE ], with T PE ∈ R + if there exists β 1 , β 2 ∈ R + , such that β 1 I t +T t (τ ) T (τ )dτ ≤ β 2 I, ∀t</formula><p>with I an identity matrix of appropriate dimensions.</p><p>There is a need to develop a learning framework to find a tuning law for Ŵ in order to achieve the convergence of <ref type="bibr" target="#b17">(18)</ref> to the (approximate) Hamiltonian ( <ref type="formula" target="#formula_28">15</ref>) along the closed-loop trajectories. However, in order to attain that, one would typically need PE (see Definition 2) for the vector ω(t) defined as</p><formula xml:id="formula_35">ω(t) := ∇φ(x(t))( f (x(t)) + g(x(t))u(t)) (<label>19</label></formula><formula xml:id="formula_36">)</formula><p>along the closed-loop trajectories <ref type="bibr" target="#b48">[49]</ref>. To weaken the need to guarantee a priori, a PE condition in the sense of Definition 2 for infinite time, we follow the approach proposed in <ref type="bibr" target="#b57">[58]</ref> that uses past recorded data, concurrently with current data. To this effect, we define the Hamiltonian error corresponding to the data collected at the current time t e(t)</p><formula xml:id="formula_37">:= Ĥ (x(t), u(t), Ŵ (t) T ∇φ(x(t))) -H (x, K (x), ∇V ) = Ĥ (x(t), u(t), Ŵ (t) T ∇φ(x(t))) ∀x, u</formula><p>where the latter equality is due to <ref type="bibr" target="#b7">(8)</ref>, and the error corresponding to the data previously collected at times t 0 , t 1 , . . . ,</p><formula xml:id="formula_38">t k &lt; t e buff i (t i , t) := Ĥ (x(t i ), u(t i ), Ŵ (t) T ∇φ(x(t i ))) := Ŵ (t) T ∇φ(x(t i )( f (x(t i )) + g(x(t i ))u(t i )) + Q(x(t i )) + R s (u(t i )).</formula><p>We draw attention to the reader that, while the error e buff i (t i , t) uses the past state and input data x(t i ) and u(t i ), respectively, it is defined based on the current weight estimates Ŵ (t).</p><p>The current and previous errors defined above can be combined into the following (normalized) global error:</p><formula xml:id="formula_39">E(t) = 1 2 e(t) 2 (ω(t) T ω(t) + 1) 2 + k i=1 e 2 buff i (t i , t) (ω(t i ) T ω(t i ) + 1) 2 ∀t</formula><p>where</p><formula xml:id="formula_40">ω(t i ) := ∇φ(x(t i )) ( f (x(t i )) + g(x(t i ))u(t i )).</formula><p>The tuning for the critic NN is obtained by a gradientdescent-like rule as follows:</p><formula xml:id="formula_41">Ẇ = -α ∂ E ∂ Ŵ = -α ω(t)e(t) (ω(t) T ω(t) + 1) 2 -α k i=1 ω(t i )e buff i (t i , t) (ω(t i ) T ω(t i ) + 1) 2 = -α ω(t)(ω(t) T Ŵ (t) + R s (u(t)) + Q(x(t))) (ω(t) T ω(t) + 1) 2 -α k i=1 ω(t i )(ω(t i ) T Ŵ (t)+ Q(x(t i ))+ R s (u(t i ))) (ω(t i ) T ω(t i ) + 1) 2<label>(20)</label></formula><p>∀t &gt; t i ≥ 0, where α &gt; 0 is a constant gain that determines the speed of convergence. Define the weight estimation error of the critic by</p><formula xml:id="formula_42">W := W -Ŵ ∈ R N . (<label>21</label></formula><formula xml:id="formula_43">)</formula><p>We conclude from (20) that the error dynamics can be written as Ẇ = -N om + P ert <ref type="bibr" target="#b21">(22)</ref> where</p><formula xml:id="formula_44">N om (t) := α ω(t)ω(t) T (ω(t) T ω(t)+1) 2 + k i=1 ω(t i )ω(t i ) T (ω(t i ) T ω(t i )+1) 2 W (t) (23)</formula><p>can be viewed as defining the nominal error dynamics and</p><formula xml:id="formula_45">P ert (t) := α ω(t) (ω(t) T ω(t) + 1) 2 H (t) + k i=1 ω(t i ) (ω(t i ) T ω(t i ) + 1) 2 H (t i ) (<label>24</label></formula><formula xml:id="formula_46">)</formula><p>a perturbation term, bounded as P ert (α/2)(k + 1) Hm that would be zero if the Hamiltonian errors H were absent. To derive this expression for Ẇ = -Ẇ , we used <ref type="bibr" target="#b19">(20)</ref> together with the fact that Q</p><formula xml:id="formula_47">(x(t)) + R s (u(t)) = -W T ω(t) + H (t),</formula><p>which is a consequence of ( <ref type="formula" target="#formula_10">8</ref>) and <ref type="bibr" target="#b15">(16)</ref>. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Theorem 2: Suppose that {ω(t 1 ), . . . , ω(t k )} contains N linearly independent vectors and that the tuning law is given by <ref type="bibr" target="#b19">(20)</ref>. Then, for every given control signal u(t), we have that</p><formula xml:id="formula_48">d W (t) 2 dt = -2 W (t) T N om ≤ -2αλ min ( ) W 2 (25) with := k i=1 (ω(t i )ω(t i ) T /(ω(t i ) T ω(t i ) + 1)</formula><p>2 ) &gt; 0 along solutions to <ref type="bibr" target="#b21">(22)</ref>.</p><p>Remark 1: Typical adaptive-optimal control algorithms <ref type="bibr" target="#b8">[9]</ref> do not have the extra past-data terms 2 ) in the error dynamics <ref type="bibr" target="#b21">(22)</ref> and, thus, need a PE condition on ω(t)/(ω(t) T ω(t) + 1) (typically of the form</p><formula xml:id="formula_49">k i=1 (ω(t i )ω(t i ) T / (ω(t i ) T ω(t i ) + 1)</formula><formula xml:id="formula_50">β 1 I t +T t (ω(t)ω(t) T /(ω(t) T ω(t) + 1) 2 ) ≤ β 2 I with constants β 1 , β 2 , T ∈ R + )</formula><p>that holds for every t from t = 0 to t = ∞. This is equivalent to requiring that the matrix t +T t (ω(t)ω(t) T /(ω(t) T ω(t) + 1) 2 ) ∈ R n×n is positive definite over any finite interval. This is equivalent to requiring that the signal ω(t) contains at least n spectral lines. This condition cannot be verified during learning, especially for nonlinear systems. In Theorem 2, the relaxed PE condition comes through the requirement that at least N of the vectors {ω(t 1 ), . . . , ω(t k )} must be linearly independent, which is equivalent to the matrix being positive definite. In practice, as one collects each additional vector ω(t i ), one adds a new term to the matrix , and one can stop recording points as soon as this matrix becomes full rank (i.e., t k time has been reached). From that point forward, one does not need to record new data, and the assumption of Theorem 2 holds, regardless of whether or not future data provide additional excitation. The selection of the times t i is somewhat arbitrary, but in our numerical simulations, we typically select these values equally spaced in time.</p><p>Remark 2: It is assumed that the maximum number of data points to be stored in the history stack (i.e., t 0 , t 1 , . . . , t k &lt; t) is limited due to memory/bandwidth limitations.</p><p>Proof of Theorem 2: Consider the following Lyapunov function:</p><formula xml:id="formula_51">L = 1 2α W (t) T W (t). (<label>26</label></formula><formula xml:id="formula_52">)</formula><p>By differentiating <ref type="bibr" target="#b25">(26)</ref> along the error dynamics system trajectories, one has</p><formula xml:id="formula_53">L = -W (t) T ω(t)ω(t) T (ω(t) T ω(t) + 1) 2 + W (t) + W (t) T P ert<label>(27)</label></formula><p>which is negative definite as long as</p><formula xml:id="formula_54">W &gt; P ert λ min ( ) .</formula><p>Equation <ref type="bibr" target="#b24">(25)</ref> follows from this and the fact that (ω(t)ω(t) T /(ω(t) T ω(t) + 1) 2 ) &gt; 0, ∀t. Since {ω(t 1 ), . . . , ω(t k )} has N linearly independent vectors, the matrix is positive definite, from which the exponential stability of the nominal system follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Actor Neural Network</head><p>One could use a single set of weights with a slidingmode controller as shown in <ref type="bibr" target="#b58">[59]</ref> to approximate both V and its gradient ∇V , but instead, we independently adjust two sets of weights: 1) the critic weights introduced in (17) to approximate V and 2) the actor weights introduced below to approximate ∇V in the expression of the optimal control policy <ref type="bibr" target="#b6">(7)</ref>. While this carries additional computational burden, the flexibility introduced by this overparameterization will enable us to establish convergence to the optimal solution and guaranteed the Lyapunov-based stability, which seems difficult using only one set of weights.</p><p>The optimal control policy ( <ref type="formula" target="#formula_8">7</ref>) can be approximated by an actor NN as follows:</p><formula xml:id="formula_55">K (x) = W u T φ u (x) + u (x) ∀x<label>(28)</label></formula><p>where W u ∈ R N 2 ×m is an ideal weight matrix, φ u (x) are the actor NN activation functions defined similar to the critic NN, N 2 is the number of neurons in the hidden layer, and u is the actor approximation error. As before, the NN activation functions must define a complete independent basis set, so that K (x) can be uniformly approximated on , as expressed by Assumption 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 2 (Actor Uniform Approximation):</head><p>The actor activation functions in φ u and the actor residual error u are all uniformly bounded on a set ⊆ R n , in the sense that there exist finite constants φ um , um ∈ R + , such that |φ u (x)| φ um and | u (x)| ≤ um , ∀x ∈ . In order to get u small, we also assume that we have a large number of basis sets, i.e., N 2 → ∞.</p><p>Since the ideal weighs W u are not known, we introduce actor estimate weights Ŵu ∈ R N 2 ×m to approximate the optimal control in (28) with K(x) [i.e., u(t) := K(x)] as</p><formula xml:id="formula_56">K(x) = Ŵ T u φ u (x) ∀x. (<label>29</label></formula><formula xml:id="formula_57">)</formula><p>Our goal is then to tune Ŵu , such that the following error is minimized:</p><formula xml:id="formula_58">E u (t) = 1 2 trace e T u (t)e u (t) ∀t<label>(30)</label></formula><p>where</p><formula xml:id="formula_59">e u := Ŵ T u φ u + θ 1 2 R -1 g T (x)∇φ T Ŵ ∈ R m</formula><p>is the error between the estimate ( <ref type="formula" target="#formula_56">29</ref>) and a version of <ref type="bibr" target="#b6">(7)</ref>, in which V is approximated by the critic's estimate <ref type="bibr" target="#b16">(17)</ref>.</p><p>The tuning for the actor NN is obtained by a gradientdescent-like rule as follows:</p><formula xml:id="formula_60">Ẇu = -α u ∂ E u ∂ Ŵu = -α u φ u e u = -α u φ u Ŵ T u φ u + θ 1 2 R -1 g T (x)∇φ T Ŵ T (<label>31</label></formula><formula xml:id="formula_61">)</formula><p>where α u &gt; 0 is a constant gain that determines the speed of convergence. Defining the weight estimation error for the actor by</p><formula xml:id="formula_62">Wu := W u -Ŵu ∈ R N 2 ×m (<label>32</label></formula><formula xml:id="formula_63">)</formula><p>Algorithm 2 Adaptive-Optimal Control Algorithm With Relaxed PE 1. Start with initial state x(0), random initial weights Ŵu (0), Ŵ (0) and i = 1 2. procedure 3.</p><p>Propagate t, x(t) using (1) and u(t) := K(x) x(t) comes from integrating the nonlinear system (1) using any ordinary differential equation (ode) solver (e.g. Runge Kutta) while the time t comes from the Runge Kutta integration process, i.e. [t i , t i+1 ], i ∈ N where t i+1 := t i + h with h ∈ R + the step size 4.</p><p>Propagate Ŵu (t), Ŵ (t) integrate Ẇu as in <ref type="bibr" target="#b30">(31)</ref> and Ẇ as in (20) using any ode solver (e.g. Runge Kutta)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Compute V (x) = Ŵ T φ(x) output of the Critic NN, 6.</p><p>Compute K(x) = Ŵ T u φ u (x) output of the Actor NN 7.</p><p>If i = k {ω(t 1 ), ω(t 2 ), . . . , ω(t i )} has N linearly independent elements and t k is the time instant that this happens 8.</p><p>Select an arbitrary data point to be included in the history stack (c.f. Remarks 1-2) 9.</p><p>i := i + 1 10.</p><p>end if when the history stack is full 11. end procedure and after considering that ( <ref type="formula" target="#formula_8">7</ref>) with ( <ref type="formula" target="#formula_25">13</ref>) is approximated by ( <ref type="formula" target="#formula_56">29</ref>), the error dynamics can be written as</p><formula xml:id="formula_64">Ẇu = -α u φ u φ T u Wu -α u φ u θ 1 2 R -1 g T (x)∇φ T W T + α u φ u θ 1 2 R -1 g T (x)∇φ T Ŵ T -α u φ u θ 1 2 R -1 g T (x)∇ T -α u φ u u . (<label>33</label></formula><formula xml:id="formula_65">)</formula><p>Remark 3: Note that the third term of ( <ref type="formula" target="#formula_64">33</ref>) is a function of Ŵ; however, since this signal appears inside the saturation function θ(•), this term is always bounded and will be treated appropriately in the stability analysis as follows.</p><p>A pseudocode (with inline comments to provide guidance following after the symbol ) that describes the proposed adaptive-optimal control algorithm has the form shown in Algorithm 2.</p><p>Remark 4: Note that the algorithm runs in real time in a plug-n-play framework and that we do not have any iterations. Everything happens simultaneously as we receive new state measurements along the trajectories. One measures the state x(t) and integrates the tuning laws ( <ref type="formula" target="#formula_41">20</ref>) and (31) using any ordinary differential equation (ode) solver (e.g., Runge-Kutta), and then computes V (x) = Ŵ T φ(x) and K(x) = Ŵ T u φ u (x). Numerical methods implemented in modern software packages are mostly adaptive algorithms where, at each step, the step size h is adjusted based on an estimate of the error at that step. In general, as h is decreased, the calculation takes longer but is more accurate. However, if h is decreased too much, the slight rounding that occurs in the computer (because it cannot represent real numbers exactly) begins to accumulate enough to cause significant errors. For many higher order systems, it is very difficult to make the Euler approximation effective. The explicit Runge-Kutta methods for nonstiff problems provide computations that are linear to the size of the problem. For stiff problems, more accurate and more elaborate techniques were developed.</p><p>Remark 5: For the proposed method, the involved computation is dominated by the training algorithm for Ŵ and Ŵu in order to approximate V (x) = Ŵ T φ(x) and K(x) = Ŵ T u φ u (x), which are all the variables of the state.</p><p>If one does the calculations of the right hand side of ( <ref type="formula" target="#formula_41">20</ref>) and <ref type="bibr" target="#b30">(31)</ref> in the order of parentheses then, for the critic one has quadratic growth with the number of basis sets N, for the actor one has linear growth with m N 2 and linear growth with the number of states n. Thus, the complexity is given as O(n + N 2 + m N 2 ) with the term N 2 , dominating the other two terms. In order to evaluate the performance of the implemented algorithm, we note that the computational complexity is similar to LSTD <ref type="bibr" target="#b35">[36]</ref> [e.g., O(n 2 )] but worst than DHP, HDP <ref type="bibr" target="#b36">[37]</ref>, and TD learning <ref type="bibr" target="#b59">[60]</ref> that all have linear complexity with respect to the total number of parameters [e.g., O(n)]. In addition, we should mention that, instead of selecting arbitrary or equally spaced data points as in our work, one can compute the singular values of the history stack matrix and update the history as in <ref type="bibr" target="#b50">[51]</ref>. However, these computations are very expensive and will boost the algorithmic complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stability Analysis</head><p>The regularity assumption is needed for the stability results presented below.</p><p>Assumption 3: The process input function g is uniformly bounded on a set ⊂ R n , i.e., g(x) &lt; 1/2, ∀x ∈ .</p><p>To remove the effect of the NN approximation errors and u (and their partial derivatives) and obtain a closedloop system with an asymptotically stable equilibrium point, one needs to add a robustifying term to the control law <ref type="bibr" target="#b28">(29)</ref> following the work of Polycarpou et al. <ref type="bibr" target="#b60">[61]</ref> and use:</p><formula xml:id="formula_66">K(x) = Ŵ T u φ u (x) + η ∀x<label>(34)</label></formula><p>where</p><formula xml:id="formula_67">η := -B x 2 1 m (A + x T x) ∀x<label>(35)</label></formula><p>with A a positive constant, and</p><formula xml:id="formula_68">B ∈ R + satisfies ∀x ∈ B x 2 ≥ A + x T x (W m φ dm + dm ) × 1 2α k + 1 2 Hm 2 + (φ um ū + φ um um ) 2 + φ um 2 (W m φ dm + dm ) 2 + 2(2φ um ū) 2 + (W m φ dm + dm ) 2 + 2 um . (<label>36</label></formula><formula xml:id="formula_69">)</formula><p>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Theorem 3 is the main result of this paper, and proves the asymptotic stability of the learning algorithm of the resulting closed-loop dynamics (1), ( <ref type="formula" target="#formula_67">35</ref>)</p><formula xml:id="formula_70">ẋ = f (x) + g(x) W u -Wu T φ u (x) + η . (<label>37</label></formula><formula xml:id="formula_71">)</formula><p>Theorem 3: Consider the closed-loop dynamics given by <ref type="bibr" target="#b36">(37)</ref> together with the tuning laws for the critic and actor NNs given by ( <ref type="formula" target="#formula_41">20</ref>) and <ref type="bibr" target="#b30">(31)</ref>, respectively. Suppose that the HJB equation ( <ref type="formula" target="#formula_10">8</ref>) has a positive definite and smooth solution, Assumptions 1, 2, and 3 hold, and that {ω(t 1 ), ω(t 2 ), . . . , ω(t k )} has N linearly independent elements. Then, there exists a triple ( x × W × W u ) ⊂ with compact, such that the solution Z := (x(t), W (t), Wu (t)) ∈ ( x × W × W u ) exists globally and converges asymptotically to zero for all the NN weights W (0) inside W , Wu (0) inside W u , and state x(0) inside x , provided that the following inequalities are satisfied:</p><formula xml:id="formula_72">α &gt; 1 8λ min k i=1 ω(t i )ω(t i ) T (ω(t i ) T ω(t i )+1) 2 (38) φ um &gt; 1 + √ 65 8 . (<label>39</label></formula><formula xml:id="formula_73">)</formula><p>When the set that appears in Assumptions 1, 2, and 3 is the whole R n , the triple W × W u × x can also be the whole R n .</p><p>Remark 6: For the inequality <ref type="bibr" target="#b37">(38)</ref> to hold, one needs to pick the tuning gain α for the critic NN sufficiently large. However, as noted in adaptive control <ref type="bibr" target="#b48">[49]</ref>, large adaptive gains can cause high-frequency oscillations in the control signal and reduced tolerance to time delays that will destabilize the system. There are not any systematic approaches to pick a satisfactory adaptation gain; hence, trial and error, intuition, or Monte Carlo simulations can serve as guidelines. Regarding <ref type="bibr" target="#b38">(39)</ref>, since φ um is simply an upper bound that appears in Assumption 2, one can have it as large as needed. However, one must keep in mind that a large value of φ um requires an appropriate value for the function B [see <ref type="bibr" target="#b35">(36)</ref>] in the robustness term in <ref type="bibr" target="#b34">(35)</ref>. This dependence is clear from the quadratic terms of φ um in <ref type="bibr" target="#b35">(36)</ref> </p><formula xml:id="formula_74">(see {(1/2α)((k + 1/2) Hm ) 2 + (φ um ū + φ um um ) 2 + (φ um /2)(W m φ dm + dm ) 2 + 2(2φ um ū) 2 + (W m φ dm + dm ) 2 + 2 um }</formula><p>) that increase when one picks larger values of φ um .</p><p>Remark 7: By denoting Z := [x T W T W T u ] T from the conclusion of Theorem 3, we have that Z → 0, which implies x → 0, and it is straightforward that as t → ∞ then from <ref type="bibr" target="#b33">(34)</ref>, we have <ref type="bibr" target="#b28">(29)</ref>.</p><p>Remark 8: In case the approximation holds over the entire space, i.e., x = R n , one can conclude the global existence of solution, provided that the HJB solution V is norm coercive (i.e., V (x) → 0 ⇒ x → 0), as this suffices to guarantee the Lyapunov function V that we use in Proof of Theorem 2 is also norm coercive (see <ref type="bibr" target="#b10">[11]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROOF OF THEOREM 3</head><p>Consider the following Lyapunov function:</p><formula xml:id="formula_75">V := V + W T W + 1 2α u trace W T u Wu (<label>40</label></formula><formula xml:id="formula_76">)</formula><p>where V is the optimal value function in <ref type="bibr" target="#b4">(5)</ref> that is the positive definite and smooth solution of (8) (see Theorem 1), and V c := W T W is the Lyapunov function considered in Theorem 2. Since V is positive definite, there exist class-K functions γ 1 (•) and γ 2 (•), and then</p><formula xml:id="formula_77">γ 1 ( Z ) ≤ V ≤ γ 2 ( Z ) for all Z := [x T W T W T u ] T ∈ B r</formula><p>, where B r ⊂ is a ball of radius r ∈ R + . By taking the time derivative of the first term with respect to the state trajectories with u(t) [see <ref type="bibr" target="#b36">(37)</ref>] and the second term with respect to the perturbed critic estimation error dynamics <ref type="bibr" target="#b22">(23)</ref>, using <ref type="bibr" target="#b24">(25)</ref>, and substituting the update for the actor <ref type="bibr" target="#b30">(31)</ref> and grouping terms together, <ref type="bibr" target="#b39">(40)</ref> becomes</p><formula xml:id="formula_78">V = ∇V (x) T f (x) -g(x) W T u φ u + g(x)( K (x) -u ) -g(x)B x 2 1 m (A + x T x) - ∂ V c ∂ W T ω(t)ω(t) T (ω(t) T ω(t)+1) 2 + k i=1 ω(t i )ω(t i ) T (ω(t i ) T ω(t i )+1) 2 W + ∂ V c ∂ W T ω(t) (ω(t) T ω(t) + 1) 2 H (t) + k i=1 ω(t i ) (ω(t i ) T ω(t i ) + 1) 2 H (t i ) + trace W T u -φ u φ T u Wu -φ u θ 1 2 R -1 g T (x)∇φ T W T -φ u θ 1 2 R -1 g T (x)∇ T -φ u u , t ≥ 0 = T 1 + T 2 + T 3 (<label>41</label></formula><formula xml:id="formula_79">)</formula><p>where the three terms T 1 -T 3 are given by ( <ref type="formula" target="#formula_84">42</ref>)- <ref type="bibr" target="#b43">(44)</ref>, as shown at the top of next page, respectively. Using the HJB equation</p><formula xml:id="formula_80">∇V (x) T f (x) = -∇V (x) T g(x) K (x) -R s ( K (x)) -Q(x) ∀x in (44) yields T 3 = -R s ( K (x)) -Q(x) -∇V (x) T g(x) W T u φ u -∇V T g(x) u -∇V T (x)g(x)B x 2 1 m (A + x T x) ≤ -R s ( K (x)) -Q(x) -(W m φ dm + dm ) × 1 2 φ um Wu + um + 1 2 B x 2 1 m A + x T x (<label>45</label></formula><formula xml:id="formula_81">)</formula><p>since A + x T x &gt; 0. The term T 3 can be further upper bounded as</p><formula xml:id="formula_82">T 3 ≤ -R s ( K (x)) -Q(x) + φ um 4 ((W m φ dm + dm )) 2 + φ um 4 Wu 2 + 1 2 (W m φ dm + dm ) 2 + 1 2 2 um -(W m φ dm + dm ) 1 2 B x 2 1 m A + x T x . (<label>46</label></formula><formula xml:id="formula_83">)</formula><formula xml:id="formula_84">T 1 := - ∂ V c ∂ W T ω(t)ω(t) T (ω(t) T ω(t) + 1) 2 + k i=1 ω(t i )ω(t i ) T (ω(t i ) T ω(t i ) + 1) 2 W + ∂ V c ∂ W T ω(t) (ω(t) T ω(t) + 1) 2 H (t) + k i=1 ω(t i ) (ω(t i ) T ω(t i ) + 1) 2 H (t i ) ≤ -2αλ min k i=1 ω(t i )ω(t i ) T (ω(t i ) T ω(t i ) + 1) 2 W 2 + 1 2α W k + 1 2 Hm ≤ -2αλ min k i=1 ω(t i )ω(t i ) T (ω(t i ) T ω(t i ) + 1) 2 W 2 + 1 4α W 2 + 1 4α k + 1 2 Hm 2 (<label>42</label></formula><formula xml:id="formula_85">)</formula><formula xml:id="formula_86">T 2 := trace W T u -φ u φ T u Wu -φ u θ 1 2 R -1 g T (x)∇φ T W T + φ u θ 1 2 R -1 g T (x)∇φ T Ŵ T -φ u θ 1 2 R -1 g T (x)∇ T -φ u u ≤ -φ 2 um Wu 2 + 2φ um ū Wu + (φ um ū + φ um um ) Wu ≤ -φ 2 um Wu 2 + (2φ um ū) 2 + (φ um ū + φ um um ) 2 2 + Wu 2 (<label>43</label></formula><formula xml:id="formula_87">)</formula><formula xml:id="formula_88">T 3 := ∇V (x) T f (x) -g(x) W T u φ u + g(x)(u (x) -u ) -g(x)B x 2 1 m (A + x T x)<label>(44)</label></formula><p>Finally, after considering the bound of B x 2 from (36), we can upper bound <ref type="bibr" target="#b40">(41)</ref> as</p><formula xml:id="formula_89">V ≤ -2αλ min k i=1 ω(t i )ω(t i ) T (ω(t i ) T ω(t i ) + 1) 2 - 1 4α W 2 -φ 2 um - φ um 4 -1 Wu 2 -R s ( K (x)) -Q(x), t ≥ 0.<label>(47)</label></formula><p>Then, by considering the inequalities ( <ref type="formula">38</ref>) and ( <ref type="formula" target="#formula_72">39</ref>), one has V ≤ 0 and t ≥ 0. From Barbalat's lemma <ref type="bibr" target="#b61">[62]</ref>, it follows that as t → ∞, Z → 0. The result holds as long as we can show that the state x(t) remains in the set ⊆ R n for all times. To this effect, define the following compact set M := {x ∈ R n |V(t) ≤ m} ⊂ R n , where m is chosen as the largest constant, so that M ⊆ . Since by assuming x 0 ∈ x and x ⊂ , we can conclude that x 0 ∈ . While x(t) remains inside , we have seen that V 0, and therefore, x(t) must remain inside M ⊂ . The fact that x(t) remains inside a compact set also excludes the possibility of finite escape time, and therefore, one has global the existence of solution.</p><p>V. SIMULATIONS This section presents two simulation examples to illustrate the effectiveness of the proposed optimal-adaptive control algorithm. In the simulations below, since the history stack is empty in the beginning, we need to add a dithering noise to the control input in the form of ρ(t) = (1/2)(sin(0.3πt) + cos(0.3πt)) for the first second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Van der Pol Oscillator</head><p>Consider a controlled Van der Pol oscillator of the form (1), with</p><formula xml:id="formula_90">f (x) = x 2 -x 1 - 1 2 x 2 1 -x 2 1 -x 2 1 x 2 , g(x) = 0 x 1<label>(48)</label></formula><p>which has an uncontrolled unstable limit cycle and a stable uncontrollable equilibrium point at the origin. It is shown in <ref type="bibr" target="#b62">[63]</ref> that the nonsaturated optimal control input, with a criterion V (x(0)) = ∞ 0 ( x 2 + u 2 )dτ , is given by K (x) = -x 1 x 2 , and that the corresponding optimal value function is given by V (x) = x 2 1 + x 2 2 . We now consider the optimal control of (48) with the input saturated, so that |u| ū = 0.1 and the cost defined as ( <ref type="formula" target="#formula_0">2</ref>)-(4) with Q(x) := x 2 and R = 1, for which the optimal feedback law is not known in a closed form. The NN weights are randomly initialized in the interval [0, 1], the activation functions were chosen to be quadratics of the form φ</p><formula xml:id="formula_91">(x) = [x 2 1 x 1 x 2 x 2 2 ]</formula><p>T and φ u (x) := ∇φ(x), and the tuning gains were set to α = 10 and α u = 2. Thus, the critic parameters converged to Ŵ = [2.4953 0.9991 2.2225]. Fig. <ref type="figure" target="#fig_0">1</ref> presents the phase plane trajectory of the system and the optimal control input, which is saturated when it reaches the maximum and minimum saturation limits. Fig. <ref type="figure" target="#fig_2">2</ref> shows the convergence of the critic parameters that take almost 5 s to converge.</p><p>It is well known that parameter convergence cannot be achieved for nonlinear systems without PE. Since PE is unverifiable for general nonlinear systems, trying to achieve parameter convergence for such systems is very difficult. In order to show the efficacy of our proposed approach  with relaxed PE compared with the tuning law <ref type="bibr" target="#b19">(20)</ref> without the second term (i.e., past data), we will compare the result shown in Fig. <ref type="figure" target="#fig_2">2</ref> in two different cases. The first case considers a strong PE (i.e., a large number of sinusoids of different frequencies and a high amplitude) that is applied for 0 ≤ t ≤ 40 s, and is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. The second case considers a weak PE (i.e., a modest number of sinusoids of different frequencies and a low amplitude) that is applied for 0 ≤ t ≤ 20 s, and is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. From Figs. <ref type="figure" target="#fig_3">3</ref> and<ref type="figure" target="#fig_4">4</ref>, one shall see how difficult is to guarantee PE throughout learning for every t ≥ 0. In particular, in the first case, the weight estimates reach the optimal solution after 20 s (compared with just 5 s with the proposed algorithm), and in the second case, the weights converge fast but get stuck in a local minimum. This happens since the PE condition given in Definition 2 is violated due to the reason that the states x reach zero either too late [i.e., ω(t) becomes zero after oscillating] or too early [i.e., ω(t) becomes zero before the weights reach the optimal solution]. Our proposed learning framework with previous data solves these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Power Plant System</head><p>Consider the power system shown in Fig. <ref type="figure" target="#fig_5">5</ref>, consisting of a turbine generator, a system load, and an automatic generation control. A simplified state-space model for this system is  Evolution of the critic parameters with a weak PE applied for 0 ≤ t ≤ 20 s. With the dashed lines, one can observe the optimal critic parameters as plotted in Fig. <ref type="figure" target="#fig_2">2</ref>. of the form <ref type="bibr" target="#b48">(49)</ref> where f G is the incremental frequency deviation, P m is the incremental change in the generator output, ᾱ is the incremental change in governor value position, and the control    <ref type="figure">-8</ref> show the state evolution, the incremental speed change in position deviation (control input), and the evolution of the critic NN weights, respectively. A perturbation of 5% is applied to the generator frequency in the interval of 7-11 s, and we can see the system's adaptation to the new load.</p><formula xml:id="formula_92">˙ ᾱ ˙ P m ˙ f G = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ - 1 T g 0 1 R g T g K t T t - 1 T t 0 0 K p T p - 1 T p ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ᾱ P m f G + ⎡ ⎣ 1 T g 0 0 ⎤ ⎦ P c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper proposed a new approximate dynamic programming algorithm for systems with bounded inputs, which relaxes the PE condition using previously stored data concurrently with current data. To suppress the effects of the critic and actor NN approximation errors, a new robustifying term was added to the controller. By considering an appropriate Lyapunov function, we prove the asymptotic stability of the overall closed-loop system. The simulation results of a controlled Van der Pol oscillator and a power system illustrate the effectiveness and efficiency of the proposed approach. Future work will be concentrated on extending the results for completely unknown systems and multiple decision makers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>PI for Nonlinear Systems 1: procedure 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Top: phase plane trajectory of the closed-loop system that shows convergence to the origin. Bottom: control input that is saturated when it reaches the saturation limits.</figDesc><graphic coords="10,73.43,244.25,201.62,151.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Convergence of the critic parameters to the optimal cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Evolution of the critic parameters with a strong PE applied for 0 ≤ t ≤ 40 s. With the dashed lines, one can observe the optimal critic parameters as plotted in Fig. 2.</figDesc><graphic coords="10,336.47,50.81,201.74,151.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.Evolution of the critic parameters with a weak PE applied for 0 ≤ t ≤ 20 s. With the dashed lines, one can observe the optimal critic parameters as plotted in Fig.2.</figDesc><graphic coords="10,336.47,247.25,201.74,151.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Power system block diagram.</figDesc><graphic coords="10,336.47,451.73,201.62,123.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Time evolution of the power system states with a perturbation of 5% applied in the generator frequency during 7-11 s.</figDesc><graphic coords="11,73.43,231.77,201.62,151.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Incremental speed change in position deviation (control input of the system).</figDesc><graphic coords="11,73.43,413.45,201.62,151.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .+ f 2 G</head><label>82</label><figDesc>Fig. 8. Convergence of the critic parameters to the optimal cost.</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This material is based upon work supported in part by ARO MURI Grant number W911NF0910553, ARO grant W911NF-09-D-0001 (Inst. for Collaborative Biotechnologies) and by grant CAPES No BEX-9586/11-3. K. G. Vamvoudakis and J. P. Hespanha are with the Center for Control, Dynamical-</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marcio</head><p>Fantini Miranda was born in Belo Horizonte, Brazil. He received the bachelor's degree in physics from the Universidade Federal de Minas Gerais (UFMG), Belo Horizonte, the master's degree in mechanical engineering from the University of Campinas, Campinas, Brazil, and the Ph.D. degree in electrical engineering from UFMG, in 2000.</p><p>He was an Assistant Professor with the Electrical Engineering and Control Engineering Department, UFMG, from 2000 to 2008. He is currently a Full Professor with the Technical School, UFMG. His current research interests include robust control, process control, multivariable control, and linear matrix inequalities. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brain-like intelligent control: From neural nets to largerscale systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th IEEE Conf. Decision Control</title>
		<meeting>36th IEEE Conf. Decision Control<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-12">Dec. 1997</date>
			<biblScope unit="page" from="3902" to="3904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intelligence in the brain: A theory of how it works and how to build it</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<title level="m">Dynamic Programming and Optimal Control</title>
		<meeting><address><addrLine>Belmont, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Athena Scientific</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lendaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saeks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="153" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate Dynamic Programming: Solving the Curses of Dimensionality</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Wiley Series in Probability and Statistics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
			<pubPlace>Princeton, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel actor-critic-identifier architecture for approximate optimal control of uncertain nonlinear systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kamalapurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vamvoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="92" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reinforcement Learning and Approximate Dynamic Programming for Feedback Control</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Wiley</publisher>
			<pubPlace>Piscataway, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="105" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online actor-critic algorithm to solve the continuous-time infinite horizon optimal control problem</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="878" to="888" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data-based optimal control for discrete-time zero-sum games of 2-D systems using adaptive critic designs</title>
		<author>
			<persName><forename type="first">Q.-L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-L</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Autom. Sinica</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="682" to="692" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Khalil</surname></persName>
		</author>
		<title level="m">Nonlinear Systems</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal control of nonlinear continuous-time systems: Design of bounded controllers via generalized nonquadratic functionals</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Lyshevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Amer. Control Conf</title>
		<meeting>Amer. Control Conf<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">Jun. 1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="205" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nearly optimal control laws for nonlinear systems with saturating actuators using a neural network HJB approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abu-Khalaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="791" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural-network-based constrained optimal control scheme for discrete-time switched nonlinear system using dual heuristic programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="839" to="849" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Actor-critic-based optimal tracking for partially unknown nonlinear discrete-time systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kiumarsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="151" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reinforcement learning controller design for affine nonlinear discrete-time systems using online approximators</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="377" to="390" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust adaptive dynamic programming for nonlinear control design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 51st Annu. Conf. Decision Control</title>
		<meeting>IEEE 51st Annu. Conf. Decision Control<address><addrLine>Maui, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12">Dec. 2012</date>
			<biblScope unit="page" from="1896" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming for a class of complex-valued nonlinear systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1733" to="1739" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wide-area measurement based dynamic stochastic optimal power flow control for smart grids with high variability and uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Harley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Smart Grid</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Experience replay for real-time reinforcement learning control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="212" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An analysis of experience replay in temporal difference learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cichosz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybern. Syst., Int. J</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="341" to="363" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tree-based batch mode reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="503" to="556" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural fitted Q iteration-First experiences with a data efficient neural reinforcement learning method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Eur. Conf. Mach. Learn</title>
		<meeting>16th Eur. Conf. Mach. Learn<address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparison of learning speed and ability to cope without exploration between DHP and TD(0)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fairbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw<address><addrLine>Brisbane, QLD, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Handbook of Learning and Approximate Dynamic Programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stable adaptive control using new critic designs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Workshop Virtual Intell</title>
		<meeting>9th Workshop Virtual Intell<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">Jun. 1998</date>
			<biblScope unit="page" from="510" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Residual algorithms: Reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Conf. Mach. Learn</title>
		<meeting>12th Int. Conf. Mach. Learn<address><addrLine>Tahoe City, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-07">Jul. 1995</date>
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ADP: Goals, opportunities and principles</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Learning and Approximate Dynamic Programming</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convergence of critic-based training</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Syst., Man, Cybern</title>
		<meeting>IEEE Int. Conf. Syst., Man, Cybern<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3057" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Consistency of HDP applied to a simple reinforcement learning problem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="189" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast gradient-descent methods for temporaldifference learning with linear function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>26th Annu. Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convergent temporal-difference learning with arbitrary smooth function approximation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1204" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An equivalence between adaptive dynamic programming with a critic and backpropagation through time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fairbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2088" to="2100" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Linear least-squares algorithms for temporal difference learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bradtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="33" to="57" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A menu of designs for reinforcement learning over time</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Webros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Control</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Miller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Sutton</surname></persName>
		</editor>
		<editor>
			<persName><surname>Werbos</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="67" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Finite-approximation-errorbased discrete-time iterative adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2820" to="2833" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Infinite horizon self-learning optimal control of nonaffine discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="866" to="879" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A novel iterative θ -adaptive dynamic programming for discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1176" to="1190" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Model-free optimal control design for a class of linear discrete-time systems with multiple delays using adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="163" to="170" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Online adaptive policy learning algorithm for H ∞ state feedback control of unknown affine nonlinear discrete-time systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2706" to="2718" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Finite horizon optimal control of non-linear discrete-time switched systems using adaptive dynamic programming with ε-error bound</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1683" to="1693" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Leader-based optimal coordination control for the consensus problem of multiagent differential games via fuzzy adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="163" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Data-driven neuro-optimal temperature control of water-gas shift reaction using stable iterative adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6399" to="6408" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multibattery optimal coordination control for home energy management systems via distributed iterative adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4203" to="4214" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A novel dual iterative Q-learning method for optimal battery management in smart residential environments</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2509" to="2518" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Approximate dynamic programming for real-time control and neural modeling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>White</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sorge</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Van Nostrand</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adaptive Control Tutorial (Advances in Design and Control)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fidan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Philadelphia, PA, USA: SIAM</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalized Hamilton-Jacobi-Bellman formulation-based neural network control of affine nonlinear discretetime systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="106" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Concurrent learning for convergence in adaptive control without persistency of excitation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 49th IEEE Conf. Decision Control</title>
		<meeting>49th IEEE Conf. Decision Control<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12">Dec. 2010</date>
			<biblScope unit="page" from="3674" to="3679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Technical update: Least-squares temporal difference learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fixed-final-time optimal control of nonlinear systems with terminal constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heydari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="61" to="71" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Integral reinforcement learning and experience replay for adaptive optimal control of partially-unknown constrained-input continuous-time systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-B</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive optimal control of unknown constrained-input systems using policy iteration and neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-B</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1513" to="1525" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On an iterative technique for Riccati equation computations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kleinman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="115" />
			<date type="published" when="1968-02">Feb. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="551" to="560" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Concurrent learning adaptive control of linear systems with exponentially convergent bounds</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yucelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mühlegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adapt. Control Signal Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="280" to="301" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optimal control of affine nonlinear continuous-time systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dierks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Amer. Control Conf</title>
		<meeting>Amer. Control Conf<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">Jun./Jul. 2010</date>
			<biblScope unit="page" from="1568" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On-line approximation control of uncertain nonlinear systems: Issues with control input saturation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Polycarpou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Amer. Control Conf</title>
		<meeting>Amer. Control Conf<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="543" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Nonlinear Dynamical Systems and Control: A Lyapunov-Based Approach</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chellaboina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Princeton Univ. Press</publisher>
			<pubPlace>Princeton, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Constrained nonlinear optimal control: A converse HJB approach</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nevistić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Primbs</surname></persName>
		</author>
		<idno>CIT-CDS 96-021</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. Control Dyn. Syst., California Inst. Technol</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Pasadena, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A multi-task automatic generation control for power regulation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Iracleous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Alexandridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electr. Power Syst. Res</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="285" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
