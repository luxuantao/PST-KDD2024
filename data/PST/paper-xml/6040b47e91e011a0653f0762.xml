<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingjian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
							<email>yang.yhx@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pre-training</term>
					<term>Language Modeling</term>
					<term>Knowledge Representation</term>
					<term>Heterogeneous Graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To enrich language models with domain knowledge is crucial but difficult. Based on the world's largest public academic graph Open Academic Graph (OAG), we pre-train an academic language model, namely OAG-BERT, which integrates massive heterogeneous entities including paper, author, concept, venue, and affiliation. To better endow OAG-BERT with the ability to capture entity information, we develop novel pre-training strategies including heterogeneous entity type embedding, entity-aware 2D positional encoding, and span-aware entity masking. For zero-shot inference, we design a special decoding strategy to allow OAG-BERT to generate entity names from scratch. We evaluate the OAG-BERT on various downstream academic tasks, including NLP benchmarks, zero-shot entity inference, heterogeneous graph link prediction and author name disambiguation. Results demonstrate the effectiveness of the proposed pre-training approach to both comprehending academic texts and modeling knowledge from heterogeneous entities. OAG-BERT has been deployed to multiple real-world applications, such as reviewer recommendations for NSFC (National Nature Science Foundation of China) and paper tagging in the AMiner system. OAG-BERT 1 is also available to the public through the CogDL package.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-trained language models such as BERT <ref type="bibr" target="#b8">[9]</ref>, GPT <ref type="bibr" target="#b36">[37]</ref> and XL-Net <ref type="bibr" target="#b44">[45]</ref> substantially promote the development of natural language processing. Besides pre-training for general purposes, more and more language models are targeting at specific domains, such as BioBERT <ref type="bibr" target="#b25">[26]</ref> for biomedical field and SciBERT <ref type="bibr" target="#b1">[2]</ref> for academic field, which establish new state-of-the-art on many domain-related benchmarks such as named entity recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, topic classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> and so on.</p><p>However, most of these models are only pre-trained over domain corpus, but ignore to integrate domain entity knowledge, which is crucial for many entity-related downstream tasks. In the author name disambiguation task, the affiliation of a paper could contribute by indicating the field-of-study of an author. For example, authors from Max Planck Institute may focus more on science and engineering rather than humanity. We may also produce fine-grained fields-of-study out of the database, by leveraging information from title, abstract, affiliations, and even their author names. This will substantially boost the effectiveness for researchers to identify their related work of interest.</p><p>As a complement to corpora, many domain knowledge graphs are available to provide knowledge. For example, OAG <ref type="bibr" target="#b46">[47]</ref> is the largest publicly available heterogeneous academic entity graph containing more than 700 million entities, including paper, author, field-of-study, venue and affiliation; and 2 billion relationships. If we can incorporate such tremendous amount of knowledge into domain language models, numerous downstream applications will be benefited. However, despite the abundant available knowledge sources, how to inject knowledge into language models has become the key problem. While many works have been concentrating on injecting the homogeneous entity and relation knowledge from large scale knowledge graphs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51]</ref>, none of them considers the problem of heterogeneous scenario which usually holds true in practice.</p><p>To bridge the gap, we propose OAG-BERT to jointly model texts and heterogeneous entity knowledge via pre-training over the OAG. We collect around 5 million full papers and 110 million abstracts as corpora, and 70 million paper ego-networks including its authors, fields of study, venues and affiliations. To handle the heterogeneity, we design the heterogeneous entity type embeddings for each type of entity respectively. To implement the masked language pre-training over entity names with various lengths, we design a novel span-aware entity masking strategy that can select to mask a continuous span of tokens according to entity lengths. To better "notice" the OAG-BERT with the entity spans and sequence order, we propose the entity-aware 2D positional encoding to take both the inter-entity sequence order and intra-entity token order into consideration.</p><p>We first evaluate the OAG-BERT on various types of downstream applications, including traditional academic NLP datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>, novel zero-shot entity inference <ref type="bibr" target="#b34">[35]</ref>, heterogeneous graph learning (link prediction) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> and author name disambiguation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50]</ref>. For zero-shot inference, we develop a special decoding strategy arXiv:2103.02410v1 [cs.CL] 3 Mar 2021 for OAG-BERT, allowing it to generate fluent sequences like GPTs. Not only do the experiment results demonstrate OAG-BERT's competitive performance to previous pre-trained models on ordinary language benchmarks, but also support its outstanding grasp of entity knowledge by outperforming over tasks that heavily depend on entity knowledge.</p><p>To sum up, we make the following contributions in this paper: • We propose to study the problem of enriching pre-trained language models with heterogeneous entity knowledge. To solve the problem, we design heterogeneous entity type embedding, spanaware entity masking and entity-aware 2D positional encoding.</p><p>We also develop a special decoding strategy for BERT-style models to generate high-quality entities from scratch. • We present the OAG-BERT, an entity knowledge augmented academic language model that is pre-trained over 5 million paper full-text, 110 million paper abstracts and billions of academic entities and relations from the OAG. It has the similar number of parameters with other BERT-based models such as SciBERT. • We conduct relatively extensive experiments to demonstrate OAG-BERT's capability of traditional language tasks, zero-shot inference, heterogeneous graph learning and author name disambiguation. OAG-BERT has been deployed as the infrastructure of AMiner system<ref type="foot" target="#foot_0">2</ref> for OAG downstream applications. It is open to the public access through the CogDL <ref type="bibr" target="#b45">[46]</ref> package. • We apply the pre-trained OAG-BERT model to several real-world applications, such as the NSFC reviewer recommendation. It is also employed as a fundamental component in the AMiner system, which is further used to improve the performance on tasks like automatic paper tagging or author name disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• We release the pre-trained OAG-BERT model in CogDL package</head><p>for open access and free use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Our proposed OAG-BERT model is based on BERT <ref type="bibr" target="#b8">[9]</ref>, a selfsupervised <ref type="bibr" target="#b28">[29]</ref> bidirectional language model. It employs multilayer transformers as its encoder and uses masked token prediction as its objective, which allows using massive unlabeled text data as training corpus. The model architecture and training scheme have been shown to be effective on various natural language tasks, such as question answering or natural language inference. BERT has many variants. Some focus on the robustness of the pre-training process, like RoBERTa <ref type="bibr" target="#b29">[30]</ref>. Some others try to incorporate more knowledge into the natural language pre-training. SpanBERT <ref type="bibr" target="#b17">[18]</ref> develops span-level masking which benefits span selection tasks. ERNIE <ref type="bibr" target="#b50">[51]</ref> introduces explicit knowledge graph inputs to the BERT encoder and achieves significant improvements over knowledge-driven tasks.</p><p>As for the academic domain, previous works such as BioBERT <ref type="bibr" target="#b25">[26]</ref> or SciBERT <ref type="bibr" target="#b1">[2]</ref> leverage the pre-training process on scientific domain corpus and achieve state-of-the-art performance on several academic NLP tasks. The S2ORC-BERT <ref type="bibr" target="#b30">[31]</ref>, applies the same method with SciBERT on a larger scientific corpus and slightly improves the performance on downstream tasks. Later works <ref type="bibr" target="#b14">[15]</ref> further show that continuous training on specific domain corpus also benefits the downstream tasks. These academic pre-training models rely on large scientific corpora. SciBERT uses the semantic scholar corpus <ref type="bibr" target="#b0">[1]</ref>. Other large academic corpora including AMiner <ref type="bibr" target="#b40">[41]</ref>, OAG <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref>, and Microsoft Academic Graph (MAG) <ref type="bibr" target="#b19">[20]</ref> also integrate massive publications with rich graph information as well, such as authors and research fields.</p><p>On academic graphs, there are some tasks that involve not only text information from papers but also structural knowledge lying behind graph links. For example, to disambiguate authors with the same names <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50]</ref>, the model needs to learn node representations in the heterogeneous graph. To better recommend papers for online academic search <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, graph information including related academic concepts and published venues could provide great benefits. To infer experts' trajectory across the world <ref type="bibr" target="#b43">[44]</ref>, associating authors with their affiliation on semantic level would help. Capturing features from paper titles or abstracts is far from enough for these types of challenges.</p><p>Targeting at graph-based problems, many graph representation learning methods were proposed in the last decade. Works like node2vec <ref type="bibr" target="#b13">[14]</ref> and ProNE <ref type="bibr" target="#b47">[48]</ref> focus on purely homogeneous graph structures and metapath2vec <ref type="bibr" target="#b10">[11]</ref> later extends the idea to heterogeneous graphs. Neural-based methods like GCN <ref type="bibr" target="#b23">[24]</ref> successfully introduce neural networks to solve the graph learning problem. Recent works including Heterogeneous Graph Transformer <ref type="bibr" target="#b16">[17]</ref> and GPT-GNN <ref type="bibr" target="#b15">[16]</ref> similarly borrow the idea from the natural language community, applying transformer blocks and pre-training scheme on graph tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>The proposed OAG-BERT is a bidirectional transformer-based pretraining model. It can encode scientific texts and entity knowledge into high dimensional embeddings, which can be used for downstream tasks such as predicting the published venue for papers. We build the OAG-BERT model on top of the conventional BERT <ref type="bibr" target="#b8">[9]</ref> model with 12 transformer <ref type="bibr" target="#b42">[43]</ref> encoder layers.</p><p>While the original BERT model only focuses on natural language, our proposed OAG-BERT also incorporates heterogeneous entity knowledge. In other words, in addition to learning from pure scientific texts such as paper title or abstract, the OAG-BERT model can comprehend other types of information, such as the published venues or the affiliations of paper authors. To achieve that, we made several modifications to the model architecture and the pre-training process. We will introduce them in the following sections. An overview of the proposed OAG-BERT model is depicted in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>The key challenge for OAG-BERT lies in how to integrate knowledge into language models. Previous approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b50">51]</ref> mainly focus on injecting homogeneous entities and relations from knowledge graphs like Wikidata, and very few of them look into situations where there are heterogeneous entities.</p><p>To augment OAG-BERT with various types of entity knowledge, we place title, abstract along other entities from the same paper in a single sequence as one training instance (see Figure <ref type="figure">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OAG-BERT</head><formula xml:id="formula_0">+ (0, 0) (0, 1) (…) (0, i-1) (0, i) (1, 0) (1, 1) (2, 0) (2, 1) (3, 0) (3, 1) (3, 2) (3, 3) (3, 4) (4, 0) (4, 1) (4, 2) (…)</formula><p>Figure <ref type="figure">2</ref>: Heterogeneous entity augmentation in OAG-BERT. 1) For different entity types, we design heterogeneous entity type embedding. 2) For comparatively long entities (such as the "Knowledge Discovery and Data Mining"), we leverage the spanaware entity masking strategy, which selects a continuous span in the entity. 3) For positional embeddings across different entities, we design an entity-aware 2D-positional embedding strategy, whose first dimension is designed for indicating interentity sequence order, and the second dimension is designed for indicating intra-entity token sequence order.</p><p>There are five types of entities in total. We treat the text features (title and abstract) of a paper as one special text entity. The published venue, authors, affiliations, and research fields are the rest four types of entities. Following the notation in OAG, we use FOS (field-of-study) to denote research fields. Thanks to OAG, for venues, authors, affiliations, and FOS, their names have been cleaned up, deduplicated, and unified, which enables OAG-BERT to learn a consistent representation for each entity.</p><p>All the entities from one paper are concatenated as an input sample. To help the OAG-BERT model distinguish them, we use another three techniques: Heterogeneous entity type embedding, Entity-aware 2D-positional encoding and Span-aware entity masking.</p><p>Heterogeneous entity type embedding. The original BERT employs the next sentence prediction loss (NSP) to learn the relationship between sequences, which requires the use of token type embeddings to distinguish two sequences from each other. Tokens from two sequences are added by different token type embeddings. However, NSP loss is believed to harm rather than improve BERT's performance, as found in later works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. Therefore, in this work, we abandon the NSP loss and discard the old token type embeddings.</p><p>On the other hand, in order to distinguish different types of entities, we propose to leverage entity type embedding in the pretraining process to indicate entity type, whose usage is similar to the token type embedding used in BERT.</p><p>For example, given the title and abstract of a paper "ArnetMiner: extraction and mining of academic social networks", we retrieve its authors, fields of studies, venues, and affiliation entities and concatenate them into a sequence less than 512 tokens. For pure text (such as title and abstract), we label them with the original entity type index (e.g., 0) to acquire its entity type embedding. For author entities (such as Jie Tang), we label them with author type index (e.g., 1). So are for other entities. What's more, because entities are order-invariant in the sequences, we shuffle their order in a sample sequence to avoid our model to learn any positional biases of these entities.</p><p>Entity-aware 2D-positional encoding. Although the transformer <ref type="bibr" target="#b42">[43]</ref> architecture has achieved great success in sequencebased tasks, it is also known that the transformer itself is permutation-invariant (i.e. is not aware of the sequence order). The critical technique of applying transformer to natural language is to add a positional embedding to indicate the sequence order, including the absolute positional embedding used in vanilla Transformer <ref type="bibr" target="#b42">[43]</ref> and BERT <ref type="bibr" target="#b8">[9]</ref>, and the relative positional embedding developed in Transformer-XL <ref type="bibr" target="#b6">[7]</ref> and XLNet <ref type="bibr" target="#b44">[45]</ref>.</p><p>However, when we want OAG-BERT to capture entity knowledge, neither of them is applicable. This is because the conventional positional embedding can not distinguish words from entities that are adjacent to each other and of the same type. For instance, if there are two affiliations "Tsinghua University" and "Unviersity of California" being placed next to each other in a sequence, the transformer would assume that there is an affiliation named "Tsinghua University University of California".</p><p>To sum up, our requests could be summarized to two points: 1) the positional embedding should imply the inter-entity sequence order (which is used to distinguish different entities) and 2) the positional embedding should indicate the intra-entity token sequence order (which is used as the traditional positional embedding).</p><p>In light of this, we design the entity-aware 2D-positional embedding that solves both the inter-entity and intra-entity problem (see Figure <ref type="figure">2</ref>). The first dimension is for inter-entity order, indicating the token is in which entity; the second dimension is for intra-entity order, indicating the sequence of tokens. For a given position, the final positional embedding is calculated by adding the two positional embeddings together.</p><p>Span-aware entity masking. When performing masking, for pure text contents such as paper title and abstract, we adopt the same random masking strategy as in BERT. However, for entities such as author names, field of study, venues and affiliations, to encourage OAG-BERT to memorize them, we develop a span-aware entity masking strategy which combines the advantages of both ERNIE <ref type="bibr" target="#b50">[51]</ref> and SpanBERT <ref type="bibr" target="#b17">[18]</ref>.</p><p>The intuition of using this strategy is that, some of the entities are too long and thus too difficult for the OAG-BERT to learn. The span-aware entity masking strategy not only alleviates the problem, but also still preserves the sequential relationship of an entity's tokens: for entity that has less than 4 tokens, we will mask the whole entity; and for others, we sample masked lengths from a geometric distribution Geo(𝑝) which satisfies:</p><formula xml:id="formula_1">𝑝 = 0.2 , and 4 ≤ Geo(p) ≤ 10 (1)</formula><p>If the sampled length is less than the entity length, we will only mask out the entity. For text contents and entity contents, we mask 15% of the tokens for each respectively.</p><p>Pre-LN BERT. Except for the previous changes to the original BERT architecture, we further adopt the Pre-LN BERT as used in deepspeed <ref type="bibr" target="#b37">[38]</ref> , where layer normalization is placed inside the residual connection instead of after the add-operation in Transformer blocks. Previous work <ref type="bibr" target="#b48">[49]</ref> demonstrates that training with Pre-LN BERT avoids vanishing gradients when using aggressive learning rates. Therefore, it is shown to be more stable than the traditional Post-LN version for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training Details</head><p>The pre-training of OAG-BERT is separated into two stages. In the first stage, we only use scientific texts (paper title, abstract, and body) as the model inputs, without using the entity augmented inputs introduced above. This process is similar to the pre-training of the original BERT model. We name the intermediate pre-trained model as the vanilla version of OAG-BERT. In the second stage, based on the vanilla OAG-BERT, we continue to train the model on the heterogeneous entities, including title, abstract, venue, authors, affiliations, and field-of-studies (FOS).</p><p>First Stage: Pre-train the vanilla OAG-BERT. In the first stage of pre-training, we construct the training corpus from two sources: one comes from the PDF storage of AMiner, which mainly consists of arXiv PDF dumps; the other comes from the PubMed XML dump.</p><p>We clean up and sentencize the corpus with SciSpacy <ref type="bibr" target="#b32">[33]</ref>. The corpus adds up to around 5 million unique paper full-text from multiple disciplines. In terms of vocabulary, we construct our OAG-BERT vocabulary using WordPiece, which is also used in original BERT implementation. This ends up with 44,000 unique tokens in our vocabulary.</p><p>For better handling the entity knowledge of authors in the OAG, in the data prepossessing we transform the author name list as a sentence for each paper and place it between the title and abstract. Therefore, compared to previous models like SciBERT, our vocabulary contains more tokens from author names.</p><p>Following the training procedures of BERT, the vanilla OAG-BERT is first pre-trained on samples with a maximum of 128 tokens. After the loss has converged, we shift to pre-training it over samples with 512 tokens.</p><p>Second Stage: Enrich OAG-BERT with entity knowledge. In the second stage of pre-training, we use papers and related entities from the OAG corpus. Compared to the corpus used in the first stage, we do not have full texts for all papers in OAG. Thus, we only use paper title and abstract as the paper text information. From this corpus, we picked all authors with at least 3 papers published. Then we filtered out all papers not linked to these selected authors. Finally, we got 120 million papers, 10 million authors, 670 thousand FOS, 53 thousand venues, and 26 thousand affiliations. Each paper and its connected entities are concatenated into a single training instance, following the input construction method described above. In this stage, we integrate the three strategies mentioned in Section 3.1 to endow OAG-BERT the ability to "notice" the entities, rather than regarding them as pure texts.</p><p>Our pre-training is conducted with 32 Nvidia Tesla V100 GPUs and an accumulated batch size of 32768. We use the default BERT pre-training configurations in deepspeed. We run 16K steps for the first stage pre-training and another 4K steps for the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we will introduce several experiments to demonstrate the effectiveness of our proposed OAG-BERT. First, to exhibit how OAG-BERT works on multi-type information, we design intuitive zero-shot inference tasks. Then, we make extensions to supervised classification tasks. We further apply the pre-trained embeddings to name disambiguation and link prediction tasks, which present the superior capability of OAG-BERT in leveraging various types of entities. Finally, on the NLP tasks used by SciBERT, we additionally verify that the proposed OAG-BERT model can also achieve competitive results with text-only information provided. An overview of the model performance is shown in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Zero-shot Inference</head><p>Although not using unidirectional decoder structure like GPT-3, we find that the bidirectional encoder-based OAG-BERT is also capable of decoding entities based on the knowledge it learned during the pre-training process. We develop a simple extension to the Masked Language Model (MLM) to achieve that.</p><p>In MLM, the token prediction task in the pre-training process can be seen as maximizing the probability of masked input tokens. It treats the predictions for each token as independent processes. The target can be denoted as maximizing 𝑤 ∈masked log 𝑃 (𝑤 |C), where masked is the collection of masked tokens and C denotes contexts, which represents the inputs of MLM, including both input tokens and position information.</p><p>In the entity decoding process, we cannot ignore the dependencies between tokens in each entity, which requires us to jointly consider the probability of all tokens in one entity as following log 𝑃 (𝑤 1 , 𝑤 2 , ..., 𝑤 𝑙 |C), where 𝑙 is the entity length and 𝑤 𝑖 is the 𝑖-th token in the entity. As MLM is not unidirectional model, the decoding order for the tokens in one entity can be arbitrary. Suppose the decoding order is 𝑤 𝑖 1 , 𝑤 𝑖 2 , ..., 𝑤 𝑖 𝑙 , where 𝑖 1 , 𝑖 2 , ..., 𝑖 𝑙 is a permutation of 1, 2, ..., 𝑙. Then the prediction target can be reformed as maximizing ∑︁</p><formula xml:id="formula_2">1≤𝑘 ≤𝑙 log 𝑃 (𝑤 𝑖 𝑘 |C, 𝑤 𝑖 1 , 𝑤 𝑖 2 , ..., 𝑤 𝑖 𝑘−1 )<label>(2)</label></formula><p>However, the number of possible decoding orders is 𝑙!, which makes it extremely expensive to calculate while dealing with long entities. Thus, we adopt two strategies to solve this problem. First, while calculating the probability for one given entity, we use greedy selection to decide the decoding order. In other words, for each round of decoding, we choose the token with maximal probability to decode. An example is depicted in Figure <ref type="figure" target="#fig_1">3</ref>. Second, when decoding an entity from scratch, we use beam search <ref type="bibr" target="#b41">[42]</ref> to search the token combinations with the highest probability. Another challenge for decoding using the MLM model is to choose the appropriate entity length. Instead of using fixed length Table <ref type="table">1</ref>: The summary of model performance for all tasks. We report the performance of only using paper titles as inputs in title-only and the best performance of using other features such as FOS or venue as inputs in mixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Zero-shot Inference 1,2 Hit@1 is reported for zero-shot inference and supervised classification. 3 NA is short for Name disambiguation. The macro pairwise f1 score is reported. 4 For link prediction tasks, we use pre-trained models to encode all types of nodes. Only title was provided for paper nodes. NDCG is reported.  while decoding from scratch, we traverse all entity lengths in a pre-defined range depending on the entity type and choose top candidates according to the calculated probability in Equation <ref type="formula" target="#formula_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OAG-BERT</head><p>We design three zero-shot inference tasks to evaluate the entity generation capability of our proposed OAG-BERT and make comparisons with SciBERT, the current state-of-the-art pre-training model in academic domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Field-of-Study (FOS) Inference</head><p>To evaluate the performance of decoding field-of-study, we adopt the research field prediction task from MAG (Microsoft Academic Graph) <ref type="bibr" target="#b38">[39]</ref>. First, we choose 19 top-level field-of-studies (FOS) such as "biology" and "computer science". Then, from the paper data which were not used in the pre-training process, we randomly select 1,000 papers for each FOS. The task is to predict which research field each paper belongs to.</p><p>For each paper, we estimate the probabilities for all FOS candidates and choose the top one. When estimating each one, we concatenate the FOS candidate with the paper title as model input and mask the FOS candidate. For example, when estimating the probability of "computer science", we add two "[MASK]" tokens to the end of the original title as the input. For OAG-BERT, we treat the newly added "[MASK]" tokens as a new entity, reset entity positions and use FOS entity type embedding additionally. Then we use Equation 2 to calculate the probability. This is denoted as the Plain method, as depicted in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>We also apply two techniques to improve the model decoding performance. The first technique is to add extra prompt word to the end of the paper title (before masked tokens). We select "Field of study:" as the prompt words in the FOS inference task. The second technique is to concatenate the paper abstract to the end of the paper title.</p><p>Venue and Affiliation Inference Similar to the FOS inference task, we create venue and affiliation inference tasks. From nonpretrained papers, we choose 30 most frequent arXiv categories and 30 affiliations as inference candidates, with 100 papers randomly selected for each candidate. Full lists of the candidates including FOS candidates are enclosed in the appendix.</p><p>The experiment settings completely follow the FOS inference task, except that we use "Journal or Venue:" and "Affiliations:" as prompt words respectively. The entity type embeddings for masked entities in OAG-BERT are also replaced by venue and affiliation entity type embeddings accordingly. We report the Hit@1 and MRR scores in Table <ref type="table" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Analysis</head><p>In Table <ref type="table" target="#tab_3">2</ref>, we can see that the proposed augmented OAG-BERT outperforms SciBERT by a large margin. Although SciBERT was not pre-trained with entity knowledge, it still performs much greater than a random guess, which means the inference tasks are not independent of the paper content information. We speculate that the pre-training process on paper content (as used in SciBERT) also helps the model learn some generalized knowledge on other types of information, such as field-of-studies or venue names. We also observe that the proposed use of abstract can always help improve the performance. On the other hand, the prompt words works well with SciBERT but only provide limited help for OAG-BERT. Besides, the affiliation inference task appears to be harder than the other two tasks. Further analysis are provided in the A.1. Two extended experiments are enclosed as well, which reveal two findings:</p><p>(1) Using the summation of token log probabilities as the entity log probability is better than using the average. (2) The out-of-order decoding is more suitable for encoder-based models like SciBERT and OAG-BERT, as compared with the left-to-right decoding. Case Study To exhibit the capability of decoding entities, we applied the method described above on the task of FOS generation. Given the paper title and abstract, we use beam search with a width of 16 to decode FOS entities. We search from single-token entities to quadruple-token entities. The top 16 generated ones are listed in Table <ref type="table" target="#tab_4">3</ref>. In Table <ref type="table" target="#tab_4">3</ref>, the gold FOS are all in the top 16. Some fine-grained entities, though not in candidates, are also generated, such as Autoregressive language model or Few shot learning. However, we can still observe some ill-formed or inappropriate entities such as Architecture or Artificial language processing. While the paper is related to Model architecture, the single-token Architecture usually refers to the science of designing buildings, which is not suitable in this case. Artificial language model, on the other hand, is more like a combination of Artificial Intelligence and Language model, which have already been generated. In summary, although our proposed OAG-BERT model is not born for decoding, it still exhibits the potential of generating highquality entities in the zero-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised Classification</head><p>In this section, we develop the supervised classification tasks on top of the datasets described above, which are enlarged by 10 times following the same generating process. The data in the zero-shot inference are kept as test sets. We construct validation sets to select the best models during fine-tuning, with the same size as the test sets. The rest data are used as training sets. The sizes of all datasets for all tasks are enclosed in the appendix.</p><p>In supervised classification tasks, we remove the masked tokens and feed the averaged output embeddings from the pre-training models to a single fully-connected layer. We apply softmax layer to make predictions at last. As for the inputs, to present the effectiveness of heterogeneous entity types, we not only use paper titles as inputs but also concatenate other entities. Besides, we also tested the model performance with and without the original pre-training model parameters frozen. We follow the standard configurations for fine-tuning BERT, which are enclosed in the appendix.</p><p>As shown in Table <ref type="table" target="#tab_5">4</ref>, the OAG-BERT outperforms SciBERT by a large margin when the parameters in pre-trained parts are frozen. When not frozen, for venue and affiliation prediction, OAG-BERT surpasses SciBERT significantly. In FOS prediction, although OAG-BERT under-performs SciBERT in some cases, the best performance for using all available entities in OAG-BERT still beats the one reached by SciBERT.</p><p>We also observe that different types of entities contribute to various tasks in dissimilar ways. For example, the use of author information is particularly helpful for affiliation prediction but not very useful in FOS prediction. On the other hand, the field of study (FOS) inputs, work pretty well in venue prediction but provide marginal improvements to affiliation prediction.</p><p>In conclusion, the proposed OAG-BERT is effective in both zeroshot tasks and supervised tasks. The additionally learned heterogeneous entities can help the model reach better performance while dealing with multiple types of inputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Name Disambiguation</head><p>Previous experiments focus on the decoding capability and finetuning performance on downstream tasks. In this section, we adopt the name disambiguation problem to validate the paper representation quality produced by OAG-BERT. In this problem, given a set of papers with authors of the same name, the designed algorithm needs to separate these papers into several clusters, where papers in the same cluster belong to the same author and different clusters represent different authors.</p><p>We use the public dataset whoiswho-v1 3 <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50]</ref> and apply the embeddings generated by pre-trained models to solve name disambiguation from scratch. Formally, for each paper, we use the paper title and other attributes such as field-of-study or published venue as model input. Then we average over all the output token embeddings for the paper and use it as the paper embedding. After that, we build a graph with all papers as the graph nodes and set a threshold to select edges. The edges are between papers where the pairwise cosine similarity of their embeddings is larger than the threshold. Finally, for each connected component in the graph, we treat it as a cluster. We searched the thresholds from 0.65 to 0.95 on the validation set. The threshold from the best validation results is used on test set evaluation. We calculated the macro pairwise f1 score following previous works.</p><p>The results in Table <ref type="table" target="#tab_6">5</ref> indicate that the embedding of OAG-BERT is significantly better than the SciBERT embedding while directly used in the author name disambiguation. We also observe that for SciBERT the best threshold is always 0.8 while this value for OAG-BERT is 0.9, which reflects that the paper embeddings produced by OAG-BERT are generally closer than the ones produced by SciBERT.</p><p>In Table <ref type="table" target="#tab_6">5</ref> we only list the results with title, field-of-study, and venue as inputs. Though we attempted to use the abstract, author, and affiliation information, there is no performance improvement as expected. We speculate it is because these types of information are more complex to use, which might require additional classifier head or fine-tuning, as the supervised classification task mentioned above. In addition, we also report the top 1 score in the name disambiguation challenge leaderboard 4 and find that our proposed OAG-BERT reaches close performance as compared with the top 1.</p><p>3 https://www.aminer.cn/whoiswho 4 https://www.biendata.xyz/competition/aminer2019/leaderboard/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Link Prediction</head><p>In previous sections, we present the effectiveness of using OAG-BERT individually. In this section, we apply the heterogeneous entity embeddings of OAG-BERT as pre-trained initializations for node embeddings on the academic graph and show that OAG-BERT can also work together with other types of models. Specifically, we take the heterogeneous graph transformer (HGT) model from <ref type="bibr" target="#b16">[17]</ref> and combine it with the pre-trained embeddings from OAG-BERT.</p><p>To make predictions for the links in the heterogeneous graph, the authors of HGT first extract node features and then apply HGT layers to encode graph features. For paper nodes, the authors use XLNet <ref type="bibr" target="#b44">[45]</ref> to encode titles as input features. For other types of nodes, HGT use metapath2vec <ref type="bibr" target="#b10">[11]</ref> to initialize the features.</p><p>However, there are two problems with using XLNet on the heterogeneous academic graph. First, the XLNet was pre-trained on universal language corpus, which is lack of academic domain data. Second, XLNet can only encode paper nodes by using their titles and is unable to generate useful embeddings for other types of nodes like author or affiliation.</p><p>To this end, we propose to replace the original XLNet encoder with our OAG-BERT model, which can tackle the two challenges mentioned above. We use the OAG-BERT model to encode all types of nodes and use the generated embeddings as their node features. To prove the effectiveness of OAG-BERT on encoding heterogeneous nodes, we also compare the performance of SciBERT with OAG-BERT. We experimented on the CS dataset released by HGT <ref type="foot" target="#foot_1">5</ref> . The details of the dataset are delivered in the appendix.</p><p>The NDCG and MRR scores for the Paper-Field and Paper-Venue link prediction are reported in Table <ref type="table" target="#tab_8">7</ref>. It shows that SciBERT surpasses the original XLNet performance significantly, due to the pre-training on the large scientific corpus. Our proposed OAG-BERT made further improvements on top of that, as it can better understand the entity knowledge on the heterogeneous graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">NLP Tasks</head><p>Previous experiments have demonstrated the superiority of OAG-BERT on tasks involving multi-type entities. In this section, we will further explore the performance of OAG-BERT on natural language processing tasks, which only contain text-based information such as paper titles and abstracts. We will show that although pre-trained with heterogeneous entities, the OAG-BERT can still perform competitive results with SciBERT on NLP tasks.</p><p>We made comparisons over three models, including SciBERT (both the original paper results and the reproduced results), S2ORC (similar to SciBERT except pre-trained with more data), and OAG-BERT (both the vanilla version and the augmented version).</p><p>In accord with SciBERT <ref type="bibr" target="#b1">[2]</ref>, we evaluate the model performance on the same 12 NLP tasks, including Named Entity Recognition (NER), Dependency Parsing (DEP), Relation Extraction (REL), PICO Extraction (PICO), and Text Classification (CLS). These tasks only focus on single sentence representation so we add another three sequential sentence classification (SSC) tasks used in <ref type="bibr" target="#b4">[5]</ref>, to further verify the capability of pre-training models on long texts. The evaluation metrics are also accord with the usage in SciBERT <ref type="bibr" target="#b1">[2]</ref> and 1 Samples refers to the number of training samples in the dataset. 2 We run the fine-tuning process for 5 times with different random seeds and report the mean and standard deviation. The results in the original paper of SciBERT do not report this. The results for NER tasks in the original SciBERT model use a different casing version of pre-trained model while all other results are achieved by uncased pre-trained models. Sequential-Sentence-Classification <ref type="bibr" target="#b4">[5]</ref>, which can be found in the appendix along with the task details and hyper-parameter settings.</p><p>The results in Table <ref type="table" target="#tab_7">6</ref> show that the proposed OAG-BERT is competitive with SciBERT and a bit behind the S2ORC. Comparing with the reproduced SciBERT, our vanilla OAG-BERT only shows clear disadvantages on the SciERC REL task and the ACL-ARC CLS task, where datasets are relatively small and are sensitive to a few swinging samples. We ascribe the minor differences in other tasks to the differences in training corpus and the data cleaning techniques. The augmented OAG-BERT, although trained with heterogeneous entities that differ from the inputs of downstream NLP tasks, still presents similar performance to the vanilla version.</p><p>In summary, despite the fact that the OAG-BERT does not surpass the previous state-of-the-art academic pre-training model on NLP tasks, it still keeps the knowledge on these language dedicated tasks even after pre-training with multiple types of entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DEPLOYED APPLICATIONS</head><p>In this section, we will introduce several real-world applications where our OAG-BERT is employed.</p><p>First, the results on the name disambiguation tasks indicate that the OAG-BERT is relatively strong at encoding paper information with multi-type entities, which further help produce representative embeddings for the paper authors. Thus, we apply the OAG-BERT to the NSFC reviewer recommendation problem <ref type="bibr" target="#b5">[6]</ref>. The National Natural Science Foundation of China is one of the largest funders of basic science, where an enormous number of applications are reviewed every year. Finding appropriate reviewers for applications is time-consuming and laborious. To tackle this problem, we collaborate with Alibaba and develop a practical algorithm on top of the OAG-BERT which can automatically assign proper reviewers to applications and greatly benefits the reviewing process.</p><p>In addition to that, we also integrate the OAG-BERT as a fundamental component for the AMiner <ref type="bibr" target="#b40">[41]</ref> system. In AMiner, we utilize OAG-BERT to handle rich information on the academic heterogeneous graph. For example, with the ability of decoding FOS entities, we use the OAG-BERT to automatically generate FOS candidates for unlabeled papers. Besides, we similarly amalgamate the OAG-BERT into the name disambiguation framework. Finally, we employ OAG-BERT to recommend related papers for users, leveraging its capability in encoding paper embeddings.</p><p>Moreover, we release the OAG-BERT model in CogDL package, helping users take advantages of our OAG-BERT model in their own applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSTION</head><p>In conclusion, we propose a new pre-training model in the academic domain, called OAG-BERT. Compared to previous models like SciBERT, the OAG-BERT incorporates entity knowledge during pre-training, which benefits lots of downstream tasks that involve multi-type entities, such as name disambiguation or link prediction on the heterogeneous academic graph. We apply OAG-BERT to real-world applications, which improves the efficiency of these applications. We finally release the pre-trained model in CogDL, providing free use to arbitrary users.</p><p>There are still some problems remained. First, although OAG-BERT can decode entities, it is hard to generate long entities efficiently, due to the exhaustive search for the entity length. Second, the learning for sparse entities such as author names is much less A EXPERIMENT SUPPLEMENTARY A.1 Zero-shot Inference Use of Prompt Word As shown in Table <ref type="table" target="#tab_3">2</ref>, the use of proposed prompt words in the FOS inference task, turns out to be fairly useful for SciBERT to decode paper fields (FOS). We conjecture it is because the extra appended prompt words can help alter the focus of the pre-training model while making predictions on masked tokens. However, the improvement for SciBERT is marginal on affiliation inference. When decoding venue, it even hurts the performance. This is probably due to the improper choice of prompt words.</p><p>For OAG-BERT, this technique has limited help as our expectation. Instead of using continuous positions as SciBERT, OAG-BERT encodes inter-entity positions to distinguish different entities and paper texts. Thus the additional appended prompt word is treated as part of the paper title and is not adjacent to the masked entities for OAG-BERT.</p><p>Use of Abstract The use of abstract can greatly improve the model inference performance in both SciBERT and OAG-BERT. Both models frequently accept long text inputs in the pre-training process, which makes them naturally favor abstracts. Besides, abstracts contain rich text information which can help the pre-training model capture the main idea of the whole paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Comparisons</head><p>The affiliation generation task appears to be much harder than the other two tasks. This is probably due to the weak semantic information contained in affiliation names. The words in field-of-studies can be seen as sharing the same language with paper contents and most venue names also contain informative concept words such as "Machine Learning" or "High Energy". This is not always true for affiliation names. For universities like "Harvard University" or "University of Oxford", their researchers could focus on multiple unrelated domains which are hard for language models to capture. For companies and research institutes, some may focus on a single domain but it is not necessary to have such descriptions in their names, which also confuses the pre-training language model. Discussion for Entity Probability In Equation <ref type="formula" target="#formula_2">2</ref>, we use the sum of log probabilities of all tokens to calculate the entity log probability. This method seems to be unfair for entities with longer lengths as the log probability for each token is always negative. However, for MLM-based models, the encoding process not only encodes "[MASK]" tokens but also captures the length of the masked entity and each token's position. Therefore, if the pre-training corpus has fewer long entities than short entities, in the decoding process, the decoded tokens in a long entity will generally receive higher probability, compared to the ones in a short entity.</p><p>Even so, the sum of log probabilities is still not necessary to be the best choice depending on the entity distribution in the pretraining corpus. We conduct a simple experiment to test different average methods. We reform the calculation of entity log probability in Equation 2 as 1 𝐿 𝛼 1≤𝑘 ≤𝑙 log 𝑃 (𝑤 𝑖 𝑘 |C, 𝑤 𝑖 1 , 𝑤 𝑖 2 , ..., 𝑤 𝑖 𝑘−1 ), where 𝐿 denotes the length of target entity. When 𝛼 = 0, this equation degrades to the summation version, which is used in previous tasks. When 𝛼 = 1, this equation degrades to the average version.</p><p>We compare different averaging methods by using various 𝛼 and test their performance on the zero-shot inference tasks. We select the input features with the best performance according to Table <ref type="table" target="#tab_3">2</ref>. For SciBERT, we use both abstract and prompt word for FOS and affiliation inference. We do not use the prompt word for venue inference. For OAG-BERT, we only use abstract as the prompt word does not work well. The results in Table <ref type="table" target="#tab_9">8</ref> show that for the most time, using the summation strategy outperforms the average strategy significantly. The simple average (𝛼 = 1) appears to be the worst choice. However, for some situations, a moderate average (𝛼 = 0.5) might be beneficial. Discussion for Decoding Order In our designed decoding process, we do not strictly follow the left-to-right order as used in classical decoder models. The main reason is that for encoder-based BERT model, the decoding for each masked token relies on all bidirectional context information, rather than only prior words. We compare the performance of using left-to-right decoding and out-of-order decoding in Table <ref type="table">9</ref>.</p><p>The results show that for FOS, there is no significant difference between two decoding orders, since the candidate FOS only have one or two tokens inside. As for venue and affiliation, it turns out that the out-of-order decoding generally performs much better than left-to-right decoding, except when OAG-BERT is using abstract where differences are relatively small as well. We also present the results for models using left-to-right decoding and prompt words in Table <ref type="table">9</ref>, which indicates that the left-to-right decoding will sometimes undermine the effectiveness of prompt words significantly, especially for OAG-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Supervised Classification</head><p>In terms of training, we use the slanted triangular scheduler to adjust the learning rate dynamically and the AdamW optimizer with a maximal learning rate at 2e-5. We run the fine-tuning process for 5 epochs with 10% of the training steps used for warm-up. For each model and each task setting, the averaged accuracy (Hit@1) and standard deviations for 5 runs with different random seeds are reported in Table <ref type="table" target="#tab_5">4</ref>. The number of samples in the classification datasets is shown in Table <ref type="table" target="#tab_11">11</ref>.</p><p>Table <ref type="table">9</ref>: The results for using left-to-right decoding and outof-order decoding order. Hit@1 and MRR are reported. Results with difference larger than 1% Hit@1 were bolded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>FOS Venue Affiliation Hit@1 MRR Hit@1 MRR Hit@1 MRR</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SciBERT</head><p>Left-to-Right </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 NLP Tasks</head><p>Task Description Among all 15 NLP tasks, 9 tasks concentrate on the field of Biology and Medicine (Bio). Another 4 tasks use paper samples from computer science domain (CS). The rest two tasks involve a mixture of multi-domain data (Multi).</p><p>Tasks including NER and PICO require models to make predictions on each token and identify which tokens are part of entities. Some datasets like BC5CDR <ref type="bibr" target="#b26">[27]</ref> only need span range identification while other datasets like EBM-NLP <ref type="bibr" target="#b33">[34]</ref> also need entity type recognition. For sequence token classification tasks, a Conditional Random Field (CRF) layer is added on top of token outputs from the pre-training model, to better capture the dependencies between sequence labels. The DEP task <ref type="bibr" target="#b20">[21]</ref> also uses the token outputs from the pre-training model. The token embeddings, produced by the pre-training model, are fed to a biaffine matrix attention block and used to make further predictions on dependency arc type and direction. The REL and CLS tasks are sequence prediction tasks. The model only needs to make one prediction on the whole sequence. For example, in Paper Field prediction task <ref type="bibr" target="#b38">[39]</ref>, the model accepts paper title as inputs and output the research fields of that paper. The REL tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>, although not directly asking the label of the input sequence, can be reformed into sequence prediction as well. In this type of tasks, the model makes predictions for the entity relation types by categorizing the whole sequence, where the focused entity pairs are encapsulated with special tokens. The SSC tasks are multi-sequence prediction tasks. Given a list of sentences such as abstract, the model needs to predict the functionality for each internal sentence. These tasks always involve long sequences and also benefits from using CRF layer on top of the sentence embeddings.</p><p>Table <ref type="table">10</ref>: A full list of used candidates in zero-shot inference tasks and supervised classification tasks.</p><p>FOS: Art, Biology, Business, Chemistry, Computer science, Economics, Engineering, Environmental science, Geography, Geology, History, Materials science, Mathematics, Medicine, Philosophy, Physics, Political science, Psychology, Sociology Venue: Arxiv: algebraic geometry, Arxiv: analysis of pdes, Arxiv: astrophysics, Arxiv: classical analysis and odes, Arxiv: combinatorics, Arxiv: computer vision and pattern recognition, Arxiv: differential geometry, Arxiv: dynamical systems, Arxiv: functional analysis, Arxiv: general physics, Arxiv: general relativity and quantum cosmology, Arxiv: geometric topology, Arxiv: group theory, Arxiv: high energy physicsexperiment, Arxiv: high energy physics -phenomenology, Arxiv: high energy physics -theory, Arxiv: learning, Arxiv: materials science, Arxiv: mathematical physics, Arxiv: mesoscale and nanoscale physics, Arxiv: nuclear theory, Arxiv: number theory, Arxiv: numerical analysis, Arxiv: optimization and control, Arxiv: probability, Arxiv: quantum physics, Arxiv: representation theory, Arxiv: rings and algebras, Arxiv: statistical mechanics, Arxiv: strongly correlated electrons  Evaluation Metrics We use the same evaluation metrics with the SciBERT <ref type="bibr" target="#b1">[2]</ref> paper and the Sequential-Sentence-Classification <ref type="bibr" target="#b4">[5]</ref> paper. For NER and PICO tasks, we compare the span-level and token-level macro F1 scores respectively, except using micro-F1 for ChemProt <ref type="bibr" target="#b24">[25]</ref>. For REL, CLS, and SSC tasks, we compare sentencelevel macro F1 scores. For the DEP task, we compare LAS (labeled attachment score) and UAS (unlabeled attachment score).</p><p>Hype-parameters In SciBERT, the authors claimed that the best results for most downstream tasks were produced by fine-tuning 2 or 4 epochs and using 2e-5 learning rate after searching between 1 to 4 epochs with a maximum learning of 1e-5, 2e-5, 3e-5, 5e-5, as stated in <ref type="bibr" target="#b30">[31]</ref>. In our experiments, we follow the same settings and select the optimal hyper-parameters on validation sets and report the corresponding test sets results. Pre-training on 512-token samples During fine-tuning on NLP tasks, we also observe that the pre-training on inputs with 512 tokens is essential for the SSC tasks with up to 10% performance boost, which is much larger than the performance boost for other types of tasks as shown in Table <ref type="table" target="#tab_13">13</ref>. It is because the SSC tasks require the model to comprehend multiple sentences in a long paragraph rather than a single sentence in other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: OAG-BERT outperforms SciBERT on a range of entity-related tasks by 2.75%-21.0% (Absolute Gain).</figDesc><graphic url="image-1.png" coords="1,317.27,199.64,240.92,72.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The decoding process of OAG-BERT. The left figure indicates that OAG-BERT decodes the masked token "science" at the second position with the highest probability (0.625) for the first round. Then it decodes "political" at the first position with highest probability (0.526) for the second round as shown in the right figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Affiliation:</head><label></label><figDesc>Al azhar university, Bell labs, Carnegie mellon university, Centers for disease control and prevention, Chinese academy of sciences, Electric power research institute, Fudan university, Gunadarma university, Harvard university, Ibm, Intel, Islamic azad university, Katholieke universiteit leuven, Ludwig maximilian university of munich, Max planck society, Mayo clinic, Moscow state university, National scientific and technical research council, Peking university, Renmin university of china, Russian academy of sciences, Siemens, Stanford university, Sun yat sen university, Tohoku university, Tsinghua university, University of california berkeley, University of cambridge, University of oxford, University of paris</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The results for zero-shot inference tasks.</figDesc><table><row><cell>Method</cell><cell cols="2">FOS Hit@1 MRR</cell><cell cols="2">Venue Hit@1 MRR</cell><cell cols="2">Affiliation Hit@1 MRR</cell></row><row><cell>SciBERT</cell><cell>19.93%</cell><cell>0.37</cell><cell>9.87%</cell><cell>0.22</cell><cell>6.93%</cell><cell>0.19</cell></row><row><cell>+prompt</cell><cell>29.59%</cell><cell cols="2">0.47 10.03%</cell><cell>0.21</cell><cell>8.00%</cell><cell>0.20</cell></row><row><cell>+abstract</cell><cell>25.66%</cell><cell cols="2">0.43 18.00%</cell><cell cols="2">0.32 10.33%</cell><cell>0.22</cell></row><row><cell>+both</cell><cell>35.33%</cell><cell>0.52</cell><cell>9.83%</cell><cell cols="2">0.22 12.40%</cell><cell>0.25</cell></row><row><cell>OAG-BERT</cell><cell>34.36%</cell><cell cols="2">0.51 21.00%</cell><cell cols="2">0.37 11.03%</cell><cell>0.24</cell></row><row><cell>+prompt</cell><cell>37.33%</cell><cell cols="2">0.55 22.67%</cell><cell cols="2">0.39 11.77%</cell><cell>0.25</cell></row><row><cell cols="7">+abstract 49.59% 0.67 39.00% 0.57 21.67% 0.38</cell></row><row><cell>+both</cell><cell cols="6">49.51% 0.67 38.47% 0.57 21.53% 0.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The generated FOS for the paper of GPT-3. The gold FOS are bolded. FOS not in the original OAG FOS candidate list are underlined.</figDesc><table><row><cell></cell><cell>Artificial language processing</cell></row><row><cell>Gold FOS</cell><cell>Language model, Computer science, Linguistics</cell></row></table><note>TitleLanguage Models are Few-Shot Learners Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally...Generated FOSNatural language processing, Autoregressive language model, Computer science, Sentence, Artificial intelligence, Domain adaptation, Language model, Few shot learning, Large corpus, Arithmetic, Machine learning, Architecture, Theoretical computer science, Data mining, Linguistics,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The results of the classification task.</figDesc><table><row><cell>Tasks</cell><cell cols="4">Freeze SciBERT OAG-BERT SciBERT OAG-BERT Finetune</cell></row><row><cell>FOS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">title only 33.25 0.25</cell><cell>43.28 0.12</cell><cell>55.13 0.30</cell><cell>54.54 0.29</cell></row><row><cell>+author</cell><cell>30.15 0.07</cell><cell>41.87 0.06</cell><cell>55.63 0.42</cell><cell>55.30 0.43</cell></row><row><cell>+venue</cell><cell>34.77 0.17</cell><cell>46.99 0.15</cell><cell>63.18 0.18</cell><cell>63.53 0.08</cell></row><row><cell>+aff</cell><cell>32.83 0.13</cell><cell>43.07 0.11</cell><cell>55.06 0.21</cell><cell>54.65 0.38</cell></row><row><cell>+all</cell><cell>32.83 0.08</cell><cell>45.47 0.16</cell><cell>63.43 0.15</cell><cell>64.22 0.38</cell></row><row><cell>Venue</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">title only 24.62 0.52</cell><cell>32.87 1.47</cell><cell>61.86 0.32</cell><cell>63.03 0.46</cell></row><row><cell>+author</cell><cell>21.21 0.82</cell><cell>30.91 0.96</cell><cell>62.62 0.34</cell><cell>63.46 0.48</cell></row><row><cell>+aff</cell><cell>24.38 0.49</cell><cell>32.32 1.36</cell><cell>62.13 0.43</cell><cell>62.65 0.49</cell></row><row><cell>+fos</cell><cell>40.49 1.25</cell><cell>52.61 0.79</cell><cell>78.05 0.14</cell><cell>78.47 0.25</cell></row><row><cell>+all</cell><cell>39.92 1.17</cell><cell>51.33 0.44</cell><cell>77.88 0.16</cell><cell>78.34 0.62</cell></row><row><cell>Affiliation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">title only 13.88 0.83</cell><cell>19.72 0.64</cell><cell>35.44 0.45</cell><cell>35.04 0.61</cell></row><row><cell>+author</cell><cell>20.65 1.04</cell><cell>32.19 0.92</cell><cell>52.68 0.18</cell><cell>53.33 0.43</cell></row><row><cell>+venue</cell><cell>16.57 0.60</cell><cell>25.23 0.72</cell><cell>43.13 0.36</cell><cell>43.65 0.40</cell></row><row><cell>+fos</cell><cell>17.39 0.86</cell><cell>22.06 0.37</cell><cell>37.05 0.80</cell><cell>37.60 0.51</cell></row><row><cell>+all</cell><cell>24.02 0.87</cell><cell>32.49 0.50</cell><cell>56.04 0.95</cell><cell>57.63 0.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The Macro Pairewise F1 scores for the name disambiguation task.</figDesc><table><row><cell>Inputs</cell><cell cols="2">SciBERT OAG-BERT</cell></row><row><cell>title</cell><cell>0.3690</cell><cell>0.4120</cell></row><row><cell>+fos</cell><cell>0.4101</cell><cell>0.4643</cell></row><row><cell>+venue</cell><cell>0.3603</cell><cell>0.4247</cell></row><row><cell>+fos+venue</cell><cell>0.3903</cell><cell>0.4823</cell></row><row><cell cols="2">Leader Board Top 1</cell><cell>0.4900</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The results for NLP Tasks.</figDesc><table><row><cell cols="2">Field Task</cell><cell>Dataset</cell><cell>Samples 1</cell><cell>S2ORC</cell><cell cols="2">SciBERT Reported 2 Reproduced</cell><cell cols="2">OAG-BERT Vanilla Augmented</cell></row><row><cell></cell><cell></cell><cell>BC5CDR [27]</cell><cell>3942</cell><cell>90.04 0.06</cell><cell>90.01</cell><cell>89.77 .23</cell><cell>89.71 .13</cell><cell>89.71 .12</cell></row><row><cell></cell><cell>NER</cell><cell>JNLPBA [22]</cell><cell>16807</cell><cell>77.70 .25</cell><cell>77.28</cell><cell>77.29 .38</cell><cell>75.81 .20</cell><cell>76.99 .04</cell></row><row><cell></cell><cell></cell><cell>NCBI-disease [10]</cell><cell>5424</cell><cell>88.70 .52</cell><cell>88.57</cell><cell>88.10 .06</cell><cell>87.90 .12</cell><cell>88.77 .56</cell></row><row><cell></cell><cell cols="2">PICO EBM-NLP [34]</cell><cell>27879</cell><cell>72.35 .95</cell><cell>72.28</cell><cell>72.52 .71</cell><cell>72.22 .24</cell><cell>71.74 .49</cell></row><row><cell>Bio</cell><cell>DEP</cell><cell>GENIA -LAS [21] GENIA -UAS [21]</cell><cell>14326</cell><cell>90.80 .19 92.31 .18</cell><cell>90.43 91.99</cell><cell>90.57 .08 92.12 .07</cell><cell>89.99 .10 91.57 .08</cell><cell>90.12 .11 91.63 .09</cell></row><row><cell></cell><cell>REL</cell><cell>ChemProt [25]</cell><cell>4169</cell><cell>84.59 .93</cell><cell>83.64</cell><cell>83.46 .28</cell><cell>82.14 1.12</cell><cell>80.21 1.42</cell></row><row><cell></cell><cell>SSC</cell><cell cols="2">Pubmed-RCT-20k [8] 15130 NICTA-piboso [23] 735</cell><cell>--</cell><cell>92.90 84.80</cell><cell>92.86 .12 83.93 .58</cell><cell>92.80 .05 83.02 .67</cell><cell>92.73 .08 84.00 .32</cell></row><row><cell></cell><cell>NER</cell><cell>SciERC [32]</cell><cell>1861</cell><cell>68.93 .19</cell><cell>67.57</cell><cell>66.28 .20</cell><cell>67.80 .24</cell><cell>66.75 .77</cell></row><row><cell>CS</cell><cell>REL CLS</cell><cell>SciERC [32] ACL-ARC [19]</cell><cell>3219 1688</cell><cell>81.77 1.64 68.45 2.47</cell><cell>79.97 70.98</cell><cell>80.21 .88 70.34 3.07</cell><cell>76.59 0.75 66.13 1.58</cell><cell>78.63 .06 64.79 3.35</cell></row><row><cell></cell><cell>SSC</cell><cell>CSAbstruct [5]</cell><cell>1668</cell><cell>-</cell><cell>83.10</cell><cell>82.40 .33</cell><cell>82.48 .44</cell><cell>82.59 .67</cell></row><row><cell cols="2">Multi CLS</cell><cell>Paper Field [39] SciCite [4]</cell><cell>84000 7320</cell><cell>65.99 .08 84.76 .37</cell><cell>65.71 85.49</cell><cell>65.77 .13 85.65 .54</cell><cell>64.67 .14 85.25 .38</cell><cell>64.95 .10 84.95 .32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The result of link prediction tasks.</figDesc><table><row><cell>Tasks</cell><cell cols="2">Paper-Field NDCG MRR</cell><cell cols="2">Paper-Venue NDCG MRR</cell></row><row><cell>XLNet</cell><cell>0.3939</cell><cell>0.4473</cell><cell>0.4385</cell><cell>0.2584</cell></row><row><cell>SciBERT</cell><cell>0.4740</cell><cell>0.5743</cell><cell>0.4570</cell><cell>0.2834</cell></row><row><cell cols="5">OAG-BERT 0.4892 0.6099 0.4844 0.3131</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The results for using different average methods while calculating entity log probabilities. Hit@1 and MRR are reported.</figDesc><table><row><cell>Method</cell><cell>𝛼 = 0</cell><cell>𝛼 = 0.5</cell><cell>𝛼 = 1</cell></row><row><cell>SciBERT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FOS</cell><cell cols="3">35.33%, 0.52 32.07%, 0.51 14.85%, 0.36</cell></row><row><cell>Venue</cell><cell cols="3">18.00%, 0.32 19.30%, 0.33 7.07%, 0.23</cell></row><row><cell>Affiliation</cell><cell cols="2">12.40%, 0.25 10.83%, 0.23</cell><cell>9.23%, 0.21</cell></row><row><cell>OAGBERT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FOS</cell><cell cols="3">49.59%, 0.67 48.08%, 0.66 45.36%, 0.63</cell></row><row><cell>Venue</cell><cell cols="3">39.00%, 0.57 38.20%, 0.57 36.13%, 0.55</cell></row><row><cell>Affiliation</cell><cell cols="3">21.67%, 0.38 19.90%, 0.36 16.47%, 0.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>The sizes for datasets used in supervised classification tasks.</figDesc><table><row><cell>Task</cell><cell cols="2">Categories Train</cell><cell cols="2">Validation Test</cell></row><row><cell>FOS</cell><cell>19</cell><cell cols="2">152000 19000</cell><cell>19000</cell></row><row><cell>Venue</cell><cell>30</cell><cell>24000</cell><cell>3000</cell><cell>3000</cell></row><row><cell cols="2">Affiliation 30</cell><cell>24000</cell><cell>3000</cell><cell>3000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Details for the CS heterogeneous graph used in the link prediction.</figDesc><table><row><cell></cell><cell>Papers</cell><cell>Authors</cell><cell>FOS</cell></row><row><cell>Nodes</cell><cell>544244</cell><cell>510189</cell><cell>45717</cell></row><row><cell cols="2">1116163 Venues</cell><cell>Affiliations</cell><cell></cell></row><row><cell></cell><cell>6934</cell><cell>9079</cell><cell></cell></row><row><cell></cell><cell>#Paper-Author</cell><cell>#Paper-FOS</cell><cell>#Paper-Venue</cell></row><row><cell>#Edges</cell><cell>1862305</cell><cell>2406363</cell><cell>551960</cell></row><row><cell cols="4">6389083 #Author-Affiliation #Paper-Paper #FOS-FOS</cell></row><row><cell></cell><cell>519268</cell><cell>992763</cell><cell>56424</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>The performance of vanilla OAG-BERT with and without training on 512-token samples. All results in this table were produced by fine-tuning with 2 epochs and 2e-5 learning rates.</figDesc><table><row><cell cols="2">Task Dataset</cell><cell cols="2">Vanilla OAG-BERT w/o 512 w/ 512</cell><cell>Gain</cell></row><row><cell></cell><cell>BC5CDR</cell><cell>89.62 .16</cell><cell>89.33 .12</cell><cell>-0.29</cell></row><row><cell>NER</cell><cell>NCBI-disease</cell><cell>87.63 .62</cell><cell cols="2">87.92 1.08 +0.29</cell></row><row><cell></cell><cell>SciERC</cell><cell>67.64 .52</cell><cell>67.19 .34</cell><cell>-0.45</cell></row><row><cell>REL</cell><cell>ChemProt SciERC</cell><cell cols="3">77.50 1.99 77.99 2.50 +0.49 69.87 1.51 69.88 .77 +0.01</cell></row><row><cell>SSC</cell><cell cols="2">NICTA-piboso 77.62 .87 CSAbstruct 72.65 .40</cell><cell>80.01 .24 82.30 .47</cell><cell>+2.39 +9.65</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://www.aminer.cn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">https://github.com/acbull/pyHGT</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>effective than other entities due to the lack of pre-training, which hinders the downstream tasks to fully leverage the entity information. We leave these problems for future explorations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Construction of the literature graph in semantic scholar</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02262</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">SciBERT: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CONNA: Addressing Name Disambiguation on The Fly</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Structural scaffolds for citation intent classification in scientific publications</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Field</forename><surname>Cady</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01608</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pretrained language models for sequential sentence classification</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04054</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial intelligence is selecting grant reviewers in China</title>
		<author>
			<persName><forename type="first">David</forename><surname>Cyranoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">569</biblScope>
			<biblScope unit="page">7756</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixedlength context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts</title>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06071</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NCBI disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName><forename type="first">Rezarta</forename><surname>Islamaj Doğan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Polar: Attention-based cnn for one-shot personalized article recommendation</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">POLAR++: Active One-shot Personalized Article Recommendation</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measuring the evolution of a scientific field through citation frames</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raine</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A scalable hybrid research paper recommender system for microsoft academic</title>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GENIA corpus -a semantically annotated corpus for bio-textmining</title>
		<author>
			<persName><forename type="first">J-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to the bio-entity recognition task at JNLPBA</title>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<editor>JNLPBA. Citeseer</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic classification of sentences to support evidence based medicine</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Cavedon</surname></persName>
		</author>
		<author>
			<persName><surname>Yencken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ChemProt-3.0: a global chemical biology diseases mapping</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Kringelum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonny</forename><surname>Kim Kjaerulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><surname>Brunak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><forename type="middle">I</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Taboureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02782</idno>
		<title level="m">S2orc: The semantic scholar open research corpus</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09602</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07669</idno>
		<title level="m">Scispacy: Fast and robust models for biomedical natural language processing</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Junyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roma</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>NIH Public Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases?</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Nina</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulli</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03681</idno>
		<title level="m">E-bert: Efficient-yeteffective entity embeddings for bert</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Olatunji Ruwase, and Yuxiong He</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00309</idno>
		<title level="m">CoLAKE: Contextualized Language and Knowledge Embedding</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Word Reordering and a Dynamic Programming Beam Search Algorithm for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="97" to="133" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Where Have You Been? Inferring Career Trajectory from Academic Social Network</title>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang Yukuo Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00959</idno>
		<title level="m">CogDL: An Extensive Toolkit for Deep Learning on Graphs</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Oag: Toward linking large-scale heterogeneous entity graphs</title>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ProNE: Fast and Scalable Network Representation Learning</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping</title>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13369</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop</title>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m">ERNIE: Enhanced language representation with informative entities</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
