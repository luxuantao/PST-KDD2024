<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Action Recognition with Spatio-Temporal Visual Attention on Skeleton Image Sequences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Sci-ence</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<addrLine>1 2 22 23 24 25 1 2 22 23 24 25 1 2 22 23 24 25</addrLine>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">887D3622BEDC1892C5666A7C34DA4474</idno>
					<idno type="DOI">10.1109/TCSVT.2018.2864148</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2018.2864148, IEEE Transactions on Circuits and Systems for Video Technology</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action and Activity Recognition</term>
					<term>Video Understanding</term>
					<term>Human Analysis</term>
					<term>Visual Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action recognition with 3D skeleton sequences became popular due to its speed and robustness. The recently proposed Convolutional Neural Networks (CNN) based methods shown good performance in learning spatio-temporal representations for skeleton sequences. Despite the good recognition accuracy achieved by previous CNN based methods, there existed two problems that potentially limit the performance. First, previous skeleton representations were generated by chaining joints with a fixed order. The corresponding semantic meaning was unclear and the structural information among the joints was lost. Second, previous models did not have an ability to focus on informative joints. The attention mechanism was important for skeleton based action recognition because different joints contributed unequally towards the correct recognition. To solve these two problems, we proposed a novel CNN based method for skeleton based action recognition. We first redesigned the skeleton representations with a depth-first tree traversal order, which enhanced the semantic meaning of skeleton images and better preserved the associated structural information. We then proposed the general two-branch attention architecture that automatically focused on spatio-temporal key stages and filtered out unreliable joint predictions. Based on the proposed general architecture, we designed a Global Long-sequence Attention Network (GLAN) with refined branch structures. Furthermore, in order to adjust the kernel's spatio-temporal aspect ratios and better capture long term dependencies, we proposed a Sub-Sequence Attention Network (SSAN) that took sub-image sequences as inputs. We showed that the two-branch attention architecture could be combined with the SSAN to further improve the performance. Our experiment results on the NTU RGB+D dataset and the SBU Kinetic Interaction dataset outperformed the state-of-the-art. The model was further validated on noisy estimated poses from the subsets of the UCF101 dataset and the Kinetics dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>included. One limitation is that manually labeling skeleton sequences is too expensive, while automatic annotation methods may yield inaccurate predictions. Given the above advantages and the fact that skeletons can now be more reliably predicted <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, skeleton based human action recognition is becoming increasingly popular. The major goal for skeleton based recognition is to learn a representation that best preserves the spatio-temporal relations among the joints. With a strong ability of modeling sequential data, Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) neurons outperform the previous hand-crafted feature based methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Each skeleton frame is converted into a feature vector and the whole sequence is fed into the RNN. Despite the strong ability in modeling temporal sequences, RNN structures lack the ability to efficiently learn the spatial relations between the joints. To better use spatial information, a hierarchical structure is proposed in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> that feeds the joints into the network as several pre-defined body part groups. However, the pre-defined body regions still limit the effectiveness of representing spatial relations. A spatiotemporal 2D LSTM (ST-LSTM) network <ref type="bibr" target="#b13">[14]</ref> is proposed to learn the spatial and temporal relations simultaneously. Furthermore, a two-stream RNN structure <ref type="bibr" target="#b14">[15]</ref> is proposed to learn the spatio-temporal relations with two RNN branches.</p><p>CNN has a natural ability to learn representations from 2D</p><p>1051-8215 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. arrays. The works in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> first propose to represent the skeleton sequences as 2D gray scale images and use CNN to jointly learn a spatio-temporal representation. Each gray scale image corresponds to one axis in the joint coordinates. For example, the coordinates in the x-axis throughout a skeleton sequence generate one single-channel image. Each row is a spatial distribution of coordinates at a certain time-stamp, and each column is the temporal evolution of a certain joint. The generated 2D arrays are then scaled and resized into a fixed size. The gray scale images generated from the same skeleton sequence are concatenated together and processed as a multichannel image, which is called the skeleton image.</p><p>Despite the large boost in recognition accuracy achieved by previous CNN based methods, there exist two problems. First, previous skeleton image representations lose spatial information. In previous methods, each row represents skeleton's spatial information by chaining all joints with a fixed order. This concatenation process lacks semantic meaning and leads to a loss in skeleton's structural information. Although a good chain order can perverse more spatial information, it is impossible to find a perfect chain order that maintains all spatial relations in the original skeleton structure. We propose a Tree Structure Skeleton Image (TSSI) to preserve spatial relations. TSSI is generated by traversing a skeleton tree with a depth-first order. We assume the spatial relations between joints are represented by the edges that connect them in the original skeleton structure, as shown in Figure <ref type="figure" target="#fig_0">1</ref> (a). The fewer edges there are, the more relevant the joint pair is. Thus we prove that TSSI best preserves the spatial relation.</p><p>Second, previous CNN based methods do not have the ability to focus on spatial or temporal key stages. In skeleton based action recognition, certain joints and frames are more informative, like the joints on the arms in action 'waving hands'. Furthermore, certain joints may be inaccurately predicted and should be neglected as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Therefore, it is important to include attention mechanisms. Attention masks <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> learned from natural images are 2D weight matrices that amplify the visual features from the regions of importance and depress the others. Similarly, the idea of learning attention masks can be adopted on 'skeleton images'. The skeleton image representation has a natural ability to represent spatiotemporal importance jointly with 2D attention masks, where each row represents the spatial importance of key joints and each column represents the temporal importance of key frames. Based on this, we propose a two-branch architecture for visual attention on single skeleton images. One branch of the architecture is designed with a larger receptive field and generates the predicted attention mask. The other branch maintains and refines the CNN feature. We first introduce the two-branch architecture with a base attention model. A Global Long-sequence Attention Network (GLAN) is then proposed with refined branch structures. Experiments on public datasets prove the effectiveness of the two improvements. The recognition accuracy is superior to the state-of-the-art methods. The GLAN alone achieves an accuracy of 80.1% on NTU RGB+D and 95.6% on SBU Kinect Interaction, compared to the 79.6% and 93.6% reported in CNN+MTLN <ref type="bibr" target="#b15">[16]</ref>.</p><p>Despite the effectiveness of the two-branch attention structure, representing an entire sequence as one skeleton image lacks the ability to adjust kernels' spatio-temporal resolutions and learn the long-term dependencies. The original resolutions are determined by the number of joints and the length of the sequence. Furthermore, there is information loss with a sequence longer than the height of the skeleton image. Therefore, we represent the skeleton sequence as several overlapped sub skeleton images and propose a Sub-Sequence Attention Network (SSAN) based on the representation. Furthermore, we show that the GLAN can be combined with the SSAN to further improve the performance.</p><p>Our main contributions include the following: to adjust spatio-temporal aspect ratios and better learn long-term dependencies. We further show that the GLAN and the SSAN can be well combined. • The proposed method is compatible with both 3D skeletons and 2D poses. We evaluate the model on both Kinect recorded skeletons and noisy estimated poses.</p><formula xml:id="formula_0">•</formula><p>Experiments prove the effectiveness of the method and the robustness against noisy inputs. This paper contains published contents from an early conference paper <ref type="bibr" target="#b19">[20]</ref>. The key differences include the followings. The conference version discusses visual attention on a single skeleton image. Although the attention framework is effective, it lacks the ability to adjust the spatio-temporal resolution along the two axis of skeleton images. In this paper, we propose to split a single skeleton image into a skeleton image sequence and extend the GLAN with a temporal attention network. Experiments show the effectiveness of the proposed model and the importance of a proper spatio-temporal resolution. Furthermore, the proposed model is evaluating on estimated pose data with self-paced learning techniques. Finally, more experiment results are included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Historically, RGB video action recognition <ref type="bibr" target="#b20">[21]</ref> consisted of a feature extraction stage, a feature encoding stage and a classification stage. Hand-crafted features including HOG <ref type="bibr" target="#b21">[22]</ref>, MBH <ref type="bibr" target="#b22">[23]</ref> and etc were designed to represent activity features. Various feature encoding methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> were also studied. With the developments of deep neural networks <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, more deep models were designed to solve the action recognition problem. Adopting ConvNets to encode framelevel feature and using LSTM to learn the temporal evolution had been proved to be an effective approach <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Optical flow <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> was another way of representing temporal information. Furthermore, C3D <ref type="bibr" target="#b1">[2]</ref> was proposed to learn the spatial and temporal information simultaneously with 3D convolutional kernels.</p><p>Compared to other frequently used modalities including RGB videos <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and optical flow <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, skeleton sequences required much less computation and were more robust across views and datasets. With the advanced methods to acquire reliable skeletons from RGBD sensors <ref type="bibr" target="#b6">[7]</ref> or even a single RGB camera <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, skeleton-based action recognition became increasingly popular. Many previous skeleton-based action recognition methods <ref type="bibr" target="#b32">[33]</ref> modeled the temporal pattern of skeleton sequences with Recurrent Neural Networks (RNN). Hierarchical structures <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> better represented the spatial relations between body parts. Other works <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> adopted attention mechanisms to locate spatial key joints and temporal key stages in skeleton sequences. Liu et al. <ref type="bibr" target="#b13">[14]</ref> proposed a 2D LSTM network to learn spatial and temporal relations simultaneously. Wang et al. <ref type="bibr" target="#b14">[15]</ref> modeled spatio-temporal relations with a two-stream RNN structure. Other effective approaches included lie groups <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b35">[36]</ref> and nearest neighbor search <ref type="bibr" target="#b36">[37]</ref>. Recently, graphical neural networks <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> achieved the state-of-the-art performance on the skeleton based recognition. A performance summary on two frequently used datasets NTU RGB+D <ref type="bibr" target="#b12">[13]</ref> and SBU Kinect Interaction <ref type="bibr" target="#b39">[40]</ref> was shown in Table <ref type="table" target="#tab_0">I</ref>.</p><p>As shown in Table <ref type="table" target="#tab_0">I</ref>, the recently proposed CNN based approaches showed a better performance in learning skeleton representations compared to RNN based methods. Ke et al. <ref type="bibr" target="#b15">[16]</ref> proposed to convert human skeleton sequences into gray scale images, where the joint coordinates were represented by the intensity of pixels. Liu et al. <ref type="bibr" target="#b40">[41]</ref> proposed to generate skeleton images with 'Skepxels' to better represent the joint correlations. In this paper, we further improved the design of skeleton images with a depth-first traversal on skeleton trees.</p><p>Attention mechanisms were important for skeleton based action recognition. Previous LSTM based methods <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> learned attention weights between the stacked LSTM layers. For CNN based methods, we proposed that general visual attention can be directly adopted to generate 2D attention masks, where each row represented the spatial importance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Approach NTU RGB+D Cross Subject</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SBU Kinect Interaction</head><p>Two-stream RNN <ref type="bibr" target="#b14">[15]</ref> RNN 71.3% 94.8% Ensemble TS-LSTM <ref type="bibr" target="#b32">[33]</ref> RNN 76.0% -Clips+CNN+MTLN <ref type="bibr" target="#b15">[16]</ref> CNN 79.6% 93.6% Skepxels <ref type="bibr" target="#b40">[41]</ref> CNN 81.3% -GLAN <ref type="bibr" target="#b19">[20]</ref> CNN 80.1% 95.6% A 2 GNN <ref type="bibr" target="#b37">[38]</ref> Graphic NN 72.7% -ST-GCN <ref type="bibr" target="#b38">[39]</ref> Graphic NN 81.5% and each column represented the temporal importance. Visual attention had achieved successes in many areas, including image captioning <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b41">[42]</ref>, RGB based action recognition <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b42">[43]</ref>, image classification <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, sentiment analysis <ref type="bibr" target="#b45">[46]</ref> and etc. Many visual attention methods took an image sequence as input <ref type="bibr" target="#b18">[19]</ref>, or used extra information from another modality like text <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Because a single skeleton image already represented a spatio-temporal sequence and there was no need for an extra modality, we proposed a single frame based visual attention structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we first introduced the previous design of skeleton images and the base CNN structure, before an improved Tree Structure Skeleton Image (TSSI) was proposed. Later, we proposed the idea of two-branch visual attention and introduced a Global Long-sequence Attention Network (GLAN) based on the idea. Finally, we introduced the Sub-Sequence Attention Network (SSAN) to learn long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Base Model</head><p>In CNN based skeleton action recognition, joint sequences were arranged as 2D arrays that were processed as gray scale images. We named such a generated image the 'Skeleton Image'. For a channel in skeleton images, each row contained the chaining of joint coordinates at a certain time-stamp. Each column represented the coordinates of a certain joint throughout the entire video clip. The chain order of joints was pre-defined and fixed. A typical arrangement of the 2D array was shown in Figure <ref type="figure" target="#fig_0">1 (c</ref>). The generated 2D arrays were then scaled into 0 to 255, and resized into a fixed size of 224 * 224. The processed 2D arrays were processed as gray scale images, where each channel represented an axis of joint coordinates. The skeleton images were then fed into CNNs for action recognition. ResNet-50 <ref type="bibr" target="#b46">[47]</ref> was adopted as the base ConvNet model. Compared to RNN based or graph neural network based method, CNN based methods could better learn the spatio-temporal relations among joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tree Structure Skeleton Image</head><p>A shortcoming in the previous skeleton images was that each row was arranged by an improper fixed order. Each row contained the concatenation of all joints with a predefined chain order. CNN had a feature that the receptive field grows larger at higher levels. Therefore, the adjacent joints in each row or column were learned first at lower levels. This implied that the adjacent joints shared more spatial relations in original skeleton structure, which did not hold frequently. In previous skeleton images, a generated array had 25 columns representing the joint coordinates 1 to 25 with a joint index shown in Figure <ref type="figure" target="#fig_0">1 (a</ref>). An arrangement of the skeleton image was shown in Figure <ref type="figure" target="#fig_0">1 (c</ref>). In this case, a convolutional kernel would cover joints <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> at a certain level since these joints were adjacent in skeleton images. However, these joints had less spatial relations in original skeleton structures and should not be learned together at lower levels.</p><p>To solve this problem, we proposed a Tree Structure Skeleton Image (TSSI) inspired by a recent LSTM based study <ref type="bibr" target="#b13">[14]</ref>. The basic assumption was that the spatially related joints in original skeletons had direct graph links between them. The less edges required to connect a pair of joints, the more related was the pair. The human structure graph was defined with semantic meanings as shown in 1 (a). In the proposed TSSI, the direct concatenation of joints was replaced by a depth-first tree traversal order. One possible skeleton tree was defined in Figure <ref type="figure" target="#fig_0">1</ref> (b) and the corresponding arrangement of TSSI was shown in Figure <ref type="figure" target="#fig_0">1 (d)</ref>. Based on the shown skeleton tree, the depth-first tree traversal order for each row was <ref type="bibr">[</ref> other torso nodes could also be selected as the root of skeleton trees and the corresponding traversal order would be different. Using different torso nodes (node 1, 2 or 21) as tree roots generated a maximum accuracy difference of 0.6% on NTU RGB+D with the cross subject setting.</p><p>With the depth-first tree traversal order, the neighboring columns in skeleton images were spatially related in original skeleton structures. This proved that the TSSI better preserved the spatial relations. With TSSI, the spatial relations between related joints were learned first at lower levels of CNN and the relations between less relevant joints were learned later at high levels when the receptive field became larger. An example of the generated TSSI was shown in Figure <ref type="figure" target="#fig_0">1 (e)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Networks</head><p>In skeleton sequences, certain joints and frames were particularly distinguishable and informative for recognizing actions. For example in action 'waving hands', the joints in arms were more informative. These informative joints and frames were referred to as 'key stages'. Furthermore, noise existed in the captured joint data and deteriorated the recognition accuracy. The inaccurate joints should be automatically filtered out or ignored by the network.</p><p>To alleviate the effect of data noises and to focus on informative stages, skeleton based methods should adjust weights for different inputs automatically. We proposed the idea of two-branch visual attention and further designed a Global Long-sequence Attention Network (GLAN) based on the idea. In this section, we first introduced the basic idea of the two-branch attention architecture with a base attention model. Then the structure of Global Long-sequence Attention Network (GLAN) was introduced.</p><p>Base Attention Model. Skeleton images naturally represented both spatial and temporal information of skeleton sequences. Therefore a 2D attention mask could represent spatio-temporal importance simultaneously, where the weights in each row represented the spatial importance of joints and the weight in each column represented the temporal importance of frames. In order to generate the attention masks, we proposed a two-branch attention architecture that learned attention masks from a single skeleton image. The two-branch structure contained a 'mask branch' and a 'residual branch'. Taking previous CNN feature blocks as inputs, the mask branch learned a 2D attention mask and the residual branch refined previous CNN feature. The two branches were then merged to generate the weighted CNN feature block. To be specific, the mask branch learned attention masks with structures that had larger receptive fields. The residual branch was designed to maintain and refine the input CNN features. The two branches were fused at the end of each attention block with elementwise multiplication and summation.</p><p>We first introduced the base attention model, which was the simplest version of two-branch attention structures. As shown in Figure <ref type="figure" target="#fig_3">3</ref> (a), the mask branch in the base model gained a larger receptive field with a single convolutional layer. Softmax or Sigmoid functions were used for mask generation. The residual branch preserved the input CNN features with a direct link. The 'attention block' was defined as a module with one mask branch and one residual branch as in Figure <ref type="figure" target="#fig_3">3</ref> (a). The whole framework was built by mixing the proposed attention blocks with the convolutional blocks from ResNet.</p><p>In the base attention model, attention blocks were inserted between ResNet-50's residual blocks, with the structure of residual blocks unchanged.</p><p>Global Long-sequence Attention Network (GLAN). Based on the proposed two-branch structure, we improved the designs of both branches to learn attention masks and CNN features more effectively. Inspired by the hourglass structure  [48], <ref type="bibr" target="#b44">[45]</ref>, we proposed a Global Long-sequence Attention Network (GLAN) as shown in Figure <ref type="figure" target="#fig_4">4</ref>. The hourglass structure was adopted in mask branches to quickly adjust the feature size and efficiently gain a larger receptive field. As shown in Figure <ref type="figure" target="#fig_3">3</ref> (b), the hourglass structure consisted of a series of down-sampling units followed by up-sampling units. In each hourglass mask branch, input CNN features were first down-sampled to the lowest spatial resolution of 7 * 7 and recovered back to the original size. Max pooling was used for down-sampling and bilinear interpolation was used for upsampling. Each down-sampling unit included a max pooling layer, a followed residual unit and a link connection to the recovered feature with a same size. Each up-sampling unit contained a bilinear interpolation layer, a residual unit and a element-wise sum with the link connection. We showed that the Convolution-Deconvolution structure gained a large receptive field effectively and therefore could better learn an attention mask. For residual branches, we added two residual units to further refine the learned CNN features. All residual units were the same as ResNet <ref type="bibr" target="#b46">[47]</ref>, which contains three convolutional units and a direct residual link. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, three GLAN attention blocks were added between the four residual blocks in ResNet-50 to build the GLAN network. The depth of each GLAN blocks varied due to the different input feature sizes. Furthermore, we reduced the number of residual units in each residual block to keep a proper depth of the GLAN network, since GLAN blocks were much deeper than the base attention blocks. Only one residual unit was kept for the first three residual blocks. The final residual block kept all three residual units as in ResNet-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~Sigmoid/Softmax</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Long-term Dependency Model</head><p>Although single-frame-based two-branch attention structure achieved a good performance, it lacked the ability to learn long-term dependences. The generated skeleton image had a fix height of 224. This implied an information loss with a sequence longer than 224 frames, which is around 7 seconds with a frame rate of 30 fps. To better learn long-term dependencies, we proposed a CNN + LSTM model with sub skeleton image sequences. We first split skeleton sequences into several overlapped sub-sequences and generated a series of sub skeleton images for a skeleton sequence. CNN features were first extracted from each sub-sequence skeleton image and the long-term dependencies were modeled with RNNs.</p><p>Furthermore, in the original two-branch attention structures, both the spatial and temporal resolutions in skeleton images were fixed by the number of joints and the length of the sequence. However, the kernel should be able to adjust the number of joints and frames it jointly looked at to achieve the best performance. The proposed sub-image model could adjust the relative resolution flexibly by adjusting the number of sub-images and the overlapping rate. This adjustment was equivalent to adjusting the width and height of CNN kernels, while it did not require retraining the model for each dataset.</p><p>Sub-Sequence Attention Network (SSAN). To further improve the performance of the proposed sub-image model, an long-term attention module was adopted. Inspired by <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b17">[18]</ref>, a Sub-Sequence Attention Network (SSAN) was proposed with a structure shown in Figure <ref type="figure" target="#fig_5">5</ref>. Long Short-Term Memory (LSTM) was adopted as the RNN cells. The LSTM implementation was based on <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_1">    i t f t o t g t     =     σ σ σ tanh     T d+D,4d h t-1 x t<label>(1)</label></formula><formula xml:id="formula_2">c t = f t c t-1 + i t g t<label>(2)</label></formula><formula xml:id="formula_3">h t = o t tanh(c t )<label>(3)</label></formula><p>where i t , f t , o t , c t , h t were the input gates, forget gates, output gates, cell states and hidden states of the LSTM. g t was an intermediate representation for updating cell states c t . T was an affine transformation, where D was the depth of the CNN feature block and d was the dimension of all LSTM states. x t was the weighted CNN feature that input to the LSTM at time t with length D.</p><p>Based on the LSTM model, the 2D attention map l t at time t was defined as a K * K mask, where K was the output width and height of the CNN feature block: Inspired by <ref type="bibr" target="#b44">[45]</ref>, we also adopted the spatial-channel attention with sigmoid activation, where i ∈ 1 . . . K 2 , z ∈ 1 . . . D.</p><formula xml:id="formula_4">l t,i = exp(W i h t-1 ) K * K j=1 exp(W j h t-1 ) i ∈ 1 . . . K 2<label>(4)</label></formula><formula xml:id="formula_5">l t,i,z = Sigmoid(W i,z h t-1 )<label>(5)</label></formula><p>The weighted CNN feature x t at time t was the element-wise multiplication of attention mask l t and original CNN output X t following Equation 6. In the SSAN, Resnet-50 was selected as the CNN model.</p><formula xml:id="formula_6">x t = K 2 j=1 l t,i X t,i<label>(6)</label></formula><p>GLAN + SSAN. Furthermore, we showed that the GLAN could replace Resnet-50 as the CNN structure in the long-term dependency model. The combination of SSAN and GLAN enabled the framework to generate attentions both in CNN layers with a bottom-up approach and in LSTM layers with a top-down approach. Experiments showed the effectiveness of the proposed combination, and further proved the possibility of using the proposed modules as atomic parts in other frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The proposed method is evaluated on both clean datasets captured by Kinect and noisy datasets where the poses are estimated from RGB videos. We adopt the NTU RGB+D dataset <ref type="bibr" target="#b12">[13]</ref> and the SBU Kinect Interaction Dataset <ref type="bibr" target="#b39">[40]</ref> for clean dataset evaluation. The estimated poses on UCF101 <ref type="bibr" target="#b49">[50]</ref> and Kinetics <ref type="bibr" target="#b50">[51]</ref> are used to measure the performance with potentially incomplete and noisy poses. We further evaluate the effectiveness of each proposed module separately. The experiments show that both the TSSI and the attention network generate a large boost in the action recognition accuracy to outperform the state-of-the-art.</p><p>A. Datasets NTU RGB+D. The NTU RGB+D dataset <ref type="bibr" target="#b12">[13]</ref> is so far the largest 3D skeleton action recognition dataset. RGB+D has 56880 videos collected from 60 action classes, including 40 daily actions, 9 health-related actions and 11 mutual actions. The dataset is collected with Kinect and the recorded skeletons include 25 joints. The train/val/test split follows <ref type="bibr" target="#b12">[13]</ref>. Samples with missing joints are discarded as in that paper.</p><p>SBU Kinect Interaction. The SBU Kinect Interaction dataset <ref type="bibr" target="#b39">[40]</ref> contains 282 skeleton sequences and 6822 frames. We follow the standard experiment protocol of 5-fold cross validations with the provided splits. The dataset contains eight classes. There are two persons in each skeleton frame and 15 joints are labeled for each person. The two skeletons are processed as two data samples during training and the averaged prediction score is calculated for testing. During training, random cropping is applied for data augmentation. Prediction scores from the five crops in center and four corners are averaged as testing prediction.</p><p>Kinetics-Motion. The Kinetics dataset <ref type="bibr" target="#b50">[51]</ref> is the largest RGB action recognition dataset, containing around 300,000 video clips from 400 action classes. The videos are collected from YouTube and each clip is around 10 seconds long. To conduct joints based action recognition, we use the precalculated estimated poses provided by <ref type="bibr" target="#b38">[39]</ref>. Videos are first converted into a fixed resolution of 340×256 with a frame rate of 30 FPS, and poses are then estimated with the OpenPose toolbox <ref type="bibr" target="#b30">[31]</ref>. Because the joint sequence contains no background context or object appearance, it fails to distinguish certain classes defined in RGB action recognition datasets. To better evaluate skeleton based methods on estimated joints, et al. <ref type="bibr" target="#b38">[39]</ref> proposes a 'Kinetics-Motion' dataset, which is a 30-class-subset of Kinetics with action labels strongly related to body motion. We evaluate the proposed method on the Kinetics-Motion dataset. The selected 30 classes are: belly dancing, punching bag, capoeira, squat, windsurfing, skipping rope, swimming backstroke, hammer throw, throwing discus, tobogganing, hopscotch, hitting baseball, roller skating, arm wrestling, snatch weight lifting, tai chi, riding mechanical bull, salsa dancing, hurling (sport), lunge, skateboarding, country line dancing, juggling balls, surfing crowd, dead lifting, clean and jerk, crawling baby, push up, front raises, pull ups.</p><p>UCF101-Motion. The UCF101 dataset <ref type="bibr" target="#b49">[50]</ref> contains 13,320 videos from 101 action categories. Videos have a fixed frame rate and resolution of 25 FPS and 320 × 240. Using RGB videos as inputs, we estimate 16-joint-poses with AlphaPose toolbox <ref type="bibr" target="#b51">[52]</ref>. The toolbox provides 2D joint locations and the confidence values for predictions. Similar to Kinetics-Motion, we argue the problem also exists on UCF101 that certain predefined action classes such as 'cutting in kitchen' are more relevant to objects and scenes. To prove this, we follow the procedure in ST-GCN <ref type="bibr" target="#b38">[39]</ref> and propose a subset from UCF-101 named 'UCF-Motion'. UCF-Motion contains 23 classes that are strongly related to body motions with 3172 videos in total. The selected classes are: playing dhol, clean and jerk, writing on board, playing flute, playing cello, playing guitar, bowling, ice dancing, playing piano, punch, playing tabla, soccer juggling, tai chi, boxing-speed bag, salsa spins, jump rope, boxing-punching bag, hammer throw, rafting, push ups, juggling balls, golf swing, baby crawling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>To prove the effectiveness of the TSSI and the proposed attention networks, we separately evaluate each proposed module with results shown in Table <ref type="table" target="#tab_4">III</ref>. Each component of the framework is evaluated on NTU RGB+D with the cross subject setting. NTU RGB+D is selected for component evaluations because it is the largest and the most challenging dataset so far. Similar results are observed on other datasets.</p><p>Traditional Skeleton Image + ConvNet. As a baseline, we adopt the previous skeleton image representation from <ref type="bibr" target="#b15">[16]</ref> and use ResNet-50 as a base CNN model to train spatio-temporal skeleton representations. We test the three spatial joint orders proposed by Sub-JHMDB <ref type="bibr" target="#b52">[53]</ref>, PennAction <ref type="bibr" target="#b53">[54]</ref> and NTU RGB+D <ref type="bibr" target="#b12">[13]</ref>. Experiments show that the NTU RGB+D's order generates a better accuracy of 1.3% than the rest two orders. Therefore, we adopt the joint order proposed by NTU RGB+D for baseline comparison. The order is shown in Figure <ref type="figure" target="#fig_0">1 (a)</ref>.</p><p>TSSI + ConvNet. The effectiveness of the proposed Tree Structure Skeleton Image (TSSI) is compared to the baseline design of skeleton images. TSSI is the skeleton image generated with a depth-first tree traversal order. The skeleton tree structure, TSSI arrangement and a TSSI example is shown in Figure <ref type="figure" target="#fig_0">1</ref> (b), (d), (e). A large boost in accuracy is observed from 68.0% to 73.1%, which proves the effectiveness of TSSI.</p><p>TSSI + Base Attention. The base attention model provides a baseline for two-branch attention networks. The base attention blocks with and without residual links are inserted at three different locations in ResNet-50, that is at the front after the first convolutional layer, in the middle after the second residual block and in the end after the final residual block. The input feature blocks to the three attention blocks have the shapes of 112 * 112 * 64, 28 * 28 * 512 and 7 * 7 * 2048. The recognition accuracy boosts from 73.1% to 74.9%. This experiment shows that even the simplest two-branch attention network can improve the recognition accuracy. TSSI + GLAN. We evaluate the proposed Global Longsequence Attention Network (GLAN). The number of link connections and the depth of the hourglass mask branch can be manually adjusted. In experiments, we first down-sample the feature blocks to a lowest resolution of 7 * 7 and then up-sample them back to the input size. Each max pooling layer goes with one residual unit, one link connection and one up-sampling unit. With a GLAN structure shown in Figure <ref type="figure" target="#fig_4">4</ref>, the recognition accuracy increases from 73.1% to 80.1% compared to TSSI without attention mechanisms.</p><p>TSSI + SSAN. The SSAN is one of the two attention networks we proposed. The number of sub-sequences and the overlapping rate for the sub-sequences are two hyperparameters that are tuned with validation set. With a subsequence number of 5 and an overlapping rate of 0.5, the attention network achieves an accuracy of 80.9% from 73.1% compared to the base TSSI structure.</p><p>TSSI + GLAN + SSAN. Individually, the GLAN and SSAN san achieve a similar improvement in recognition accuracy. Moreover, we show that the GLAN and SSAN can be well combined to further improve the performance. By replacing the Resnet-50 with the proposed GLAN, the framework achieved an accuracy of 82.4%. This experiment also shows that the proposed two branch attention structure can be adopted as atomic CNN structure in various frameworks to achieve a better performance. Furthermore, we analyze the hyper-parameters in the SSAN, i.e. the overlapping rate, the number and length of sub-images. The relation of these parameters is:</p><formula xml:id="formula_7">T = t sub * [1 + (1 -α) * (n -1)]<label>(7)</label></formula><p>where t sub is the number of frames in each sub-image or the sub-image length, T is the number of frames in the whole sequence, α is the overlapping rate and n is the number of sub-images. We design two sets of experiments with fixed sub-image lengths or fixed sub-image numbers to interpret the effectiveness of the SSAN and find the best set of hyper parameters. Experiments are conducted on NTU RGB+D with the TSSI + GLAN + SSAN framework.</p><p>Starting from the optimal hyper-parameters of a 50% overlapping rate and 5 sub-images, we report the performances under different hyper parameters with either sub-image numbers or sub-image lengths unchanged. For the fixed length experiment as shown in Table <ref type="table" target="#tab_0">II</ref>, the length of sub-images are fixed and the number of sub-images changes from 3 to 9 by adjusting the overlapping rate. We observe the accuracy drops 1.6% from 82.4% to 80.8%. In the fixed sub-image number experiment, the number of sub-images is fixed as five where the best performance is achieved. The length of sub-images varies from T /5 to T /2 with different overlapping rates. A larger drop of accuracy of 3.8% is observed in the fixed subimage number experiment.</p><p>According to the experiment results, the length of subimages influence the performance of the SSAN most. This implies that the SSAN produces a large boost in accuracy mainly with its ability to flexibly adjusting the spatial-temporal resolutions. The optimal hyper parameters of a 50% overlapping rate, 5 sub-images and the T /3 temporal length works best with the 25 joints on NTU RGB-D. The optimal hyper parameters vary on different datasets with different averaged sequence lengths and joint numbers. Furthermore, the SSAN also better learns the long term dependencies. Most results with the SSAN as shown in Table <ref type="table" target="#tab_0">II</ref> outperform the methods with single skeleton frames such as TSSI + GLAN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-of-the-art</head><p>Cross Subject</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross View</head><p>Lie Group <ref type="bibr" target="#b10">[11]</ref> 51.0 52.8 HBRNN <ref type="bibr" target="#b11">[12]</ref> 59.1 64.0 Part-aware LSTM <ref type="bibr" target="#b12">[13]</ref> 62.9 70.3 Trust Gate LSTM <ref type="bibr" target="#b13">[14]</ref> 69.2 77.7 Two-stream RNN <ref type="bibr" target="#b14">[15]</ref> 71.3 79.5 TCN <ref type="bibr" target="#b16">[17]</ref> 74.3 83.1 Global Attention LSTM <ref type="bibr" target="#b33">[34]</ref> 74.4 82.8 A 2 GNN <ref type="bibr" target="#b37">[38]</ref> 72.7 82.8 Clips+CNN+MTLN <ref type="bibr" target="#b15">[16]</ref> 79.6 84.8 Ensemble TS-LSTM <ref type="bibr" target="#b32">[33]</ref> 76.0 82.6 Skepxels <ref type="bibr" target="#b40">[41]</ref> 81.3 89.2 ST-GCN <ref type="bibr" target="#b38">[39]</ref> 81. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluations on Clean Datasets</head><p>NTU RGB+D. The middle column of Table <ref type="table" target="#tab_4">III</ref> shows the results of the NTU RGB+D cross subject setting. The base model with naive skeleton images already outperforms a number of previous LSTM based method, without adopting the attention mechanism. This shows that CNN based methods are promising for skeleton based action With the improved TSSI, the cross subject accuracy achieves 73.1%, which is comparable to the state-of-the-art LSTM methods. The proposed two-branch attention architecture further improves the performance and the GLAN outperforms the stateof-the-art. Experiments prove the effectiveness of the proposed CNN based action recognition method.</p><p>Furthermore, we show that generating sub-sequences and adopting the long-term dependency model (SSAN) can achieve a better results. The SSAN with ResNet-50 achieves a cross subject accuracy of 80.9%. By replacing the ResNet with the proposed GLAN to provide the spatial attention, the framework improves the state-of-the-art to 82.4%. Similar results are observed in the NTU RGB+D cross view setting, as shown in the right column of Table <ref type="table" target="#tab_4">III</ref>.</p><p>SBU Kinect Interaction. Similar to the performance on the NTU RGB+D dataset, the proposed TSSI and attention framework generates a large boost in the recognition accuracy on the SBU Kinect Interaction dataset that outperforms the state-of-the-art. The performances are shown in Table <ref type="table" target="#tab_7">IV</ref>. The proposed TSSI+SSAN+GLAN achieves an accuracy of 95.7± 1.7% on the five splits provided by SBU Kinect Interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Error Case Analysis</head><p>To better understand the successful and failure cases, experiments are conducted to analyze the performance of each class in NTU RGB+D. As shown in  State-of-the-art Accuracy (%) Raw Skeleton <ref type="bibr" target="#b39">[40]</ref> 49.7 HBRNN <ref type="bibr" target="#b11">[12]</ref> 80.4 Trust Gate LSTM <ref type="bibr" target="#b13">[14]</ref> 93.3 Two-stream RNN <ref type="bibr" target="#b14">[15]</ref> 94.8 Global Attention LSTM <ref type="bibr" target="#b33">[34]</ref> 94.1 Clips+CNN+MTLN <ref type="bibr" target="#b15">[16]</ref> 93. show that the actions with dynamic body movements, such as standing, sitting and walking, can be well classified with skeletons, while the classes with less motion like reading, writing and clapping usually have a poor result. The first, middle and last frames from these classes are visualized in the first row of Figure <ref type="figure" target="#fig_6">6</ref>. This follows human intuition that skeletons are more useful for distinguishing dynamic actions, while additional background context information is necessary for recognizing the actions with less motion.</p><p>The results also show that the proposed TSSI, GLAN and SSAN all generate a large boost in performance in all the listed classes. On the righthand side of the table, statistics of the best and worst classes are listed. Results show that TSSI + GLAN + SSAN greatly improve the accuracy in challenging classes. The top 1 worst class in TSSI + GLAN + SSAN has an accuracy of 42.8%, which is even better than the averaged accuracy of the worst 10 in base model. For the best classes, the top 1 accuracy between the baseline and TSSI + GLAN + SSAN are similar. The improvements are mainly obtained through the improvements in the challenging classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Self-Paced Learning on Noisy Datasets</head><p>The model is then evaluated on large scale RGB datasets with estimated and thus noisy poses. For a fair comparison, we do not use any pre-processing methods like interpolation to reduce the noise. To better learn a noise robust system, we adopt self-paced learning <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref> during the training process. The model is first trained with a small portion of reliable pose estimates and then gradually take more noisy data as inputs. The average pose estimation confidence values provided by Openpose is used as the indication of reliability and the level of noises. The model starts with a high confidence threshold of 0.5, i.e. all estimated pose sequences with an average confidence lower than 0.5 are eliminated in the training process. We then fine-tune the model step by step by feeding more unreliable noisy data. Experiments show that self-paced learning can both accelerate the convergence speed and improve the final accuracy.</p><p>Kinetics-Motion. As shown in Table <ref type="table" target="#tab_9">V</ref>, the proposed long term dependency model with attention is comparable to the state-of-the-art performances. The recognition accuracy also similar to the methods using other modalities including RGB State-of-the-art Accuracy (%) RGB CNN <ref type="bibr" target="#b50">[51]</ref> 70.4 Flow CNN <ref type="bibr" target="#b50">[51]</ref> 72.8 ST-GCN <ref type="bibr" target="#b38">[39]</ref> 72.4 Proposed Model Accuracy (%) With TSSI 58.8 TSSI + GLAN <ref type="bibr" target="#b19">[20]</ref> 67.2 TSSI + SSAN + GLAN 68.7 State-of-the-art Accu. (%) RGB Flow Keypoints HLPF <ref type="bibr" target="#b52">[53]</ref> 71.4 --LRCN <ref type="bibr" target="#b0">[1]</ref> 81.6 --3D-ConvNet <ref type="bibr" target="#b1">[2]</ref> 75.2 --Flow CNN <ref type="bibr" target="#b50">[51]</ref> 85.1 --Two-Stream <ref type="bibr" target="#b50">[51]</ref> 91.3 -Proposed Model Accu. (%) With TSSI 87.9 --TSSI + GLAN <ref type="bibr" target="#b19">[20]</ref> 91.0 --TSSI + SSAN + GLAN 91.7 -and optical flow. This experiment proves that the proposed GLAN + SSAN framework is noise robust. The first, middle and last frames from example videos are shown for success and failure cases in the third row of Figure <ref type="figure" target="#fig_6">6</ref>. Observations show that failure cases are mostly caused by missing or incorrect pose estimates. UCF101-Motion. As shown in Table <ref type="table" target="#tab_10">VI</ref>, we evaluate the framework on the proposed UCF-Motion dataset. The proposed framework outperforms previous methods that use a single modality <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> or both appearance feature and optical flow <ref type="bibr" target="#b3">[4]</ref>. The experiment proves that joint is an effective modality for recognizing motion related actions, although joints alone are insufficient for distinguishing all defined action classes since recognizing certain classes requires object and scene appearances. Furthermore, the recognition accuracy is still limited by the imperfect pose estimations. Example frames are shown in the second row of Figure <ref type="figure" target="#fig_6">6</ref>. The compared performances are based on released codes or our reimplementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>Using CNN for skeleton based action recognition is a promising approach. In this work, we address the two major problems with previous CNN based methods, i.e., the improper design of skeleton images and the lack of attention mechanisms. The design of skeleton images is improved by introducing the Tree Structure Skeleton Image (TSSI). The two-branch attention structure is then introduced for visual attention on the skeleton image. A Global Long-sequence Attention Network (GLAN) is proposed based on the twobranch attention structure. We further propose the long-term dependency model with a Sub-Sequence Attention Network (SSAN). The effectiveness of combining the GLAN and the SSAN is also validated. Experiments show that the proposed enhancement modules greatly improve the recognition accuracy, especially on the challenging classes. Extended ablation studies prove that the improvements are achieved by better retaining the skeletal information and focusing on informative joints or time stamps. Moreover, the model shows the robustness against noisy estimated poses. Next on our agenda is to further improve the robustness against incomplete and inaccurate estimated poses. Another direction for further exploration is extending TSSI to automatically learn refined joint relations.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Tree Structure Skeleton Image (TSSI): (a) The example skeleton structure and order in NTU RGB+D, (b) One possible skeleton tree for TSSI generating, (c) Joint arrangements of naive skeleton images, (d) Joint arrangements of TSSI based on the shown skeleton tree, and (e) An example frame of TSSI. Different colors represent different body parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of temporal key stages and inaccurate keypoint predictions on NTU RGB+D. The actions in (a) are: throwing, crossing hands in front and hand waving. Keypoint errors in (b) could lead to incorrect action predictions.</figDesc><graphic coords="2,216.48,155.00,68.23,80.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We propose Tree Structure Skeleton Image (TSSI) to better preserve the spatial relations in skeleton sequences. TSSI improves the previous concatenation skeleton image generation with the depth-first tree traversal. • We propose a two-branch visual attention architecture for skeleton based action recognition. A Global Longsequence Attention Network (GLAN) is introduced based on the proposed architecture. • We propose a Sub-Sequence Attention Network (SSAN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A base attention module and a GLAN module: (a) A base attention block, (b) An expanded plot for the Hourglass mask branch in GLAN, (c) An attention block with GLAN structure, short for 'GLAN block'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The structure of the Global Long-sequence Attention Network (GLAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The structure of the Sub-Sequence Attention Network (SSAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example frames from clean and noisy datasets. In the first row from left to right contains classes from NTU RGB+D: standing up, kicking something, clapping and writing. The second and third row contains predicted noisy poses of success and failure cases. The second row is from UCF101 and the third row is from Kinetics. Success cases are shown on the left side and the failure cases are shown on the right side.</figDesc><graphic coords="10,181.05,389.90,50.54,79.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>PERFORMANCE SUMMARY OF THE STATE-OF-THE-ART ON SKELETON BASED ACTION RECOGNITION.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2, 21, 3, 4, 3, 21, 5, 6, 7, 8, 22, 23, 22, 8, 7, 6, 5, 21, 9, 10, 11, 12, 24, 25, 24, 12, 11, 10, 9, 21, 2, 1, 13, 14, 15, 16, 15, 14, 13, 1, 17, 18, 19, 20, 19, 18, 17, 1, 2]. It is worth noticing that</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III THE</head><label>III</label><figDesc>ACTION RECOGNITION ACCURACY COMPARED TO THE STATE-OF-THE-ART METHODS ON THE NTU RGB+D DATASET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VII, two parts of analysis are conducted. First, eight classes that constantly perform the best or worst are selected on the left side of Table VII. Results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV THE</head><label>IV</label><figDesc>RECOGNITION ACCURACY COMPARED TO THE STATE-OF-THE-ART METHODS ON THE SBU KINETIC INTERACTION DATASET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V THE</head><label>V</label><figDesc>RECOGNITION ACCURACY COMPARED TO THE STATE-OF-THE-ART METHODS ON THE KINETICS-MOTION DATASET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI THE</head><label>VI</label><figDesc>RECOGNITION ACCURACY COMPARED TO THE STATE-OF-THE-ART METHODS ON THE UCF-MOTION DATASET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII THE</head><label>VII</label><figDesc>STATISTICS AND NAMES OF CLASSES WITH THE HIGHEST AND LOWEST RECOGNITION ACCURACY. EXPERIMENTS ARE CONDUCTED ON NTU RGB+D WITH A CROSS SUBJECT SETTING. THE LEFTHAND TABLE SHOWS THE CLASSES THAT CONSTANTLY HAVE A GOOD OR BAD PERFORMANCE. THE RIGHTHAND TABLE SHOWS THE STATISTICS OF THE TOP AND BOTTOM CLASSES.</figDesc><table><row><cell>Selected Best Classes</cell><cell>Base.</cell><cell>TSSI</cell><cell>GLAN</cell><cell>GLAN + SSAN</cell><cell>Best Classes Stat.</cell><cell>Base.</cell><cell>TSSI</cell><cell>GLAN</cell><cell>GLAN + SSAN</cell></row><row><cell>standing up</cell><cell>85.4</cell><cell>94.1</cell><cell>97.1</cell><cell>96.3</cell><cell>Top 1</cell><cell>96.0</cell><cell>99.3</cell><cell>97.8</cell><cell>97.1</cell></row><row><cell>sitting down</cell><cell>91.6</cell><cell>91.6</cell><cell>93.8</cell><cell>93.8</cell><cell>Top 3 Avg.</cell><cell>93.6</cell><cell>96.1</cell><cell>96.8</cell><cell>96.8</cell></row><row><cell>walking apart</cell><cell>90.6</cell><cell>91.3</cell><cell>93.1</cell><cell>96.0</cell><cell>Top 5 Avg.</cell><cell>92.0</cell><cell>94.3</cell><cell>96.2</cell><cell>96.6</cell></row><row><cell>kicking something</cell><cell>80.8</cell><cell>91.7</cell><cell>92.4</cell><cell>92.8</cell><cell>Top 10 Avg.</cell><cell>87.3</cell><cell>92.0</cell><cell>94.8</cell><cell>95.5</cell></row><row><cell>Selected Worst Classes</cell><cell cols="2">Base. TSSI</cell><cell cols="2">GLAN GLAN + SSAN</cell><cell>Worst Classes Stat.</cell><cell cols="2">Base. TSSI</cell><cell cols="2">GLAN GLAN + SSAN</cell></row><row><cell>writing</cell><cell>52.2</cell><cell>26.5</cell><cell>39.7</cell><cell>45.6</cell><cell>Top 1</cell><cell>17.2</cell><cell>25.3</cell><cell>39.7</cell><cell>42.8</cell></row><row><cell>reading</cell><cell>25.6</cell><cell>26.0</cell><cell>39.9</cell><cell>42.8</cell><cell>Top 3 Avg.</cell><cell>23.8</cell><cell>25.9</cell><cell>45.2</cell><cell>49.9</cell></row><row><cell>clapping</cell><cell>17.2</cell><cell>36.6</cell><cell>39.7</cell><cell>63.0</cell><cell>Top 5 Avg.</cell><cell>27.9</cell><cell>31.6</cell><cell>49.5</cell><cell>55.0</cell></row><row><cell>playing with phone</cell><cell>31.6</cell><cell>43.6</cell><cell>56.0</cell><cell>66.2</cell><cell>Top 10 Avg.</cell><cell>39.6</cell><cell>42.2</cell><cell>56.4</cell><cell>61.2</cell></row><row><cell>Overall</cell><cell>68.0</cell><cell>73.1</cell><cell>80.1</cell><cell>82.4</cell><cell>Overall</cell><cell>68.0</cell><cell>73.1</cell><cell>80.1</cell><cell>82.4</cell></row></table><note><p>(a). Clean data; Best classes (b). Clean data; Worst classes (c). Success cases with noisy data (d). Failure cases with noisy data</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the support of New York State through the Goergen Institute for Data Science, our corporate research sponsors Snap and Cheetah Mobile, and NSF Award #1704309.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Personalized pose estimation for body language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Eigenjoints-based action recognition using naive-bayes-nearest-neighbor,&quot; in Computer vision and pattern recognition workshops (CVPRW), 2012 IEEE computer society conference on</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
	<note>Ntu rgb+ d: A large scale dataset for 3d human activity analysis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4570" to="4579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">NIPS Time Series Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action recognition with visual attention on skeleton images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="1996" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2005</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global contextaware attention lstm networks for 3d action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit</title>
		<meeting>Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6099" to="6108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Spatio-temporal naive-bayes nearestneighbor (st-nbnn) for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action-attending graphic neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3657" to="3670" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-person interaction detection using body-pose features and multiple instance learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Skepxels: Spatio-temporal image representation of human skeleton joints for action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05941</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rpan: An end-to-end recurrent poseattention network for action recognition in videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3725" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visual sentiment analysis by attending on local image regions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="231" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recurrent neural network regularization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1409.2329</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised texture segmentation using complementary types of features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2071" to="2082" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Easy samples first: Self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
