<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Jam-O-Drum Interactive Music System: A Study in Interaction Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tina</forename><surname>Blaine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">RhythMix Cultural Works</orgName>
								<address>
									<addrLine>638 Viona Avenue Oakland</addrLine>
									<postCode>94610</postCode>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tim Perkis</orgName>
								<address>
									<addrLine>1050 Santa Fe Avenue Albany</addrLine>
									<postCode>94706</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Jam-O-Drum Interactive Music System: A Study in Interaction Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E34AC6DE75F698B7FE1557430EDFB42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>collaborative</term>
					<term>musical improvisation</term>
					<term>computer graphics</term>
					<term>interactive music system</term>
					<term>input device</term>
					<term>interaction design</term>
					<term>multi-user</term>
					<term>novice</term>
					<term>social interaction</term>
					<term>velocity sensitive DIS &apos;00</term>
					<term>Brooklyn</term>
					<term>New York</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper will describe the multi-user interactive music system known as the Jam-O-Drum developed at Interval Research Corporation. 1 By combining velocity sensitive input devices and computer graphics imagery into an integrated tabletop surface, up to six simultaneous players are able to participate in a collaborative approach to musical improvisation. We demonstrate that this interactive music system embraces both the novice and musically trained participants by taking advantage of their intuitive abilities and social interaction skills. In this paper and accompanying video, we present conclusions from user testing of this device along with examples of interaction design methods and prototypes of interpretive musical and game-like development schemes. Our research was conducted in two phases with two different development teams and will accordingly be addressed herein as Phase One and Phase Two development.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>This project began in July 1998 to explore new ways for people to make music collaboratively. Integrating crosscultural music was inspired by the authors' experiences of communal music making, both in the context of non-western cultures and contemporary computer music <ref type="bibr" target="#b0">[1]</ref>. Prior to this undertaking, Blaine built electronic midi percussion instruments and interactive "show-toys" that integrated music and computer graphics to create unusual opportunities for audience participation with the ensemble D'CuCKOO <ref type="bibr" target="#b1">[2]</ref>.</p><p>Perkis came to the project with an extensive background of work in computer-mediated collaborative music making. His group The Hub connected six electronic musicians in a computer network, and performed experiments in new forms of collective musical improvisation throughout the '80s and '90s <ref type="bibr" target="#b2">[3]</ref>. Both were interested in further experimentation with spontaneous improvisation and interactive audiovisual collaboration. By changing the context of making music to a casual group experience, we particularly hoped to provide novice players the experience of musical interaction in an ensemble setting.</p><p>Several basic goals of the project had been established before the particular configuration of a drum table was conceived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our intention was:</head><p>• To explore music and motion in graphics • To make collective musical experiences available to novices • To experiment with different musical and visual styles • To bring a group of people together for a collaborative approach to music-making • To inspire physical movement and non-self conscious behavior in the players From these objectives, the community drum circle emerged as a metaphor to guide the form and content of our work. The drum circle is a format in which every person has an equal position as observer and participant; and we saw this as an excellent context for creating "in the moment music" <ref type="bibr" target="#b3">[4]</ref>. Another important element of our work included the integration of universal design concepts for accessibility and usability. By creating an interface with a very simple control gesture -hitting a flat surface -we sought to create an experience which would engage people with a wide range of capabilities. Although most of the sound design was built using percussive soundsby integrating melodic long tones in more ambient compositions we hoped to provide a role for those undisposed or unable to engage in vigorous percussive playing. We began thinking of creating an immersive musical environment with a projection display on a wall as inspired by Myron Kruger's augmented reality systems designed for full body interaction, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Eventually we envisioned a shared physical object around which players could gather, in essence, turning the immersive space "inside-out." The Jam-O-Drum prototype design incorporated six drum pads in a seven foot diameter circular table used as an integrated video projection surface. The design supports face-to-face audio and visual collaboration by enabling users to play on drum pads embedded in the surface, simultaneously creating rhythmical musical events and visual effects.</p><p>Over a six month period in 1998, we conducted dozens of participatory demonstrations with people in the Interval community and outside visitors using this graphical user interface. Although time and budget constraints kept us from formal user testing, on the basis of user feedback from these experiences we were able to explore and refine several different interaction schemes. These iterative approaches to interaction design will be referred to herein as Phase One and Phase Two interaction design experiments The two phases correspond to major changes in the software prototyping environment used: the Phase One system relied on Opcode MAX for MIDI programming and Macromedia Director for the visuals; our streamlined Phase Two platform used MIDI sequencers and custom C programs for MIDI and visual processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TIMELINE:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7/98</head><p>Concept and Physical Design </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardware Environment</head><p>The basic system architecture consisted of two separate Power Macintosh G3/266 computers; one for music and the other for graphics. The machines wereconnected to each other using the MIDI serial data protocol, as well as to an Akai S2000 sampler to play the musical sounds, and an Alesis D4 Drum Trigger Module, which received signals from the drum pads embedded in the table. These velocity messages gave a 7bit (0-127) number telling how hard the pad was hit. Each pad contained piezoelectric sensors, which required an audible strike to output a signal but were not pressure sensitive.</p><p>The graphic subsystem was straightforward: a second Macintosh G3 computer, equipped with a simple MIDI interface to communicate with the musical subsystem, and an LCD video projector receiving the Mac video output (and looping it back to a monitor for programmer use). The projector was mounted near a 15' ceiling, and a large, 4.5 ft. square mirror, tilted at a 45˚angle, reflected the projected image down onto the table [Fig. <ref type="figure" target="#fig_0">1</ref>].</p><p>In the final phases of the project, several additional game prototypes were developed on a Windows NT machine, which temporarily replaced the G3 graphics machine in the system.</p><p>The Jam-O-Drum table was built on a seven foot diameter welded steel framework with 10" drum pads mounted directly onto the frame. The composition board surface of the tabletop was designed to leave extra space around the pads to avoid acoustic coupling with the table. An additional six holes were cut in the table surface to allow the mounting of separate speakers in front of each player's station. The entire tabletop is wrapped with a custom fitted cover which feels like a drumhead. Polyfoam padding was used to soften the drum pad surface to allow comfortable hand drumming. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software Environment</head><p>In our early prototypes, Opcode's MAX was used to process MIDI information from pads, control playback of backing tracks, and forward control information to the graphical subsystem [Figure <ref type="figure" target="#fig_1">2</ref>]. MAX is a visual programming system designed to implement real-time control networks, primarily used forMIDI-based musical composition. Akai's MESA II software was used to program the Akai S2000 sampler and sampler library. Emagic Logic, a combined digital audio and MIDI sequencer program, was used in the phase two prototypes to play back MIDI sequences which served to "script" both the audio and graphic events.</p><p>For the graphics software, Macromedia Director 6.0, with HyperMIDI XCMD extension, was used for all prototypes in phase one. Metrowerks CodeWarrior was selected as the C development environment and the Opcode OMS developers kit was integrated for phase two prototypes. The freeware library Spriteworld was utilized in the "bouncing ball" prototypes of phase two, which were written in C.</p><p>Other non-dedicated software-Bias Peak, Steinberg's ReCycle and ReBirth-were used to edit and shape the custom library of individual hit and looping samples utilized in the project demos. Microsoft Visual C++ was used for the card game prototype which ran on a Windows NT machine. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FSR Development</head><p>Initially, we experimented with using two dimensional force sensitive resistors (FSR's) and our own circuitry to build custom trigger pads to provide location and velocity information with regard to surface pressure. Although we believed that an FSR interface would dramatically expand the capabilities of the table, we eventually decided to use out-ofthe-box commercially available drum pads with a midi interface device to enable the immediate investigation of the social computing premises of this research. In some ways, this decision limited our options later on in the project, but it allowed us to move ahead quickly to address content and interaction design issues. The two-dimensional FSR research used a standard PIC chip development suite, the TDE, MPLAB and C Compiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sound Distribution Experiments</head><p>We explored several different methods of routing the custom musical sounds and special effects via the sampler's eight channel output board. These sound distribution experiments were conducted over several different graphical prototypes that will be discussed individually in the Interaction Design sections.</p><p>The following sound distribution methods were tested:</p><p>• Global Mix: Sources were mixed down to a stereo pair of signals played back via speakers in the room.</p><p>• Distributed Sound Sources: A matrix of audio distribution amplifiers and line mixers were used to provide a separate headphone mix for each station [Fig. <ref type="figure" target="#fig_0">1</ref>]. Each headphone mix placed the player's own signal in the center foreground, with a mix of the other player's signals spatialized in the headphone image to match the relative physical position of the other pads from that station.</p><p>• Individual Speakers: Though untested on the six person prototype, we successfully mounted speakers into a smaller three pad prototype table directly in front of each station with sound reinforcement via surround sound speakers and a subwoofer underneath the Jam-O-Drum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>During the preliminary demos, many people had difficulty identifying their own audio contribution within a global mix. We explored the spatialized sound distribution methods not only to solve the problem of player self-identification, but also with awareness that ambient noise would be a factor in the placement of the Jam-O-Drum in a museum environment. As we suspected, headphones made people communicate less with each other, feel more isolated from other players and removed from the overall experience. However, we expected the tradeoff of providing a spatialized method of sound distribution to allow easy identification of each player's individual input.</p><p>In the work of Kendall, Martens and Decker, they found that spatiotemporal patterns create the context in which judgements of direction and distance are made <ref type="bibr" target="#b6">[7]</ref>. If mental models of an acoustic space are formed through abnormal exposure in a novel environment, localization accuracy is significantly degraded <ref type="bibr" target="#b7">[8]</ref>. Our results showed that musicians were better able to hear themselves, but most non-musicians still had difficulty identifying their effect on the system. For those that were unable to easily identify their audio contributions, the participant's focus frequently shifted more toward the graphics for visual feedback. In general, it seemed that people were more comfortable talking and socializing about the graphics and music when they weren't wearing headphones, but tended to be more visually focused on the graphics when they were. We concluded that a combination of direct feedback via individual speakers mounted in front of each player with supporting sound reinforcement (sub-woofer and surround sound speakers) in close proximity might be the most effective approach to sound distribution. From our observations of people's levels of engagement as measured by body movement, gestures, and general levels of excitation, we also felt that this method would allow the most opportunities for social interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTERACTION DESIGN-PHASE ONE</head><p>Our first interaction experiments were done in the context of the MAX programming environment. As stated above, one of the overriding goals of the project was to find a way for nonmusical people to have the experience of music creation. We believed that one of the main impediments to this was that musically untrained people were not able to play accurately. As a consequence, our challenge was to provide the opportunity to get a sense of what it was like to play in a sophisticated musical situation, to lock into a groove. Therefore, we spent a certain amount of time in this period developing schemes that would correct people's playing, or provide a more musical response to their unskilled performance actions than a traditional instrument would.This entailed developing different interpretations of the MIDI data coming from the pad hits and redefining them into what we considered to be more accurate musical responses within a group context</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rhythmic Quantization</head><p>Rhythmic quantization was based on the simple assumption that if people are playing out of time, the response can be quantized to occur at the next incidence of the music's intended beat. We tried varying quantization levels (64th, 32nd, 16th, and 8 th notes), to recalibrate the timing of the players' performance. Although quantization is a common feature of MIDI sequencers used for recording, these schemes were ineffectual in a live performance setting, primarily for two reasons:</p><p>• Unskilled players tend to hit late in relation to the pulse, and quantizing to the NEXT beat subdivision accentuated the late response.</p><p>• Hitting is a most unforgiving gesture that demands immediate feedback. When someone hits a drum pad, there is a precise expectation of a reaction at a specific time. Anything other than that expected reaction intuitively sounds wrong, and makes it more difficult for players to identify their influence on the system.</p><p>In the work of Roh and Wilcox <ref type="bibr" target="#b8">[9]</ref>, a Markov model based recognizer was used to transform a single user's drumming input to play back a grammatically correct tabla pattern as a teaching method. It seems plausible that devices such as Max Matthews' radio baton <ref type="bibr" target="#b9">[10]</ref> or Don Buchla's Marimba Lumina <ref type="bibr" target="#b10">[11]</ref> which sense surface proximity, might be able to execute this form of "pre-quantization." Though untested, we theorize that causing an anticipated hit to sound on time might be effective if its clear that a "behind the beat" player is imminently approaching the pad. Though we had hoped that quantization would contribute to a less chaotic, more coherent musical output from a group of inexperienced players, we found that even subtle quantization was perceptible and distracting for par-ticipants. By diminishing the player's control over their hits, quantization made the overall result less satisfying leading to diminished user interest over time regardless of the graphics programs we tried to accompany this performance method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rhythmic Emphasis Weighting</head><p>Based on what we learned from the failings of temporal quantization, we tried using the pads to set the rhythmic emphasis or volume envelope on a user's playing. As shown in Figure <ref type="figure">3</ref>, the closer a player came to hitting on the beat, the louder the sound associated with their drumpad played back in the overall mix. Inaccurately timed hits had lower volume, which made it more difficult for participants to influence the rhythmic pulse of the music if their hits were inaccurate. As a measure of overall sonic quality, the technique worked well within a group context of novice players; unfortunately, it also made it difficult for players to perceive their effect on the system If a player hit "off beat" consistently, they were then unable to hear their instruments as loudly and often responded with harder hits on the pads in an effort to hear their sounds. This method was introduced to players in connection with the "Techno Rhythm Monitor" prototype which provided a graphic visualization of the tempo in the form of a scrolling bar graph to further assist players to play in time. (The Techno Rhythm Monitor is discussed more below.)</p><p>The Beat</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3 Rhythmic Emphasis Weighting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Volume Envelope</head><p>Related to Rhythmic Emphasis, we tried using the pads to control a volume envelope on an ongoing musical drumloop recorded by an expert player. When a player strikes a pad, the volume of the ongoing loop is turned up for a period of time and fades to zero shortly thereafter. The volume envelope proved to be a musically interesting method that worked best with continuous pulsating material, until people realized they could get the same results by regularly playing on the downbeat. Prior to this discovery, we observed that this method appeared to give people the impression that they were playing exceptionally well. This method proved to be confusing to more experienced players, who easily detected that they were not controlling the detailed rhythmic behavior of their "voice." This method was integrated with animations of dancers that were intended to be played at a particular tempo in order to keep the playback of each frame of a video sequence and the resulting music in rhythm. (The Dancer prototypes are discussed further below.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Velocity to Filter and Pitch Bend Mappings</head><p>Velocity, or the strength with which the pad is hit, generally controlled only the volume of the sample played. We tried altering that simple response by applying resonant low pass filters and pitch bend to the samples based on velocity. To simulate the characteristic of a drum, one combination of effects for drum sounds was to have the peak low pass filter frequency go lower for harder hits, while the overall pitch of the sample went upward. When applied indiscriminately, the unexpected responses resulted in confusion and unsatisfying experiences for the players; but in cases where the effect was applied in a subtle, barely perceptible way, the increased realism proved to be an engaging enhancement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Cycle</head><p>In this interaction method, the MAX patch keeps a count of a cycle of individual hits edited from the performance of an expert player. On each hit from the player, the next note in the cycle is played. Unlike some of our previous loop-based interaction methods, the timing of the sequence cycle is wholly determined by the user. Regulating the speed of playback and associated technique of a "real performance" provided a strong sense of being in control and a generally positive experience for the players. However, certain sound design groupings with more timbral or melodic content had the effect of the player noticing that they were "locked" into a predetermined cycle of events. Occasionally, a player might hit a pad forcefully at a place in the sequence cycle where there was a softer series of notes or vice versa. The potential for dynamic inconsistency between the user's performance and the associated sample put a strong constraint on the range of useful sequences that could be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>From our observations of novice musical behavior on the Jam-O-Drum, we believed it would be of the utmost importance to integrate both audio and visual cues to accommodate synchronization of the user's playback in tempo with the other players. The expectations of synchrony from players when hitting a surface are severe, and these preliminary investigations of musical interaction led us to speculate that softer performance gestures, like rubbing, squeezing, moving a linear controller, knob or wheel, might create looser expectations and permit a broader range of system responses. However, since our original design objectives were to explore a percussive interface, we decided to fully explore the possibilities of the community drum circle metaphor before changing direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRAPHICS PROTOTYPES -PHASE ONE</head><p>In combination with the above-mentioned musical interaction schemes, in phase one we also developed a range of graphical prototypes using the Director Lingo programming environ-ment. While many visual designs were tried, we have chosen to discuss here the two design exercises that we believe were most representative of this period of development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dancers</head><p>For the dancer prototype [Fig. <ref type="figure" target="#fig_2">4</ref>], the graphics team created a Director animation using a process called rotoscoping, in which frames of recorded video are traced by hand. With each hit of a pad, an animation of a dancer in front of each station would advance one frame and increase or decrease in size as a function of velocity. The African mask image overlying the pads would also momentarily shrink to give visual feedback as to the velocity of each hit.The original video footage was recorded with a pair of stereoscopic cameras focused on an energetic Afro-Brazilian dancer with colorful costume changes.</p><p>By setting the optimal playback of the dancer animations to the same tempo as the backing tracks, we hoped these graphics would encourage more active physical movement and spontaneous dancing amongst the players. We also thought that even if the players were not attuned to the musical backing track tempo, they would notice the spasmodic playback of the animations at other speeds and naturally fall into the implied rhythm of the dancer. Occasionally, these anticipated behaviors would be observed, but neither occurred with regularity. While most of our graphics designs were abstract, this prototype design was our first experimentation with the projection of physical motions. Although the animation sequences were hand drawn, they were based on the physical behavior and choreography of a professional dancer. Despite these efforts to encourage rhythmic playing and movement through an audiovisual representation of dance, our results showed that after participants had advanced through a dance sequence several times, it ceased to reward sustained exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Techno Rhythm Monitor</head><p>In this prototype [Fig. <ref type="figure">5</ref>], we were interested in exploring visual cues as a means of enabling the participation of people without any musical knowledge. Each player had a scrolling bar graph of their activity in front of their station. Bar height corresponded to the player's velocity of attack and a red square indicated the downbeat to match along the grid at 120 beats per minute. We thought that providing a continually scrolling visual representation of the musical tempo would allow novices to get a sense of where they were in relation to the pulse. When a player hit directly on the downbeat, they were rewarded with a graphic image in the center of the table. While we tried to convey the look and feel of a video game supported by a techno musical style, the scrolling bar graph confused both visual and aurally-oriented users. The rhythm monitor, though seeming at first to be a clear and simple design, turned out to be an ineffective scheme for helping performers to play in time. Furthermore, it appeared that the players' efforts to match a moving cursor inhibited rather than increased the opportunities for casual musical interaction with others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5. Techno Rhythm Monitor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTERACTION DESIGN EXPERIMENTS -PHASE TWO</head><p>As a consequence of the complex phase one experiments using MAX and Director, we modified the models of interaction, changed music and graphic software, and chose to focus on simpler interaction schemes. The prototypes in this phase generally used combinations of prerecorded MIDI sequences and simple, unprocessed MIDI input from the drum pads to trigger both musical and graphical events. In reengineering our prototyping system, we had these specific goals in mind: </p><formula xml:id="formula_0">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>To provide clearer feedback, allowing players to more easily identify their own contribution to the music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Call and Response</head><p>The Call and Response system illustrated in Figure <ref type="figure" target="#fig_4">6</ref> was developed in response to the chaotic interaction and lack of direction in earlier demos. As a result of our earlier work, we set out to orchestrate the design of rhythmic and visual patterns that would inspire "follow the leader" behavior. We believe this sociable method of phrase recognition and imitation shows some of the potential of the Jam-O-Drum for educational applications. The call and response animation consisted of several elements:</p><p>• Background: overall visual changes to indicate new musical sections • Caller: provides the lead music example to be matched by the players • Response Cues: Indicates the desired rhythm to be matched by the players • "Your Turn" Indicators: Arrow selectors to cue players when to play and when to listen allowing players to take different roles in the audiovisual composition. Pad Visuals: To give visual reinforcement and repre-sentation of the audio selections.The sequencer would play short rhythmic patterns that triggered synchronized flashing of the "call" area in the center of the screen. The call patterns were followed by space for players to copy the pattern, directed by response cues. "Your Turn" indicators allowed the table to play all together, to be split into subgroups, or support solo sections. Once the players caught on to the overall arrangement of when to play and when to listen, opportunities would emerge for more experienced players to improvise within the compositional form. While some players found the rhythmic learning experience too structured to be entertaining, others enjoyed the "Simon" aspect of game-like interaction this prototype introduced. In an effort to create sound sets that would emulate playing on an acoustic percussion instrument, some of the player's sounds were split into different sample zones based on velocity. Some participants found this entertaining, while others were sonically confused by the changes in their own sound.</p><p>Of all the interaction methods we explored, the call and response patterns were the most successful in bringing both novice and expert players together for musical collaboration. From our observations, this method appeared to inspire the most animated physical response as people tried to mimic the rhythmic patterns, and led to spontaneous rhythmical improvisation within the demo groups. In general, novice players seemed less intimidated about playing out and became more active participants in the digital drum circle when engaged in the Call and Response system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bouncing Balls ("HexaPong")</head><p>The bouncing ball prototype displayed in Figure <ref type="figure">7</ref> was originally conceived to be a six way musical pong game. Our design goal was to explore the correlation between musical events and the physical behavior of bouncing, moving and spinning balls. Players were given a limited set of four balls that were shot toward an opposite wall as the drum pad was hit. This playing motion also caused a series of gamelan bell samples to be triggered. Once the balls have been set in motion, the audio corresponding to each ball cannot be reactivated until the balls return to their place of origin.</p><p>The ball paths are implemented in the code as parametric curves which can follow slightly different curving paths to their destinations. The speed of each ball is adjusted so that no matter how long a trip it is taking, it takes the same amount of time. This has the interesting side-effect that a sequence of balls forms a "platoon" visually representing the rhythm that was originally played. As these balls hit their target and bounce, they sound again, the sequence of hits forming an echo of the original rhythm.</p><p>We found that the combination of 3D-like sprite objects coupled with the bell sounds did give people the impression of a game and also inspired playful behavior with others gathered around the Jam-O-Drum. Many people assumed they were partners with the person across the table, probably due to the fact that the balls travel to opposite positions. Although the periodic physical and musical timing of the ball bounces caused some musical mayhem, we believe that limiting the number of objects a player controls would help minimize the chaos. Conversely, limiting the number of sounds a player can make at one time makes it harder to generate a specific rhythmic pattern. It was difficult to find the right balance giving rich possibilities without leading to musical chaos. Were we to revisit this interaction scheme in the future, we would explore the possibility of having players work together towards clearly defined collective goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 7: Bouncing Balls Prototype</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BlissPaint Collaborative Drawing</head><p>BlissPaint <ref type="bibr" target="#b11">[12]</ref> is a commercially available color animated drawing program that was modified by its creator for the Jam-O-Drum environment. A series of drawing modes are automatically traversed using a pre-scripted MIDI graphics sequence. Players cause color, hue saturation and brightness changes in an overall fluid, kaleidoscopic atmosphere [Fig. <ref type="figure">8</ref>]. This freeform approach to visual and collaborative improvisation was supported by an ambient sound design less rhythmical than that of our other prototypes. Participants were free to play at any time, but due to the high level of visual interest, people were generally less attentive to the act Figure <ref type="figure">8</ref>. Bliss Paint of playing music together. Despite the lack of musical focus, the BlissPaint schemes were perceived as the most visually impressive prototypes and held user interest for extended periods of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Our work on a collaborative approach to musical interaction is inspired by many accomplished examples of rhythmic input devices <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, reactive tables <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, computer vision <ref type="bibr" target="#b4">[5]</ref>, augmented environments <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, interface design <ref type="bibr" target="#b20">[21]</ref>, and alternate controllers <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, although not all of these models were intended to engage naive users or necessarily even involve music.</p><p>Toshio Iwai, has designed a number of elegant interactive audiovisual installations which generate musical sounds and computer-generated graphics. For example, Iwai's Resonance of 4 <ref type="bibr" target="#b12">[13]</ref> and Composition on the Table <ref type="bibr" target="#b13">[14]</ref> mixed reality installations share many of the same goals as the Jam-O-Drum project in that players compose interactive music based on reactions to other players. However, Iwai's audiovisual compositions integrate a network of devices with non-musical input devices such as trackballs, mice, dials, or switches with primarily non-rhythmic content. We believe our work differs from Iwai's in that we built the Jam-O-Drum as a system for rhythmically-oriented musical collaboration with six or more participants on one shared device using computer graphics visuals and MIDI input devices.</p><p>MIT's interactive computer vision project PingPongPlus uses a "reactive table" that incorporates sensors, sound and projection technologies to encourage full body motion during gameplay <ref type="bibr" target="#b14">[15]</ref>. While there are many areas of common interest, we believe this prior research is not focused primarily on multi-user collaborative music-making methods for novices as the main point of convergence between mixed media usage. Tod Machover's research in HyperInstruments <ref type="bibr" target="#b15">[16]</ref> that fueled the Brain Opera, featured an installation area called the Mind Forest with numerous input devices for musical interaction to augment a networked and live performance event <ref type="bibr" target="#b16">[17]</ref>. Although large numbers of people could share the installation space and multiple input devices were available, many of the devices were geared toward solo or dual user interaction, such as the Gesture Wall <ref type="bibr" target="#b17">[18]</ref>. One notable exception, the Rhythm Tree, was designed to accommodate large-scale interaction <ref type="bibr" target="#b18">[19]</ref>. Up to fifty users played vocal sounds, words and non-rhythmically oriented audio samples using 300 networked drum pads on multiple "branches" of a tree with LEDs rather than computer graphics imagery. Again, this work also differs from the research objectives of the Jam-O-Drum music system in that our focus was on creating combined graphical and musical interaction methods on one shared device with embedded triggers to implement a coherent collaborative rhythmic experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OBSERVATIONS AND FUTURE WORK</head><p>One of the interesting side effects of this research was the feeling of community that players developed over time. People appeared to be more comfortable socializing and engaging with strangers as they gathered around a shared object. Even when people who were reluctant to play, merely sharing the circular space facing other participants invited opportunities for engagement that might not otherwise present themselves in a public setting. While all the interaction schemes presented gave visual and audio cues in tandem, some people were simply more comfortable following visual cues, while others found it easier to synchronize with the group by focusing their attention on the audio cues.</p><p>We believed that the development of the Phase Two interaction schemes based on the outcomes of the Phase One prototypes were leading the design team closer to our goals of inspiring collective audiovisual collaborations, but we still needed more time to refine these ideas when the project ended. People really enjoyed the social aspects of playing music together and for some, it was their first experience of ensemble playing. Several participants commented that they would be ordinarily be too self-conscious to play a musical instrument in public, but felt that the Jam-O-Drum helped them overcome these emotions because it was inviting and fun to play.</p><p>One area of design where we went astray was in not responding quickly enough to user feedback requesting more responsive controllers. Although we discussed the features of other controllers, we neglected to integrate more capable control surfaces early on in the project. In retrospect, we should have explored a variety of control surfaces from the beginning, but due to time constraints, never had an opportunity to fully explore the effect of incorporating more responsive controllers. Our reliance upon continual iteration and casual user testing, proved to be an invaluable aspect of our design method Although the primary focus of our research was on musical interaction, we developed an audiovisual platform capable of supporting a wide variety of content. We believe there is promise in several application domains for this and similar devices, including education, scientific visualization, meeting facilitation, networked gaming, web-browsing and of course, collaborative musical improvisation. A version of the Jam-O-Drum has been placed at the Experience Music Project Museum in Seattle. The installation features a modified version of the Call and Response interaction method for six to twelve participants. A smaller three person version of the Jam-O-Drum was featured at Siggraph 2000 in New Orleans and will be donated by Interval Research to the Entertainment Technology Center at Carnegie Mellon University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>We have presented the interaction design and implementation of collaborative musical improvisation based on the Jam-O-Drum interactive music system. We discovered that the quality of the users' experience was more dependent upon the group's interaction as a whole rather than any individual's musical ability. Despite our research and application of many complex interaction schemes that we suspected would yield positive communal and musical results, instead we found that less is more: our simplest and most direct interaction methods yielded the best results. The Call &amp; Response prototype was the most effective interaction scheme we employed in terms of getting people to play together. The Bliss Paint prototypes were the most successful in terms of holding user interest for long periods of time and creating unique social situations for playing visual music.</p><p>Finding the proper balance between control behaviors or directives for users that would also allow enough capability for creative expression was a consistent goal of the design team. Finally, we have indicated several promising directions for future work and potential improvements to this system. It is our hope that this interactive music system will continue to facilitate musical collaboration and social interaction, particularly for novice users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1 :</head><label>1</label><figDesc>Fig 1: System Design Overview</figDesc><graphic coords="2,317.88,381.60,250.68,226.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Max Patch used in Phase One Development</figDesc><graphic coords="3,55.44,269.40,242.88,219.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 4 :</head><label>4</label><figDesc>Fig 4: Dancers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>To show more game-like interaction • To show educational, structured activities which would encourage more mutual regard and communication among the players • To develop a basic software framework which would support the rapid development of variant prototypes • To more carefully orchestrate the experience, in order to avoid chaotic interaction • To expand the level of social engagement between players by creating more goal-oriented interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Call and Response Prototype</figDesc><graphic coords="6,317.88,412.56,239.88,213.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank all the team members on this project for their invaluable contributions: Greg Beattie, Hila Dar, Eiko DoEspiritoSanto, Kris Force, Greg Jalbert, Charles Lassiter, Ignazio Moresco, Michael Naimark, Steve Rubin, Stuart Sharpe, and Tricia Wright. We would also like to thank Interval Research Corporation and our esteemed colleagues at IRC for their support and feedback. We are pleased that the Experience Music Project in Seattle is the new home for the Jam-O-Drum installation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Hackers Hit the Stage</title>
		<author>
			<persName><forename type="first">G</forename><surname>Robair</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
		</imprint>
	</monogr>
	<note>Electronic Musician</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D'</forename><surname>Cuckoo</surname></persName>
		</author>
		<ptr target="http://www.dcuckoo.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">The</forename><surname>Hub</surname></persName>
		</author>
		<ptr target="http://www.artifact.com/perkis/w_hubEM.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Facilitating Human Potential Through Rhythm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="21" to="52" />
			<pubPlace>Drum Circle Spirit, White Cliffs Media Tempe, AZ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Krueger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Reality</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<date type="published" when="1990">1990</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Full-Body Interactive Exhibits</title>
		<author>
			<persName><forename type="first">Myron</forename><forename type="middle">W</forename><surname>Kruger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hypermedia and Interactivity in Museums. Archives and Museums Informatics</title>
		<meeting><address><addrLine>Pittsburgh PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="222" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Current Directions in Computer Music Research</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Decker</surname></persName>
		</author>
		<editor>Max Matthews &amp; John Pierce</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monoaural localization: an analysis of practice effects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Musicant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Butler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception and Psychophysics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="236" to="240" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring Tabla Drumming Using Rhythmic Input</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI &apos;95</title>
		<meeting>CHI &apos;95<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995-05">May 1995. 1995</date>
			<biblScope unit="page" from="310" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Matthews</surname></persName>
		</author>
		<ptr target="http://www-ccrma.stanford.edu/CCRMA/Courses/252/sensors/node2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Buchla</surname></persName>
		</author>
		<author>
			<persName><surname>Marimbalumina</surname></persName>
		</author>
		<ptr target="http://www.buchla.com/mlumina.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Jalbert</surname></persName>
		</author>
		<author>
			<persName><surname>Blisspaint</surname></persName>
		</author>
		<ptr target="http://www.imaja.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Toshio</forename><surname>Iwai</surname></persName>
		</author>
		<ptr target="http://www.iamas.ac.jp/~iwai/artworks/resonance.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Toshio</forename><surname>Iwai</surname></persName>
		</author>
		<ptr target="http://www.ntticc.or.jp/newschool/+-/iwai_e.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PingPongPlus: Design of an Athletic-Tangible Interface for Computer-Supported Cooperative Play</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wisneski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orbanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paradiso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI &apos;99</title>
		<meeting>CHI &apos;99<address><addrLine>Pittsburgh, PA USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999-05">May 1999. 1999</date>
			<biblScope unit="page" from="394" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Machover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><surname>Hyperinstruments</surname></persName>
		</author>
		<title level="m">Musically Intelligent and Interactive performance and Creativity Systems. Intl. Computer Music Conference</title>
		<meeting><address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Machover</surname></persName>
		</author>
		<author>
			<persName><surname>Brainopera</surname></persName>
		</author>
		<ptr target="http://parallel.park.org/Events/BrainOpera" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Paradiso</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ciliberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Brain</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Opera</surname></persName>
		</author>
		<ptr target="http://www.media.mit.edu/~joep/TTT.BO/wall.html" />
		<imprint>
			<publisher>Gesture Wall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Brain Opera: Rhythm Tree</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paradiso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ciliberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Denckla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knaian</surname></persName>
		</author>
		<ptr target="http://www.media.mit.edu/~joep/TTT.BO/tree.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WorldBeat: Designing a Baton-Based Interface for an Interactive Music</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI &apos;97</title>
		<meeting>CHI &apos;97<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997-03">March 1997</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring Color in Interface Design</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Falck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactions</title>
		<imprint>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="36" to="48" />
			<date type="published" when="1996-08">July + August 1996</date>
			<publisher>ACM Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">computer Augmented Environments: Back to the Real World</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commuications of the ACM</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Special Issue on Augmented Environments</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
	<note>Communications of the ACM</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Alembic: Virtual Unrealities</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<ptr target="http://www-crd.rca.ac.uk/~richardb" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
