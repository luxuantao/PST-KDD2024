<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The necessity and sufficiency of anytime capacity for stabilization of a linear system over a noisy communication link Part I: scalar systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-01-04">4 Jan 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anant</forename><surname>Sahai</surname></persName>
							<email>sahai@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sanjoy</forename><surname>Mitter</surname></persName>
							<email>mitter@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The necessity and sufficiency of anytime capacity for stabilization of a linear system over a noisy communication link Part I: scalar systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-01-04">4 Jan 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">A92A739D2B5C8B85E970087AD98546AF</idno>
					<idno type="arXiv">arXiv:cs.IT/0601007v1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Real-time information theory</term>
					<term>reliability functions</term>
					<term>error exponents</term>
					<term>feedback</term>
					<term>anytime decoding</term>
					<term>sequential coding</term>
					<term>control over noisy channels</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We review how Shannon's classical notion of capacity is not enough to characterize a noisy communication channel if the channel is intended to be used as part of a feedback loop to stabilize an unstable scalar linear system. While classical capacity is not enough, another sense of capacity (parametrized by reliability) called "anytime capacity" is shown to be necessary for the stabilization of an unstable process. The required rate is given by the log of the unstable system gain and the required reliability comes from the sense of stability desired. A consequence of this necessity result is a sequential generalization of the Schalkwijk/Kailath scheme for communication over the AWGN channel with feedback.</p><p>In cases of sufficiently rich information patterns between the encoder and decoder, adequate anytime capacity is also shown to be sufficient for there to exist a stabilizing controller. These sufficiency results are then generalized to cases with noisy observations, delayed control actions, and without any explicit feedback between the observer and the controller. Both necessary and sufficient conditions are extended to continuous time systems as well. We close with comments discussing a hierarchy of difficulty for communication problems and how these results establish where stabilization problems sit in that hierarchy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The necessity and sufficiency of anytime capacity for stabilization of a linear system over a noisy communication link Part I: scalar systems</p><formula xml:id="formula_0">I. INTRODUCTION</formula><p>For communication theorists, Shannon's classical channel capacity theorems are not just beautiful mathematical results, they are useful in practice as well. They let us summarize a diverse range of channels by a single figure of merit: the capacity. For most non-interactive point-to-point communication applications, the Shannon capacity of a channel provides an upper bound on performance in terms of end-toend distortion through the distortion-rate function. As far as distortion is concerned, all that matters is the channel capacity and the nature of the source. Given enough tolerance for end-to-end delay, the source can be encoded into bits and those bits can be reliably transported across the noisy channel if the rate is less than the Shannon capacity. As long as the source, distortion, and channel are well-behaved <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, there is asymptotically no loss in separating the problems of source and channel coding. This provides a justification for the layered architecture that lets engineers isolate the problem of reliable communication from that of using the communicated information. Recent advances in coding theory have also made it possible to approach the capacity bounds very closely in practical systems.</p><p>In order to extend our understanding of communication to interactive settings, it is essential to have some model for interaction. Schulman and others have studied interaction in the context of distributed computation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The interaction there is between computational agents that have access to some private data and wish to perform a global computation in a distributed way. The computational agents can only communicate with each other through noisy channels. In Schulman's formulation, capacity is not a question of major interest since constant factor slowdowns are considered acceptable. <ref type="foot" target="#foot_0">1</ref>Fundamentally, this is a consequence of being able to design all the system dynamics. The rich field of automatic control provides an interactive context to study capacity requirements since the plant dynamics are given, rather than something that can be designed. In control, we consider interaction between an observer that gets to see the plant and a controller that gets to control it. These two can be connected by a noisy channel.</p><p>Shannon himself had suggested looking to control problems for more insight into reliable communication <ref type="bibr" target="#b4">[5]</ref>. ". . . can be pursued further and is related to a duality between past and future<ref type="foot" target="#foot_1">2</ref> and the notions of control and knowledge. Thus we may have knowledge of the past and cannot control it; we may control the future but have no knowledge of it."</p><p>We are far from the first to attempt to bring together information and control theory. In <ref type="bibr" target="#b6">[7]</ref>, Ho, Kastner, and Wong drew out a detailed diagram in which they summarized the then known relationships among team theory, signaling, and information theory from the perspective of distributed control. Rather than taking such a broad perspective, we instead ask whether Shannon's classical capacity is the appropriate characterization for communication channels arising in distributed control systems. Our interest is in understanding the fundamental relationship between problems of stabilization and problems of communication.</p><p>Tatikonda's recent work on sequential rate distortion theory provides an information-theoretic lower-bound on the achievable performance of a control system over a channel. Because this bound is sometimes infinite, it also implies that there is a fundamental rate of information production, namely the sum of the logs of the unstable eigenvalues of the plant, that is invariantly attached to an unstable linear discrete-time process <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This particular notion of rate was justified by showing how to stabilize the system over a noiseless feedback link with capacity greater than the intrinsic rate for the unstable process. <ref type="foot" target="#foot_2">3</ref> Nair et al. extended this to cover the case of unbounded disturbances and observation noise under suitable conditions <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In addition to noiseless channels, the results were extended for almost-sure stabilization in the context of undisturbed <ref type="foot" target="#foot_3">4</ref> control systems with bounded initial conditions being stabilized over certain noisy channels <ref type="bibr" target="#b11">[12]</ref>.</p><p>We had previously showed that it is possible to stabilize persistently disturbed controlled Gauss-Markov processes over suitable power-constrained AWGN (Additive White Gaussian Noise) channels <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> where it turns out that Shannon capacity is tight and linear observers and controllers are sufficient to achieve stabilization <ref type="bibr" target="#b14">[15]</ref>. In contrast, we showed that the Shannon capacity of the binary erasure channel (BEC) is not sufficient to check stabilizability and introduced the anytime capacity as a candidate figure of merit <ref type="bibr" target="#b15">[16]</ref>. Following up on our treatment of the BEC case, Martins et al. have studied more general erasure-type models and have also incorporated bounded model uncertainty in the plant <ref type="bibr" target="#b16">[17]</ref>. There is also related work by Elia that uses ideas from robust control to deal with communication uncertainty in a mixed continuous/discrete context, but restricting to linear operations <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Basar and his students have also considered such problems and have studied the impact of a noisy channels on both the observations and the controls <ref type="bibr" target="#b19">[20]</ref>. The area of control with communications constraints continues to attract attention and the reader is directed to the recent September 2004 issue -Communication Problems over Noisy Channels with Noiseless Feedback with Noisy Feedback Channels Stabilization Problems Fig. <ref type="figure">1</ref>. The "equivalence" between stabilization over noisy feedback channels and reliable communication over noisy channels with feedback is the main result established in this paper.</p><p>of IEEE Transactions on Automatic Control and the articles therein for a more comprehensive survey.</p><p>Many of the issues that arise in the control context also arise for the conceptually simpler problem of merely estimating an unstable open-loop process 5 , across a noisy channel. For this estimation problem in the limit of large, but finite, end-to-end delays, we have proved a source coding theorem that shows that the distortion-rate bound is achievable. Furthermore, it is possible to characterize the information being produced by an unstable process <ref type="bibr" target="#b22">[23]</ref>. It turns out that such processes produce two qualitatively distinct types of information when it comes to transport over a noisy channel. In addition to the classical Shannon-type of information found in traditional ratedistortion settings 6 , there is an essential core of information that captures the unstable nature of the source. While classical Shannon reliability suffices for the classical information, this unstable core requires anytime reliability for transport across a noisy channel. 7 As also discussed in this paper, anytime reliability is a sense of reliable transmission that lies between Shannon's classical ǫ-sense of reliable transmission and his zero-error reliability <ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b22">[23]</ref>, we also review how the sense of anytime reliability is linked to classical work on sequential tree codes with bounded delay decoding. 8  The new feature in control systems is their essential interactivity. The information to be communicated is not a message known in advance that is used by some completely separate entity. Rather, it evolves through time and is used to control the very process being encoded. This introduces two interesting issues. First, causality is strictly enforced. The encoder and controller must act in real time and so taking the limit of large delays must be interpreted very carefully. Second, it is unclear what the status of the controlled process is. If the controller succeeds in stabilizing the process, it is no longer unstable. As explored in Section II-D, a purely external non-interactive observer could treat the question of encoding the controlled closed-loop system state using classical tools for the encoding and communication of a stationary ergodic 5 The unstable open-loop processes discussed here are first-order nonstationary autoregressive processes <ref type="bibr" target="#b20">[21]</ref>, of which an important special case is the Wiener process considered by Berger <ref type="bibr" target="#b21">[22]</ref>. 6 In <ref type="bibr" target="#b22">[23]</ref>, we show how the classical part of the information determines the shape of the rate-distortion curve, while the unstable core is responsible for a shift of this curve along the rate axis. 7 How to communicate such unstable processes over noisy channels had been an open problem since Berger had first developed a source-coding theorem for the Wiener process <ref type="bibr" target="#b23">[24]</ref>. Berger had conjectured that it was impossible to transport such processes over generic noisy channels with asymptotically finite end-to-end distortion using traditional means. 8 Reference <ref type="bibr" target="#b25">[26]</ref> raised the possibility of such a connection early on.</p><p>process. Despite having to observe and encode the exact same closed-loop process, the observer internal to the control system requires a channel as good as that required to communicate the unstable open-loop process. This seemingly paradoxical situation illustrates what can happen when the encoding of information and its use are coupled together by interactivity.</p><p>In this paper (Part I), the basic equivalence between feedback stabilization and reliable communication is established. The scalar problem (Figure <ref type="figure" target="#fig_0">2</ref>) is formally introduced in Section II where classical capacity concepts are also shown to be inadequate. In Section III, it is shown that adequate feedback anytime capacity is necessary for there to exist an observer/controller pair able to stabilize the unstable system across the noisy channel. This connection is also used to give a sequential anytime version of the Schalkwijk/Kailath scheme for the AWGN channel with noiseless feedback.</p><p>Section IV shows the sufficiency of feedback anytime capacity for situations where the observer has noiseless access to the channel outputs. In Section V, these sufficiency results are generalized to the case where the observer only has noisy access to the plant state. Since the necessary and sufficient conditions are tight in many cases, these results show the asymptotic equivalence between the problem of control with "noisy feedback" and the problem of reliable sequential communication with noiseless feedback. In Section VI, these results are further extended to the continuous time setting. Finally, Section VII justifies why the problem of stabilization of an unstable linear control system is "universal" in the same sense that the Shannon formulation of reliable transmission of messages over a noisy channel with (or without) feedback is universal. This is done by introducing a hierarchy of communication problems in which problems at a given level are equivalent to each other in terms of which channels are good enough to solve them. Problems high in the hierarchy are fundamentally more challenging than the ones below them in terms of what they require from the noisy channel.</p><p>In Part II, the necessity and sufficiency results are generalized to the case of multivariable control systems on an unstable eigenvalue by eigenvalue basis. The role of anytime capacity is played by a rate region corresponding to a vector of anytime reliabilities. If there is no explicit channel output feedback, the intrinsic delay of the control system's input-output behavior plays an important role. It shows that two systems with the same unstable eigenvalues can still have potentially different channel requirements. These results establish that in interactive settings, a single "application" can fundamentally require different senses of reliability for its data streams. No single number can adequately summarize the channel and any layered communication architecture should allow applications to adjust reliabilities on bitstreams.</p><p>There are many results in this paper. In order not to burden the reader with repetitive details and unnecessarily lengthen this paper, we have adopted a discursive style in some of the proofs. The reader should not have any difficulty in filling in the omitted details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM DEFINITION AND BASIC CHALLENGES</head><p>Section II-A formally introduces the control problem of stabilizing an unstable scalar linear system driven by both a control signal and a bounded disturbance. In Section II-B, classical notions of capacity are reviewed along with how to stabilize an unstable system with a finite rate noiseless channel. In Section II-C, it is shown by example that the classical concepts are inadequate when it comes to evaluating a noisy channel for control purposes. Shannon's regular capacity is too optimistic and zero-error capacity is too pessimistic. Finally, Section II-D shows that the core issue of interactivity is different than merely requiring the encoders and decoders to be delay-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The control problem</head><formula xml:id="formula_1">X t+1 = λX t + U t + W t , t ≥ 0 (1)</formula><p>where</p><formula xml:id="formula_2">{X t } is a IR-valued state process. {U t } is a IR-valued control process and {W t } is a bounded noise/disturbance process s.t. |W t | ≤ Ω 2 .</formula><p>This bound is assumed to hold with certainty. For convenience, we also assume a known initial condition X 0 = 0.</p><p>To make things interesting, consider λ &gt; 1 so the openloop system is exponentially unstable. The distributed nature of the problem (shown in Figure <ref type="figure" target="#fig_0">2</ref>) comes from having a noisy communication channel in the feedback path. The observer/encoder system O observes X t and generates inputs a t to the channel. It may or may not have access to the control signals U t or past channel outputs B t-1 as well. The decoder/controller <ref type="foot" target="#foot_4">9</ref> system C observes channel outputs B t and generates control signals U t . Both O, C are allowed to have unbounded memory and to be nonlinear in general.</p><p>Definition 2.1: A closed-loop dynamic system with state</p><formula xml:id="formula_3">X t is f -stable if P(|X t | &gt; m) &lt; f (m) for all t ≥ 0.</formula><p>This definition requires the probability of a large state value to be appropriately bounded. A looser sense of stability is given by: Definition 2.2: A closed-loop dynamic system with state</p><formula xml:id="formula_4">X t is η-stable if there exists a constant K s.t. E[|X t | η ] ≤ K for all t ≥ 0.</formula><p>In both definitions, the bound is required to hold for all possible sequences of bounded disturbances {W t } that satisfy the given bound Ω. We do not assume any specific probability model governing the disturbances. Rather than having to specify a specific target for the tail probability f , holding the η-moment within bounds is a way of keeping large deviations rare. The larger η is, the more strongly very large deviations are penalized. The advantage of η-stability is that it allows constant factors to be ignored while making sharp asymptotic statements. Furthermore, Section III-C shows that for generic DMCs, no sense stronger than η-stability is feasible.</p><p>The goal in this paper is to find necessary and sufficient conditions on the noisy channel for there to exist an observer O and controller C so that the closed loop system shown in Figure <ref type="figure" target="#fig_0">2</ref> is stable in the sense of definitions 2.1 or 2.2. The problem is considered under different information patterns corresponding to different assumptions about what information is available at the observer O. The controller is always assumed to just have access to the entire past history<ref type="foot" target="#foot_5">10</ref> of channel outputs.</p><p>For discrete-time linear systems, the intrinsic rate of information production (in units of bits per time) equals the sum of the logarithms (base 2) of the unstable eigenvalues <ref type="bibr" target="#b8">[9]</ref>. In the scalar case studied here, this is just log 2 λ. This means that it is generically <ref type="foot" target="#foot_6">11</ref> impossible to stabilize the system in any reasonable sense if the feedback channel's Shannon classical capacity C &lt; log 2 λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classical notions of channels and capacity Definition 2.3:</head><p>A discrete time channel is a probabilistic system with an input. At every time step t, it takes an input a t ∈ A and produces an output b t ∈ B with probability<ref type="foot" target="#foot_7">12</ref> p(B t |a t 1 , b t-1 1 ) where the notation a t 1 is shorthand for the sequence a 1 , a 2 , . . . , a t . In general, the current channel output is allowed to depend on all inputs so far as well as on past outputs.</p><p>The channel is memoryless if conditioned on a t , B t is independent of any other random variable in the system that occurs at time t or earlier. All that needs to be specified is p(B t |a t ).</p><p>The maximum rate achievable for a given sense of reliable communication is called the associated capacity. Shannon's classical reliability requires that after a suitably large end-toend delay 13 n that the average probability of error on each bit is below a specified ǫ. Shannon classical capacity C can also be calculated in the case of memoryless channels by solving an optimization problem:</p><formula xml:id="formula_5">C = sup P(A) I(A; B)</formula><p>where the maximization is over the input probability distribution and I(A; B) represents the mutual information through the channel <ref type="bibr" target="#b0">[1]</ref>. This is referred to as a single letter characterization of channel capacity for memoryless channels. Similar formulae exist using limits in cases of channels with memory. There is another sense of reliability and its associated capacity C 0 called zero-error capacity which requires the probability of error to be exactly zero with sufficiently large n. It does not have a simple single-letter characterization <ref type="bibr" target="#b24">[25]</ref>.</p><p>Example 2.1: Consider a system <ref type="bibr" target="#b0">(1)</ref> with Ω = 1 and λ = </p><formula xml:id="formula_6">(B t = 1|a t = 1) = p(B t = 0|a t = 0) = 1 while p(B t = 1|a t = 0) = p(B t = 0|a t = 1) = 0. This channel has C 0 = C = 1 &gt; log 2 3 2 . Use a memoryless observer O(x) = 0 if x ≤ 0 1 if x &gt; 0</formula><p>and memoryless controller</p><formula xml:id="formula_7">C(B) = + 3 2 if B = 0 -3 2 if B = 1</formula><p>Assume that the closed loop system state is within the interval [-2, +2]. If it is positive, then it is in the interval [0, +2]. At the next time, 3  2 X + W would be in the interval [-1 2 , 7  2 ]. The applied control of -3 2 shifts the state back to within the interval [-2, +2]. The same argument holds by symmetry on the negative side. Since it starts at 0, by induction it will stay within [-2, +2] forever. As a consequence, the second moment will stay less than 4 for all time, and all the other moments will be similarly bounded.</p><p>In addition to the Shannon and zero-error senses of reliability, information theory has various reliability functions. Such reliability functions (or error exponents) are traditionally considered an internal matter for channel coding and were viewed as mathematically tractable proxies for the issue of implementation complexity <ref type="bibr" target="#b0">[1]</ref>. Reliability functions study how fast the probability of error goes to zero as the relevant system parameter is increased. Thus, the reliability functions for block-codes are given in terms of the block length, reliability functions for convolutional codes in terms of the constraint length <ref type="bibr" target="#b26">[27]</ref>, and reliability functions for variablelength codes in terms of the expected block length <ref type="bibr" target="#b27">[28]</ref>. With the rise of sparse code constructions and iterative decoding, the prominence of error exponents in channel coding has 13 Traditionally, the community has used block-length for a block code as the fundamental quantity rather than delay. It is easy to see that doing encoding and decoding in blocks of size n corresponds to a delay of between n and 2n on the individual bits being communicated.</p><p>diminished since the computational burden is not superlinear in the block-length.</p><p>For memoryless channels, the presence or absence of feedback does not alter the classical Shannon capacity <ref type="bibr" target="#b0">[1]</ref>. More surprisingly, for symmetric DMCs, the fixed block coding reliability functions also do not change with feedback, at least in the high rate regime <ref type="bibr" target="#b28">[29]</ref>. From a control perspective, this is the first indication that neither Shannon's capacity nor block-coding reliability functions are the perfect fit for control applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Counterexample showing classical concepts are inadequate</head><p>We use erasure channels to construct a counterexample showing the inadequacy of the Shannon classical capacity in characterizing channels for control. While both erasure and AWGN channels are easy to deal with, it turns out that AWGN channels can not be used for a counterexample since they can be treated in the classical LQG framework <ref type="bibr" target="#b14">[15]</ref>. The deeper reason for why AWGN channels do not provide a counterexample is given in Section III-C.4.</p><p>1) Erasure channels: The packet erasure channel models situations where errors can be reliably detected at the receiver. In the model, sometimes the packet being sent does not make it through with probability δ, but otherwise it makes it through correctly. Explicitly:</p><formula xml:id="formula_8">Definition 2.4: The L-bit packet erasure channel is a mem- oryless channel with A = {0, 1} L , B = {0, 1} L ∪ {∅} and p(x|x) = 1 -δ while p(∅|x) = δ.</formula><p>It is well known that the Shannon capacity of the packet erasure channel is (1δ)L bits per channel use regardless of whether the encoder has feedback or not <ref type="bibr" target="#b0">[1]</ref>. Furthermore, because a long string of erasures is always possible, the zeroerror capacity C 0 of this channel is 0. There are also variablelength packet erasure channels where the packet-length is something the encoder can choose. See <ref type="bibr" target="#b29">[30]</ref> for a discussion of such channels.</p><p>To construct a simple counterexample, consider a further abstraction:</p><p>Definition 2.5: The real packet erasure channel has A = B = IR and p(x|x) = 1δ while p(0|x) = δ. This model has also been explored in the context of Kalman filtering with lossy observations <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. It has infinite classical capacity since a single real number can carry arbitrarily many bits within its binary expansion, while the zero-error capacity remains 0.</p><p>2) The inadequacy of Shannon capacity: Consider the problem from example 2.1, except over the real erasure channel instead of the one bit noiseless channel. The goal is for the second moment to be bounded (η = 2) and recall that λ = 3 2 . Let δ = 1 2 so that there is a 50% chance of any real number being erased. Assume the bounded disturbance W t , assume that it is zero-mean and iid with variance σ 2 . By assuming an explicit probability model for the disturbance, the problem is only made easier as compared to the arbitrarilyvarying but bounded model introduced earlier.</p><p>In this case, the optimal control is obvious -set a t = X t as the channel input and use U t = -λB t as the control.  With every successful reception, the system state is reset to the initial condition of zero. For an arbitrary time t, the time since it was last reset is distributed like a geometric-1 2 random variable. Thus the second moment is:</p><formula xml:id="formula_9">E[|X t+1 | 2 ] &gt; t i=0 1 2 ( 1 2 ) i E[( i j=0 ( 3 2 ) j W t-j ) 2 ] = t i=0 1 2 ( 1 2 ) i i j=0 i k=0 ( 3 2 ) j+k E[W t-j W t-k ] = t i=0 ( 1 2 ) i+1 i j=0 ( 9 4 ) j σ 2 = 4σ 2 5 t i=0 ( 9 8 ) i+1 -( 1 2 ) i+1</formula><p>This diverges as t → ∞ since 9 8 &gt; 1. Notice that the root of the problem is that</p><formula xml:id="formula_10">( 3 2 ) 2 ( 1 2 ) &gt; 1.</formula><p>Intuitively, the system is exploding faster than the noisy channel is able to give reliability. This causes the second moment to diverge. In contrast, the first moment</p><formula xml:id="formula_11">E[|X t |] is bounded for all t since ( 3 2 )( 1 2 ) &lt; 1.</formula><p>The adequacy of the channel depends on which moment is required to be bounded. Thus no single-number characterization like classical capacity can give the figure-of-merit needed to evaluate a channel for control applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Non-interactive observation of a closed-loop process</head><p>Consider the system shown in Figure <ref type="figure">3</ref>. In this, there is an additional passive joint source-channel encoder E p watching the closed loop state X t and communicating it to a passive estimator D p through a second independent noisy channel. Both the passive and internal observers have access to the same plant state and we can also require the passive encoder and decoder to be causal -no end-to-end delay is permitted.</p><p>At first glance, it certainly appears that the communication situations are symmetric. If anything, the internal observer is better off since it also has access to the control signals while the passive observer is denied access to them.</p><p>Suppose that the closed-loop process (1) had already been stabilized by the observer and controller system of 2.1, so that the second moment E[X 2 t ] ≤ K for all t. Suppose that the noisy channel facing the passive encoder is the real 1  2 -erasure channel of the previous section. It is interesting to consider how well the passive observer does at estimating this process.</p><p>The optimal encoding rule is clear, set a t = X t . It is certainly feasible to use X t = B t itself as the estimator for the process. This passive observation system clearly achieves E[( X t -X t ) 2 ] ≤ K 2 &lt; K since the probability of non-erasure is 1  2 . The causal decoding rule is able to achieve a finite endto-end squared error distortion over this noisy channel in a causal and memoryless way.</p><p>This example makes it clear that the challenge here is arising from interactivity, not simply being forced to be delay-free. The passive external encoder and decoder do not have to face the unstable nature of the source while the internal observer and controller do. An error made while estimating X t by the passive decoder has no consequence for the next state X t+1 while a similar error by the controller does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ANYTIME CAPACITY AND ITS NECESSITY</head><p>Anytime reliability is introduced and related to classical notions of reliability in <ref type="bibr" target="#b22">[23]</ref>. Here, the focus is on the maximum rate achievable for a given sense of reliability rather than the maximum reliability possible at a given rate. The two are of course related since fundamentally there is an underlying region of feasible rate/reliability pairs.</p><p>Since the open-loop system state has the potential to grow exponentially, the controller's knowledge of the past must become certain at a fast rate in order to prevent a bad decision made in the past from continuing to corrupt the future. When viewed in the context of reliably communicating bits from an encoder to a decoder, this suggests that the estimates of the bits at the decoder must become increasingly reliable with time. The sense of anytime reliability is made precise in Section III-A. Section III-B then establishes the key result of this paper relating the problem of stabilization to the reliable communication of messages in the anytime sense. Finally, some consequences of this connection are studied in Section III-C. Among these consequences is a sequential generalization of the Schalkwijk/Kailath scheme for communication over an AWGN channel that achieves a doubly-exponential convergence to zero of the probability of bit error universally over all delays simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anytime reliability and capacity</head><p>The entire message is not assumed to be known ahead of time. Rather, it is made available gradually as time evolves. For simplicity of notation, let M i be the R bit message that the channel encoder gets at time i. At the channel decoder, no target delay is assumed -i.e. the channel decoder does not necessarily know when the message i will be needed by the Both the encoder E and decoder D are causal maps and the decoder in principle provides updated estimates for all past messages. These estimates must converge to the true message values appropriately rapidly with increasing delay.</p><p>application. A past message may even be needed more than once by the application. Consequently, the anytime decoder produces estimates M i (t) which are the best estimates for message i at time t based on all the channel outputs received so far. If the application is using the past messages with a delay d, the relevant probability of error is P( M t-d</p><formula xml:id="formula_12">1 (t) = M t-d<label>1</label></formula><p>). This corresponds to an uncorrected error anywhere in the distant past (ie on messages M 1 , M 2 , . . . , M t-d ) beyond d channel uses ago.</p><p>Definition 3.1: As illustrated in figure <ref type="figure" target="#fig_2">4</ref>, a rate R communication system over a noisy channel is an encoder E and decoder D pair such that:</p><p>• R-bit message M i enters 14 the encoder at discrete time i • The encoder produces a channel input at integer times based on all information that it has seen so far. For encoders with access to feedback with delay 1 + θ, this also includes the past channel outputs B t-1-θ 1 .</p><p>• The decoder produces updated channel estimates M i (t) for all i ≤ t based on all channel outputs observed till time t. A rate R sequential communication system achieves anytime reliability α if there exists a constant K such that:</p><formula xml:id="formula_13">P( M i 1 (t) = M i 1 ) ≤ K2 -α(t-i)<label>(2)</label></formula><p>holds for every i, t. The probability is taken over the channel noise, the R bit messages M i , and all of the common randomness available in the system. If (2) holds for every possible realization of the messages M , then the system is said to achieve uniform anytime reliability α. 14 In what follows, messages are considered to be composed of bits for simplicity of exposition. The i-th bit arrives at the encoder at time i R and thus M i is composed of the bits S ⌊iR⌋ ⌊(i-1)R⌋+1 .</p><p>Communication systems that achieve anytime reliability are called anytime codes and similarly for uniform anytime codes.</p><p>We could alternatively have bounded the probability of error by 2 -α(d-log 2 K) and interpreted log 2 K as the minimum delay imposed by the communication system. Definition 3.2: The α-anytime capacity C any (α) of a channel is the least upper bound of the rates R (in bits) at which the channel can be used to construct a rate R communication system that achieves uniform anytime reliability α.</p><p>Feedback anytime capacity is used to refer to the anytime capacity when the encoder has access to noiseless feedback of the channel outputs with unit delay.</p><p>The requirement for exponential decay in the probability of error with delay is reminiscent of the block-coding reliability functions E(R) of a channel given in <ref type="bibr" target="#b0">[1]</ref>. There is one crucial difference. With standard error exponents, both the encoder and decoder vary with blocklength or delay n. Here, the encoding is required to be fixed and the decoder in principle has to work at all delays since it must produce updated estimates of the message M i at all times t &gt; i.</p><p>This additional requirement is why it is called "anytime" capacity. The decoding process can be queried for a given bit at any time and the answer is required to be increasingly accurate the longer we wait. The anytime reliability α specifies the exponential rate at which the quality of the answers must improve. The anytime sense of reliable transmission lies between that represented by classical zero-error capacity C 0 (probability of error becomes zero at a large but finite delay) and classical capacity C (probability of error becomes something small at a large but finite delay). It is clear that ∀α, C 0 ≤ C any (α) ≤ C.</p><p>By using a random coding argument over infinite tree codes, it is possible to show the existence of anytime codes without using feedback between the encoder and decoder for all rates less than the Shannon capacity. This shows:</p><formula xml:id="formula_14">C any (E r (R)) ≥ R</formula><p>where E r (R) is Gallager's random coding error exponent calculated in base 2 and R is the rate in bits <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Since feedback plays an essential role in control, it turns out that we are interested in the anytime capacity with feedback. It is interesting to note that in many cases for which the blockcoding error exponents are not increased with feedback, the anytime reliabilities are increased considerably <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Necessity of anytime capacity</head><p>Anytime reliability and capacity are defined in terms of digital messages that must be reliably communicated from point to point. Stability is a notion involving the analog value of the state of a plant in interaction with a controller over a noisy feedback channel. At first glance, these two problems appear to have nothing in common except the noisy channel. Even on that point there is a difference. The observer/encoder O in the control system may have no explicit access to the noisy output of the channel. It can appear to be using the noisy channel without feedback. Despite this, it turns out that the relevant digital communication problem involves access to the noisy channel with noiseless channel feedback coming back to the message encoder.</p><p>Theorem 3.3: For a given noisy channel and η &gt; 0, if there exists an observer O and controller C for the unstable scalar system that achieves E[|X t | η ] &lt; K for all sequences of bounded driving noise |W t | ≤ Ω 2 , then the channel's feedback anytime capacity C any (η log 2 λ) ≥ log 2 λ bits per channel use.</p><p>The proof of this spans the next few sections. Assume that there is an observer/controller pair (O, C) that can η-stabilize an unstable system with a particular λ and are robust to all bounded disturbances of size Ω. The goal is to use the pair to construct a rate R &lt; log 2 λ anytime encoder and decoder for the channel with noiseless feedback, thereby reducing 15  the problem of anytime communication to a problem of stabilization.</p><p>The heart of the construction is illustrated in figure <ref type="figure" target="#fig_3">5</ref>. The "black-box" observer and controller are wrapped around a simulated plant mimicking <ref type="bibr" target="#b0">(1)</ref>. Since the {U t } must be generated by the black-box controller C and the λ is prespecified, the disturbances {W t } must be used to carry the message. So, the encoder must embed the messages {M t } into an appropriate sequence {W t }, taking care to stay within the Ω size limit.</p><p>While both the observer and controller can be simulated at the encoder thanks to the noiseless channel output feedback, at the decoder only the channel outputs are available. Consequently, these channel outputs are connected to a copy of the black-box controller C, thereby giving access to the controls {U t } at the decoder. To extract the messages from these control signals, they are first causally preprocessed through a simulated copy of the unstable plant, except with no disturbance input. All past messages are then estimated from the current state of this simulated plant.</p><p>The key is to think of the simulated plant state as the sum of the states of two different unstable LTI systems. The first, with state denoted X t , is driven entirely by the controls and starts in state 0.</p><formula xml:id="formula_15">X t+1 = λ X t + U t<label>(3)</label></formula><p>X is available at both the decoder and the encoder due to the presence of noiseless feedback. 16 The other, with state denoted Xt , is driven entirely by a simulated driving noise that is generated from the data stream to be communicated.</p><formula xml:id="formula_16">Xt+1 = λ Xt + W t<label>(4)</label></formula><p>The sum X t = ( X t + Xt ) behaves exactly like it was coming from (1) and is fed to the observer which uses it to generate inputs for the noisy channel.</p><p>The fact that the original observer/controller pair stabilized the original system implies that |X t | = | X -(-X t )| is small and hence -X t stays close to Xt . 15 In traditional rate-distortion theory, this "necessity" direction is shown by going through the mutual information characterizations of both the ratedistortion function and the channel capacity function. In the case of stabilization, mutual information is not discriminating enough and so the reduction of anytime reliable communication to stabilization must be done directly. 16 If the controller is randomized, then the randomness is required to be common and shared between the encoder and decoder. The messages are used to generate the {Wt} inputs which are causally combined to generate { Xt} within the encoder. The channel outputs are used to generate control signals at both the encoder and decoder. Since the simulated plant is stable, -X and X are close to each other. The past message bits are estimated from the X at the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Encoding data into the state:</head><p>As long as the bound Ω is satisfied, the encoder is free to choose any disturbance 17 for the simulated plant. The choice will be determined by the data rate R and the specific messages to be sent. Rather than working with general messages M i , consider a bitstream S i with bit i becoming available at time i R . Everything generalizes naturally to non-binary alphabets for the messages, but the notation is cleaner in the binary case with S i = ±1.</p><p>Xt is the part of X t driven only by the {W t }.</p><formula xml:id="formula_17">Xt = λ Xt-1 + W t-1 = t-1 i=0 λ i W t-1-i = λ t-1 t-1 j=0 λ -j W j</formula><p>This looks like the representation of a fractional number in base λ which is then multiplied by λ t-1 . This is exploited in the encoding by choosing the bounded disturbance sequence 17 In <ref type="bibr" target="#b22">[23]</ref>, a similar strategy is followed assuming a specific density for the iid disturbance Wt. In that context, it is important to choose a simulated disturbance sequence that behaves stochastically like Wt. This is accomplished by using common randomness shared between the encoder and decoder to dither the kind of disturbances produced here into ones with the desired density. so that: 18</p><formula xml:id="formula_18">Xt = γλ t ⌊Rt⌋ k=0 (2 + ǫ 1 ) -k S k (5)</formula><p>where S k is the k-th bit 19 of data that the anytime encoder has to send and ⌊Rt⌋ is just the total number of bits that are available by time t. γ, ǫ 1 are constants to be specified.</p><p>To see that ( <ref type="formula">5</ref>) is always possible to achieve by appropriate choice of W , use induction. <ref type="bibr" target="#b4">(5)</ref> clearly holds for t = 0. Now assume that it holds for time t and consider time t + 1:</p><formula xml:id="formula_19">Xt+1 = λ Xt + W t = γλ t+1 ( ⌊Rt⌋ k=0 (2 + ǫ 1 ) -k S k ) + W t So setting W t = γλ t+1 ⌊R(t+1)⌋ k=⌊Rt⌋+1 (2 + ǫ 1 ) -k S k<label>(6)</label></formula><p>gives the desired result. Manipulate ( <ref type="formula" target="#formula_19">6</ref>) to get</p><formula xml:id="formula_20">W t = γλ t+1 (2 + ǫ 1 ) -⌊Rt⌋ ⌊R(t+1)⌋-⌊Rt⌋ j=1 (2 + ǫ 1 ) -j S ⌊Rt⌋+j = γλ (2 + ǫ 1 ) Rt-(⌊Rt⌋) λ -t(1-R log 2 (2+ǫ 1 ) log 2 λ ) ⌊R(t+1)⌋-⌊Rt⌋ j=1 (2 + ǫ 1 ) -j S ⌊Rt⌋+j</formula><p>To keep this bounded, choose</p><formula xml:id="formula_21">ǫ 1 = 2 log 2 λ R -2<label>(7)</label></formula><p>which is strictly positive if R &lt; log 2 λ. Applying that substitution gives</p><formula xml:id="formula_22">|W t | = |γλ(2 + ǫ 1 ) Rt-(⌊Rt⌋) ⌊R(t+1)⌋-⌊Rt⌋ j=1 (2 + ǫ 1 ) -j S ⌊Rt⌋+j | &lt; |γλ(2 + ǫ 1 )| = |γλ 1+ 1 R | So by choosing γ = Ω 2λ 1+ 1 R (8)</formula><p>the simulated disturbance is guaranteed to stay within the specified bounds. 18 For a rough understanding, ignore the ǫ 1 and suppose that the message were encoded in binary. It is intuitive that any good estimate of the Xt state is going to agree with Xt in all the high order bits. Since the system is unstable, all the encoded bits eventually become high-order bits as time goes on. So no bit error could persist for too long and still keep the estimate close to Xt. The ǫ 1 in the encoding is a technical device to make this reasoning hold uniformly for all bit strings, rather than merely "typical" ones. This is important since we are aiming for exponentially small bounds and so cannot neglect rare events. 19 For the next section, it is convenient to have the disturbances balanced around zero and so we choose to represent the bit S i as +1 or -1 rather than the usual 1 or 0.</p><p>2) Extracting data bits from the state estimate: Lemma 3.1: Given a channel with access to noiseless feedback, for any rate R &lt; log 2 λ, it is possible to encode bits into the simulated scalar plant so that the uncontrolled process behaves like (5) by using disturbances given in <ref type="bibr" target="#b5">(6)</ref> and the formulas ( <ref type="formula" target="#formula_21">7</ref>) and <ref type="bibr" target="#b7">(8)</ref>. At the output end of the noisy channel, it is possible to extract estimates S i (t) for the i-th bit sent for which the error event</p><formula xml:id="formula_23">{ω|∃i ≤ j, S i (t) = S i (t)} ⊆ {ω||X t | ≥ λ t-j R γǫ 1 1 + ǫ 1 }<label>(9</label></formula><p>) and thus:</p><formula xml:id="formula_24">P( S j 1 (t) = S j 1 (t)) ≤ P(|X t | ≥ λ t-j R γǫ 1 1 + ǫ 1 )<label>(10)</label></formula><p>Proof: Here ω is used to denote members of the underlying sample space. <ref type="foot" target="#foot_8">20</ref>The decoder has -X t = Xt -X t which is close to X since X t is small. To see how to extract bits from -X t , first consider how to recursively extract those bits from Xt .</p><p>Starting with the first bit, notice that the set of all possible Xt that have S 0 = +1 is separated from the set of all possible Xt that have S 0 = -1 by a gap of</p><formula xml:id="formula_25">γλ t   (1 - ⌊Rt⌋ k=1 (2 + ǫ 1 ) -k ) -(-1 + ⌊Rt⌋ k=1 (2 + ǫ 1 ) -k )   &gt; γλ t 2(1 - ∞ k=1 (2 + ǫ 1 ) -k ) = γλ t 2(1 - 1 1 + ǫ 1 ) = λ t 2ǫ 1 γ 1 + ǫ 1</formula><p>Fig. <ref type="figure">6</ref>. The data bits are used to sequentially refine a point on a Cantor set. Its natural tree structure allows bits to be encoded sequentially. The Cantor set also has finite gaps between all points corresponding to bit sequences that first differ in a particular bit position. These gaps allow the uniformly reliable extraction of bit values from noisy observations.</p><p>Notice that this worst-case gap <ref type="foot" target="#foot_9">21</ref> is a positive number that is growing exponentially in t. If the first i -1 bits are the same, then both sides can be scaled by (2+ǫ 1 ) i = λ i R to get the same expressions above and so by induction, it quickly follows that the minimum gap between the encoded state corresponding to two sequences of bits that first differ in bit position i is given by gap</p><formula xml:id="formula_26">i (t) = inf S: Si =Si | Xt (S) -Xt ( S)| &gt; λ t-i R 2γǫ1 1+ǫ1 if i ≤ ⌊Rt⌋ 0 otherwise<label>(11)</label></formula><p>Because the gaps are all positive, <ref type="bibr" target="#b10">(11)</ref> shows that it is always possible to perfectly extract the data bits from Xt by using an iterative procedure. <ref type="foot" target="#foot_10">22</ref> To extract bit information from an input I t : 1) Initialize threshold T 0 = 0 and counter i = 0.</p><p>2) Compare input</p><formula xml:id="formula_27">I t to T i . If I t ≥ T i , set S i (t) = +1. If I t &lt; T i , set S i (t) = -1. 3) Increment counter i and update threshold T i = γλ t i-1 k=0 (2 + ǫ 1 ) -k S k 4)</formula><p>Goto step 2 as long as i ≤ ⌊Rt⌋ Since the gaps given by ( <ref type="formula" target="#formula_26">11</ref>) are always positive, the procedure works perfectly if applied to input I t = Xt . At the decoder, apply the procedure to</p><formula xml:id="formula_28">I t = -X t instead.</formula><p>With this, ( <ref type="formula" target="#formula_23">9</ref>) is easy to verify by looking at the complemen-</p><formula xml:id="formula_29">tary event {ω||X t | &lt; λ t-j R γǫ1</formula><p>1+ǫ1 }. The bound <ref type="bibr" target="#b10">(11)</ref> thus implies that we are less than halfway across the minimum gap for bit j at time t. Consequently, there is no error in the step 2 comparison of the procedure at iterations i ≤ j.</p><p>3) Probability of error for bounded moment and other senses of stability: Proof of Theorem 3.3: Using Markov's inequality:</p><formula xml:id="formula_30">P(|X t | &gt; m) = P(|X t | η &gt; m η ) ≤ E[|X t | η ]m -η &lt; Km -η</formula><p>Combining with Lemma 3.1, gives:</p><formula xml:id="formula_31">P( S i 1 (t) = S i 1 (t)) ≤ P(|X t | ≥ λ t-i R γǫ 1 1 + ǫ 1 ) &lt; K( 1 γ + 1 γǫ 1 ) η λ -η(t-i R ) = (K( 1 γ + 1 γǫ 1 ) η )2 -(η log 2 λ)(t-i R )</formula><p>Since t -i R represents the delay between the time that bit i was ready to be sent and the decoding time, the theorem is proved.</p><p>All that was needed from the bounded moment sense of stability was some bound on the probability that X t took on large values. Thus, the proof above immediately generalizes to other senses of stochastic stability if we suitably generalize the sense of anytime capacity to allow for other bounds on the probability of error with delay.</p><p>Definition 3.4: A rate R communication system achieves g-anytime reliability given by a function g(d) if</p><formula xml:id="formula_32">P( M t-d 1 (t) = M t-d 1 (t)) &lt; g(d)</formula><p>g(d) is assumed to be 1 for all negative values of d. The g-anytime capacity C g-any (g) of a noisy channel is the least upper bound of the rates R at which the channel can be used to construct a sequential communication system that achieves g-anytime reliability given by the function g(d).</p><p>Notice that for α-anytime capacity, g(d) = K2 -αd for some K.</p><p>Theorem 3.5: For a given noisy channel and decreasing function f (m), if there exists an observer O and controller C for the unstable scalar system that achieves P(|X t | &gt; m) &lt; f (m) for all sequences of bounded driving noise |W t | ≤ Ω 2 , then C g-any (g) ≥ log 2 λ for the noisy channel considered with the encoder having access to noiseless feedback and g(d) having the form g(d) = f (Kλ d ) for some constant K. Proof: For any rate R &lt; log 2 λ,</p><formula xml:id="formula_33">P( S i 1 (t) = S i 1 (t)) ≤ P(|X t | ≥ λ t-i R γǫ 1 1 + ǫ 1 ) = f ( γǫ 1 1 + ǫ 1 λ t-i R )</formula><p>Since the delay d = t -i R , the theorem is proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implications</head><p>At this point, it is interesting to consider a few implications of Theorem 3.5.</p><p>1) Weaker senses of stability than η-moment: There are senses of stability weaker than specifying a specific η-th moment or a specific tail decay target f (m). An example is given by the requirement lim m→∞ P(|X t | &gt; m) = 0 uniformly for all t. This can be explored by taking the limit of C any (α) as α ↓ 0. We have shown elsewhere <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b22">[23]</ref> that:</p><formula xml:id="formula_34">lim α↓0 C any (α) = C</formula><p>where C is the Shannon classical capacity. This holds for all discrete memoryless channels since the α-anytime reliability goes to zero at Shannon capacity but is &gt; 0 for all lower rates even without feedback being available at the encoder. Thus, classical Shannon capacity is the natural candidate for the relevant figure of merit.</p><p>To see why Shannon capacity can not be beaten, it is useful to consider an even more lax sense of stability. Suppose the requirement were only that lim m→∞ P(|X t | &gt; m) = 10 -5 &gt; 0 uniformly for all t. This imposes the constraint that the probability of a large state stays below 10 -5 for all time. Theorem 3.5 would thus only requires the probability of decoding error to be less than 10 -5 . However, Wolfowitz' strong converse to the coding theorem <ref type="bibr" target="#b0">[1]</ref> implies that since the block-length in this case is effectively going to infinity, the Shannon capacity of the noisy channel still must satisfy C ≥ log 2 λ. Adding a finite tolerance for unboundedly large states does not get around the need to be able to communicate log 2 λ bits reliably.</p><p>2) Stronger senses of stability than η-moment: Having f decrease only as a power law might not be suitable for certain applications. Unfortunately, this is all that can be hoped for in generic situations. Consider a DMC with no zero entries in its transition matrix. Define ρ = min i,j p(i, j). For such a channel, with or without feedback, the probability of error after d time steps is lower bounded by ρ d since that lower bounds the probability of all channel output sequences of length d. This implies that the probability of error can drop no more than exponentially in d for such DMCs. Tighter upper-bounds on anytime reliability with feedback are available in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b5">[6]</ref>.</p><p>Theorem 3.5 therefore implies that the only f -senses of stability which are possible over such channels are those for which:</p><formula xml:id="formula_35">f (Kλ d ) ≥ ρ d f (m) ≥ ρ log 2 ( m K ) log 2 λ f (m) ≥ K ′ m - log 2 1 ρ log 2 λ</formula><p>which is a power law. This rules out the "risk sensitive" sense of stability in which f is required to decrease exponentially. In the context of Theorem 3.3, this also implies that there is an η beyond which all moments must be infinite! Corollary 3.1: If any unstable process is controlled over a discrete memoryless channel with no feedback zero-error capacity, then the resulting state can have at best a power-law bound (Pareto distribution) on its tail. This is very much related to how sequential decoding must have computational effort distributions with at best a Pareto distribution <ref type="bibr" target="#b34">[35]</ref>. In both cases, the result follows from the interaction of two exponentials. The difference is that the computational search effort distributions assumed a particular structure on the decoding algorithm while the bound here is fundamental to the stabilization problem regardless of the observers or controllers.</p><p>Thus for DMCs and a given λ, we are either limited to a power-law tail for the controlled state because of an anytime reliability that is at most singly exponential in delay or it is possible to hold the state inside a finite box since there is adequate feedback zero-error capacity. Nothing in between can happen with a DMC.</p><p>3) Limiting the controller effort or memory: If there was a hard limit on actuator effort (|U | ≤ U for some U &gt; 0), then the only way to maintain stability is to also have a hard limit on how big the state X can get. Theorem 3.5 immediately gives a fundamental requirement for feedback zero-error capacity ≥ log 2 λ since g(d) = 0 for sufficiently large d.</p><p>Similarly, consider limited-memory time-invariant controllers which only have access to the past k channel outputs. If the channel has a finite output alphabet and no randomization is permitted at the controller, limited memory immediately translates into only a finite number of possible control inputs. Since there must be a largest one, it reduces to the case of having a hard limit on actuator effort.</p><p>We conjecture that even with randomization and timevariation, finite memory at the controller implies that the channel must have feedback zero-error capacity ≥ log 2 λ. Intuitively, if the channel has zero-error capacity &lt; log 2 λ, it can misbehave for arbitrarily long times and build up a huge "backlog" of uncertainty that can not be resolved at the controller. With finite memory, the controller has no way of knowing what uncertainty it is actually facing and so is unable to properly interpret the channel outputs to devise the proper control signals.</p><p>4) The AWGN case with an average input power constraint: The tight relationship between control and communication established in Theorem 3.5 allows the construction of sequential codes for noisy channels with noiseless feedback if we know how to stabilize linear plants over such channels. Consider the problem of stabilizing an unstable plant driven by finite variance driving noise over an AWGN channel. A linear observer and controller strategy achieve mean-square stability for such systems since the problem fits into the standard LQG framework <ref type="bibr" target="#b13">[14]</ref>.</p><p>By looking more closely at the actual tail probabilities achieved by the linear observer/controller strategy, we obtain a natural anytime generalization of Schalkwijk and Kailath's scheme <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> for communicating over the power constrained additive white Gaussian noise channel with noiseless feedback. Its properties are summarized in Figure <ref type="figure">7</ref>, but the highlight is that it achieves doubly exponential reliability with delay, universally over all sufficiently long delays.</p><p>Theorem 3.6: It is possible to communicate bits reliably across a discrete-time average-power constrained AWGN channel with noiseless feedback at any rate R &lt; 1 2 log 2 (1+ P σ 2 ) while achieving a g-anytime reliability of at least</p><formula xml:id="formula_36">g(d) = 2e -K(4 Rd -O(2 Rd ))<label>(12)</label></formula><p>for some constant K that depends only on the rate R, power constraint P , and channel noise power σ 2 .</p><p>Proof: To avoid having to drag σ 2 around, just normalize units so as to consider power constraint P ′ = P σ 2 and a channel with iid unit variance noise N t . Choose the λ for the simulated (1) so that R &lt; log 2 λ &lt; 1 2 log 2 (1 + P ′ ). The observer/encoder used is a linear map:</p><formula xml:id="formula_37">a t = βX t (<label>13</label></formula><formula xml:id="formula_38">)</formula><p>so the channel output B t = βX t + N t . Use a linear controller:</p><formula xml:id="formula_39">U t = -λφB t (<label>14</label></formula><formula xml:id="formula_40">)</formula><p>giving the closed-loop system:</p><formula xml:id="formula_41">X t+1 = λ(1 -βφ)X t + W t -λφN t<label>(15)</label></formula><p>where the β, φ are constants to be chosen. For the closed-loop system to be stable:</p><formula xml:id="formula_42">0 &lt; λ(1 -βφ) &lt; 1<label>(16)</label></formula><p>Thus βφ ∈ (1 -1 λ , 1). Assuming ( <ref type="formula" target="#formula_42">16</ref>) holds and temporarily setting the W t = 0 for analysis, it is clear that the closedloop X t is Gaussian with a growing variance asymptotically tending to</p><formula xml:id="formula_43">σ 2 x = λ 2 φ 2 1 -λ 2 (1 -βφ) 2<label>(17)</label></formula><p>The channel input power satisfies:</p><formula xml:id="formula_44">E[a 2 t ] ≤ λ 2 (βφ) 2 1 -λ 2 (1 -βφ) 2</formula><p>Since λ 2 &lt; 1 + P ′ , define P ′′ = λ 2 -1 &lt; P ′ and substitute to get:</p><formula xml:id="formula_45">E[a 2 t ] ≤ (P ′′ + 1)(βφ) 2 1 -(P ′′ + 1)(1 -βφ) 2<label>(18)</label></formula><p>By setting βφ = P ′′ P ′′ +1 , the left hand side of ( <ref type="formula" target="#formula_45">18</ref>) is identically P ′′ as desired. All that remains is to verify the stability condition <ref type="bibr" target="#b15">(16)</ref>:</p><formula xml:id="formula_46">λ(1 -βφ) = λ P ′′ + 1 = √ P ′′ + 1 P ′′ + 1 = 1 √ P ′′ + 1 &lt; 1</formula><p>So the closed loop system is stable and the channel noise alone results in an average input power of at most P ′′ &lt; P ′ .</p><p>Rather than optimizing the choice of β and φ to get the best tradeoff point, just set β = 1 and φ = P ′′ P ′′ +1 for simplicity. In that case, σ 2</p><p>x = P ′′ . Now consider the impact of the W t alone on the closedloop control system. These are going through a stable system and so by expanding the recursion <ref type="bibr" target="#b14">(15)</ref> and setting N t = 0,</p><formula xml:id="formula_47">|X w t | ≤ ∞ i=0 (λ(1 -βφ)) i Ω 2 = ∞ i=0 1 √ P ′′ + 1 i Ω 2 = Ω 2(1 - 1 √ P ′′ +1 ) = Ω √ P ′′ + 1 2( √ P ′′ + 1 -1)</formula><p>which is a constant that can be made as small as desired by choice of Ω. Assume that the data stream S to be transmitted is independent of the channel noise N . Then, the total average input power is bounded by:</p><formula xml:id="formula_48">σ 2 x + β 2 |X w t | 2 ≤ P ′′ + ( Ω √ P ′′ + 1 2( √ P ′′ + 1 -1) ) 2 ≤ P ′′ + Ω 2 P ′′ + 1 4(P ′′ + 2(1 - √ P ′′ + 1))</formula><p>Since P ′′ &lt; P ′ , we can choose an Ω small enough so that the channel input satisfies the average power constraint regardless of the message bits to be sent. All that remains is to see what f (m) this control system meets for such arbitrary, but bounded, disturbances. X t is asymptotically the sum of a Gaussian with zero mean and variance P ′′ together with the closed-loop impact of the disturbance X w (t). Since the total impact of the disturbance part is bounded:</p><formula xml:id="formula_49">P(|X t | &gt; m) ≤ P(|N σ 2 x | &gt; m - Ω √ P ′′ + 1 2( √ P ′′ + 1 -1) ) = P(|N | &gt; 1 √ P ′′ (m - Ω √ P ′′ + 1 2( √ P ′′ + 1 -1) )) ≤ 2e -1 2P ′′ (m- Ω √ P ′′ +1 2( √ P ′′ +1-1)</formula><p>) 2   Ignoring the details of the constants, this gives an ) for all α &gt; 0 on the AWGN channel. If the additive channel noise were not Gaussian, but had bounded support with the same variance, then this proof immediately reveals that the zero-error capacity of such a bounded noise channel with feedback satisfies: C 0 ≥ 1 2 log 2 (1 + P σ 2 ). In the Gaussian case, it is not immediately clear whether there are ideas analogous to those in <ref type="bibr" target="#b37">[38]</ref> that can be used to further boost the g-anytime reliability beyond double exponential. It is clear that if it were possible, it would require nonlinear control strategies.</p><formula xml:id="formula_50">f (m) = 2e -K1(m-K2) 2 = 2e -K1(m 2 -2K2m-K3)</formula><p>The AWGN case is merely one example. Theorem 3.5 gives a way to lower-bound the anytime capacity for channels with feedback in cases where the optimal control behavior is easy to see. The finite moments of the closed-loop state reveal what anytime reliability is being achieved. Often, there is a simple upper-bound that matches up with the lowerbound thereby giving the anytime capacity itself. The BEC case discussed in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b5">[6]</ref> is such an example. In addition, Theorem 3.5 gives us the ability to mix and match communication and control tools to study a problem. This is exploited in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref> to understand the feedback anytime capacity of constrained packet erasure channels and the power constrained AWGN+erasure channel. In <ref type="bibr" target="#b39">[40]</ref>, these results are extended to the Gilbert-Eliot channel with feedback. It is also exploited in <ref type="bibr" target="#b33">[34]</ref> to lower bound the anytime reliability achieved by a particular code for the BSC with feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE SUFFICIENCY OF ANYTIME CAPACITY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>When characterizing a noisy channel for control, the choice of information pattern <ref type="bibr" target="#b40">[41]</ref> can be critical <ref type="bibr" target="#b13">[14]</ref>. The sufficiency result is first established for cases with an explicit noiseless feedback path from the channel outputs back to the observer. Section IV-E takes a quick look at the simpler problem of almost-sure stabilization when the system is undisturbed and all the uncertainty comes from either the channel or the initial condition. Then, in Section IV-F, the impact of viewing time in blocks of size n and only acting on the slower time-scale is examined. Finally, Sections IV-G and IV-H give models for boundedly noisy or quantized controls and/or observations and show that such bounded noise can be tolerated.</p><p>To prove the sufficiency theorem addressing the situation illustrated in figure <ref type="figure" target="#fig_0">2</ref>, we need to design an observer/controller pair that deals with the analog plant and communicates across the channel by using an anytime communication system. The anytime communication system works with noiseless feedback from the channel output available at the bit encoder and is considered a "black box."</p><p>Theorem 4.1: For a given noisy channel, if there exists an anytime encoder/decoder pair with access to noiseless feedback that achieves C g-any (g) ≥ log 2 λ, then it is possible to stabilize an unstable scalar plant with parameter λ that is driven by bounded driving noise through the noisy channel by using an observer that has noiseless access to the noisy channel outputs. Furthermore, there exists a constant K so that P(|X t | &gt; m) ≤ g(K + log λ m).</p><p>To prove this theorem, explicit constructions are given for the observer and controller in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Observer</head><p>Since the observer has access to the channel outputs, it can run a copy of the controller and hence has access to the control signals U t . Since W t = X t+1 -λX t -U t , and the observer receives X t from the plant, the observer also effectively has access to the W t . However, it is not sufficient to merely encode the W t independently to some precision. 23  Instead, the observer will act as though it is working with a virtual controller through a noiseless channel of finite rate R in the manner of example 2.1. The resulting bits will be sent through the anytime code.</p><p>The observer is constructed to keep the state uncertainty at the virtual controller inside a box of size ∆ by using bits at the rate R. It does this by simulating a virtual process Xt governed by:</p><formula xml:id="formula_51">Xt+1 = λ Xt + W t + Ūt (<label>19</label></formula><formula xml:id="formula_52">)</formula><p>where the Ūt represent the computed actions of the virtual controller. This gives rise to a virtual counterpart of X t</p><formula xml:id="formula_53">X Ū t+1 = λX Ū t + Ūt (<label>20</label></formula><formula xml:id="formula_54">)</formula><p>23 This is because the unstable plant will eventually blow up even tiny uncorrected discrepancies between the encoded and actual Wt. which satisfies the relationship Xt = Xt +X Ū t . Because Xt will be kept within a box, it is known that -X Ū t is close to Xt . The actual controller will pick controls designed to keep X t close to X Ū t . Because of the rate constraint, the virtual control Ūt takes on one of 2 ⌊R(t+1)⌋-⌊Rt⌋ values. For simplicity of exposition, we ignore the integer effects and consider it to be one of 2 R values <ref type="foot" target="#foot_11">24</ref>  </p><formula xml:id="formula_55">λ 2 R ∆ + Ω ≤ ∆<label>(21)</label></formula><p>To get the minimum ∆ required as a function of R, we can solve for (21) being an equality. This occurs <ref type="foot" target="#foot_12">25</ref> when ∆ = Ω 1-λ2 -R for every case where R &gt; log 2 λ. Since the slope λ 2 R on the left hand side of ( <ref type="formula" target="#formula_55">21</ref>) is less than 1, any larger ∆ also works.</p><p>Since they arose from dividing the uncertainty window to 2 R disjoint segments, it is clear that the virtual controls Ūt can be encoded causally using R bits per unit time. These bits are sent to the anytime encoder for transport over the noisy channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Controller</head><p>The controller uses the updated bit estimates from the anytime decoder to choose a control to attempt to make the true state X t stay close to the virtual state Xt . It does this by having a pair of internal models as shown in figure <ref type="figure">9</ref>.</p><p>The first, X t from (3), models the unstable system driven only by the actual controls. based on the current bit estimates from the anytime decoder, of where the unstable system should be driven only by the virtual controls Ūt . Of course, the controller does not have the exact virtual controls, only its best estimates U t 1 (t) for them.</p><formula xml:id="formula_56">X t+1 (t) = t i=0 λ i U t-i (t)<label>(22)</label></formula><p>This is not given in recursive form since all of the past estimates for the virtual controls are subject to re-estimation at the current time t. The control U t is chosen to make X t+1 = X t+1 (t).</p><formula xml:id="formula_57">U t = X t+1 (t) -λ X t<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluating stability</head><p>Proof of Theorem 4.1: With controls given by ( <ref type="formula" target="#formula_57">23</ref>), the true state X t can be written as:</p><formula xml:id="formula_58">X t = Xt + X t = Xt + X t (t -1) = t-1 i=0 λ i (W t-i + U t-i (t -1))</formula><p>Notice that the actual state X t differs from the virtual state Xt only due to errors in virtual control estimation due to channel noise. If there were no errors in the prefix U t-d 1 and arbitrarily bad errors for U t t-d+1 , then we could start at Xt-d and see how much the errors could have propagated since then:</p><formula xml:id="formula_59">X t = λ d Xt-d + d-1 i=0 λ i (W t-i + U t-i (t -1))</formula><p>Comparing this with Xt , and noticing that the maximum possible difference between two virtual controls is λ∆ gives:</p><formula xml:id="formula_60">|X t -Xt | = | d-1 i=0 λ i ( Ūt-i -U t-i (t -1))| ≤ d-1 i=0 λ i | Ūt-i -U t-i (t -1)| ≤ d-1 i=0 λ i+1 ∆ &lt; ∆λ d ∞ i=0 λ -i = λ d ∆ 1 -λ -1 Since | Xt | ≤ ∆ 2</formula><p>, if we know that there were no errors in the prefix of estimated virtual controls until d time steps ago, then</p><formula xml:id="formula_61">{ U t-d 0 (t -1) = Ū t-d 0 } ⇒ {|X t | &lt; λ d 2∆ 1 -λ -1 }<label>(24)</label></formula><p>(24) immediately gives:</p><formula xml:id="formula_62">P(|X t | ≥ m) = P(|X t | ≥ λ log 2 m log 2 λ λ log 2 (1-λ -1 )-log 2 (2∆) log 2 λ 2∆ 1 -λ -1 ) ≤ P(|X t | ≥ λ log 2 m+log 2 (1-λ -1 )-log 2 (2∆) log 2 λ 2∆ 1 -λ -1 ) ≤ g( log 2 m + log 2 (1 -λ -1 ) -log 2 (2∆) log 2 λ ) ≤ g(K ′′ + log 2 m log 2 λ )</formula><p>where g bounds the probability of error for the g-anytime code and K ′′ is some constant.</p><p>Specializing to the case of α-anytime capacity, it is clear that:</p><formula xml:id="formula_63">P(|X t | ≥ m) ≤ K ′′′ 2 -α log 2 m log 2 λ = K ′′′ m -α log 2 λ</formula><p>which gives a power-law bound on the tail. If the goal is a finite η-th moment,</p><formula xml:id="formula_64">E[|X t | η ] = ∞ 0 P(|X t | η ≥ m)dm = ∞ 0 P(|X t | ≥ m 1 η )dm ≤ 1 + K ′′′ ∞ 1 m - α η log 2 λ dm</formula><p>As long as α &gt; η log 2 λ, the integral above converges and hence the controlled process has a bounded η-moment.</p><p>Theorem 4.2: It is possible to control an unstable scalar process driven by a bounded disturbance over a noisy channel so that the η-moment of |X t | stays finite for all time if the channel has feedback anytime capacity C any (α) &gt; log 2 λ for some α &gt; η log 2 λ and the observer is allowed to observe the noisy channel outputs and the state exactly.</p><p>Aside from the usual gap between &gt; and ≥, this shows that the necessity condition in Theorem 3.3 is tight. Since there are no assumptions on the disturbance process except for its boundedness, the sufficiency theorems here automatically cover the case of stochastic disturbances having any sort of memory structure as long as they remain bounded in support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Almost-sure stability</head><p>Control theorists are sometimes interested in an even simpler problem for which there is no disturbance (i.e. W t = 0 for all t) but the initial condition X 0 is unknown to within some bound Ω. For this problem, the goal is ensuring that the state X t tends to zero almost surely. This short section constructively shows that any sufficiency result for η-stability also extends to almost-sure stabilization. To do this, we consider the system:</p><formula xml:id="formula_65">X ′ t+1 = λ ′ X ′ t + U ′ t + W ′ t (<label>25</label></formula><formula xml:id="formula_66">)</formula><p>and use it to prove a key lemma: Lemma 4.1: If it is possible to η ′ -stabilize a persistently disturbed system from (25) when driven by any driving noise W ′ bounded by Ω, then there exists a time-varying observer with noiseless access to the state and a time-varying controller so that any undisturbed system (1) with initial condition |X 0 | ≤ Ω 2 , W t = 0, and 0 &lt; λ &lt; λ ′ can be stabilized in the sense that there exists a K so that:</p><formula xml:id="formula_67">E[|X t | η ′ ] ≤ K( λ λ ′ ) η ′ t<label>(26</label></formula><p>) Proof: Since W t = 0 for t &gt; 0, it is immediately clear that the system of (25) can be related to the original system of (1) by the following scaling relationships:</p><formula xml:id="formula_68">W ′ 0 = X 0 W ′ t = 0 if t &gt; 0 X ′ 0 = 0 X ′ t = ( λ ′ λ ) t-1 X t-1 if t &gt; 0 U ′ t = ( λ ′ λ ) t-1 U t-1</formula><p>It is possible to use an observer/controller design for the system of (25) to construct one for the original system (1) through the same mapping. The input to the observer constructed with X ′ in mind will just be ( λ ′ λ ) t X t and the controls U ′ just need to be scaled down by a factor ( λ λ ′ ) t so that they will properly apply to the X t system. Since (25) can be η ′ -stabilized, there exists a K ′ so that for all t ≥ 0,</p><formula xml:id="formula_69">K ′ ≥ E[|X ′ t | η ′ ] = E[( λ ′ λ ) (t-1)η ′ |X t-1 | η ′ ] = ( λ ′ λ ) η ′ (t-1) E[|X t-1 | η ′ ]</formula><p>which immediately yields <ref type="bibr" target="#b25">(26)</ref>.</p><p>Lemma 4.1 can be used to get almost-sure stability by noticing that:</p><formula xml:id="formula_70">E[ ∞ t=0 |X t | η ′ ] = ∞ t=0 E[|X t | η ′ ] ≤ ∞ t=0 K( λ λ ′ ) η ′ t ≤ K 1 -( λ λ ′ ) η ′ which is bounded. It immediately follows that: lim t→∞ |X t | η ′ = 0 almost surely lim t→∞ X t = 0 almost surely</formula><p>which is summarized in the following theorem:</p><p>Theorem 4.3: If it is possible to η ′ -stabilize a persistently disturbed system from (25) when driven by any driving noise W ′ bounded by Ω, then there exists a time-varying observer with noiseless access to the state and a time-varying controller so that any undisturbed system (1) with initial condition |X 0 | ≤ Ω 2 , W t = 0, and 0 &lt; λ &lt; λ ′ can be stabilized in the almost-sure<ref type="foot" target="#foot_13">26</ref> sense:</p><formula xml:id="formula_71">lim t→∞ X t = 0 almost surely</formula><p>The important thing to notice about Lemma 4.1 and Theorem 4.3 is that they do not depend on the detailed structure of the original problem except for the need to observe the state perfectly at the encoder and to be able to apply controls with perfect precision. It is clear that if either the state observation or the control application was limited in precision, then there would be no way to drive the state to zero almost surely.</p><p>Theorem 4.3 is used in Section V to get Corollary 5.3 which says that for almost-sure stabilization of an undisturbed plant across a discrete memoryless channel (DMC), Shannon capacity larger than log 2 λ suffices regardless of the information pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Time in blocks and delayed observations</head><p>In the discussion so far, time has operated at the same scale for channel uses, system dynamics, plant observations, and control application. Furthermore, the only structural delay in the system was the one-step-delay across the noisy channel needed to allow the interconnection of the controller, observer, channel, and plant to make sense. It is interesting to consider different parts of the system operating at slightly different time scales and to see the impact of fixed and known delays in the system.</p><p>1) Observing and controlling the plant on a slower time scale: In the control context, it is natural to consider cases where the plant evolves on a slower time scale than communication. Formally, suppose that time is grouped into blocks of size n and the observer is restricted to only encode the value of X t at times that are integer multiples of n. Similarly, suppose that the controller only takes an action 27 immediately before the observer will sample the state. The effective system dynamics change to</p><formula xml:id="formula_72">X n(k+1) = λ n X nk + U n(k+1)-1 + W ′ k (27)</formula><p>where</p><formula xml:id="formula_73">W ′ k = n-1 j=0 λ n-1-j W nk+j .</formula><p>Observe that |W ′ k | is known to be bounded within an interval of size Ω ′ &lt; λ n Ω λ-1 . Essentially, everything has just scaled up by a factor of λ n . Thus all the results above continue to hold above for a system described by <ref type="bibr" target="#b26">(27)</ref> at times which are integer multiples of n. The rate must be larger than log 2 λ n = n log 2 λ bits per n time steps which translates to log 2 λ bits per time step. The anytime reliability α &gt; η log 2 λ n = n(η log 2 λ) for delay measured in units of n time-steps translates into α &gt; η log 2 λ for delay measured in unit time steps. This is the same as it was for the system described by <ref type="bibr" target="#b0">(1)</ref>.</p><p>The only remaining question is what happens to the state at times within the blocks since no controls are being applied while the state continues to grow on its own. At such times, the state has just grown by a factor of at most λ n with an additive term of at most</p><formula xml:id="formula_74">λ n Ω λ-1 . E[(λ n (X nk + Ω λ -1 )) η ] = λ ηn E[(X nk + Ω λ -1 ) η ] ≤ λ ηn E[ 2 max(|X nk |, Ω λ -1 ) η ] = λ ηn 2 η ∞ 0 P(max(|X nk | η , ( Ω λ -1 ) η ) ≥ τ )dτ = λ ηn 2 η ( Ω λ -1 ) η + ∞ ( Ω λ-1 ) η P(|X nk | η ≥ τ )dτ &lt; λ ηn 2 η ( Ω λ -1 ) η + ∞ 0 P(|X nk | η ≥ τ )dτ = λ ηn 2 η ( Ω λ -1 ) η + E[|X nk | η ]</formula><p>which is finite since the original is finite. Thus: Theorem 4.4: If for all Ω &gt; 0, it is possible to stabilize a particular unstable scalar system with gain λ n and arbitrary disturbance signal bounded by Ω when we are allowed n uses of a particular channel between when the control-system evolves, then for any Ω &gt; 0 it is also possible to stabilize an unstable scalar system with gain λ that evolves on the same time scale as the channel using an observer restricted to only observe the system every n time steps. By simple application of Theorem 4.4, it is known that Theorem 4.2 and similarly Theorem 3.3 continue to hold even if the observers/controllers only get access to the analog system at timesteps that are integer multiples of some n. This is used when considering noisy observations in Section IV-H and in the context of vector-valued states in Part II. 27 The controller can take "no action" by setting Ut = 0.</p><p>2) Known fixed delays: Similarly, we can study cases where the assumed "round trip delay" is larger than one. Suppose the control signal applied at time t depends only on channel outputs up to time tv for some v &gt; 0.</p><p>It is easy to see that while this sort of deterministic delay does degrade performance, it does not change stability. The proof of Theorem 4.1 goes through as before. Specifically, in Section IV-C, <ref type="bibr" target="#b21">(22)</ref> will change to:</p><formula xml:id="formula_75">X t+1 (t) = t i=0 λ i U t-i (t -v)<label>(28)</label></formula><p>Everything else proceeds as before, just that in place of d for the probability of error we will have d + v. Specifically, in place of <ref type="bibr" target="#b23">(24)</ref>, we now know only that:</p><formula xml:id="formula_76">|X t | &lt; λ d+v 2∆ 1 -λ -1 = λ d 2∆λ v 1 -λ -1<label>(29)</label></formula><p>This is just a change in the constant factor and results in a smaller (more negative) constant K to deal with the larger uncertainty. This change of constant does not make a bounded η-moment become unbounded. The result is summarized in the following theorem: Theorem 4.5: Theorems 4.1 and 4.2, continue to hold if the control signal U t is required to depend only on the channel outputs up through time t-v where v ≥ 0. Only the constants grow larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Noisy or quantized controls</head><p>The control signals U t may not be able to be set by the controller to infinite precision. The applied control U t at the plant might be different from the intended control U i t generated at the controller. This section considers the case of Γ c -precise controls where the difference is bounded so |U t -U i t | ≤ Γc 2 for some constant Γ c to reflect the noise at the controller. It is easy to see that the plant dynamics now effectively change from 1 to:</p><formula xml:id="formula_77">X t+1 = λX t + U i t + W t + (U t -U i t )</formula><p>where the term W t + (U t -U i t ) can be considered the new bounded disturbance for the system. So in place of Ω, we simply use the new bound Ω + Γ c . Thus, all the previous results continue to hold in the case of boundedly noisy control signals.</p><p>Theorem 4.6: If for all Ω &gt; 0, it is possible to stabilize a particular unstable scalar system with arbitrary disturbance signal bounded by Ω given the ability to apply precise control signals, then for all Γ c &gt; 0 and Ω &gt; 0, it remains possible to stabilize the same unstable scalar system with arbitrary disturbance signal bounded by Ω given the ability to apply only Γ c -precise control signals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Noisy or quantized observations</head><p>The observer of Section IV-B has exact knowledge of the state X t . Suppose that the observation is instead X noisy (t) = X t + N t where N t is known to be within a bound ( -Γ 2 , +Γ 2 ). For example, this models situations where the input to the encoder has already been quantized to some resolution. <ref type="foot" target="#foot_14">28</ref>The observer needs to ensure that the virtual state X is within an interval of size ∆. To do this, just choose a large enough ∆ &gt; 2Γ so that X noisy (t) and X t both pick out the same interval for the state. As figure 10 illustrates, this is not quite enough since the intervals used in Section IV-B are partitions of the real line. Meanwhile, each observation of X noisy (t) gives rise to an uncertainty window for X t ∈ (X noisy (t)-Γ 2 , X noisy (t)+ Γ 2 ) that might straddle a boundary of the partition. <ref type="foot" target="#foot_15">29</ref> Doubling the number of intervals and having them overlap by half ensures that the uncertainty window can always fit inside a single interval. Such a doubling increases the data rate by at most an additional bit. To amortize this additional bit, Theorem 4.4 from Section IV-F is used and time is considered in blocks of size n. Then, the required rate for achievability with blocked time is R &gt; 1 + log 2 λ n bits per n time-steps or R &gt; 1 n + log 2 λ bits per time step. Since n can be large enough, R &gt; log 2 λ is good enough. Delayed control actions also causes no new concerns. Thus, we get the following corollary to Theorems 4.2 and 4.5:</p><p>Corollary 4.1: It is possible to control an unstable scalar process driven by a bounded disturbance over a noisy channel so that the η-moment of |X t | stays finite for all time if the channel has feedback anytime capacity C any (α) &gt; log 2 λ for some α &gt; η log 2 λ and the observer is allowed to observe the noisy channel outputs exactly and has a boundedly noisy view of the state. This is true even if the control U t is only allowed to depend on channel outputs up through time tv where v ≥ 0. Control over a noisy communication channel without explicit feedback of channel outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELAXING FEEDBACK</head><p>In this section, we relax the (unrealistic) assumption that the observer can observe the outputs of the noisy channel directly. This change of information pattern has the potential to make the problem more difficult. In distributed control, this was first brought out in <ref type="bibr" target="#b41">[42]</ref> by the famous Witsenhausen counterexample. This showed that even in the case of LQG problems, nonlinear solutions can be optimal when the information patterns are not classical. This same example also showed how the "control" signals can start to play a dual role -simultaneously being used for control and to communicate missing information from one party to another <ref type="bibr" target="#b42">[43]</ref>. Information theory also has experience with the new challenges that arise in distributed problems of source and channel coding <ref type="bibr" target="#b43">[44]</ref>.</p><p>This section restricts the information pattern in stages. First, we consider the problem of Figure <ref type="figure">11</ref> in which the observer can see the controls but not the channel outputs. Then, we consider the problem of Figure <ref type="figure" target="#fig_7">12</ref> that restricts the observer to only see the states X t . This section is divided based on the approach rather than the problem.</p><p>In Section V-A, the solutions are based on anytime codes without feedback. These give rise to sufficient conditions that are more restrictive than the necessary conditions of Theorem 3.3. The main result is Theorem 5.2 -a random construction that shows it is possible, in the case of DMCs, to have nearly memoryless time-varying observers and still achieve stability without any feedback. All the complexity can in principle be shifted to the controller side.</p><p>In Section V-B, the solutions are based on explicitly communicating the channel outputs back to the observer through either the control signals or by making the plant itself "dance" in a stable way that communicates limited information noiselessly with no delay. Such solutions give rise to tight sufficient conditions. These are not as constructive, but serve to establish the fundamental connection between stabilization and communication with noiseless feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Using anytime codes without feedback</head><p>Noisefree access to the control signals is not problematic in the case of Corollary 4.1 since the control signals are calculated from the perfect channel feedback. Without such perfect feedback, it is more realistic to consider only noisy access to the control signals. Furthermore, observe that in Section IV-B, knowledge of the actual applied controls is used to calculate W t from the observed X t+1 , X t-1 , U t . Thus, any bounded observation noise on the control signals U t just translates into an effectively larger Γ bound on the state observation noise. By Corollary 4.1, any finite Γ can be dealt with and thus:</p><p>Corollary 5.1: It is possible to control an unstable scalar process driven by a bounded disturbance over a noisy channel so that the η-moment of |X t | stays finite for all time if the channel without feedback has C any (α) &gt; log 2 λ for some α &gt; η log 2 λ and the observer is allowed noisy access to the control signals and the state process as long as the noise on both is bounded.</p><p>As discussed in <ref type="bibr" target="#b5">[6]</ref>, without noiseless feedback the anytime capacity will tend to be considerably lower for a given α, and so there will be a gap between the necessary condition established in Theorem 3.3 and the sufficient condition in Corollary 5.1.</p><p>Next, consider the problem of figure <ref type="figure" target="#fig_7">12</ref> that restricts the observer to only see the states X t . The challenge is that the observer of Section IV-B needs to know the controls in order to remove their effect so as to focus only on encoding the virtual process Xt . As such, a new type of observer is required: Definition 5.1: A ∆-lattice based quantizer is a map (depicted in Figure <ref type="figure" target="#fig_8">13</ref> that maps inputs X to integer bins j. The j-th bin spans (∆ j 2 , ∆( j 2 + 1)] and is assigned to X ∈ (∆( j 2 + 1 4 ), ∆( j 2 + 3 4 )] near the center of the bin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">6</head><p>Periodically repeating labels for bins a priori uncertainty for next state given past observations and controls A L-regularly-labeled ∆-lattice based quantizer is one which outputs j mod L when the input is assigned to bin j -one for which the L bin labels repeat periodically.</p><p>A randomly-labeled ∆-lattice based quantizer is one which outputs A j when the input it assigned to bin j where the A j are drawn iid from a specified distribution.</p><p>Lattice based quantizers have some nice properties:</p><formula xml:id="formula_78">Lemma 5.1: a. If X noisy (t) = X t + N t with observation noise N t ∈ ( -Γ 2 , +Γ<label>2</label></formula><p>), then as long as ∆ &gt; 2Γ, the bin j selected by a ∆-lattice based quantizer facing input X noisy (t) is guaranteed to contain X t . b. There exists a constant K depending only on λ, ∆, Ω so that if X t is within a single particular bin, then X t+n can be in no more than Kλ n possible adjacent bins whose positions are a function of the control inputs applied during those n time periods as well as the original bin index for X t . c. If L &gt; Kλ n then knowing the L-regular label assigned to X noisy (t + n) is enough to determine a bin guaranteed to contain X t+n assuming knowledge of a bin containing X t as well as the control inputs applied during those n time periods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of [a]:</head><formula xml:id="formula_79">X noisy (t) ∈ (∆( j 2 + 1 4 ), ∆( j 2 + 3 4 )] implies X t ∈ (∆( j 2 + 1 4 )-Γ 2 , ∆( j 2 + 3 4 )+ Γ 2 ]. But Γ 2 &lt; ∆ 4 by assumption and hence X t ∈ (∆( j 2 + 1 4 )-∆ 4 , ∆( j 2 + 3 4 )+ ∆ 4 ] = (∆ j 2 , ∆( j 2 + 1)</formula><p>] which is the extent of the bin j.</p><p>Proof of [b]: First, suppose that the control actions were all zero during the interval in question. Because the system is linear, without loss of generality, assume that we start in the j = 0 bin, [0, ∆]. After n time-steps, this can reach at most [0, λ n ∆] without disturbances. The bounded disturbances can contribute at most</p><formula xml:id="formula_80">n-1 i=0 λ i Ω 2 &lt; λ n Ω 2 ∞ i=1 λ -i = λ n Ω 2(λ -1)</formula><p>to each side, resulting in an interval of with total length λ n (∆ + Ω λ-1 ). By linearity, the effect of any control inputs is a simple translation and is therefore just translates the interval by some positive or negative amount. Because of the overlapping nature of the bins, a single interval can overlap with at most 2 additional partial bins at the boundaries.</p><p>Since the bins are spaced by ∆ 2 , the number of possible bins the state can be in is bounded by 2 + λ n (2 + 2Ω ∆(λ-1) ) and so</p><formula xml:id="formula_81">K = 4 + 2Ω ∆(λ-1) makes property [b] true. Proof that [a],[b] ⇒ [c]: [a]</formula><p>guarantees that the bin corresponding to X noisy (t+n) is guaranteed to contain X t+n .</p><p>[b] guarantees there are only at most Kλ n &lt; L adjacent bins that the state could be in. Since the modulo operation used to assign regular labels only assigns the same label to a bin L positions away or further, all of the Kλ n positions have distinct labels and hence the labeling of X noisy (t + n) picks out the unique correct bin. Lemma 5.1 allows the observer to just use regular ∆-lattice quantizer to translate the state positions into bins since the control actions are side-information that is known perfectly at the intended recipient (the controller). The overhead implied by the constant K can be amortized by looking at time in blocks of n and so does not asymptotically cost any rate. This can be used to extend Corollary 5.1 to cases without any access to the control. Every n time-units, the observer can just apply the appropriate regular ∆-lattice quantizer and send the bin labels through an anytime code that operates without feedback. However, anytime codes without feedback have a natural tree structure since the impact of the distant past must never die out. In the stabilization context, this tree structure forces the observer/encoder to remember the bin sequence corresponding to all the past states. This seems wasteful since closed-loop stability implies that the plant state will keep returning to the bins in the neighborhood of the origin. This suggests that this memory at the observer is not necessary.</p><p>Theorem 5.2: It is possible to control an unstable scalar process driven by a bounded disturbance over a DMC so that the η-moment of |X t | stays finite for all time if the channel without feedback has random coding error exponent E r (R) &gt; η log 2 λ for some R &gt; log 2 λ and the observer is allowed boundedly noisy access to the state process.</p><p>Furthermore, there exists an n &gt; 0 so this is possible by using an observer consisting of a time-varying randomly-labeled ∆-lattice based quantizer that samples the state every n time steps and outputs a random label for the bin index. The random labels are chosen iid from A n according to the distribution that maximizes the random coding error exponent at R. The controller must have access to the common randomness used to choose the random bin labels. Proof: Fix a rate R &gt; log 2 λ for which E r (R) &gt; η log 2 λ. Lemma 5.1 applies to our quantizer. Pick n, ∆ large enough so that 2 nR &gt; Kλ n where the K comes from property <ref type="bibr">[b]</ref> above. This gives: d. Conditioned on actual past controls applied, the set of possible paths that the states X 0 , X n , X 2n , . . . could have taken through the quantization bins is a subset of a trellis that has a maximum branching factor of 2 nR Furthermore, the total length covered by the d-stage descendants of any particular bin is bounded above by Kλ dn . Not all such paths through the trellis are necessarily possible, but all possible paths do lie within the trellis. Figure <ref type="figure" target="#fig_2">14</ref> shows what such a trellis looks like and Figure <ref type="figure" target="#fig_3">15</ref> shows its tree like local property. Furthermore, the labels on each bin are iid through both time and across bins. P P P q -1 P P P q -1 -1 P P P q -1 &gt; 1 &gt; 7 7 -P P P q Z Z Z PP P q Z Z Z SS S S w Z Z Z SS S S w J J J J J ^6 ? P P P q -1 P P P q -1 -1 P P P q -1 &gt; 1 &gt; 7 7 -P P P q Z Z Z PP P q Z Z Z SS S S w Z Z Z SS S S w J J J J J ^6 ? P P P q -1 P P P q -1 -1 P P P q -1 &gt; 1 &gt; 7 7 -P P P q Z Z Z PP P q Z Z Z SS S S w Z Z Z SS S S w J J J J J ^6 ? P P P q -1 P P P q -1 -1 P P P q -1 &gt; 1 &gt; 7 7</p><formula xml:id="formula_82">- P P P q Z Z Z PP P q Z Z Z SS S S w Z Z Z SS S S w J J J J J ^PP P q - 1 P P P q - 1 - 1 P P P q - 1 &gt; 1 &gt; 7 7 - P P P q Z Z Z PP P q Z Z Z SS S S w Z Z Z SS S S w J J J J J ? 6 t=0 t=1 t=2 t=3 t=4 ∆ R = log 2 3</formula><p>Fig. <ref type="figure" target="#fig_2">14</ref>. A short segment of the randomly labeled regular trellis from the point of view of the controller that knows the actual control signals applied in the past. The example has R = log 2 3 and λ ≈ 2.4 with ∆ large. Fig. <ref type="figure" target="#fig_3">15</ref>. Locally, the trellis looks like a tree with the nodes corresponding to the intervals where the state might have been and the levels of the tree correspond to the time. It is not a tree because paths can remerge, but all labels on disjoint paths are chosen so that they are independent of each other.</p><p>Call two paths of length t through the trellis disjoint with depth d if their last common node was at depth td and the paths are disjoint after that. Consequently: e. If two paths are disjoint in the trellis at a depth of d, then the channel inputs corresponding to the past dn channel uses are independent of each other. The suboptimal controller just searches for the ML path through the trellis. The trellis itself is constructed based on the controller's memory of all past applied controls. Once an ML path has been identified, a control signal is applied based on the bin estimate at the end of the ML path. The control signal just attempts to drive the center of that bin to zero.</p><p>Consider an error event at depth d. This represents the case that the maximum likelihood path last intersected with the true path dn time steps ago. By property [d] above, the control will be based on a state estimate that can be at most Kλ dn bins away from the true state. Thus: f. If an error event at depth d occurs at time t, the state |X t+n | can be no larger than K ′ λ (d+1)n for some constant K ′ = 2∆K that does not depend on d or t.</p><p>Property [f] plays the role of ( <ref type="formula" target="#formula_61">24</ref>) in this proof.</p><p>By property [d], there are no more than 2 dnR possible false paths that last intersected the true path d stages ago. By the memorylessness of the channel, the log-likelihood of each path is the sum of the likelihood of the "prefix" of the path leading up to d stages ago and the "suffix" of the path from that point onward. For a path that is disjoint from the true path at a depth of d to beat all paths that end up at the true final state, the false path must have a suffix log-likelihood that beats the suffix log-likelihood of at least the true path. Property [e] guarantees that the channel inputs corresponding to the false paths are pairwise independent of the true inputs for the past dn channel uses.</p><p>All that is required to apply Gallager's random block-coding analysis of Chapter 5 in <ref type="bibr" target="#b0">[1]</ref> is such a pairwise independence 30  between the true and false codewords for a code of length dn.</p><p>g. The probability that the ML path diverges from the true path at depth d is no more than 2 -dnEr(R) .</p><p>All that remains is to analyze the η-moment by combining [g] and [f] and using the union bound to compute the expectation.</p><formula xml:id="formula_83">E[|X t+n | η ] ≤ t n d=0 2 -dnEr(R) (K ′ λ (d+1)n ) η &lt; (K ′ λ n ) η ∞ d=0 2 -dnEr(R) )λ ηdn = (K ′ λ n ) η ∞ d=0 2 -dn(Er(R)-η log 2 λ) = K ′′ &lt; ∞</formula><p>where the final geometric sum converges since E r (R) &gt; η log 2 λ.</p><p>Although the condition in Theorem 5.2 is not tight, the result has several nice features. First, it allows easy verification of sufficiency for a good channel since E r (R) is easy to calculate. Structurally, it demonstrates that there is no need to use very complex observers. The intrinsic memory in the plant can play the role of the memory that would otherwise need to be implemented in a channel code. The complexity can be shifted to the controller, and even that complexity is not too bad. Sequential decoding can be used at the controller since it is known to have the same asymptotic performance with respect 30 Notice that pairwise independence is also obtained if the random labels were assigned using an appropriate random time-varying infinite constraintlength convolutional code (with the symbol-merging tricks of Figure <ref type="figure">6</ref>.2.1 of <ref type="bibr" target="#b0">[1]</ref> to match the desired channel input-distribution) applied to the binary expansion of the integer j corresponding to the selected bin at each stage. Since the closed-loop system is stable, the state is presumably small and the bin is close to 0. As such, all of the higher-order bits in the binary expansion of the bin label are zeros and do not cause any computational burden when operating the convolutional code. This is related to the feedback convolutional codes with variable constraint-lengths discussed further in <ref type="bibr" target="#b5">[6]</ref>. Because of this, the computational burden of running this observer is non-increasing with time.</p><p>to delay as the ML decoder <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Because the closedloop system is stable and thereby renews itself constantly, the computational burden of running sequential decoding (and hence the controller) does not grow unboundedly with time <ref type="bibr" target="#b46">[47]</ref>.</p><p>Since E r (R, Q) &gt; 0 for all R &lt; C and the capacityachieving distribution Q, Theorem 5.2 can also be recast in a weaker Shannon capacity-centric form:</p><p>Corollary 5.2: If the observer is allowed boundedly noisy access to the plant state, and the noisy channel is a DMC with Shannon capacity C &gt; log 2 λ, then there exists some η &gt; 0 and an observer/controller pair that stabilizes the system in closed loop so that the η-moment of |X t | stays finite for all time.</p><p>Furthermore, there exists an n &gt; 0 so this is possible by using an observer consisting of a time-varying randomlylabeled ∆-lattice based quantizer that samples the state every n time steps and outputs a random label for the bin index. This random labels are chosen iid from the A n according to the capacity-achieving input distribution. The controller must have access to the common randomness used to choose the random bin labels.</p><p>Applying Theorem 4.3 to Corollary 5.2 immediately results in the following new corollary:</p><p>Corollary 5.3: If the observer is allowed perfect access to the plant state, and the noisy channel is a DMC with Shannon capacity C &gt; log 2 λ, then there exists an observer/controller pair that stabilizes the system (1) in closed loop so that: lim t→∞ X t = 0 almost surely as long as the initial condition |X 0 | ≤ Ω 2 and the disturbances W t = 0.</p><p>Furthermore, there exists an n &gt; 0 so this is possible by using an observer consisting of a time-varying randomlylabeled ∆ t -lattice based quantizer that samples the state every n time steps and outputs a random label for the bin index. The ∆ t shrink geometrically with time, and the random labels are chosen iid from the A n according to the capacity-achieving input distribution. The controller must have access to the common randomness used to choose the random bin labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Communicating the channel outputs back to the observer</head><p>In this section, the goal is to recover the tight condition on the channel from Theorem 4.2. To do this, we construct a controller that explicitly communicates the noisy channel outputs to the observer using whatever "channels" are available to it. First we consider using a noiseless control signal to embed the feedback information. This motivates the technique used to communicate the feedback information by making the plant itself dance in a stable way that tells the observer the channel output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Using the controls to communicate the channel outputs:</head><p>The idea is to "cheat" 31 and communicate the channel outputs through the controls. The control signal is thus serving dual purposes -stabilization of the system and the communication of channel outputs. Suppose the observer had noiseless access to the control signals. The controller can choose to quantize its real-valued controls to some suitable level and then use the infinite bits remaining in the fractional part to communicate the channel outputs to the observer. The observer can then extract these bits noiselessly and give them to the anytime encoder as noiseless channel feedback.</p><p>Of course, this additional fractional part will introduce an added disturbance to the plant. One approach is to just consider the quantization and channel output communication terms together as a bounded noise on the control signals considered in Section IV-G. This immediately yields:</p><p>Corollary 5.4: It is possible to control an unstable scalar process driven by a bounded disturbance over a noisy channel so that the η-moment of |X t | stays finite for all time if the channel has feedback anytime capacity C any (α) &gt; log 2 λ for some α &gt; η log 2 λ and the observer is allowed to observe the control signals perfectly.</p><p>However, the additional disturbance introduced by the quantization of the original control signal and the introduction of the new fractional part representing the channel output is known perfectly at the controller end. Meanwhile, the output of the virtual-process based observer does not depend on the actual applied controls anyway since it subtracts them off. So rather than compensating for this quantization+signaling by expanding the uncertainty Ω and thus changing the ∆ at the observer, the controller can just clean up after itself. This idea allows us to eliminate all access to the control signals at the observer and generalizes to many cases of countably large channel output output alphabets.</p><p>2) Removing noiseless access to the controls at the observer: There are two tricks involved. The first is the idea of making the plant "dance" appropriately and using the moves in the dance to communicate the channel outputs. The second idea is to introduce an artificial delay of 1 time step in the determination of the "non-dance" component of the control signals. This makes the non-dance component completely predictable by the observer and allows the observer to clearly see the dance move corrupted only by the bounded process disturbance. Putting it together gives: Theorem 5.3: Given a noisy channel with a countable alphabet, identify the channel output alphabet with the integers and suppose that there exist K &gt; 0, β &gt; η so that the channel 31 We call this "cheating" since it violates the spirit of the requirement against access to the channel outputs. However, it is important to establish this result because it points out the need for a serious future study where the communication constraints back from the controller to the observer are modeled more carefully. A more realistic model for the problem should have a sensor observing the plant connected via a communication channel to the controller. The controller is then connected to an actuator through another communication channel. The actuator finally acts upon the plant itself. With no complexity constraints, this reduces to the case studied here with the controller merely playing the role of a relay bridging together two communication channels. The relay anytime reliability will become the relevant quantity to study.  Then, it is possible to control an unstable scalar plant driven by a bounded disturbance over that channel so that the ηmoment of |X t | stays finite for all time if the channel has feedback anytime capacity C any (α) ≥ log 2 λ for some α &gt; η log 2 λ even if the observer is only allowed to observe the state X t corrupted by bounded noise. Proof: The overall strategy is illustrated in Figure <ref type="figure">16</ref>. The channel output extraction at the observer is illustrated in Figure <ref type="figure">17</ref> in the context of a channels with output alphabet size |B| = 5.</p><p>Let U t (b t-1 0 ) be the control that would be applied from Theorem 4.5 as transformed by the action of Theorem 4.6 if necessary. It only depends on the strictly past channel outputs.</p><p>Let b t be the current channel output. The control applied is:</p><formula xml:id="formula_84">U ′ t (b t 0 ) = U t (b t-1 0 ) + F (b t ) -λ U ′ t-1 (b t-1 0 ) -U t-1 (b t-2 0 )<label>(30</label></formula><p>) where the function F (b t ) is the "dance move" corresponding to the channel output.</p><p>First consider the case that perfect state observations X t are available at observer. At time t the observer can see the control signal only as it is corrupted by the process disturbance since U t-1 + W t-1 = X t -λX t-1 . By observing X perfectly, the observer has in effect gained boundedly noisy access to the U with Γ u = Ω. Now suppose that the observations of X were boundedly noisy with some Γ. In that case: ) to eliminate the effect of the past communication. The final control signal applied is shifted slightly to encode which bt was received. The decoder uses the past b t-1 0 to align its decoding regions and then reads off bt by using X t+1 -λXt.</p><formula xml:id="formula_85">|U t-1 -(X noisy (t) -λX noisy (t -1))| = |U t-1 -(X t -λX t-1 ) + (λN t-1 -N t )| = | -W t-1 + (λN t-1 -N t )| ≤ Ω + (λ +</formula><p>In this case, the effective observation noise on the controls is bounded by Γ u = Ω + (λ + 1)Γ.</p><p>Just by looking at the state and its history, the observer has access to U o t with the property that</p><formula xml:id="formula_86">|U ′ t -U o t | ≤ Γ u .</formula><p>To ensure decodability of b t , set F (b t ) = 3Γ u b t so the channel outputs are modulated to be integer multiples of 3Γ u .</p><p>At time t = 0, the observer is unchanged since there is nothing for it to learn and no applied controls. At time t = 1, because of the induced delay of 1 extra time step, there are no delayed controls ready to apply either and so the applied control only consists of 3Γ u b 0 . This is observed up to precision Γ u and so the observer can uniquely recover b 0 and feed it to its anytime encoder.</p><p>Assume now that the observer was successful in learning b t-1 0 in the past. Then it can compute the U t (b t-1 0 ) term as well as the U ′ t-1 (b t-2 0 )-U t-1 (b t-1 0 ) using this knowledge and can subtract both of them from its observed U o t . This leaves only the 3Γ u b t term which can be uniquely decoded given that the observation noise is no more than Γ u in either direction. By induction, the observer can effectively recover the past channel outputs from its noiseless observations of the control signal and can thereby operate the feedback anytime-encoder successfully.</p><p>The communication of each channel output b t only impacts the very next state by shifting it by 3Γ u b t . At the next time, it is canceled out by the correction term η+ǫ) for some K ′ and ǫ &gt; 0.</p><formula xml:id="formula_87">-λ U ′ t-1 (b t-1 0 ) -U t-1 (b t-2 0 ) . The non-dancing controlled state X ′ t+1 = (X t+1 -3Γ u B t ) has at least a power-law tail P(X ′ t+1 ≥ x) ≤ K ′ x -(</formula><p>Then</p><formula xml:id="formula_88">E[|X| η ] = = ∞ 0 P(|X ′ + 3Γ u B| ≥ m 1 η )dm ≤ ∞ 0 P 2 max(|X ′ |, 3Γ u |B|) ≥ m 1 η dm ≤ ∞ 0 P |X ′ | ≥ 1 2 m 1 η dm + +P |B| ≥ 1 6Γ u m 1 η dm ≤ ∞ 0 K ′ 1 2 m 1 η -(η+ǫ) + K 1 6Γ u m 1 η -β</formula><p>dm Since β &gt; η, this converges and so the η-moment of X also exists.</p><p>The channel output condition in 5.3 is clearly satisfied whenever the channel has a finite output alphabet. Beyond that case, it is satisfied in generic situations when the input alphabet is finite and the transition probabilities p(b|a) individually have an light enough tail for each one of the finite a values. <ref type="foot" target="#foot_16">32</ref> When the channel input alphabet is itself countable, the condition is harder to check.</p><p>If information must flow noiselessly from the controller to the observer, the key question is to quantify the instantaneous zero-error capacity of the effective channel through the plant.</p><p>Here, the bounded support of W and the unconstrained nature of U are critical since they allow the instantaneous zero-error capacity of that effective channel to be infinite. Of course, there remains the problem of the dual-nature of the control signal -it is simultaneously being asked to stabilize the plant as well as to feedback information about the channel outputs. The theorem shows that the ability of the controller to move the plant provides enough feedback to the encoder in the case of finite channel output alphabets or channels with uniformly exponentially bounded output statistics.</p><p>At an abstract level, the controller is faced with the problem of causal "writing on dirty paper" <ref type="bibr" target="#b47">[48]</ref> where the information it wishes to convey in one time step is the channel output and the dirty paper consists of the control signals it must apply to keep the system stable and to counteract the effect of the writing it did in previous time steps. Here, the problem is finessed by introducing the artificial delay at the controller to ensure that the "dirt" is side-information known both to the transmitter and the receiver. For finite output alphabets, it is also possible to take a direct "precoding" approach to do this by encoding the channel outputs by placing the control to the appropriate value modulo 3Γ u (|B| + 1). This is a bounded perturbation of the control inputs and Theorem 4.6 tells us that this does not break stability if the ∆ is adjusted appropriately.</p><p>Finally, it might seem that this particular "dance" by the plant will be a disaster for performance metrics beyond stabilization. This is probably true, but we conjecture that such implicit feedback through the plant will be usable without much loss of performance. If it has memory, the observer can notice when and how the channel has misbehaved since the plant's state will start growing rather than staying near 0. The ∆-lattice based quantizer used in the observer for Theorem 5.2 could not exploit this because it was memoryless and used uniformly sized bins regardless of whether the state was large or small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONTINUOUS TIME SYSTEMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>So far, we have considered a discrete-time model (1) for the dynamic system that must be stabilized over the communication link. This has simplified the discussion by having a common clock that drives both the system and the uses of the noisy channel. In general, there will be a τ c that represents the time between channel uses. This allows translating everything into absolute time units.</p><formula xml:id="formula_89">Ẋ(t) = λX(t) + U (t) + W (t), t ≥ 0<label>(31)</label></formula><p>where the bounded disturbance |W (t)| ≤ Ω 2 and there is a known initial condition X(0) = 0. If the open-loop system is unstable, then λ &gt; 0.</p><p>Sampling can be used to extend both the necessity and sufficiency results to the continuous time case. The basic result is that stability requires an anytime capacity greater than λ nats per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Necessity</head><p>For necessity, we are free to choose the disturbance signal W (t) and consequently can restrict ourselves to piecewise constant signals 33 that stay constant for time τ . By sampling at the rate 1 τ , the sampled state evolves as X(τ</p><formula xml:id="formula_90">(i + 1)) = e λτ X(τ i) + ( e λτ -1 λ W i ) + (i+1)τ iτ U<label>(</label></formula><p>s)e λ(τ (i+1)-s) ds (32) Notice that (32) is just a discrete time system with λ ′ = e λτ taking the role of λ in (1), and the disturbance is bounded by Ω ′ = Ω(e λτ -1) λ . All that remains is to reinterpret the earlier theorem.</p><p>By setting τ = τ c to match up the sampling times to the channel use times, it is clear that the appropriate anytime capacity must exceed log 2 λ ′ = τ c λ log 2 e bits per channel use. By converting units to nats per second 34 , we get the intuitively appealing result that the anytime capacity must be greater that λ nats/sec. 35 Similarly, to hold the η-th moment constant, the probability of error must drop with delay faster than K2 -(ηλ log 2 e)dτc where d is in units of channel uses and thus dτ c has units of seconds. Thus, we get the following pair of theorems: Theorem 6.1: For a given noisy channel and η &gt; 0, if there exists an observer O and controller C for the unstable scalar continuous time system that achieves E[|X(t)| η ] &lt; K for all t and bounded driving noise signals |W (t)| ≤ Ω 2 , then 33 zero order hold 34 Assuming that Ẋ is in per second units. 35 This truly justifies nats as the "natural" unit of information! the channel's feedback anytime capacity C any (ηλ log 2 e) ≥ λ nats per second. Theorem 6.2: For a given noisy channel and decreasing function f (m), if there exists an observer O and controller C for the unstable continuous-time scalar system that achieves P(|X(t)| &gt; m) &lt; f (m) for all t and all bounded driving noise signals |W (t)| ≤ Ω 2 , then C g-any (g) ≥ λ nats per second for the noisy channel considered with the encoder having access to noiseless feedback and g(d) having the form g(d) = f (Ke λd ) for some constant K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sufficiency</head><p>For sufficiency, the disturbance is arbitrary but we are free to sample the signal as desired at the observer and apply piecewise constant control signals. Sampling every τ units of time gives rise to <ref type="bibr" target="#b31">(32)</ref> only with the roles of W and U reversed. It is clear that W i = (i+1)τ iτ W (s)e λ(τ (i+1)-s) ds is still bounded by substituting in the upper and lower bounds and then noticing that |W i | ≤ Ω(e λτ -1) 2λ . Thus, the same argument above holds and the sufficiency Theorems 4.1, 4.2, and 5.3 as well as Corollaries 5.4 and 5.1 translate cleanly into continuous time. In each, the relevant anytime capacity must be greater than λ nats per second. Since the necessary and sufficient conditions are right next to each other, it is clear that the choice of sampling time does not impact the sense of stability that can be achieved. Of course, this need not be optimal in terms of performance.</p><p>Finally, if the channel we face is an input power-constrained ∞-bandwidth AWGN channel, more can be said. Section III-C.4 makes it clear that nothing special is required in this case: using linear controllers and observers is good enough if the average power constraint is high enough. But what if the channel had a hard amplitude constraint that allowed the encoder no more than P power per unit time? In this case, it is possible to generalize Theorem 5.2 in an interesting way.</p><p>In <ref type="bibr" target="#b48">[49]</ref> we give an explicit construction of a feedback-free anytime code for the infinite bandwidth AWGN channel that uses a sequential form of orthogonal signaling. In the ∞bandwidth AWGN channel, pairwise orthogonality between codewords plays the role that pairwise independence does for DMCs. Applying that principle through the proof of Theorem 5.2, the observer/encoder can simply be a timeinvariant regular partition of the state space with the bins being labeled with orthogonal pulses, each with an energy equal to the hard limit for the channel. 36 The encoder just pieces together pulses with shapes corresponding to where the state is at the sampling times. The controller then searches for the most likely path based on the channel output signal as well as the past control values, and then applies a control based on the current estimate. This approach allows the use of occasional bandwidth expansion to deal with unlucky streaks of channel 36 In particular, the following sequence of pulses work with an appropriate scaling. For 0 ≤ t ≤ τ , set g i,τ (t) = 1 τ sgn sin( 4πi τ t) and g -i,τ (t) = 1 τ sgn sin( 2π(2i-1) τ t) and zero everywhere else. Here τ is the time between taking samples of the state. The g i,τ functions are orthogonal, and the i-th function is the channel input corresponding to the i-th lattice bin for the plant state observation.</p><p>noise while keeping the channel input power constant. The details of this approach are given in <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. A HIERARCHY OF COMMUNICATION PROBLEMS</head><p>In this final section, we interpret some of the results in a different way inspired by the approach used in computational complexity theory. There, the scarce resource is the time and space available for computation and the asymptotic question is whether or not a certain family of problems (indexed by n) can be solved using the limited amount of resource available. While explicit algorithms for solving problems do play a role, "reductions" from one problem to another also feature prominently in relating the resource requirements among related problems <ref type="bibr" target="#b50">[51]</ref>.</p><p>In communication, the scarce resource can be thought of as being the available channel. 37 Problems should be ordered by what channels are good enough for them. We begin with some simple definitions and then see how they apply to classical results from information theory. Finally, we interpret our current results in this framework.</p><p>Definition 7.1: A communication problem is a partially specified random system together with an information pattern and a performance objective. This is specified by a triple: (S, I, V). The partially specified random system S = (S 0 , S 1 , . . .) in which S i are real valued functions on [0, 1] i+1 × IR i . The output of the S i function is denoted X i . The information pattern I identifies what variables each of the i-th encoders and decoders has access to. The performance objective V is a statement that must evaluate to either true or false once the entire random system is specified.</p><p>As depicted in Figure <ref type="figure" target="#fig_4">18</ref>, the communication problem is thus an open system that awaits interconnection with encoder, channel, and decoder maps. The channel is a measurable map f c from [0, 1] × IR into IR. The encoder and decoder are both represented by a possibly time-varying sequence of real valued functions compatible with the information pattern I.</p><p>Once all the maps are specified, the random system becomes completely specified by tying them to an underlying probability space consisting of three iid sequences (W i , V i , R i ) of continuous uniform random variables on [0, 1]. The W i 1 are connected to the first input of S i while V i is connected to the first input of the memoryless channel. As is usual, the output of the encoder is connected to the remaining input of the channel, and all the past outputs of the channel are connected to the decoding functions as per the information patterns. Finally, assume that common randomness R i is made available to both the encoder and decoder so that they may do random coding if desired. Once everything is connected, it is possible to evaluate the truth or falsehood of V.</p><p>Definition 7.2: A channel is said to solve the problem if there exist suitable encoder and decoder maps compatible with the given information pattern so that the combined random system satisfies the performance objective V.</p><p>Communication problem A is harder than problem B if any channel f c that solves A also solves B. 37 This might in turn be related to other more primitive scarce resources like power or bandwidth available for communication.  <ref type="figure" target="#fig_4">18</ref>. Abstractly, a communication problem consists of a partially specified random system consisting of a known and possibly interactive source together with an information pattern. The noisy channel and encoder/decoders need to be specified before all the random variables become properly defined.</p><p>Each particular communication problem therefore divides channels into two classes: those that solve it and those that do not. Suitable families of communication problems, ordered by hardness, can then be used to sort channels as well. Channels that solve harder problems are better than ones that do not. The equivalence of certain families of communication problems means that they induce the same orderings on communication channels. This will become clearer by the examples of the next few sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classical Examples</head><p>1) The Shannon communication problem: Shannon identified the problem of communicating bits reliably as one of the core problems of communication. In our framework, this problem is formalized as follows:</p><formula xml:id="formula_91">• X i = 1 if W i &gt; 1</formula><p>2 and X i = 0 otherwise. The functions S i ignore all other inputs. • The information pattern I specifies that D i has access to Z i 1 . The encoder information pattern is complete in the case of communication with feedback: E i has access to X i 1 as well as Z i-1 1 . Without feedback, E i has access only to X i 1 .</p><p>• The performance objective V(ǫ, d) is satisfied if P(X i = U i+d ) ≤ ǫ for every i ≥ 0. The Shannon communication problem naturally comes in a pair of families A f ǫ,d with feedback and A nf ǫ,d without feedback. These families are indexed by the tolerable probability of bit error ǫ and end-to-end delay d.</p><p>To obtain other rates R &gt; 0, adjust the source functions as follows:</p><p>• X i = j 2 ⌊Ri⌋-⌊R(i-1)⌋ if W i ∈ [ j 2 ⌊Ri⌋-⌊R(i-1)⌋ , j+1 2 ⌊Ri⌋-⌊R(i-1)⌋ ) for integer j ≥ 0. The possibly time-varying functions S i ignore all other inputs. These naturally result in families A f R,ǫ,d and A nf R,ǫ,d for the feedback and feedback-free cases respectively. It is immediately clear that A nf R,ǫ,d is harder than A f R,ǫ,d and furthermore problems with smaller ǫ or d are harder than those with larger ones. It is also true that A f R,ǫ,d is harder than A f R ′ ,ǫ,d whenever R ≤ R ′ in that it is more challenging to communicate reliably at a high rate rather than a low one.</p><p>The set of channels with classical Shannon feedback capacity of at least R is therefore:</p><formula xml:id="formula_92">C f R = ǫ&gt;0 R ′ &lt;R d&gt;0 {f c |f c solves A f R ′ ,ǫ,d }<label>(33)</label></formula><p>and similarly for C nf R . The classical result that feedback does not increase capacity tells us that C f R = C nf R . Because of this, we just call them both C R .</p><p>2) The zero-error communication problem: A second problem is the one of zero error communication. It is defined exactly the same as the Shannon communication problem above, except that ǫ = 0.</p><p>The channels that have feedback zero-error capacity of at least R with feedback are therefore:</p><formula xml:id="formula_93">C f 0,R = R ′ &lt;R d&gt;0 {f c |f c solves A f R ′ ,0,d }<label>(34)</label></formula><p>and similarly for C nf 0,R . In this case, the result with and without feedback can be different and furthermore, C nf 0,R ⊂ C f 0,R ⊂ C R <ref type="bibr" target="#b24">[25]</ref>. In this sense, zero-error communication is fundamentally a harder problem than ǫ-error communication.</p><p>3) Estimation problems with distortion constraints: Consider iid real valued sources with cumulative distribution functions F X (t) = P(X ≤ t).</p><p>• X i = F -1 X (W i ) ignoring all the other inputs. This gives the desired source statistics. (for the cases with/without feedback) and once again associate them with the set of channels that solve them in the limit of large delays: </p><p>where R(D) is the information-theoretic rate-distortion curve. The interpretation of this separation theorem is that in the limit of large delays, estimation problems with a fidelity constraint are no harder or easier than Shannon communication problems dealing with bits. Both families of problems induce essentially the same partial order on channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Anytime communication problems</head><p>The anytime communication problems are natural generalizations of the binary data communication problems above. Everything remains as in the Shannon communication problem, only the performance measure changes. Let U t = 0. X 0 (t), X 1 (t), X 2 (t), . . . when written out in binary notation. This can always be done and the parsing of the string is unique no matter what the rate is.</p><p>• V (K,α) is satisfied if P(X i = X i (i + d)) ≤ K2 -αd for every i ≥ 0, d ≥ 0. Call these problems A f (R,α,K) when feedback is allowed and A nf (R,α,K) when it is not permitted. Once again, it is clear that the non-feedback problems are harder than the corresponding feedback problems. Furthermore, A (R,α,K) is harder than A (R,α ′ ,K) if α ′ ≤ α in addition to the usual fact of A (R,α,K) being harder than A (R ′ ,α,K) if R ′ ≤ R. Similarly, smaller K values are harder than larger ones.</p><p>The channels with α-anytime feedback capacity of at least R are then given by:</p><formula xml:id="formula_95">C f a,(R,α) = R ′ &lt;R α ′ &lt;α K&gt;0 {f c |f c solves A f (R ′ ,α ′ ,K) } (37)</formula><p>with a similar definition for C nf a,(R,α) . It is immediately clear that C f 0,R ⊆ C f a,(R,α) ⊆ C R The case of α = 0 is defined as the limit:</p><formula xml:id="formula_96">C f a,(R,0) = α&gt;0 C f a,(R,α)<label>(38)</label></formula><p>It turns out in this case that C f a,(R,0) = C nf a,(R,0) = C R since infinite random tree codes can be used to communicate reliably at all rates below the Shannon capacity <ref type="bibr" target="#b22">[23]</ref>.</p><p>However, for other α &gt; 0,</p><formula xml:id="formula_97">C nf 0,R ⊂ C nf a,(R,α) ⊂ C f a,(R,α) ⊂ C R and C nf 0,R ⊂ C f 0,R ⊂ C f a,(R,α) ⊂ C R</formula><p>with all of these being strict inclusion relations. C f 0,R and C nf a,(R,α) are not subsets of each other in general. In this sense, there is a non-trivial hierarchy of problems with Shannon communication as the easiest example and zeroerror communication as the hardest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Control and the relation to anytime communication</head><p>The stabilization problems considered in this paper are different in that they are interactive. The formulation should be apparent by comparing Figure <ref type="figure" target="#fig_4">18</ref> with Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>• X i represents the state of the scalar control problem with unstable system dynamics given by λ &gt; 1. The W t is the bounded disturbance and U i represents the control signal used to generate X i+1 . • The information pattern with and without feedback is as before.</p><formula xml:id="formula_98">• The performance objective V (η,K) is satisfied if E[|X i | η ] ≤ K for all i ≥ 0.</formula><p>Call this problem A f (λ,η,K) for cases with feedback and A nf (λ,η,K) for cases without feedback available at the encoder. The problem without feedback is harder than the problem with feedback. It is also clear that A f (λ,η,K) is harder than A f (λ ′ ,η,K) whenever λ ≥ λ ′ and similarly for A nf . The same holds if η is made larger or K is made smaller. Finally, notice how the mapping from (λ, η) to (R, α) is one-to-one and onto. By setting λ = 2 R and η = α R it is possible to translate in the opposite direction and this does provide some additional insight. For example, in the anytime communication problem, it is clear that increasing R from 2 to 3 while keeping α constant at 6 results in a harder problem. When translated to stabilization, without the results established here, it is far from obvious that the equivalent move from λ = 4 to λ = 8 with a simultaneous drop in the required η from 3 to 2 is also a move in a fundamentally harder direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>Traditionally, this hierarchy of communication problems had not been explored since there were apparently only two interesting levels: problems equivalent to classical Shannon communication and those equivalent to zero-error communication. Anytime communication problems are intermediate between the two. Though feedback anytime communication problems are interesting on their own, the equivalence with feedback stabilization makes them even more fundamental.</p><p>It is interesting to consider where Schulman's interactive computation problems fit in this sort of hierarchy. Because a constant factor slowdown is permitted by the asymptotics, such problems of interactive computation do not distinguish between channels of different Shannon capacity. In the language of this section, this means that Shannon communication problems are harder than those of interactive computation considered in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Furthermore, the noisy channel definition given here can be extended to include channels with memory. Simply make the current channel output depend on all the current and past V t and Y t . In that case, (40) will continue to hold. Since the finiteoutput alphabet constructions never needed memorylessness, (41) will also hold.</p><p>The constructive nature of the proofs for the underlying theorems makes them akin to the "reductions" used in theoretical computer science to show that two problems belong to the same complexity class. They are direct translations at the level of problems and solutions. In contrast, the classical separation results go through the mutual information characterization of R(D) and C. It would be interesting to study a suitable analog of (36) for channels with memory. Feedback can now increase the capacity so the with-feedback and feedback-free problems are no longer equivalent. However, it would be nice to see a direct reduction of Shannon's communication problem to an estimation problem that encompasses such cases as well. The asymptotic equivalence situation is likely even richer in the multiuser setting where traditional separation theorems do not hold.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 2 .</head><label>2</label><figDesc>Suppose that the memoryless communication channel is a noiseless one bit channel. So A = B = {0, 1} and p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. The problem of communicating messages in an anytime fashion. Both the encoder E and decoder D are causal maps and the decoder in principle provides updated estimates for all past messages. These estimates must converge to the true message values appropriately rapidly with increasing delay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. The construction of a feedback anytime code from a control system. The messages are used to generate the {Wt} inputs which are causally combined to generate { Xt} within the encoder. The channel outputs are used to generate control signals at both the encoder and decoder. Since the simulated plant is stable, -X and X are close to each other. The past message bits are estimated from the X at the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Virtual controller for R=1. How the virtual state X evolves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. With noisy observations, no strict partition of the line can adequately capture the uncertainty since it can straddle the boundary of two regions. By doubling the number of bins, it is guaranteed that the uncertainty arising from observation noise can be contained inside a single bin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 11.Control over a noisy communication channel without explicit feedback of channel outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Control over a noisy communication channel without any explicit feedback path from controller to observer except through the plant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. A 15-regularly-labeled lattice based quantizer. If the observer had known the controls, it would have centered the lattice to cover the top bar exactly. Because it does not, one additional quantization bin must be added at the end so that the uncertainty never covers two bins bearing the same label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Fig.<ref type="bibr" target="#b17">18</ref>. Abstractly, a communication problem consists of a partially specified random system consisting of a known and possibly interactive source together with an information pattern. The noisy channel and encoder/decoders need to be specified before all the random variables become properly defined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>•</head><label></label><figDesc>The information patterns remain as in the Shannon problem.• The performance objective V(ρ, D, d) is satisfied if lim n→∞ 1 n E[ n i=1 ρ(X i , U i+d )] ≤ D.Call these estimation problems A f (FX ,ρ,D,d) and A nf (FX ,ρ,D,d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>FX ,ρ,D) = D ′ &gt;D d&gt;0 {f c |f c solves A f (FX ,ρ,D ′ ,d) } (35)and similarly for C nf e,(FX ,ρ,D) . For cases where the distortion ρ is bounded, the existing separation result can be interpreted as follows: C R(D) = C nf e,(FX ,ρ,D) = C f e,(FX ,ρ,D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>λ,η) = λ ′ &lt;λ η ′ &lt;η K&gt;0 {f c |f c solves A f (λ ′ ,η ′ ,K) }(39)with a similar definition for C nf c,(λ,η) . The necessity result of Theorem 3.3 establishes thatC nf c,(λ,η) ⊆ C f c,(λ,η) ⊆ C f a,(log 2 λ,η log 2 λ)while Theorem 4.2 establishes the other direction for the case of feedback:C nf c,(λ,η) ⊆ C f c,(λ,η) = C f a,(log 2 λ,η log 2 λ)(40)Meanwhile without feedback and restricting to the set of finite output alphabet channels (ie. where the range of f c has finite cardinality.) denoted C fin , Theorem 5.3 implies:C f a,(log 2 λ,η log 2 λ) ∩ C fin ⊆ C nf c,(λ,η) ∩ C fin Combining with<ref type="bibr" target="#b39">(40)</ref> gives the following result for finite output alphabet channels:C nf c,(λ,η) ∩C fin = C f c,(λ,η) ∩C fin = C f a,(log 2 λ,η log 2 λ) ∩C fin<ref type="bibr" target="#b40">(41)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2. Control over a noisy communication channel. The unstable scalar system is persistently disturbed by Wt and must be kept stable in closed-loop through the actions of O, C.</figDesc><table><row><cell></cell><cell>W t-1</cell><cell></cell><cell></cell></row><row><cell>U t-1</cell><cell cols="4">9 Noisy Possible Channel Feedback 6 Designed 6 Observer 7 -? Possible Control Knowledge 9 ? 8 -6 6 Delay 1 Step O Xt Scalar System</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Channel</cell></row><row><cell>Delay 1 Step</cell><cell></cell><cell></cell><cell>8</cell><cell>7</cell></row><row><cell>6</cell><cell>Ut</cell><cell>Designed Controller</cell><cell>?</cell></row><row><cell></cell><cell>Control Signals</cell><cell>C</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Rd .</figDesc><table><row><cell>Scheme:</cell><cell cols="3">Schalkwijk [37]</cell><cell>Theorem 3.6</cell></row><row><cell>Message:</cell><cell cols="3">known in advance</cell><cell>streams in</cell></row><row><cell>Delay:</cell><cell cols="3">prespecified</cell><cell>universal</cell></row><row><cell>Error exponent:</cell><cell cols="4">double-exponential double-exponential</cell></row><row><cell>Channel:</cell><cell cols="3">known AWGN</cell><cell>known AWGN</cell></row><row><cell>Constraint:</cell><cell cols="3">average power</cell><cell>average power</cell></row><row><cell>Noiseless</cell><cell cols="3">required</cell><cell>required</cell></row><row><cell>Feedback:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Initialization:</cell><cell cols="3">2 nR -PAM</cell><cell>none</cell></row><row><cell></cell><cell cols="3">+ ML feedback</cell></row><row><cell>Ongoing:</cell><cell cols="3">MMSE feedback</cell><cell>MMSE feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+ small R-PAM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>perturbations</cell></row><row><cell>Channel input:</cell><cell cols="3">Gaussian</cell><cell>Perturbed Gaussian</cell></row><row><cell>Decoding:</cell><cell cols="4">Minimum distance Minimum distance</cell></row><row><cell>Equivalent</cell><cell cols="3">unstable</cell><cell>unstable</cell></row><row><cell>Plant:</cell><cell cols="3">R &lt; log 2 λ &lt; C</cell><cell>R &lt; log 2 λ &lt; C</cell></row><row><cell>Initial condition:</cell><cell cols="3">bounded</cell><cell>zero</cell></row><row><cell>Disturbance:</cell><cell></cell><cell cols="2">zero</cell><cell>bounded</cell></row><row><cell>Stability sense:</cell><cell cols="3">almost-sure [18]</cell><cell>exponential tail</cell></row><row><cell cols="5">Fig. 7. Quick comparison of the Schalkwijk/Kailath scheme to the anytime</cell></row><row><cell cols="2">generalization in this paper.</cell><cell></cell><cell></cell></row><row><cell cols="5">Since the convergence is double exponential, it is faster than</cell></row><row><cell cols="2">any exponential and hence</cell><cell></cell><cell></cell></row><row><cell cols="2">C any (α) =</cell><cell>1 2</cell><cell>log 2 (1 +</cell><cell>P σ 2</cell></row></table><note><p>. Applying Theo-rem 3.5 immediately gives (12) since λ d &gt; 2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and proceed by induction. Assume that Xt is known to lie within [-∆ 2 , ∆ 2 ]. Then λ Xt will lie within [-λ∆ 2 , λ∆ 2 ]. By choosing 2 R control values uniformly spaced within that interval, it is guaranteed that λ Xt + Ūt will lie within [-λ∆ 2 R+1 , λ∆ 2 R+1]. Finally, the state will be disturbed by W t and so Xt+1 will be known to lie within[-λ∆  </figDesc><table><row><cell>2 , λ∆ 2 R+1 + Ω 2 ]. 2 R+1 -Ω Since the initial condition has no uncertainty, induction will</cell></row><row><cell>be complete if</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The second is its best estimate X t , The controller remembers what it did in the past and uses the anytime decoder to get an updated sense of where the observer wants it to go. It then applies a control designed to correct for any past errors and move the state to be close to the virtual state controlled by the observer.</figDesc><table><row><cell>Bt</cell><cell>-</cell><cell>Anytime channel decoder</cell><cell cols="2">-of net impact of virtual controls Best estimate X 1 (t) U t</cell><cell>X t+1 (t)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>k ? +</cell><cell>-</cell><cell>Ut</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell></row><row><cell></cell><cell>-</cell><cell>Internal model of past controls for impact</cell><cell>-Xt</cell><cell>-λ by Multiply</cell></row><row><cell>Fig. 9.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Overlaying messages onto the control signal and recovering the messages at the observer. The control signal is generated based on unit-delayed channel outputs with the current output being communicated back. outputs B t satisfy: P(|B t | ≥ i) ≤ Ki -β for all t regardless of the channel inputs.</figDesc><table><row><cell></cell><cell></cell><cell>Channel Output Extractor</cell><cell cols="2">Z t-1 -</cell><cell>Joint Decoder Controller Copy</cell></row><row><cell></cell><cell>W t-1</cell><cell>6</cell><cell></cell><cell cols="2">U t-1</cell><cell>?</cell><cell>?</cell></row><row><cell>U t-1</cell><cell>Scalar System X t</cell><cell></cell><cell></cell><cell></cell><cell>Process Virtual X Simulator</cell><cell>Ūt -</cell><cell>Feedback Anytime Encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Noisy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Channel</cell></row><row><cell></cell><cell></cell><cell cols="2">U 1 t-1 ?</cell><cell cols="2">(t -1)</cell></row><row><cell>U t</cell><cell>6 e +</cell><cell>Controller C Tracking</cell><cell></cell><cell></cell><cell>Feedback Decoder Anytime</cell><cell>Z t</cell></row><row><cell></cell><cell>Channel</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Output</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Encoder</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Joint Decoder/Controller</cell></row><row><cell>Fig. 16.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Furthermore, such constant factor slowdowns appear to be unavoidable when facing the very general class of interactive computational problems.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The differing roles of the past and future are made clear in<ref type="bibr" target="#b5">[6]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The sequential rate-distortion bound is generally not attained even at higher rates except in the case of perfectly matched channels.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>In seminal work<ref type="bibr" target="#b11">[12]</ref>, there is no persistent disturbance acting on the unstable plant.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4"><p>Because the decoder and controller are both on the same side of the communication channel, they can be lumped together into a single box.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_5"><p>In Section III-C.3, it is shown that anything less than that can not work in general.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_6"><p>There are pathological cases where it is possible to stabilize a system with less rate. These occur when the driving disturbance is particularly structured instead of just being unknown but bounded. An example is when the disturbance only takes on values ±1 while λ = 4. Clearly only one bit per unit time is required even though log 2 λ = 2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_7"><p>This is a probability mass function in the case of discrete alphabets B, but is more generally an appropriate probability measure over the output alphabet B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_8"><p>If the bits to be sent are deterministic, this is the sample space giving channel noise realizations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_9"><p>The typical gap is larger and so the probability of error is actually lower than this bound says it is.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_10"><p>This is a minor twist on the procedure followed by serial A/D converters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_11"><p>For the details of how to deal with fractional R, please see the causal source code discussion in<ref type="bibr" target="#b32">[33]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_12"><p>In reality, the uncertainty approaches this from below since the system starts at the known initial condition 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_13"><p>Here, the probability is over the channel's noisy actions and any randomness present at the observer and controller. The convergence holds for every possible initial condition and so it does not matter if the initial condition is included in the probability model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_14"><p>The quantization is assumed to be coarse, but with infinite dynamic range. Section III-C tells us that finite dynamic range will impose the requirement of zero-error capacity on the link.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_15"><p>This will not arise for statically quantized states since those will have fixed boundaries. In that case, nothing needs to be done except ensuring that the partitions respect those boundaries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_16"><p>For example, an AWGN channel with a hard-input constraint and quantized outputs.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Mukul Agarwal, Shashibhushan Borade, Devavrat Shah, and Lav Varshney for comments on earlier versions of this paper. We thank Nicola Elia for several constructive discussions about the subject matter of this paper and Sekhar Tatikonda for many discussions over a long period of time which have influenced this work in important ways. Finally, we thank the anonymous reviewers for a careful reading of the paper and helpful feedback.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Support for S.K. Mitter was provided by the Army Research Office under the MURI Grant: Data Fusion in Large Arrays of Microsensors DAAD19-00-1-0466 and the Department of Defense MURI Grant: Complex Adaptive Networks for Cooperative Control Subaward #03-132 and the National Science Foundation Grant CCR-0325774.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Information Theory and Reliable Communication</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>John Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The source-channel separation theorem revisited</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vembu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Steinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coding for interactive communication</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1745" to="1756" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A coding theorem for distributed computation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coding theorems for a discrete source with a fidelity criterion</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE National Convention Record</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="142" to="163" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Why block length and delay are not the same thing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<ptr target="http://www.eecs.berkeley.edu/˜sahai/Papers/FocusingBound.pdf" />
	</analytic>
	<monogr>
		<title level="m">To be submitted</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teams, signaling, and information theory</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="312" />
			<date type="published" when="1978-04">Apr. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Control under communication constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Control under communication constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1056" to="1068" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Communication-limited stabilization of linear systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th IEEE Conference on Decision and control</title>
		<meeting>the 39th IEEE Conference on Decision and control<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-12">Dec. 2000</date>
			<biblScope unit="page" from="1005" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stabilizability of stochastic linear systems with finite feedback data rates</title>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="413" to="436" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Control over noisy channels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1196" to="1201" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Control of LQG systems under communication constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 American Control Conference</title>
		<meeting>the 1999 American Control Conference<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="2778" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic linear control over a communication channel</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1549" to="1561" />
			<date type="published" when="2004-09">Sept. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous design of measurement and control strategies for stochastic systems with feedback</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Basar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="679" to="694" />
			<date type="published" when="1999-09">Sept. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating channels for control: Capacity reconsidered</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 American Control Conference</title>
		<meeting>the 2000 American Control Conference<address><addrLine>Chicago, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
			<biblScope unit="page" from="2358" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stabilization of uncertain systems in the presence of a stochastic digital link</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Dahleh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">When Bode meets Shannon: control-oriented feedback communication schemes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Elia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1477" to="1488" />
			<date type="published" when="2004-09">Sept. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Remote stabilization over fading channels</title>
	</analytic>
	<monogr>
		<title level="j">Systems and Control Letters</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="249" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Achievable rates for stability of LTI systems over noisy forward and feedback channels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Basar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Conference on Information Sciences and Systems</title>
		<meeting>the 2005 Conference on Information Sciences and Systems<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-03">Mar. 2005</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Information rates of autoregressive processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="412" to="421" />
			<date type="published" when="1970-07">July 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information rates of Wiener processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="134" to="139" />
			<date type="published" when="1970-03">Mar. 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Source coding and channel requirements for unstable processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitter</surname></persName>
		</author>
		<ptr target="http://www.eecs.berkeley.edu/˜sahai/Papers/anytime.pdf" />
	</analytic>
	<monogr>
		<title level="m">To be submitted</title>
		<imprint/>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<title level="m">Rate Distortion Theory</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The zero error capacity of a noisy channel</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1956-09">Sept. 1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LQG control with communication constraints</title>
		<author>
			<persName><forename type="first">V</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications, Computation, Control, and Signal Processing: a Tribute to Thomas Kailath</title>
		<meeting><address><addrLine>Norwell, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="365" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional codes II. maximum-likelihood decoding</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="266" />
			<date type="published" when="1974-07">July 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data transmission over a discrete channel with feedback, random transmission time</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Burnashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy Perdachi Informatsii</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="10" to="30" />
			<date type="published" when="1976-12">Oct./Dec. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An asymptotic bound for the probability error of information transmission through a channel without memory using the feedback</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Dobrushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy Kibernetiki</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="161" to="168" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The anytime reliability of constrained packet erasure channels with feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Allerton Conference on Communication, Control, and Computing</title>
		<meeting>the 42nd Allerton Conference on Communication, Control, and Computing<address><addrLine>Monticello, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">Sept. 2004</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kalman filtering with intermittent observations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sinopoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schenato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franceschetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Poolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1453" to="1464" />
			<date type="published" when="2004-09">Sept. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kalman filtering with partial observation losses</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Any-time information theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Anytime channel coding with feedback</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Simsek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A lower bound to the distribution of computation for sequential decoding</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Berlekamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="174" />
			<date type="published" when="1967-04">Apr. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A coding scheme for additive noise channels with feedback -I: No bandwidth constraint</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P M</forename><surname>Schalkwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="182" />
			<date type="published" when="1966-04">Apr. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A coding scheme for additive noise channels with feedback -II: Band-limited signals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P M</forename><surname>Schalkwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="189" />
			<date type="published" when="1966-04">Apr. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving communication reliability by use of an intermittent feedback channel</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="1969-01">Jan. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The anytime reliability of the AWGN+erasure channel with feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Allerton Conference on Communication, Control, and Computing</title>
		<meeting>the 42nd Allerton Conference on Communication, Control, and Computing<address><addrLine>Monticello, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">Sept. 2004</date>
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Anytime communication over the Gilbert-Eliot channel with noiseless feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avestimehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Symposium on Information Theory</title>
		<meeting>IEEE International Symposium on Information Theory<address><addrLine>Adelaide, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">Sept. 2005</date>
			<biblScope unit="page" from="1783" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Separation of estimation and control for discrete time systems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Witsenhausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1971-11">Nov. 1971</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1557" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A counterexample in stochastic optimum control</title>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="147" />
			<date type="published" when="1968-01">Jan. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Information and control: Witsenhausen revisited</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning, Control and Hybrid Systems: Lecture Notes in Control and Information Sciences</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yamamoto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Hara</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page" from="281" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>John Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional codes III. sequential decoding</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="297" />
			<date type="published" when="1974-07">July 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Upper bounds on sequential decoding performance parameters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="239" />
			<date type="published" when="1974-03">Mar. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A simple encoding and decoding strategy for stabilization over discrete memoryless channels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Palaiyanur</surname></persName>
		</author>
		<ptr target="http://www.eecs.berkeley.edu/˜sahai/Papers/allerton05-sequential-stabilization.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Allerton Conference on Communication, Control, and Computing</title>
		<meeting>the Allerton Conference on Communication, Control, and Computing<address><addrLine>Monticello, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">Sept. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Writing on dirty paper</title>
		<author>
			<persName><forename type="first">M</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="441" />
			<date type="published" when="1983-05">May 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Anytime coding on the infinite bandwidth AWGN channel: a sequential semi-orthogonal code</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<ptr target="http://www.eecs.berkeley.edu/˜sahai/Papers/ciss05wideband.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Conference on Information Sciences and Systems</title>
		<meeting>the 2005 Conference on Information Sciences and Systems<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-03">Mar. 2005, paper 196</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stabilization over discrete memoryless and wideband channels using nearly memoryless observations</title>
		<ptr target="http://www.eecs.berkeley.edu/˜sahai/Papers/memorylesscontrol.pdf" />
	</analytic>
	<monogr>
		<title level="m">submitted to the 44th IEEE Conference on Decision and Control</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<title level="m">Introduction to Automata Theory, Languages, and Computation</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
