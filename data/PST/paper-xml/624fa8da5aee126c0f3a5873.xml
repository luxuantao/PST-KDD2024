<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</title>
				<funder ref="#_5pWynTV #_TbSzVHP #_yVEc2js">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">fund of Joint Laboratory of HUST</orgName>
				</funder>
				<funder ref="#_CZUVNJN">
					<orgName type="full">CCF-AFSG Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-06">6 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Liang</surname></persName>
							<email>shuoliang@hust.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
							<email>weiw@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<email>wangfei@ict.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Computig Technology</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyong</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Naval University of Engineering</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-06">6 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.03117v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference. It is challenging because a sentence may contain multiple aspects or complicated (e.g., conditional, coordinating, or adversative) relations. Recently, exploiting dependency syntax information with graph neural networks has been the most popular trend. Despite its success, methods that heavily rely on the dependency tree pose challenges in accurately modeling the alignment of the aspects and their words indicative of sentiment, since the dependency tree may provide noisy signals of unrelated associations (e.g., the "conj" relation between "great" and "dreadful" in Figure <ref type="figure">2</ref>). In this paper, to alleviate this problem, we propose a Bi-Syntax aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully exploits the syntax information (e.g., phrase segmentation and hierarchical structure) of the constituent tree of a sentence to model the sentiment-aware context of every single aspect (called intracontext) and the sentiment relations across aspects (called inter-context) for learning. Experiments on four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the stateof-the-art methods consistently.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-based sentiment analysis (ABSA) aims to identify the sentiment polarity towards a given aspect in the sentence. Many previous works <ref type="bibr" target="#b29">(Yang et al., 2018;</ref><ref type="bibr" target="#b15">Li et al., 2019)</ref>   The food is great but the service and the environment are dreadful". Two separate ellipses encircle its two clauses. The "conj" edge between "great" and "dreadful" is a noise.</p><p>more likely to be related to its sentiment. However, the assumption might not be valid as exemplified in Figure <ref type="figure">1</ref> (a), "service" is obviously closer to "great" rather than "dreadful", and these methods may assign the irrelevant opinion word "great" to "service" mistakenly.</p><p>To mitigate this problem, there already exists several efforts <ref type="bibr">(Wang et al., 2020a;</ref><ref type="bibr" target="#b1">Chen et al., 2020)</ref> dedicated to research on how to effectively leverage non-sequential information (e.g., syntactic information like dependency tree) via Graph Neural Networks (GNNs). Generally, a dependency tree (i.e., Dep.Tree), linking the aspect terms to the syntactically related words, stays valid in the long-distance dependency problem. However, the inherent nature of Dep.Tree structure may introduce noise like the unrelated relations across clauses, such as "conj" relation between "great" and "dreadful" in Figure <ref type="figure">2</ref>, which discourages capturing the sentiment-aware context of each aspect, i.e., intra-context. More-Figure <ref type="figure">3</ref>: Constituent tree of the sentence "The food is great but the service and the environment are dreadful". Context words are in rectangles and parsed phrase types are in rounded rectangles.</p><p>over, the Dep.Tree structure only reveals relations between words and, thereby, in most cases, is incapable of modeling complicated (e.g., conditional, coordinating, or adversative) relations of sentences, therefore failing to capture sentiment relations between aspects, i.e., inter-context.</p><p>Hence, in this paper, we consider fully exploiting the syntax information of the constituent tree to tackle the problem. Typically, a constituent tree (i.e., Con.Tree) often contains precise and discriminative phrase segmentation and hierarchical composition structure, which are helpful for correctly aligning the aspects and their corresponding words indicative of sentiment. The former can naturally divide a complicated sentence into multiple clauses, and the latter can discriminate different relations among aspects to infer the sentiment relations of different aspects. We illustrate this with an example in Figure <ref type="figure">3:</ref> (1) Clause "The food is great" and the clause "the service and environment are dreadful" are segmented by the phrase segmentation term "but"; (2) In Layer-1, the term "and" indicates the coordinating relation of "service" and "environment", while the term "but" in Layer-3 reflects the adversative relation towards "food" and "service" (or "environment").</p><p>Thus, to better align aspect terms and corresponding sentiments, we propose a new framework, Bi-Syntax aware Graph Attention Network (BiSyn-GAT+), to effectively leverage the syntax information of constituent tree by modeling intracontext and inter-context information. In particular, BiSyn-GAT+ employs: 1) a syntax graph embedding to encode the intra-context of each aspect based on the fusion syntax information within the same clause in a bottom-up way, which combines the phrase-level syntax information of its constituent tree and the clause-level syntax information of its dependency tree. 2) an aspect-context graph consisting of phrase segmentation terms and all aspects to model the inter-context of each aspect. Specifically, it aggregates the sentiment infor-mation of other aspects according to the influence between the current aspect and its neighbor aspects, which is calculated based on aspect representations learned from bi-directional relations over the aspect context graph, respectively.</p><p>Our main contributions are as follows:</p><p>(1) To the best of our knowledge, this is the first work to exploit syntax information of constituent tree (e.g., phrase segmentation and hierarchical structure) with GNNs for ABSA. Moreover, it shows superiority in the alignments between aspects and corresponding words indicative of sentiment.</p><p>(2) We propose a framework, Bi-Syntax aware Graph Attention Network (BiSyn-GAT+), to fully leverage syntax information of constituent tree (or, and dependency tree) by modeling the sentimentaware context of each single aspect and the sentiment relations across aspects.</p><p>(3) Extensive experiments on four datasets show that our proposed model achieves state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentiment analysis is an important task in the field of natural language processing <ref type="bibr" target="#b32">(Zhang et al., 2018;</ref><ref type="bibr" target="#b30">Yang et al., 2020)</ref> and can be applied in downstream tasks, like emotional chatbot <ref type="bibr" target="#b27">(Wei et al., 2019;</ref><ref type="bibr">Li et al., 2020a;</ref><ref type="bibr" target="#b11">Lan et al., 2020;</ref><ref type="bibr" target="#b28">Wei et al., 2021)</ref>, recommendation system <ref type="bibr" target="#b35">(Zhao et al., 2022;</ref><ref type="bibr">Wang et al., 2020b)</ref>, QA system <ref type="bibr" target="#b26">(Wei et al., 2011;</ref><ref type="bibr" target="#b20">Qiu et al., 2021)</ref>. Here we focus on a fine-grained sentiment analysis task -ABSA. Recently, deep learning methods have been widely adopted for ABSA task. These works can be divided into two main categories: methods without syntax information (i.e., Syntax-free methods) and methods with syntax information (i.e., Syntax-based methods).</p><p>Syntax-free methods: Neural networks with attention mechanisms <ref type="bibr" target="#b24">(Wang et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2017;</ref><ref type="bibr" target="#b21">Song et al., 2019)</ref> have been widely used. <ref type="bibr" target="#b3">Chen et al. (2017)</ref> adopts a multiple-attention mechanism to capture sentiment features. <ref type="bibr" target="#b21">Song et al. (2019)</ref> uses an attentional encoder network (AEN) to excavate rich semantic information from word embeddings.</p><p>Syntax-based methods: Recently, utilizing dependency information with GNNs has become an effective way for ABSA. <ref type="bibr" target="#b31">Zhang et al. (2019)</ref> uses graph convolutional networks (GCN) to learn node representations from Dep.Tree. <ref type="bibr" target="#b22">Tang et al. (2020)</ref> proposes a dependency graph enhanced dual-transformer network (DGEDT) by jointly considering representations from Transformers and corresponding dependency graph. <ref type="bibr">Wang et al. (2020a)</ref> constructs aspect-oriented dependency trees and proposes R-GAT, extending the graph attention network to encode graphs with labeled edges. <ref type="bibr" target="#b13">Li et al. (2021)</ref> proposes a dual graph convolutional networks (DualGCN) model, simultaneously considering syntax structures and semantic correlations. All above works use syntax information of Dep.Tree, which may introduce noise, as we said before. Thus, we exploit syntax information of Con.Tree with GNNs. Precisely, we follow the Con.Tree to aggregate information from words within the same phrases in a bottom-up way and capture intra-context information.</p><p>Moreover, some works resort to modeling aspectaspect relations. Some <ref type="bibr" target="#b8">(Hazarika et al., 2018;</ref><ref type="bibr" target="#b17">Majumder et al., 2018)</ref> adopt aspect representations to model relations by RNNs or memory networks, without utilizing context information. And some <ref type="bibr" target="#b7">(Fan et al., 2018;</ref><ref type="bibr" target="#b9">Hu et al., 2019)</ref> propose alignment loss or orthogonal attention regulation to constrain aspect-level interactions, which fail when aspects have no explicit opinion expressions or multiple aspects share same opinion words. Recently, there are some works utilizing GNNs to model aspect relations. <ref type="bibr" target="#b16">Liang et al. (2020)</ref> constructs an inter-aspect graph based on relative dependencies between aspects. <ref type="bibr" target="#b34">Zhao et al. (2020)</ref> constructs a sentiment graph, where each node represents an aspect, and each edge represents the sentiment dependency relation. However, these works fail to explicitly use phrase segmentation information, such as conjunction words. Thus, we propose an aspect-context graph consisting of all aspects and phrase segmentation terms to model inter-context information.</p><p>GNNs with constituent tree: To our knowledge, we are the first work to utilize the constituent tree for ABSA task. But in aspect-category sentiment analysis task, which predicts sentiment polarity towards a given predefined category in the text, <ref type="bibr">Li et al. (2020b)</ref> proposes a Sentence Constituent-Aware Network (SCAN) that generates representations of the nodes in Con.Tree. Unlike SCAN, we view parsed phrases as different spans of the input text instead of individual nodes. So we don't introduce any inner nodes of Con.Tree (e.g., "NP","VP" of Figure <ref type="figure">3</ref>) into the representation space, decreas-ing the computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Problem Statement. Let s = {w i } n and A = {a j } m be a sentence and a predefined aspect set, where n and m are the number of words in s and the number of aspects in A, respectively. For each s, A s = {a i |a i ? A, a i ? s} denotes the aspects contained in s. We treat each multiple-word aspect as a single word for simplicity, so a i also means the i-th word of s. The goal of ABSA is to predict the sentiment polarity y i ? {positive, negative, neural} for each aspect a i ? A s .</p><p>Architecture. As shown in Figure <ref type="figure">4</ref>, our proposed architecture takes the sentence and all aspects that appear in the text as the input, and outputs the sentiment predictions of the aspects. It contains three components: 1) the intra-context module encodes the input {w i } to obtain aspect-specific representations of the target aspects, which contains two encoders: a context encoder that outputs contextual word representations and a syntax encoder that utilizes syntax information of the parsed constituent tree (or, and dependency tree). 2) the inter-context module includes a relation encoder applied to the constructed aspect-context graph to output relation-enhanced representations. The aspect-context graph composes all aspects of the given sentence and phrase segmentation terms obtained from a designed rule-based map function applied to the constituent tree. 3) the sentiment classifier takes output representations of the above two modules to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intra-Context Module</head><p>In this part, we utilize a context encoder and a syntax encoder to model the sentiment-aware context of every single aspect and generate aspectspecific representation for each aspect. Note that for multi-aspect sentences, we use this module multiple times, as each time deals with one aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Context Encoder</head><p>We use BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> to generate contextual word representations. Given target aspect a t , we follow BERT-SPC <ref type="bibr" target="#b21">(Song et al., 2019)</ref> to construct a BERT-based sequence: It takes the sentence and all aspects as input and outputs sentiment predictions for all aspects. It has three components: 1) the intra-context module contains two encoders: a context encoder that outputs contextual word representations and a syntax encoder that utilizes syntax information of the parsed constituent tree (or, and dependency tree). Output representations from two encoders are fused to generate aspectspecific representations; 2) the inter-context module includes a relation encoder applied to the constructed aspectcontext graph to obtain relation-enhanced representations. The aspect-context graph includes all aspects and phrase segmentation terms obtained from a designed rule-based map function applied to the constituent tree. 3) the sentiment classifier takes the outputs from two modules to make predictions.</p><formula xml:id="formula_0">BERT _seq t = [CLS]+{w i }+[SEP]+a t +[SEP] ,<label>(1)</label></formula><p>Then, the output representation is obtained by,</p><formula xml:id="formula_1">h t = h t 0 , h t 1 , . . . , h t n , . . . , h t n +2+m t (2)</formula><p>where n and m are lengths of input text and target aspect a t after BERT tokenizer separately, h t 0 is "BERT pooling" vector representing the BERT sequence, h t i is the contextual representation of each token. Note that w i may be split into multiple sub-words by BERT tokenizer. So we calculate the contextual representation of w i as follows,</p><formula xml:id="formula_2">?t i = 1 |BertT (w i )| k?BertT (w i ) h t k ,<label>(3)</label></formula><p>where BertT (w i ) returns an index set of w i 's subwords in BERT sequence, and | | returns its length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Syntax Encoder</head><p>The above representations only consider semantic information, so we propose a syntax encoder to utilize rich syntax information. Our syntax encoder is stacked by several designed Hierarchical Graph ATtention (HGAT) blocks, and each block consists of multiple graph attention (i.e., GAT) layers that encode syntax information hierarchically under the guidance of the constituent tree (or, and the dependency tree). The key point is the construction of corresponding graphs.</p><p>Graph construction. As Figure <ref type="figure">4</ref> shows, we follow the syntax structure of Con.Tree in a bottomup way. Each layer l of Con.Tree consists of several phrases ph l u that compose the input text, and each phrase represents an individual semantic unit. e.g., ph 3 in Figure <ref type="figure">3</ref> is {The food is great, but, the service and the environment are dreadful}. We construct corresponding graphs based on those phrases. i.e., For layer l that consists of phrases ph l u , we construct the adjacent matrix CA that shows word connections:</p><formula xml:id="formula_3">CA l i,j = 1 if w i , w j in same phrase of ph l u 0 otherwise ,<label>(4</label></formula><p>) which is exemplified as Con.Graphs in Figure <ref type="figure">5</ref>.</p><p>HGAT block. A HGAT block aims to encode syntax information into word representations hierarchically. As Figure <ref type="figure">5</ref> shows, a HGAT block is stacked by several GAT layers that utilize a masked selfattention mechanism to aggregate information from neighbors and a fully connected feed forward network to map representations to the same semantic space. Attention mechanism can handle the diversity of neighbors with higher weights assigned to more related words. It can be formulated as follows, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?t,l</head><formula xml:id="formula_4">i = F C(g t,l i + ?t,l-1 i ),<label>(5)</label></formula><formula xml:id="formula_5">g t,l i = Z z=1 ? ? ? j?N t,l (i) ? lz ij W lz g ?t,l-1 j ? ? ,<label>(6)</label></formula><formula xml:id="formula_6">? lz ij = exp f ?t,l-1 i , ?t,l-1 j j ?N l (i) exp f ?t,l-1 i , ?t,l-1 j ,<label>(7)</label></formula><p>where N l (i) is the set of neighbors of w i in layer l, ?t,l i is the final representation of w i in layer l, F C is fully connected feed forward network. g t,l i is the representation of w i after masked self-attention mechanism. || denotes vector concatenation. Z is the number of attention heads, ? is activation function. W lz g is trainable parameter of the zth head of layer l. f is a score function that measures the correlation of two words. Stacked HGAT block takes the output of previous one as the input, and the input of the first HGAT block is ?t . The output of syntax encoder is defined as ?t for simplicity.</p><p>With dependency information. We also explore the fusion of two syntax information. Following previous works, we consider the Dep.Tree as an undirected graph and construct adjacent matrix DA, which is formulated as follows, Tree can shorten paths between aspect words and relevant opinion words, e.g., "food" and "great" in Figure <ref type="figure">3</ref>.</p><formula xml:id="formula_7">DA i,j = 1 if w i , w j link</formula><formula xml:id="formula_8">FA = CA + DA<label>(10)</label></formula><p>C. conditional position-wise add. This operation considers phrase-level syntax information of Con.Tree and clause-level syntax information of Dep.Tree. Specifically, it first deletes all dependency edges that are across clauses (e.g., the edge between "great" and "dreadful" in Figure <ref type="figure">2</ref>) and then conducts position-wise add operation with the remaining dependency edges.</p><formula xml:id="formula_9">FA = CA ? DA (11)</formula><p>Thus, the output of the intra-context module contains both contextual information and syntax information, which is formulated as follows,</p><formula xml:id="formula_10">v as t = ?t t + ?t t ; h t 0 (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter-Context Module</head><p>The intra-context module ignores the mutual influence of aspects. Thus, in inter-context module, we construct an aspect-context graph to model the relations across aspects. This module only works for multi-aspect sentences, with aspect-specific representations of all aspects from intra-context module as input and outputs relation-enhanced representation of each aspect.</p><p>Phrase segmentation. Aspect relations can be revealed by some phrase segmentation terms, like conjunction words. Thus, we design a rule-based map function P S that returns phrase segmentation terms of two aspects: Given two aspects, it first finds their lowest common ancestor (LCA) in the Con.Tree, which contains information of two aspects and has the least irrelevant context. We call branches from LCA that between sub-trees which two aspects are separately in as "inner branches". P S returns all text words in the inner branches if they exist; else, it returns words between two aspects of the input text. It is formulated as follows,</p><formula xml:id="formula_11">P S(a i , a j ) = {w k } , if |Br(a i , a j )| = 0 Br(a i , a j ), otherwise ,<label>(13)</label></formula><p>where i &lt; k &lt; j and Br(a i , a j ) returns text words in the inner branches of a i and a j . e.g., in Figure <ref type="figure">3</ref>, given aspects food and service, the LCA node is S of Layer-4 that has three branches, with food in the first and service in the third. So "but" in the second branch (inner branch) is the phrase segmentation term that reflects sentiment relation of two aspects.</p><p>Aspect-context graph construction. We notice that the influence range of one aspect should be continuous, and the mutual influence of aspects attenuates with distance. Considering all aspect pairs introduces noise caused by long distance and increases computational overhead. So we only model relations across neighbor aspects. After extracting phrase segmentation terms of neighbor aspects by P S function, we construct an aspect-context graph by linking aspects with corresponding phrase segmentation terms to help infer relations. To distinguish the bi-directional relations over the aspectcontext graph, we build two corresponding adjacent matrices. The first handles influence from aspects in odd-index among all aspects of the sentence, to neighbor even-index aspects, the second han-  dles the opposite. An example is shown in Figure <ref type="figure" target="#fig_4">6</ref>. Then, taking {v as t , t ? A s } and corresponding phrase segmentation terms representations encoded by BERT as the input, the above HGAT blocks are applied as the relation encoder to obtain relationenhanced representation v aa t for each aspect a t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>The outputs of the intra-context module and inter-context module are combined to form the final representations, which are later fed to a fully connected layer (i.e., sentiment classifier) with a softmax activation function, generating the probabilities over the three sentiment polarities:</p><formula xml:id="formula_12">o t = v as t + v aa t , (14) p(t) = sof tmax(W p o t + b p ),<label>(15)</label></formula><p>where W p , b p are parameters of the classifier<ref type="foot" target="#foot_0">1</ref> . The loss is defined as the cross-entropy loss between golden polarity labels and predicted polarity distributions of all (sentence, aspect) pairs: L(?) Sentiment =s at?As loss(p(t), y(t)), (16) where a t is the aspect and also the t-th word in s, loss is the standard cross-entropy loss, ? represents model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Setup</head><p>We evaluate our models on four English dataset: Laptop, Restaurant datasets from SemEval2014 (Task 4) <ref type="bibr" target="#b19">(Pontiki et al., 2014)</ref>, MAMS <ref type="bibr" target="#b10">(Jiang et al., 2019)</ref>, and Twitter <ref type="bibr" target="#b5">(Dong et al., 2014)</ref>. Laptop and Restaurant contain both multi-aspect and singleaspect sentences. Each sentence in MAMS contains at least two aspects with different sentiments. Twitter contains only one-aspect sentences. Dataset statistics are shown in Table <ref type="table">1</ref>. We adopt SuPar 2 as parser. Specifically, we use CRF constituency parser <ref type="bibr" target="#b33">(Zhang et al., 2020)</ref> to get the constituent tree; and following previous works <ref type="bibr">(Wang et al., 2020a;</ref><ref type="bibr" target="#b0">Bai et al., 2020)</ref>, we use deep Biaffine Parser <ref type="bibr" target="#b6">(Dozat and Manning, 2017)</ref> to get the dependency tree. Our context encoder is BERT-base-uncased 3 model. Adam optimizer is adopted with a learning rate 2 ? 10 -5 and a L 2 regulation 10 -5 for model training. Number of GAT layers of one HGAT block is 3, and number of HGAT blocks is in range [1,3] on different datasets. "Accuracy" and "Macro-Averaged F1" are evaluation metrics. More details are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our model with the following models:</p><p>1) Syntax-free baselines: BERT-SPC <ref type="bibr" target="#b21">(Song et al., 2019)</ref>, AEN-BERT <ref type="bibr" target="#b21">(Song et al., 2019)</ref>;</p><p>2) Syntax-based baselines: R-GAT <ref type="bibr">(Wang et al., 2020a)</ref>, RGAT+ <ref type="bibr" target="#b0">(Bai et al., 2020)</ref>, DGEDT <ref type="bibr" target="#b22">(Tang et al., 2020)</ref>, DualGCN <ref type="bibr" target="#b13">(Li et al., 2021)</ref>;</p><p>3) Baselines that model aspect-aspect relations: SDGCN-BERT <ref type="bibr" target="#b34">(Zhao et al., 2020)</ref> ( <ref type="bibr" target="#b16">Liang et al., 2020)</ref>; Ours are also syntax-based, including: a) BiSyn-GAT+: our full model, which contains the intra-context module that combines two syntax information by conditional position-wise add operation, inter-context module, and sentiment classifier to make predictions; b) BiSyn-GAT: full model without inter-context module;</p><p>Baselines and our models are all BERT-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>Table <ref type="table" target="#tab_2">2</ref> shows results of the baselines and our models. For fairness of comparison, we present the reported results of those baselines. Observations are: 1) Our proposed models outperform most baselines, and our full model BiSyn-GAT+ achieves state-  of-the-art performances in all datasets, especially 1.27 and 1.75 F1 improvements on Restaurant and MAMS. 2) Models with syntax information outperform those without, which means syntax structure is helpful. 3) Our models show superiority to those that only use dependency information, which implies that constituent tree can provide profitable information. 4) BiSyn-GAT+ shows consistent improvement compared to BiSyn-GAT, which means modeling aspect-aspect relations can improve performance, especially when more multi-aspect sentences are available, e.g., 0.8 and 1.06 F1 improvements on Restaurant and MAMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We also conduct an ablation study to verify the effectiveness of our proposed method. The results are shown in Table <ref type="table" target="#tab_3">3</ref>. We set the context encoder of our model as the base model, i.e., BERT+. The observations are that: 1) BERT+ achieves the lowest performance, which shows syntax information is helpful in ABSA task. 2) In category w/o AA, w/o con. is inferior to w/o dep., which means syntax information of Con.Tree is useful. Moreover, the comparison between w/o con. and con.?dep. verifies that some dependency edges that cross the phrases indeed bring noise, as the former considers all dependency edges and the latter ignores those across phrases obtained from Con.Tree for each layer.</p><p>3) Fusing two syntax information in the proper ways can boost performance. In category w/o AA, con.+dep. and con.?dep. both outperform w/o dep. and w/o con. in all datasets. However, con.?dep. is inferior to w/o dep.. One possible reason is that the position-wise dot operation ignores most connections within phrases, causing the graphs to be more sparse. It also verifies that words within the same phrases of Con.Tree are essential for aligning aspects and corresponding opinions. 4) Modeling aspect-aspect relations is beneficial from the comparison between w/ AA and w/o AA, especially in Restaurant and MAMS that contain more multi-aspect sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Effects of Aspect-context graph</head><p>We also investigate the effects of our bi-relational modeling of the proposed aspect-context graph. Firstly, we use BiSyn-GAT as base model to see whether the approach modeling aspects relations improves the performance; Secondly, based on our proposed aspect-context graph, we consider two variants: (a) w/ Bi-relation, a directed one that distinguishes the influence one aspect imposes on other aspects and is received from other aspects, i.e., our full model BiSyn-GAT+; (b) w/o Bi-relation, an undirected one that ignores the direction of the influence; Thirdly, inspired by <ref type="bibr" target="#b34">Zhao et al. (2020)</ref>, we define the aspect graph as the graph with all aspects as its nodes, i.e., our aspect-context graph without any segmentation terms. Based on the aspect graph, we propose three variants: (c) adjacent aspect graph, an undirected one where neighbor aspects are connected; (d) bi-adjacent aspect graph, a directed one where neighbor aspects are connected; (e) global aspect graph, an undirected one where all aspects are connected; The above five variants are illustrated in Figure <ref type="figure" target="#fig_5">7</ref>. Experimental One possible reason is that the number of samples that contain at least three aspects is very limited, as shown in Table <ref type="table" target="#tab_9">8</ref> of Appendix. And adjacent aspect graph equals global aspect graph when faced with two aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effects of Parsing</head><p>We conduct experiments to study the influence of paring accuracy on model performance. Two parsers are selected: (a) Stanford Parser <ref type="bibr" target="#b18">(Manning et al., 2014)</ref>, a well-known toolkit; it has transitionbased dependency parser <ref type="bibr" target="#b2">(Chen and Manning, 2014)</ref> and shift-reduce constituency parser <ref type="bibr" target="#b36">(Zhu et al., 2013)</ref>; (b) SuPar, which RGAT+ <ref type="bibr" target="#b0">(Bai et al., 2020)</ref> and our proposed models adopt; it has deep biaffine dependency parser <ref type="bibr" target="#b6">(Dozat and Manning, 2017)</ref> and neural CRF constituency parser <ref type="bibr" target="#b33">(Zhang et al., 2020)</ref>. Generally, SuPar has better parsing performances than Stanford Parser. We use BERT+ as the base model and compare the performance of model w/o dep, Bisyn-GAT, BiSyn-GAT+ when using different parsers. The results are shown in Table <ref type="table" target="#tab_5">5</ref>. Observations are that: 1) With Stanford Parser, our models can also achieve good performance. 2) Models with SuPar perform better than models with Stanford Parser, which is correlated with the parsing accuracy of two parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case Study</head><p>As shown in Figure <ref type="figure" target="#fig_4">6</ref>, we present four examples to help better understand our proposed model, especially inter-context module when faced with complex sentences. The first is a comparative sentence with two clauses connected by the conjunction "but". Both models make correct predictions for atmosphere. However, BiSyn-GAT predicts wrong over outside while BiSyn-GAT+ still makes a correct prediction, which show the inter-context module correctly captures the reversed sentiment relation between outside and atmosphere by phrase segmentation terms ", but". The rest examples all show that inter-context module can use relations across aspects to help correct the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose the BiSyn-GAT+ framework to model the sentiment-aware context of each aspect and sentiment relations across aspects for learning by fully exploiting the syntax information of the constituent tree. It includes two welldesigned modules: 1) intra-context module that fuses related semantic and syntax information hierarchically; 2) inter-context module that models relations across aspects with the constructed aspectcontext graph. To the best of our knowledge, it is the first work to exploit the constituent tree with GNNs for the ABSA task. Moreover, our proposed model achieves state-of-the-art performances on four benchmark datasets.   </p><formula xml:id="formula_13">- 8 3 - - - 6 - 1 9 1 - - - 1 - - 10 - - - - 1 1 1 11 - - - - 1 1 1 13 - 1 1 - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset and Implementation Detail</head><p>A.1 Statistics of constituent tree depth</p><p>Table <ref type="table" target="#tab_8">7</ref> shows more detailed statistics about four benchmark datasets at the aspect level. We define the "constituent tree depth" as the number of nodes in the path from the aspect term node to the root node in the Con.Tree. It means we treat the layer that the aspect term is in as the bottom layer for constituent graph construction and drop layers below it. The aspect term has no other neighbors in those layers and thus fails to update its representation through the graph encoder. According to the constituent tree depth statistics, we set the number of GAT layers of one HGAT block in the syntax encoder to 3, the most common depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Multi-aspect Distribution of datasets</head><p>Table <ref type="table" target="#tab_9">8</ref> shows the multi-aspect distribution of the Restaurant, Laptop, and MAMS datasets. This can explain the improvement of BiSyn-GAT+ compared to BiSyn-GAT on different datasets: MAMS &gt; Restaurant &gt; Laptop. MAMS contains the most multi-aspect sentences that our proposed Intercontext module can fully utilize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Training Detail</head><p>The numbers of parameters of BiSyn-GAT and BiSyn-GAT+ are 112M and 233M. Each epoch takes about 60s or 70s in RTX 2080 Ti. We test the model that performs best on validation data, and for datasets without official validation data, we follow the dataset settings of previous work <ref type="bibr" target="#b0">(Bai et al., 2020)</ref>. We use the grid search to find the best parameters for our model and report the maximum results. The number of HGAT blocks within our relation encoder is in range <ref type="bibr">[1,</ref><ref type="bibr">3]</ref> on different datasets and the number of its inner GAT layers is set to 2; the dropout rate is 0.1 for the input and output and is in the range [0.2, 0.7] between layers;</p><p>In each HGAT block of our syntax encoder, for samples with fewer constituent tree layers, we only adopt the same number of GAT layers to encode; for samples with more constituent tree layers, we prune them to three layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Discussion about phrase segmentation term</head><p>We firstly provide more cases about the phrase segmentation terms in this section. For each case, the aspects are displayed in bold and phrase segmentation words are underlined between the corresponding two aspects: 1) However, we went for lunch and were the only ones eatting there and yet the service seemed eager for use to be done and to get out.</p><p>2) We were so excited since I was reading great review of this place, however we were disappointed with the taste of the food.</p><p>3) Then the manager gave us lemon juice instead of ceasar dressing for a ceasar salad which ruined the salad.</p><p>4) The only drawback was slow service, but the food and ambiance are so nice that your wait is a ) pleasant and b ) worth it.</p><p>5) Compared to the soup of average taste, the rice is better in this restaurant.</p><p>The top 4 cases show that our approach can capture words, such as "and", "but", "yet", "however", "instead of" to help infer aspects relations.</p><p>However, we also notice there is a limitation of our method: it can only find the phrase segmentation terms within the two aspects, failing to capture some important words indicative of relations that appear in other locations. e.g., in case 5), our approach capture "," instead of "compared to", while only the latter can show the reversed sentiment of two aspects. We leave this problem as the future work, considering that our current approach is simple and can also achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Limitations and future work</head><p>This section discusses some improvements that can be made in future work. 1) Our full model adopts two BERT encoders, one in Intra-context module for encoding input text and aspects and one in Intercontext module for encoding the phrase segmentation terms. The pros are that our Inter-context can easily generalize to other ABSA models, taking their output aspect representations and generating the relation enhanced representations. However, this causes the parameters of BiSyn-GAT+ up to 233M. We will consider other encoding strategies instead of simply using another BERT; 2) We notice that the label information from Con.Tree can also provide valuable information, e.g., NP node and VP node, which together form the S node, may contain the aspect term and corresponding opinion words separately, as shown in Figure <ref type="figure">3</ref>. It is worth trying to utilize more information from Con.Tree, and we will continue to explore it in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )Figure 2 :</head><label>a2</label><figDesc>Figure 1: Examples of ABSA task. Each underlined aspect is classified to corresponding sentiment polarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure4: Overall architecture. It takes the sentence and all aspects as input and outputs sentiment predictions for all aspects. It has three components: 1) the intra-context module contains two encoders: a context encoder that outputs contextual word representations and a syntax encoder that utilizes syntax information of the parsed constituent tree (or, and dependency tree). Output representations from two encoders are fused to generate aspectspecific representations; 2) the inter-context module includes a relation encoder applied to the constructed aspectcontext graph to obtain relation-enhanced representations. The aspect-context graph includes all aspects and phrase segmentation terms obtained from a designed rule-based map function applied to the constituent tree. 3) the sentiment classifier takes the outputs from two modules to make predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 5: HGAT Block. It is stacked by several GAT layers, and each GAT layer is applied to the graph obtained from one layer of the constituent tree (or, and the dependency tree).</figDesc><graphic url="image-3.png" coords="5,106.48,242.83,59.64,53.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>directly in Dep.Tree 0 otherwise (8) We consider three operations: position-wise dot, position-wise add, and conditional positionwise add. Each corresponding adjacent matrix FA is shown as follows, A. position-wise dot. For each layer of Con.Tree, this operation only considers neighbors of the Dep.Tree that are also in the same phrase. FA = CA ? DA (9) B. position-wise add. For each layer of Con.Tree, this operation considers words in the same phrases and neighbors of the Dep.Tree. Some edges of Dep.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of an aspect-context graph and corresponding two adjacent matrices for distinguishing the bi-directional relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Illustrations of variants when investigating the effects of aspect-context graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>mainly focus on extracting sequence features via Recurrent Neural Networks (RNNs) or Convolution Neural Networks (CNNs) with attention mechanisms, which often assume that words closer to the target aspect are</figDesc><table><row><cell>Corresponding Author</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of models on four datasets. The best are in bold, and second-best are underlined.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell></cell></row><row><cell cols="2">Category</cell><cell>Model</cell><cell cols="2">Restaurant</cell><cell cols="2">Laptop</cell><cell cols="2">MAMS</cell><cell></cell><cell>Twitter</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">Acc.(%) F1.(%) Acc.(%) F1.(%) Acc.(%) F1.(%) Acc.(%) F1.(%)</cell></row><row><cell>w/o Syn.</cell><cell></cell><cell>BERT-SPC</cell><cell>84.46</cell><cell>76.98</cell><cell>78.99</cell><cell>75.03</cell><cell>82.82</cell><cell cols="2">81.90</cell><cell>73.55</cell><cell>72.14</cell></row><row><cell></cell><cell></cell><cell>AEN-BERT</cell><cell>83.12</cell><cell>73.76</cell><cell>79.93</cell><cell>76.31</cell><cell>-</cell><cell>-</cell><cell></cell><cell>74.71</cell><cell>73.13</cell></row><row><cell>w/ Syn.</cell><cell></cell><cell>R-GAT</cell><cell>86.60</cell><cell>81.35</cell><cell>78.21</cell><cell>74.07</cell><cell>-</cell><cell>-</cell><cell></cell><cell>76.15</cell><cell>74.88</cell></row><row><cell></cell><cell></cell><cell>RGAT+</cell><cell>86.68</cell><cell>80.92</cell><cell>80.94</cell><cell>78.20</cell><cell>84.52</cell><cell cols="2">83.74</cell><cell>76.28</cell><cell>75.25</cell></row><row><cell></cell><cell></cell><cell>DGEDT</cell><cell>86.30</cell><cell>80.00</cell><cell>79.80</cell><cell>75.60</cell><cell>-</cell><cell>-</cell><cell></cell><cell>77.90</cell><cell>75.40</cell></row><row><cell></cell><cell></cell><cell>DualGCN</cell><cell>87.13</cell><cell>81.16</cell><cell>81.80</cell><cell>78.10</cell><cell>-</cell><cell>-</cell><cell></cell><cell>77.40</cell><cell>76.02</cell></row><row><cell></cell><cell></cell><cell>SDGCN</cell><cell>83.57</cell><cell>76.47</cell><cell>81.35</cell><cell>78.34</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>InterGCN</cell><cell>87.12</cell><cell>81.02</cell><cell>82.87</cell><cell>79.32</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell>BiSyn-GAT</cell><cell>87.49</cell><cell>81.63</cell><cell>82.44</cell><cell>79.15</cell><cell>84.90</cell><cell cols="2">84.43</cell><cell>77.99</cell><cell>76.80</cell></row><row><cell></cell><cell></cell><cell>BiSyn-GAT+</cell><cell>87.94</cell><cell>82.43</cell><cell>82.91</cell><cell>79.38</cell><cell>85.85</cell><cell cols="2">85.49</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell></cell></row><row><cell>Category</cell><cell></cell><cell>Ablation</cell><cell></cell><cell cols="2">Restaurant</cell><cell cols="2">Laptop</cell><cell cols="2">MAMS</cell><cell>Twitter</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Acc.(%) F1.(%) Acc.(%) F1.(%) Acc.(%) F1.(%) Acc.(%) F1.(%)</cell></row><row><cell>w/o AA</cell><cell cols="3">w/o syn. &amp; dep.(BERT+)</cell><cell>84.99</cell><cell>78.51</cell><cell>79.11</cell><cell>75.76</cell><cell>82.71</cell><cell>82.22</cell><cell>75.48</cell><cell>74.54</cell></row><row><cell></cell><cell></cell><cell>w/o con.</cell><cell></cell><cell>86.42</cell><cell>80.10</cell><cell>80.22</cell><cell>76.42</cell><cell>83.38</cell><cell>82.90</cell><cell>76.51</cell><cell>75.29</cell></row><row><cell></cell><cell></cell><cell>w/o dep.</cell><cell></cell><cell>86.60</cell><cell>81.51</cell><cell>81.80</cell><cell>78.48</cell><cell>84.58</cell><cell>84.09</cell><cell>76.81</cell><cell>75.86</cell></row><row><cell></cell><cell></cell><cell>con.?dep.</cell><cell></cell><cell>86.86</cell><cell>80.82</cell><cell>80.85</cell><cell>77.27</cell><cell>84.21</cell><cell>83.76</cell><cell>76.51</cell><cell>75.37</cell></row><row><cell></cell><cell></cell><cell>con.+dep.</cell><cell></cell><cell>86.86</cell><cell>81.59</cell><cell>82.12</cell><cell>78.93</cell><cell>84.73</cell><cell>84.14</cell><cell>77.40</cell><cell>76.39</cell></row><row><cell></cell><cell cols="3">con.?dep. (BiSyn-GAT)</cell><cell>87.49</cell><cell>81.63</cell><cell>82.44</cell><cell>79.15</cell><cell>84.90</cell><cell>84.43</cell><cell>77.99</cell><cell>76.80</cell></row><row><cell>w/ AA</cell><cell></cell><cell>con.+dep.</cell><cell></cell><cell>87.76</cell><cell>82.18</cell><cell>82.75</cell><cell>79.16</cell><cell>85.48</cell><cell>85.05</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">con.?dep. (BiSyn-GAT+)</cell><cell>87.94</cell><cell>82.43</cell><cell>82.91</cell><cell>79.38</cell><cell>85.85</cell><cell>85.49</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>Ablation study. Notations "con." and "dep." represent syntax information from constituent tree and dependency tree, respectively. ?, +, ? represent the position-wise dot, position-wise add, conditional positionwise add operations, respectively, when fusing two syntax information. "AA" represents modeling aspect-aspect relations. The best performances are in bold, and second-best are underlined.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>, InterGCN Performance comparison of aspect-context graph variants on Restaurant and MAMS dataset. The best performances are in bold.</figDesc><table><row><cell>2 https://github.com/yzhangcs/parser</cell></row><row><cell>3 https://github.com/huggingface/transformers</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Experiments results with different parsers.</figDesc><table><row><cell>Model</cell><cell>Parser</cell><cell>Restaurant Acc.(%) F1.(%) Acc.(%) F1.(%) MAMS</cell></row><row><cell cols="2">Base</cell><cell>84.99 78.51 82.71 82.22</cell></row><row><cell>w/o dep.</cell><cell cols="2">Stanford Parser 86.51 81.34 84.51 84.06 SuPar 86.60 81.51 84.58 84.09</cell></row><row><cell>BiSyn-GAT</cell><cell cols="2">Stanford Parser 86.66 81.56 84.88 84.31 SuPar 87.49 81.63 84.90 84.43</cell></row><row><cell>BiSyn-GAT+</cell><cell cols="2">Stanford Parser 87.84 82.39 85.78 85.40 SuPar 87.94 82.43 85.85 85.49</cell></row></table><note><p>w/o dep. is one variant of BiSyn-GAT, only using constituent information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Predictions from BiSyn-GAT and BiSyn-GAT+. The notations pos, neg, and neu in the table represent positive, negative, and neutral. For each sentence, the aspects are displayed in bold, with golden sentiment polarities as the subscripts. The phrase segmentation words are shown underline between the corresponding two aspects. False predictions are marked with while true predictions are marked with .</figDesc><table><row><cell>Sentences</cell><cell>Aspects</cell><cell cols="2">BiSyn-GAT BiSyn-GAT+</cell></row><row><cell>it doesn't look like much on the outsideneg , but the minute</cell><cell>outside</cell><cell>neu</cell><cell>neg</cell></row><row><cell>you walk inside, it's a whole other atmospherepos.</cell><cell>atmosphere</cell><cell>pos</cell><cell>pos</cell></row><row><cell>while the serviceneg and settingneg were average</cell><cell>service</cell><cell>neg</cell><cell>neg</cell></row><row><cell>, the foodpos was excellent.</cell><cell>setting</cell><cell>neu</cell><cell>neg</cell></row><row><cell></cell><cell>food</cell><cell>pos</cell><cell>pos</cell></row><row><cell>food was average, the appetizerspos were</cell><cell>appetizers</cell><cell>pos</cell><cell>pos</cell></row><row><cell>better than the main coursesneu.</cell><cell>main courses</cell><cell>pos</cell><cell>neu</cell></row><row><cell>i have no complaints about the waitpos or the servicepos</cell><cell>wait</cell><cell>neu</cell><cell>pos</cell></row><row><cell>but the pizzaneg was bit at all something to write home about.</cell><cell>service</cell><cell>neg</cell><cell>pos</cell></row><row><cell></cell><cell>pizza</cell><cell>neg</cell><cell>neg</cell></row><row><cell>results are shown in Table 4 and we can observe</cell><cell></cell><cell></cell><cell></cell></row><row><cell>that: 1) w/ Bi-relation (i.e., BiSyn-GAT+) outper-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>forms w/o Bi-relation consistently, which indicates</cell><cell></cell><cell></cell><cell></cell></row><row><cell>distinguishing the bi-relational influences is benefi-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>cial; 2) Overall, aspect-context graph shows supe-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>riority compared with aspect graph, which means</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the phrase segmentation terms can help model as-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pects relations; 3) Unlike in aspect-context graph,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bi-adjacent aspect graph does not guarantee per-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>formance improvement compared with adjacent as-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pect graph, which reflects the importance of phrase</cell><cell></cell><cell></cell><cell></cell></row><row><cell>segmentation terms when modeling aspect-aspect</cell><cell></cell><cell></cell><cell></cell></row><row><cell>relations; 4) Overall, global aspect graph performs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>better than adjacent aspect graph, which is cor-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>related with the results in Zhao et al. (2020); 5)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>In Restaurant dataset, adjacent aspect graph and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>global aspect graph show comparable performance.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Depth distribution of parsed constituent trees on four datasets. The maximums are in bold. The last row lists the max tree depth of each dataset.</figDesc><table><row><cell>Multi.</cell><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Aspect</cell><cell cols="4">Restaurant Laptop</cell><cell></cell><cell>MAMS</cell><cell></cell></row><row><cell cols="8">Distribution Train Test Train Test Train Valid Test</cell></row><row><cell>2</cell><cell cols="7">555 192 343 101 2568 285 264</cell></row><row><cell>3</cell><cell cols="7">261 73 137 33 1169 136 173</cell></row><row><cell>4</cell><cell cols="2">103 31</cell><cell>40</cell><cell cols="4">9 364 55 45</cell></row><row><cell>5</cell><cell>32</cell><cell>14</cell><cell>9</cell><cell cols="4">6 126 16 10</cell></row><row><cell>6</cell><cell>11</cell><cell>3</cell><cell>5</cell><cell>1</cell><cell>48</cell><cell>5</cell><cell>5</cell></row><row><cell>7</cell><cell>5</cell><cell>1</cell><cell>3</cell><cell>-</cell><cell>13</cell><cell>2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Multi.aspect distribution of three datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In Eq14, v aa t is set to zero in single-aspect sentence.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant No.<rs type="grantNumber">61602197</rs>, Grant No.<rs type="grantNumber">L1924068</rs>, Grant No.<rs type="grantNumber">61772076</rs>, in part by <rs type="funder">CCF-AFSG Research Fund</rs> under Grant No.<rs type="grantNumber">RF20210005</rs>, and in part by the <rs type="funder">fund of Joint Laboratory of HUST</rs> and <rs type="institution">Pingan Property &amp; Casualty Research (HPL)</rs>. The authors would also like to thank the anonymous reviewers for their comments on improving the quality of this paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5pWynTV">
					<idno type="grant-number">61602197</idno>
				</org>
				<org type="funding" xml:id="_TbSzVHP">
					<idno type="grant-number">L1924068</idno>
				</org>
				<org type="funding" xml:id="_yVEc2js">
					<idno type="grant-number">61772076</idno>
				</org>
				<org type="funding" xml:id="_CZUVNJN">
					<idno type="grant-number">RF20210005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Investigating typed syntactic dependencies for targeted sentiment classification using graph attention neural network</title>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="503" to="514" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inducing target-specific latent structures for aspect sentiment classification</title>
		<author>
			<persName><forename type="first">Chenhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5596" to="5607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent Twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-2009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. Open-Review</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-grained attention network for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Feifan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3433" to="3442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling inter-aspect dependencies for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangeshwar</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2043</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="266" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CAN: Constrained attention networks for multi-aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keke</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renhong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1467</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4601" to="4610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A challenge dataset and effective models for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Qingnan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1654</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6280" to="6285" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pone: A novel automatic evaluation metric for open-domain generative dialogue systems</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3423168</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Em-pDG: Multi-resolution interactive empathetic dialogue generation</title>
		<author>
			<persName><forename type="first">Qintong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.394</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4454" to="4466" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.494</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6319" to="6329" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cunxiang Yin, and Sheng hua Zhong. 2020b. Sentence constituent-aware aspect-category sentiment analysis with graph attention networks</title>
		<author>
			<persName><forename type="first">Yuncong</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NLPCC</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting coarse-to-fine task transfer for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33014253</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. January 27 -February 1, 2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="4253" to="4260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Jointly learning aspect-focused and inter-aspect relations with graph convolutional networks for aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongdi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="150" to="161" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">IARM: Inter-aspect relation modeling with memory networks in aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Shad</forename><surname>Akhtar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1377</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3402" to="3411" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Asif Ekbal</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<publisher>Maryland. Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)</title>
		<meeting>the 8th International Workshop on Semantic Evaluation (SemEval 2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reinforced history backtracking for conversational question answering</title>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cen-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attentional encoder network for targeted sentiment classification</title>
		<author>
			<persName><forename type="first">Youwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
		<idno>ArXiv, abs/1902.09314</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dependency graph enhanced dualtransformer structure for aspect-based sentiment classification</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiji</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6578" to="6588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relational graph attention network for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3229" to="3238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspectlevel sentiment classification</title>
		<author>
			<persName><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global context enhanced graph neural networks for session-based recommendation</title>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-07-25">2020. July 25-30, 2020</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
	<note>SIGIR 2020, Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Integrating community question and answer archives</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011-08-07">2011. August 7-11, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emotionaware chat machine: Automatic emotional response generation for human-like emotional interaction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchong</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3357937</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="1401" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Target-guided emotion-aware chat machine</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-entity aspect-based sentiment analysis with context, entity and aspect memory</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="6029" to="6036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CM-BERT: cross-modal BERT for text-audio sentiment analysis</title>
		<author>
			<persName><forename type="first">Kaicheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413690</idno>
	</analytic>
	<monogr>
		<title level="m">MM &apos;20: The 28th ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10-12">2020. October 12-16, 2020</date>
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment classification with aspectspecific graph convolutional networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1464</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4568" to="4578" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: A survey</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1253</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and accurate neural CRF constituency parsing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4046" to="4053" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Modeling sentiment dependencies with graph convolutional networks for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Pinlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ou</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Knowledge-Based Systems</publisher>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">105443</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-view intent disentangle graph networks for bundle recommendation</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<idno>abs/2202.11425</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
