<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Communication Breakdowns Between Families and Alexa</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erin</forename><surname>Beneteau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<address>
									<postCode>2019</postCode>
									<settlement>Glasgow, New York</settlement>
									<region>Scotland, UK. ACM, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Kientz</surname></persName>
							<email>jkientz@uw.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Human Centered Design and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<address>
									<postCode>2019</postCode>
									<settlement>Glasgow, New York</settlement>
									<region>Scotland, UK. ACM, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olivia</forename><forename type="middle">K</forename><surname>Richards</surname></persName>
							<email>orichards@psu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Pennsylvania State University University Park</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<address>
									<postCode>2019</postCode>
									<settlement>Glasgow, New York</settlement>
									<region>Scotland, UK. ACM, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Yip</surname></persName>
							<email>jcyip@uw.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingrui</forename><surname>Zhang</surname></persName>
							<email>mingrui@uw.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<address>
									<postCode>2019</postCode>
									<settlement>Glasgow, New York</settlement>
									<region>Scotland, UK. ACM, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Hiniker</surname></persName>
							<email>alexisr@uw.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Communication Breakdowns Between Families and Alexa</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CBE124163C28C401481B7D22B89CC11A</idno>
					<idno type="DOI">10.1145/3290605.3300473</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative interaction, Field studies, Empirical studies in HCI Digital Home Assistants</term>
					<term>Human Computer Interaction</term>
					<term>Joint Media Engagement</term>
					<term>Communication Repairs</term>
					<term>Families</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate how families repair communication breakdowns with digital home assistants. We recruited 10 diverse families to use an Amazon Echo Dot in their homes for four weeks. All families had at least one child between four and 17 years old. Each family participated in pre-and post-deployment interviews. Their interactions with the Echo Dot (Alexa) were audio recorded throughout the study. We analyzed 59 communication breakdown interactions between family members and Alexa, framing our analysis with concepts from HCI and speech-language pathology. Our findings indicate that family members collaborate using discourse scaffolding (supportive communication guidance) and a variety of speech and language modifications in their attempts to repair communication breakdowns with Alexa. Alexa's responses also influence the repair strategies that families use. Designers can relieve the communication repair burden that primarily rests with families by increasing digital home assistants' abilities to collaborate together with users to repair communication breakdowns.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The worlds of human-to-human communication and human-to-technology communication are quickly becoming blurred in everyday, family life. The increasing popularity of digital home assistants, like the Amazon Echo, and conversational assistants, like Siri, increase users' expectations of voice as an effective communication method with machines <ref type="bibr" target="#b22">[23]</ref>. However, humans must work to adapt their communication patterns to the needs of the machines, rather than machines adapting to humans <ref type="bibr" target="#b18">[19]</ref>. People shorten their sentences, use simplified language, and repeat themselves in attempts to be understood by voice interfaces <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>. As a result, users of voice interface technology become frustrated and can fail to learn the full capabilities of the technology, or abandon use altogether <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. Despite the difficulty with human-to-voice interface communication, the technology is becoming pervasive in family lives. Yarosh et. al found that 93% of the children they talked with at a state fair in Minnesota had used voice interface technology <ref type="bibr" target="#b42">[43]</ref>. Digital home assistants, which utilize voice interfaces, are anticipated to be present in 55% of American households by the year 2022 <ref type="bibr" target="#b31">[32]</ref>. Yet, the prevalence of the technology does not necessarily indicate the successful, ongoing use of the technology. True conversational capabilities have not yet been fully realized <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, and we want to better understand how human-technology "conversations" can be improved.</p><p>In our research, we blend two fields of study: humancomputer interaction (HCI) and speech-language pathology. HCI has a long history of research in how humans communicate with computers, robots, and other conversational agents <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref>. Speech language pathology has a long history of research on human-to-human communication development, communication disorders, and remediation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>. Drawing from these two fields, we aim to better understand the communication breakdowns and repair strategies used between families and digital home assistants, specifically, the Amazon Echo Dot (who we refer to as the conversational partner "Alexa").</p><p>To frame our study, we use the concepts of joint media engagement <ref type="bibr" target="#b39">[40]</ref>, conversational analysis <ref type="bibr" target="#b26">[27]</ref>, pragmatics <ref type="bibr" target="#b34">[35]</ref>, and discourse scaffolding <ref type="bibr" target="#b40">[41]</ref>. Building on these concepts, we explore the communicative relationships between digital home assistants and families. Our specific research questions are: 1) What types of communication breakdowns occur? 2) How do families repair communication breakdowns with Alexa? 3) How does Alexa repair communication breakdowns with family members?</p><p>We gave Amazon Echo Dots to 10 economically and ethnically diverse families to use in their homes for four weeks. None of the families had previously owned a digital home assistant. We conducted pre-and postdeployment interviews with all 10 families in their homes, and we captured audio of natural, unscripted family interactions with Alexa. Based on an analysis of 59 conversational interactions, we discuss three types of responses from Alexa that signal communication breakdowns and a taxonomy of five strategies that family members used to repair breakdowns. At times, Alexa initiated collaborative communication repair opportunities with her communication partner, but currently, the brunt of communication repair work lies with the family. Designers could use the analysis of communication repair types to create relevant and helpful responses for digital home assistants to use during communication breakdowns. We conclude with design suggestions that can improve digital home assistants' facilitation of conversational repairs with families.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK Human Communication and Communication Breakdowns</head><p>In the field of speech-language pathology, pragmatics incorporates the social aspects of communication, including turn taking, topic maintenance, socially appropriate speech and language use, and code switching <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. Code switching involves the recognition of different communicative expectations for different communication partners and/or different communication environments <ref type="bibr" target="#b7">[8]</ref>. The most explicit form of code switching is to switch languages to match the language of the communication partner, such as shifting from English to Spanish <ref type="bibr" target="#b12">[13]</ref>. A subtle form of code-switching, in which the rules and use of language are dependent on the communication context, is demonstrated when a teenager uses a different conversational tone and different vocabulary when talking to friends than when talking to a parent. Pragmatics, and specifically code switching, requires some understanding and intentional thought behind communication interactions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. As such, linguistic code switching often begins around ages 4-5, when children begin intentionally talking in different ways to different people, in different settings <ref type="bibr">[44]</ref>.</p><p>Communication repair refers to the work of restoring shared understanding after conversational partners misunderstand each other. Essentially, the person who is talking needs to rephrase or say something differently because the person they were talking to did not understand what they were saying <ref type="bibr" target="#b24">[25]</ref>. The ability to code switch can aid in conversational repair and can be thought of as the act of adjusting your speaking style to accommodate the listener <ref type="bibr" target="#b40">[41]</ref>. These adjustments to speaking style can take many forms and are dependent on the communicator's language abilities. For example, children often use repetition as their initial repair strategy when presented with "neutral" clarification responses, such as "huh," "what," and "I don't know" <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. Children ages nine and older might use contextual cues, such as defining terms, to repair a communication breakdown, whereas this can be more challenging for younger children <ref type="bibr" target="#b5">[6]</ref>. Adults may engage in discourse scaffolding to assist children in expanding or adjusting their speech <ref type="bibr" target="#b40">[41]</ref>. Discourse scaffolding is a mechanism to guide learning through communication strategies with the intention of gradually transferring skills and responsibility to the learner <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref>. At an early communication stage, an example of discourse scaffolding is when a child says, "cookie," and a parent responds with an expansion of the child's utterance: "you want more cookies."</p><p>Dynamics between multiple communication partners, especially those involving young children, can be heavily influenced by the communicative development ages of the communication partners and the context of the communication interaction <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>. Our earlier example of discourse scaffolding was appropriate for a very young child, whereas the communication interaction between a teenager and parent would look very different. For example, a parent might use discourse scaffolding through questioning to lead the teen to expand on their own language. The parent might ask, "What did you do after school?" and the teen might reply, "hung out with friends." The parent could then scaffold further elaboration with questions, such as, "where did you and your friends hang out?" While these examples highlight the nuances of communication interactions between family members of different ages and developmental stages, we find that the communication interaction can become even more complex when one of the communication partners is not human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human-Computer Communication</head><p>The field of HCI has a well-established body of literature on how humans verbally communicate with computers, robots, and other devices. In 1987, Suchman framed acts of human-machine interaction as a dialog between communication partners <ref type="bibr" target="#b38">[39]</ref>. From this perspective, the work of the designer is to enable human and machine collaboration towards a shared understanding through continuous acts of collaborative communication repair when breakdowns occur. Yet, despite this legacy, and improvements in human-computer communication techniques <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, research continues to demonstrate that humans adapt their communication styles and patterns to match the machine, both with robots <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref> and computers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>, rather than the other way around. Humans shorten their sentences <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref>, use repetition <ref type="bibr" target="#b0">[1]</ref>, increase volume <ref type="bibr" target="#b4">[5]</ref> and hyperarticulate <ref type="bibr" target="#b27">[28]</ref> as repair strategies. These modification strategies are motivated by a desire to achieve successful communication with computers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication Breakdowns with Voice Interfaces</head><p>In recent years, the field of HCI has begun to examine human communication with conversational agents, such as Siri, and with digital home assistants, such as the Amazon Echo <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. Despite the "conversational" interface with conversational agents, people are not yet able to talk to technology in the same way that they talk to other people <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>. Users of conversational assistants often need to shorten their queries to key words, since increased utterance length can increase the likelihood of speech recognition errors, both with conversational agents and with other humans <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. We also see young children using repetition as an initial conversational repair strategy when they are talking to a computer game <ref type="bibr" target="#b8">[9]</ref>. With current systems, the burden of ensuring a successful communication interaction with a conversational agent continues to fall to the human in the conversation, with little support from the conversational agent itself <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>The majority of research on communication breakdowns and repairs takes place in 1:1 interactions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>. One of the few studies that investigates how multiple people work together to repair communication breakdowns with a conversational agent observed friends meeting in a caf√©, using the conversational agents (such as Siri) on their phones <ref type="bibr" target="#b33">[34]</ref>. The researchers used conversational analysis as their approach, based on the principle of analyzing naturally occurring interactions with technology from the user's point of view <ref type="bibr" target="#b26">[27]</ref>. Their findings revealed that when a communication breakdown occurs with a conversational agent, multiple people attempt communication repairs, passing the phone from person to person. Our specific interest is how families repair communication breakdowns with digital home assistants in naturalistic use in their homes.</p><p>We can turn to literature on how technology is situated within family dynamics for reference, such as Takeuchi and Stevens documentation of "Joint Media Engagement" (JME) and the ways families make meaning around their technology experience <ref type="bibr" target="#b39">[40]</ref>. Family members, particularly parents, use media as shared learning opportunities, scaffolding communication around the media experience <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>. Comparatively little work has examined children's and families' interactions with digital assistants. Yarosh and colleagues found that children struggled to formulate queries to a voice interface <ref type="bibr" target="#b42">[43]</ref> and Cheng and colleagues define a taxonomy of communication repair strategies that preschoolers use when talking to a voice-driven game <ref type="bibr" target="#b8">[9]</ref>. Porcheron et al. report on a study of five families using the Amazon Echo in their homes, and they describe how a family of two adults and two children take turns in attempting to repair a communication breakdown with their Amazon Echo <ref type="bibr" target="#b32">[33]</ref>. The authors conclude that the device's responses to the communication breakdown are ineffective in supporting the participants in identifying the cause of the breakdown.</p><p>In this study, we expand the body of work on communication repair strategies with conversational agents and digital home assistants. We specifically explore how diverse families attempt to repair communication breakdowns with the conversational partner Alexa (their Echo Dot) through a close analysis of conversational interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We conducted a four-week field study with 10 families, audio recording their everyday interactions with the Amazon Echo Dot (second generation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited 10 diverse families to use the Echo Dot for a period of four weeks, intentionally recruiting families who represent a wide-spectrum of family life. Families selfidentified as never having owned a digital home assistant.</p><p>Families also reported having a total household income at or below the median for the county in which the study was conducted. Families ranged from families of two to families of five (see Table <ref type="table" target="#tab_0">1</ref>). All families had at least one child between the ages of four and 17 living in the home. Two families were bilingual, in which all family members spoke both English and Spanish in their homes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>We collected audio data throughout the four-week period, using a custom-built audio sampling system designed to record interactions every time the trigger word "Alexa" was spoken. We also conducted pre-and post-deployment interviews with families in their homes at the start of the deployment period and at the conclusion of the four weeks. We compensated families with US$100 in gift cards for completing the study, and they were able to keep their Echo Dot after the conclusion of the study. The study was approved by our university's Institutional Review Board. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Capture</head><p>To capture audio recordings of participants' interactions with the Echo Dot, we created an audio buffer system inspired by the open source code from Porcheron et al. <ref type="bibr" target="#b32">[33]</ref>. The software captures audio when the trigger word "Alexa" is spoken; it saves one minute of audio prior to the trigger word, along with three additional minutes during and after the trigger word is spoken. It then pushes a total of four minutes of audio to a secure server.</p><p>For our study, we deployed the audio recording system on a Samsung tablet computer that families placed near the Echo Dot in each of their homes. We instructed families to keep the tablet plugged in and not to use the tablet for anything other than for the study's audio recording. The recording system also had both a "record a thought" button and a "delete" button available for families to leave explicit comments for the researchers or to delete the audio capture files, based on the desires of the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>We used an inductive process to analyze the audio capture conversational samples <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>. We began with memoing and open coding during the initial transcriptions of the audio capture files. Through memoing and open coding, we noticed an emerging theme related to communication breakdowns. We then began coding communication breakdowns from transcripts of each of the 10 families, developing and revising codes as we found additional examples of communication breakdowns, reviewing a total of 14.5 hours of audio capture. We continued this process until communication breakdown codes were stable and applicable to multiple families. Once codes were stable, we reviewed transcripts from each of the 10 families for communication breakdowns again. We included communication breakdowns from each family in our corpus of 59 communication breakdown interactions, systematically going through each individual family's transcript and pulling out communication breakdowns for each code (when present). If a family had multiple communication breakdowns for a particular code type, we reviewed the instances and chose no more than three instances of that code type for deeper analysis. For example, Family C had multiple instances of adjusting prosody; we chose three instances that we felt were most representative to be included in the corpus for deeper analysis.</p><p>For our final analysis on communication breakdowns and repairs, a total of 59 conversational interactions falling under the broad themes of communication breakdowns and repairs were deeply analyzed by two researchers. Communication breakdowns were defined as interactions between family members and Alexa which did not result in an appropriate response from Alexa to the human communication partner. We drew on the HCI conversational analysis approach to analyze family conversations set in natural environments, with a focus on the user's experiences. The use of conversational analysis in HCI aims to highlight opportunities for designers to improve interactions between humans and computers by detailing the structure of interactions from the user's perspective <ref type="bibr" target="#b26">[27]</ref>.</p><p>The first author, a speech-language pathologist (SLP), used speech-language constructs when analyzing communication breakdowns and repairs. Constructs are defined in our findings below. We also consulted a practicing SLP on the codes and themes related to family communication repair strategies. The practicing SLP validated the communication repair strategies identified and validated the repair types used in the conversational samples discussed in this paper. For additional validation, we consulted a second SLP, also a researcher, who validated the repair strategy definitions used.</p><p>During our analysis, we discovered that family members were not the only communication partners to attempt communication repairs in conversations. Alexa also engaged in communication repair attempts, and therefore we have included an analysis of Alexa's communication repairs in the findings, in addition to family communication repairs.</p><p>We also want to explicitly state that in our findings and discussion, we refer to Alexa in a personified form. We do this for two reasons: 1) Alexa is a communication partner in the conversational interactions and is most easily identified by name and personal pronoun when discussing the complex interaction between multiple conversation partners. 2) We reflect our participant families' language describing Alexa, based on our interviews and audio capture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>All 10 of our families experienced communication breakdowns with Alexa. To illustrate our findings, we discuss three distinct conversational samples from three different families in which we highlight the complex communication dynamics between multiple family members and Alexa. Before our discussion of these rich conversational samples, we provide an explanation of the constructs we use to describe the communication interactions between family members and Alexa (shown in Tables <ref type="table" target="#tab_1">2</ref><ref type="table" target="#tab_2">3</ref><ref type="table" target="#tab_3">4</ref>).</p><p>Our families used a variety of specific speech and language repair strategies, which we categorize under the broader construct of "communication repair types." Communication repair types are listed in Table <ref type="table" target="#tab_1">2</ref>, with quotes from our participant families used as examples.</p><p>In addition to specific repair strategies, we define a variety of discourse scaffolds between family members that occur during collaboration, adapted from the fields of education and speech-language pathology <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref>. These scaffolds occur both between family members and between family members and Alexa. The discourse scaffolds are listed and defined in Table <ref type="table" target="#tab_2">3</ref>. Examples of discourse scaffolds are described when we discuss the three specific family communication breakdowns.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Family Collaboration on Repair Strategies</head><p>We explored the variety of communication repair strategies that family members use with Alexa, within the framing of family collaboration and joint media engagement.</p><p>We In this interaction, the son first asks his parents how to make popsicles, asking for consultation without the use of media. When he does not get the answer he is looking for, he changes communication partners and asks Alexa. At this point, the family as a unit is now engaged with media, with Alexa as a consultant. We see throughout the interaction that all family members are listening to the conversational interactions with Alexa, as evidenced by the subsequent communication by family members related to Alexa's responses.</p><p>We know the son wants generalized information based on his initial word choice, "what are popsicles made of." The use of a plural and the lack of a specific type of popsicle indicates that he is interested in popsicles in general instead of a specific popsicle type.</p><p>Alexa responds on the correct topic-popsicles -but provides too specific of a response that deviates from the son's intended topic. When the communication breakdown occurs, the son's mother actively engages in the interaction. However, her initial communication is with her son rather than with Alexa. She uses the communication breakdown as a learning opportunity to educate her son on improving his success in obtaining information from Alexa. The mother provides discourse scaffolding in the form of direct instruction. She instructs her son, given her interpretation of the source of the communication breakdown with Alexa: his poor articulation in which Alexa misheard the word "s'mores." She then provides further discourse scaffolding in the form of modeling by engaging with Alexa directly in a communication repair.</p><p>The mother makes the first repair attempt by adjusting the syntax of the question from, "what are popsicles made of" to, "how do you make a popsicle." As a result, the communication breakdown is somewhat repaired, and Alexa responds both on-topic and with specific information relevant to the question. However, Alexa continues to provide information for a specific type of popsicle rather than a generalized recipe for any popsicle. At this point, Alexa assists her communication partners by refining her response and providing a summary of the popsicle recipe. Alexa then models two possible communication responses to her communication partner: "ask for more information" or "say next." By providing choices, Alexa is performing discourse scaffolding of her own and promoting turn-taking in the communication interaction through a specific clarification response. The specific clarification further refines her response to more closely match the needs of her communication partner.</p><p>The son, the original instigator of the communication interaction, resumes his role in the interaction with Alexa and responds to Alexa with "next," one of the communicative options that Alexa provided. However, it is clear that both the mother and father are engaged with the interaction and have had enough of Alexa's responses. The mother redirects the conversation by stopping Alexa. The parents take on the roles of communication partners with their son. The family maintains the topic of conversation but abandons Alexa as a communication partner for the remainder of the conversational topic. The parents resume the discourse scaffolding that the mother started earlier and lead their son to answering his own question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication Breakdown 2: Family B</head><p>The next conversational interaction is with Family B, which begins in a similar way to Communication Breakdown 1 in Family E. One family member first consults other family members for information, but when other family members are not able to provide the information, the family turns to technology for the answer. This interaction illustrates multiple types of communication repairs that families in our study used: syntax adjustments, semantics adjustments, changes in volume, changes in prosody, and collaboration with family members. To begin, the mother consults other family members for information, and when they are not able to provide that information, the father suggests the family's go-to information resource: Google. By doing this, the father immediately provides discourse scaffolding and shifts the family's focus to include media. The mother decides to try Alexa first, essentially shifting the family's media attention from Google as a resource to Alexa. The mother's question to Alexa closely follows the syntactical model suggested by the father for the Google search, with an expansion to clarify the last key term "Southeast Asian" vs "South." This form of question is essentially an either/or type of question, in which the requester provides two options and is looking for a response that fits one of those options. Alexa's response, instead, is a neutral response, "Sorry, I'm not sure," which does not provide her communication partner with any specific information on the cause of the communication breakdown.</p><p>The daughter, originally asked as a primary information resource, actively takes part in the conversation with Alexa and takes a turn at repairing the communication breakdown. She uses her mother's query as a foundation for her interaction. The daughter employs contraction by making a syntactical change: deleting the words in the query and thus changing the nature of the question to a yes/no question ("Are Koreans Southeast Asian?"). Alexa's response to this question is an extreme version of a neutral response, that of a beep and silence. The family conversation briefly comes to a halt with this response from Alexa, causing a second, and perhaps more dramatic communication breakdown.</p><p>After the brief silence, the mother encourages her daughter to try again through direct instruction. At this point, both mother and daughter are actively engaged in jointly repairing the communication breakdown. The daughter employs repetition with increased volume in this conversational repair attempt (the third total conversational repair attempt for the family as a unit). Alexa responds with another neutral response, "I'm not quite sure how to help you with that." At this point, the mother resumes her original role in communicating with Alexa and attempts the fourth repair. She modifies her daughter's interaction through a combination of expansion and contraction. Utilizing a dramatic syntactic change, she breaks up her original either/or question into two closed ended questions, asking both "who" and "what" questions.</p><p>By now, we have seen the original question modified five times by three family members. Each time the question is modified in relation to the prior family member's version of the question. Up to this point, Alexa has provided neutral responses, which resulted in the family collaborating to make a wide variety of repair types. Without any specific responses or actions from Alexa, the family did not have any signposts to guide their repair strategies.</p><p>In response to the mother's two closed ended questions, Alexa acts on a misunderstanding. After the fourth repair attempt is unsuccessful, the daughter makes a noise of frustration and at this point the father actively joins the interaction with Alexa for the first time. His interaction indicates that he has been listening to the prior communication attempts between his family and Alexa but has waited to take his turn at communication repair. The father's communication repair employs another syntactic change, as well as increasing his volume. He shifts the question to a specific, closed-ended request: "Please define East Asian countries." He also redirects Alexa from an off-topic response (countries in Eastern Europe) back to the specific topic the family is interested in.</p><p>All three family members collaborated to achieve a successful communication interaction with Alexa. Each family member listened to the interaction between Alexa and other family members and adjusted their communication interaction based on the prior interactions. At the end, after providing the desired information, Alexa instigates turn-taking with the family by asking them a question "did that answer your question?" The mother, the initiator of the communication interaction, responds to Alexa's question. Alexa responds back to the mother with a statement that concludes the communication interaction on this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication Breakdown 3: Family D</head><p>Our final collaborative communication repair example is from one of our two bilingual families. We note that this final communication breakdown example is representative of two key themes found across our participant families: children's speech and language development impacting communication with Alexa (captured in multiple instances, but primarily with monolingual children) and Alexa struggling to understand bilingual individuals (also captured in multiple instances, but primarily with adults). Both of these themes were reinforced by family comments during our final interviews. The communication breakdown described below is the only communication breakdown that combined these two themes in one example.</p><p>This conversational interaction is from Family D, a family that speaks both English and Spanish in their home. In this example, the family has set up their Echo Dot for the first time and have just completed the guided setup through the Alexa app with the researchers in the room. The mother encourages their 5-year-old to ask a question, but the 5-year-old has trouble with a consistent production of the word "Alexa." (Mother): It's A (pause) lexa (emphasis on the last two syllables) (Child): uh (slight pause) leh (pause) ska. Is it going to rain for a little bit or is it going to be sunny for a little bit. (said quickly and quietly), (pause) or both. (no rising intonation at end to indicate a question) (pause and no response from Alexa) (Researcher): That was a good question but it might have been a little too long, too many questions I heard. (Mother): Let's see. (pause) Alexa, is it going to be rainy all day or sunny all day or . . . (recording cut off). (recording resumes with child giggling) (Mother): Alexa, what did I ask you? (Alexa): Sorry, I'm not sure (everyone laughs) (Mother): Alexa, I asked if it was going to rain all day. (Alexa): Probably not. Each day of the next 7 days . . . has at most a 30% chance of rain. (child laughs) Joint engagement with Alexa is clear for the entirety of the communication interaction, evidenced by the communication partners' reactions to all interactions with Alexa. Initially, the child's mother provides direct instruction to her child on improving the production of the word "Alexa." We found that many of the young children in our study struggled with the speech production necessary to make themselves consistently understood by Alexa. Developmentally, children at age 5 can be expected to have difficulty with the consistent production of some sounds, including the /l/ sound and /s/ sound, which are both part of the word "Alexa" <ref type="bibr">[44]</ref>. Typically developing young bilingual children can be expected to make sound substitutions from one language to another <ref type="bibr" target="#b14">[15]</ref>. We see evidence of some of these developmental speech patterns with the child in Family D in our conversational sample described earlier.</p><p>Another theme we find in this example is the linguistic construction of the question the child asks Alexa: "Is it going to rain for a little bit or is it going to be sunny for a little bit, or both?" The syntax of the question is complex, asking Alexa three different questions about the weather grouped into one. We also see that the child's use of language does not match Alexa's abilities. This is the first time this child has used a digital home assistant, and the child is most likely unaware of the need to code-switch. Therefore, the researcher provides direct instruction and suggests the child might have asked too many questions at once.</p><p>The mother then modifies her child's question, by adjusting the syntax and semantics, to attempt her communication repair. However, Alexa does not respond. The mother then makes another attempt at communication repair by asking Alexa, "what did I ask you?" Alexa provides a neutral clarification response. Joint engagement in the interaction is demonstrated by everyone laughing at Alexa's response.</p><p>In the final communication repair, the mother significantly modifies both the syntax and semantics of the question to a contracted yes/no question. Alexa then responds appropriately to the question, which ends that specific communication interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pragmatics: Code Switching with Alexa</head><p>Although we find that most family members in our study are able to adapt communication interactions with Alexa, as evidenced through the multiple types of repair strategies used, not everyone was successful in this process. In particular, children under the age of 5 tended to struggle more than older children and adults. For example, the parents of Family I report that one of their younger children (age 4) would walk up to the Echo Dot and begin talking to Alexa without first addressing her by name. The child would use long sentences and often change topic before Alexa responded. His parents felt that his communication with Alexa was similar to his communication with other people. The parents believed that their son expected Alexa to respond to him like other people do, and they reported that he became frustrated when she did not. In contrast, their older child (age 6) would say Alexa's name and then pause before continuing his communication interaction with her.</p><p>It was not only young children that had a difficult time code switching. We find that Alexa did not code switch with family members to craft her communication interactions to the needs and abilities of the person she was addressing, as evidenced most clearly through the jokes she told. For example, the child in Family J asked Alexa to tell jokes throughout the four-week deployment. She responded with a variety of jokes, such as: "A man walks into a bar. Crank. It was a heavy metal bar." During our final interview with Family J, we asked the mother and child if the jokes Alexa told were appropriate for the child (age 4). The mother replied "Yeah, there were a couple that [child's name] chuckled at but I don't think she (pause and mother directs question to child) 'do you know why the jokes were funny? Did they make sense to you?'" The child's response was "No, I don't know why."</p><p>Alexa's lack of code switching is not only evident with children. We find that Alexa is unable to code switch with multilingual users. For example, when the mother in Family D says, "Alexa, play song by Marco Antonio Solis," producing the singer's name using Spanish phonological processes instead of an English phonetic version of the singer's name. Alexa's response is "'Unbelieve,' by Andy Groun." In this case, Alexa misheard or misunderstood the request because of her inability to code switch to recognize that the requester was using the correct pronunciation for words in another language.</p><p>We can think of Alexa as being very literal with communication interactions, which implies a lack of understanding of the social nuances and uses of language, essentially, a deficit in pragmatic skills. An excellent example of Alexa's poor pragmatic skills is with Family A: Alexa did not understand that the pause in this case was in anticipation of continuing the request. In addition, Alexa did not understand that the semantic modification, from "add" to "calculate" was intended to provide a definition of the earlier request of the word "add." Multiple meanings of words are a challenge for Alexa, and for any communication partner that does not understand the context of the communication interaction. Finally, Alexa failed to recognize the contextual cue that "157" was a number and therefore the correct interpretation of "add" is to calculate. A final example of Alexa's inability to code switch is when the child in Family J asks Alexa to count to 10 in Spanish. Alexa responds by translating the word "count" from English to Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Currently, the burden of repairing communication breakdowns with Alexa lies with family members.</p><p>Repairing communication breakdowns with Alexa can take a great deal of effort on the part of multiple family members, and in some cases, might lead to abandonment. Our findings reinforce Suchman's suggestion from 1987: designers of machines should focus on designing for collaboration between the user and the machine to achieve mutual understanding <ref type="bibr" target="#b38">[39]</ref>. To alleviate the burden on family members, and to increase chances of successful conversational repairs, we recommend that digital home assistants are designed with a focus on improving communication repair strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Digital Code Switching for Human Interaction</head><p>Understanding the communicative context and the communication partners involved in a conversation incorporates the field of pragmatics <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> Other work in the area of conversational interfaces has made similar suggestions for conversational agents to provide cues and prompts to collaborate with their human conversational partners <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>. Here, we further suggest that the constructs of discourse scaffolding and code switching could be productively applied to the design of digital assistants. Discourse scaffolding can be applied to any conversational partner (adult or child), and through code switching, customized to provide the level of support that is appropriate for the specific conversational partner. Our close analysis of conversational breakdowns and repairs with family members provides an initial guide for developers to design conversational strategies which incorporate code switching and discourse scaffolding.</p><p>We realize that the application of code switching may inadvertently create new miscommunications between Alexa and her communication partners. Alexa might misidentify an adult using repetition and talking while chewing gum as following the same patterns as a young child. However, the use of specific clarification responses provides more signposts for communication repairs than neutral clarification responses. If Alexa makes a mistake, it is easier for her human communication partner to identify the cause and make the repair with specific responses. Ideally, the responses from her human communication partner allow Alexa to learn her communication partner's speech patterns over time, making code switching easier, and increasing the accuracy of communication responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Promoting Family Engagement and Communication</head><p>Based on our findings, we see how the concept of joint media engagement <ref type="bibr" target="#b39">[40]</ref> is useful for designers to consider when creating applications and skills for digital home assistants. Due to the conversational nature of the interaction, other people in the immediate environment listen to and engage in communication with digital home assistants when there is a communication breakdown, both at home and in other social settings <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Designers of digital home assistants can capitalize on this knowledge and engage families in positive, joint interactive experiences, even when only one family member initially engages with the digital home assistant. We saw from our participant families that some family members did not engage immediately with Alexa until other family members had engaged in multiple interaction attempts. For example, when designing an Alexa skill for multiple players, such as a game or competition, family members might be encouraged to join in midway through: "Is there anyone else who can jump in and help?"</p><p>Utilizing the concept of joint media engagement, digital home assistants can simultaneously build their own pragmatic skills while assisting family members with their own conversational skills. If the digital home assistant is transparent about the nature of the communication breakdown, it can increase family members' own awareness of their communication skills. In this way, the digital home assistant can model meta-pragmatic skills, in which people talk about their own social use of language and their understanding of the social use of language <ref type="bibr" target="#b40">[41]</ref>. Digital home assistants have the potential to become communication facilitation tools for communication both between family members and between family members and the digital home assistant.</p><p>Based on our analysis of diverse families' communication breakdowns and repairs with Alexa, we find many opportunities to support the growth of Alexa's communication skills. We particularly want to emphasize the potential for digital home assistants to assist their human communication partners in repairing communication breakdowns. Our interactions with technology have radically changed since Suchman's work in 1987, however, the hurdle of overcoming communication breakdowns remains the same <ref type="bibr" target="#b38">[39]</ref>. Voice interfaces provide a unique opportunity for technology to take on the burden of developing a shared understanding with their human communication partners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The corpus of communication interactions analyzed for this study is a subset of the total communication interactions captured and likely excludes additional examples of communication breakdowns and repairs. Our findings are also limited in that some families expressed an awareness of needing to "use" the Echo Dot during the study, and this may have perpetuated their attempts to repair communication breakdowns with Alexa. To mitigate that effect, our corpus includes communication breakdown segments that occurred throughout the fourweek study period. Our sample is also limited to 10 families in one area of the United States. Future work remains to conduct a larger scale analysis of families in different regions with a broader range of language and dialectical differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>The burden of repairing communication breakdowns between humans and technology continues to rest with humans. Our findings highlight how families collaborate in a variety of ways to repair communication breakdowns with digital home assistants, often requiring multiple repair attempts with more than one family member trying to successfully communicate with Alexa (the conversational name used with the Echo Dot). While Alexa makes some attempts to repair communication breakdowns through specific clarification responses, neutral clarification responses are far more common, which do not aid in helping the human communication partner repair the breakdown. We turn back to 1987 and Suchman's guidance that designers focus on improving how technology can promote shared understanding with their human communication partners. Rather than trying to anticipate a user's needs, the digital home assistant can promote conversational collaboration through improvements in repairing communication breakdowns. To this end, we suggest that designers incorporate specific processes which promote the artificial pragmatic skills of digital home assistants. Improving technology's ability to identify their human communication partners and to provide specific clarification responses will ultimately improve the ability for humans and machines to collaborate towards shared understanding in their conversational interactions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>three different families, and use speech and language constructs to analyze the specific repair strategies used in detail. Communication Breakdown 1: Family E Our first communication breakdown occurs in Family E, who are talking while music is playing on their Echo Dot. The topic of conversation is about popsicles, a flavored ice treat. The son (age 8) initiates communication with Alexa. Alexa responds by acting on a misunderstanding. The mother collaborates with her son in repairing the communication breakdown, to which Alexa responds with specific clarification responses. The family ultimately carries on the conversational topic without Alexa. (Son): Can I ask a question? (Mother): What? (Father): What's the question? (Son): Um (pause) what are popsicles made of? (Father): Why would I tell you that? (Son): Alexa, (pause and music stops) what are popsicles made of? (Alexa): Three, six servings of s'mores popsicles are, you'll need two cups cold milk, one . . . (Alexa continues to talk) (Mother, talking over Alexa): She thinks it's s'mores popsicles because you didn't articulate. (Mother, in louder voice): Alexa, (pause and Alexa stops talking) how do you make a popsicle? (Alexa): Okay, for popsicles, I recommend a top recipe called sweet pink popsicle orange summers, which take eight hours and five minutes to make. (Mother talking over Alexa): Oh my god. (Alexa): You can ask for more information, or for more recipes, say next. (Son): Next (Mother): Alexa, stop. Resume music. (Father): Hey. You're a smart kid. What do you think popsicles are made of? (Son): Ice? (Father): No. (Mother): How do you make ice? (Father): How do you make it? (Son): Water. (Mother): Plus what? (Son): Plus steam! No, plus cold. (conversation continues between parents and child about popsicles)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Mother): Alexa, add 157 (slight pause) (Alexa): I added 157 to your shopping list. (Mother): Alexa, calculate 157 plus 50 (pause) (Alexa): Sorry, I didn't catch that.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Total family members, ages, and ethnicity of participant families.</figDesc><table><row><cell>Family</cell><cell cols="2">N Adult</cell><cell>Adults</cell><cell>Child</cell><cell>Ethnicity</cell></row><row><cell>ID</cell><cell></cell><cell>N</cell><cell>age</cell><cell>ages</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ranges</cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>4</cell><cell>3</cell><cell>41-68+</cell><cell>13</cell><cell>Asian</cell></row><row><cell>B</cell><cell>4</cell><cell>3</cell><cell>18-55</cell><cell>12</cell><cell>White/Asian</cell></row><row><cell>C</cell><cell>2</cell><cell>1</cell><cell>41-55</cell><cell>9</cell><cell>White</cell></row><row><cell>D</cell><cell>4</cell><cell>2</cell><cell>26-55</cell><cell>3, 5</cell><cell>Hispanic</cell></row><row><cell>E</cell><cell>5</cell><cell>3</cell><cell>26-67</cell><cell>4, 8</cell><cell>White</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Speech and language repair strategies used by family members.Our final construct is that of Alexa's responses and how they signal communication breakdowns. We encountered three types of responses from Alexa that indicate a communication breakdown. Examples from our audio capture of Alexa's responses are listed in Table4.</figDesc><table><row><cell>Repair Type</cell><cell>Definition</cell><cell>Example</cell></row><row><cell>Prosodic</cell><cell>Adjustments to</cell><cell>"Alexa . . what is . . the</cell></row><row><cell>changes</cell><cell>the rhythm or</cell><cell>. . temperature?" [each</cell></row><row><cell></cell><cell>cadence of</cell><cell>individual word</cell></row><row><cell></cell><cell>speech, including</cell><cell>pronounced slowly</cell></row><row><cell></cell><cell>pausing and the</cell><cell>and clearly instead of</cell></row><row><cell></cell><cell>rate of speech</cell><cell>in a conversational</cell></row><row><cell></cell><cell></cell><cell>manner]-Family C</cell></row><row><cell>Overarticul-</cell><cell>Exaggerating</cell><cell>"Alexa, play 'Make it</cell></row><row><cell>ation</cell><cell>sounds, also</cell><cell>Rain' by Dack steN"</cell></row><row><cell></cell><cell>referred to as</cell><cell>[emphasis and</cell></row><row><cell></cell><cell>hyperarticul-</cell><cell>prolongation of final</cell></row><row><cell></cell><cell>ation [22]</cell><cell>consonant]-Family C</cell></row><row><cell>Semantic</cell><cell>Modifying the</cell><cell>"Alexa, play little kid</cell></row><row><cell>adjustments</cell><cell>meaning of a</cell><cell>music."</cell></row><row><cell>and</cell><cell>word or sentence,</cell><cell>(Alexa): I couldn't find</cell></row><row><cell>modification</cell><cell>including</cell><cell>any little kid songs.</cell></row><row><cell></cell><cell>providing "cues,"</cell><cell>"Alexa, play kids</cell></row><row><cell></cell><cell>such as defining</cell><cell>songs."-Family G</cell></row><row><cell></cell><cell>a word [6]</cell><cell></cell></row><row><cell>Increased</cell><cell>The speaker</cell><cell>"Alexa stop."</cell></row><row><cell>volume</cell><cell>raises their voice</cell><cell>"Alexa stop!" [louder] -</cell></row><row><cell></cell><cell>specifically for</cell><cell>Family D</cell></row><row><cell></cell><cell>the interaction</cell><cell></cell></row><row><cell></cell><cell>with Alexa</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Discourse scaffolds families used to support one another in reformulating their communication with Alexa.</figDesc><table><row><cell cols="2">Discourse Scaffold Definition</cell></row><row><cell>Direct instruction</cell><cell>Telling a family member what they</cell></row><row><cell></cell><cell>should say or why something has</cell></row><row><cell></cell><cell>happened</cell></row><row><cell></cell><cell>Producing an utterance to demonstrate</cell></row><row><cell></cell><cell>the desired response</cell></row><row><cell>Redirection</cell><cell>Refocusing the conversation on a desired</cell></row><row><cell></cell><cell>topic</cell></row><row><cell>Expansion</cell><cell>Adding on to something said by</cell></row><row><cell></cell><cell>someone else</cell></row><row><cell>Contraction</cell><cell>Shortening something that has been said</cell></row><row><cell>Consulting</cell><cell>Family members asking others for</cell></row><row><cell></cell><cell>assistance or information</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Signals from Alexa that a communication breakdown has occurred.</figDesc><table><row><cell>Response type</cell><cell>Definition</cell><cell>Example</cell></row><row><cell>Acting on</cell><cell>Performing an</cell><cell>"Alexa, what</cell></row><row><cell>Misunderstanding</cell><cell>action or</cell><cell>should we do this</cell></row><row><cell>(AoM)</cell><cell>providing a</cell><cell>night?"</cell></row><row><cell></cell><cell>response based on</cell><cell>Alexa responds</cell></row><row><cell></cell><cell>misheard or</cell><cell>with a definition</cell></row><row><cell></cell><cell>misunderstood</cell><cell>of "this night."</cell></row><row><cell></cell><cell>input</cell><cell></cell></row><row><cell>Neutral</cell><cell>Providing an</cell><cell>"Sorry, I don't</cell></row><row><cell>Clarification</cell><cell>indication that the</cell><cell>know that."</cell></row><row><cell>Response (NR)</cell><cell>communication</cell><cell>"Sorry, I'm not</cell></row><row><cell></cell><cell>partner's</cell><cell>sure."</cell></row><row><cell></cell><cell>interaction was</cell><cell>"I'm not quite sure</cell></row><row><cell></cell><cell>unclear</cell><cell>how to help you</cell></row><row><cell></cell><cell></cell><cell>with that."</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was funded, in part, by a grant from Mozilla. We also want to acknowledge our appreciation of the speechlanguage pathologists who we consulted for validity checks of our coding for communication repair types: Teresa Fleck, a practicing SLP of 20 years, and Victoria Lai, an SLP and researcher. We also appreciate the transcription and administration support of Bella Chiu, Yini Guan, Leanne Liu, Sharon Heung, Ashley Boone and Kate Yen. Finally, we sincerely thank the participant families, without whom this research would not be possible.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Interaction with an Animated Agent in a Spoken Dialogue System</title>
		<author>
			<persName><forename type="first">Linda</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Gustafson</surname></persName>
		</author>
		<ptr target="https://www.speech.kth.se/august/eur99_augdata.html" />
		<imprint>
			<date>September 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling the cost of misunderstanding errors in the CMU Communicator dialog system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Rudnicky</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU.2001.1034635</idno>
		<ptr target="https://doi.org/10.1109/ASRU.2001.1034635" />
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="252" to="255" />
		</imprint>
	</monogr>
	<note>ASRU &apos;01</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open-world dialog: Challenges, directions, and prototype</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI&apos;2009 Workshop on Knowledge and Reasoning in Practical Dialogue Systems</title>
		<meeting>IJCAI&apos;2009 Workshop on Knowledge and Reasoning in Practical Dialogue Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiparty Turn Taking in Situated Dialog: Study, Lessons, and Directions</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2132890.2132903" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2011 Conference (SIGDIAL &apos;11)</title>
		<meeting>the SIGDIAL 2011 Conference (SIGDIAL &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="98" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linguistic alignment between people and computers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Branigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">F</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><surname>Mclean</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.pragma.2009.12.012</idno>
		<ptr target="https://doi.org/10.1016/j.pragma.2009.12.012" />
	</analytic>
	<monogr>
		<title level="j">J. Pragmat</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2355" to="2368" />
			<date type="published" when="2010-09">2010. September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Development of conversational repair strategies in response to requests for clarification</title>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Brinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Fujiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><forename type="middle">Frome</forename><surname>Loeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erika</forename><surname>Winkler</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshr.2901.75</idno>
		<ptr target="https://doi.org/10.1044/jshr.2901.75" />
	</analytic>
	<monogr>
		<title level="j">J. Speech Hear. Res</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="81" />
			<date type="published" when="1986-03">1986. March 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Antony</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Charmaz</surname></persName>
		</author>
		<title level="m">The SAGE handbook of grounded theory</title>
		<meeting><address><addrLine>London; London</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">Li-Rong Lilly</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1044/leader.FTR2.12072007.8</idno>
		<ptr target="https://doi.org/10.1044/leader.FTR2.12072007.8" />
	</analytic>
	<monogr>
		<title level="m">Codes and Contexts: Exploring Linguistic, Cultural, and Social Intelligence</title>
		<imprint>
			<date type="published" when="2007-05">2007. May 2007</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="8" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why Doesn&apos;T It Work?: Voice-driven Interfaces and Young Children&apos;s Communication Repair Strategies</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Hiniker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3202185.3202749</idno>
		<ptr target="https://doi.org/10.1145/3202185.3202749" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Interaction Design and Children (IDC &apos;18)</title>
		<meeting>the 17th ACM Conference on Interaction Design and Children (IDC &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="337" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What Can I Help You with?&quot;: Infrequent Users&apos; Experiences of Intelligent Personal Assistants</title>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">R</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Pantidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Coyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Morrissey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Al-Shehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Earley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Bandeira</surname></persName>
		</author>
		<idno type="DOI">10.1145/3098279.3098539</idno>
		<ptr target="https://doi.org/10.1145/3098279.3098539" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &apos;17)</title>
		<meeting>the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Creswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Qualitative inquiry &amp; research design: choosing among five approaches</title>
		<meeting><address><addrLine>Thousand Oaks, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Fourth edi ed.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hey Google is It OK if I Eat You?&quot;: Initial Explorations in Child-Agent Interaction</title>
		<author>
			<persName><forename type="first">Stefania</forename><surname>Druga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randi</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchel</forename><surname>Resnick</surname></persName>
		</author>
		<idno type="DOI">10.1145/3078072.3084330</idno>
		<ptr target="https://doi.org/10.1145/3078072.3084330" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Interaction Design and Children (IDC &apos;17)</title>
		<meeting>the 2017 Conference on Interaction Design and Children (IDC &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="595" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Code-Switching in Highly Proficient Spanish/English Bilingual Adults: Impact on Masked Word Recognition</title>
		<author>
			<persName><forename type="first">Paula</forename><forename type="middle">B</forename><surname>Garc√≠a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Leibold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Buss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Calandruccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Rodriguez</surname></persName>
		</author>
		<idno type="DOI">10.1044/2018_JSLHR-H-17-0399</idno>
		<ptr target="https://doi.org/10.1044/2018_JSLHR-H-17-0399" />
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018-08">2018. August 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Requests and responses in children&apos;s speech*</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Garvey</surname></persName>
		</author>
		<idno type="DOI">10.1017/S030500090000088X</idno>
		<ptr target="https://doi.org/10.1017/S030500090000088X" />
	</analytic>
	<monogr>
		<title level="j">J. Child Lang</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="41" to="63" />
			<date type="published" when="1975-04">1975. April 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">E</forename><surname>Gildersleeve-Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">S</forename><surname>Kester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">D</forename><surname>Pe√±a</surname></persName>
		</author>
		<idno type="DOI">10.1044/0161-1461</idno>
		<ptr target="https://doi.org/10.1044/0161-1461" />
	</analytic>
	<monogr>
		<title level="m">English Speech Sound Development in Preschool-Aged Children From Bilingual English-Spanish Environments</title>
		<imprint>
			<date type="published" when="2008-07">2008. July 2008. 2008/030</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="314" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Plan &amp; Play: Supporting Intentional Media Use in Early Childhood</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Hiniker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bongshin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiley</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eun</forename><forename type="middle">Kyoung</forename><surname>Choe</surname></persName>
		</author>
		<idno type="DOI">10.1145/3078072.3079752</idno>
		<ptr target="https://doi.org/10.1145/3078072.3079752" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Interaction Design and Children</title>
		<meeting>the 2017 Conference on Interaction Design and Children</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
	<note>IDC &apos;17</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pragmatic Language Assessment: A Pragmatics-As-Social Practice Model</title>
		<author>
			<persName><forename type="first">Yvette</forename><forename type="middle">D</forename><surname>Hyter</surname></persName>
		</author>
		<idno type="DOI">10.1097/01.TLD.0000269929.41751.6b</idno>
		<ptr target="https://doi.org/10.1097/01.TLD.0000269929.41751.6b" />
	</analytic>
	<monogr>
		<title level="j">Top. Lang. Disord</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="128" to="145" />
			<date type="published" when="2007-04">2007. April 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Normal Language Acquisition</title>
		<author>
			<persName><forename type="first">Sharon</forename><forename type="middle">L</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Allyn &amp; Bacon</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How Do Users Respond to Voice Input Errors?: Lexical and Phonetic Query Reformulation in Voice Search</title>
		<author>
			<persName><forename type="first">Jiepu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/2484028.2484092</idno>
		<ptr target="https://doi.org/10.1145/2484028.2484092" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;13)</title>
		<meeting>the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Alan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Wilkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leona</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">S</forename><surname>Murray</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0277(88)90003-0</idno>
		<ptr target="https://doi.org/10.1016/0010-0277(88)90003-0" />
	</analytic>
	<monogr>
		<title level="j">Dialogue with machines. Cognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="37" to="72" />
			<date type="published" when="1988-10">1988. October 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Lahey</surname></persName>
		</author>
		<title level="m">Language Disorders and Language Development</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>London</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>edition ed.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Try something else!&quot; -When users change their discursive behavior in human-robot interaction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lohse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Rohlfing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wrede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sagerer</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROBOT.2008.4543743</idno>
		<ptr target="https://doi.org/10.1109/ROBOT.2008.4543743" />
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3481" to="3486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Like Having a Really Bad PA&quot;: The Gulf Between User Expectation and Experience of Conversational Agents</title>
		<author>
			<persName><forename type="first">Ewa</forename><surname>Luger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>Sellen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858288</idno>
		<ptr target="https://doi.org/10.1145/2858036.2858288" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI &apos;16)</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems (CHI &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5286" to="5297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Communication Breakdowns in Normal and Language Learning-Disabled Children&apos;s Conversation and Narration</title>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">G</forename><surname>Maclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">S</forename><surname>Chapman</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshd.5301.02</idno>
		<ptr target="https://doi.org/10.1044/jshd.5301.02" />
	</analytic>
	<monogr>
		<title level="j">J. Speech Hear. Disord</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Use of Repair Strategies by Children With and Without Hearing Impairment</title>
		<author>
			<persName><forename type="first">Tova</forename><surname>Most</surname></persName>
		</author>
		<idno type="DOI">10.1044/0161-1461(2002/009</idno>
		<ptr target="https://doi.org/10.1044/0161-1461(2002/009" />
	</analytic>
	<monogr>
		<title level="j">Lang. Speech Hear. Serv. Sch</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="112" to="123" />
			<date type="published" when="2002-04">2002. April 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human Values and the Design of Computer Technology</title>
		<author>
			<persName><forename type="first">Clifford</forename><forename type="middle">I</forename><surname>Nass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngme</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Morkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eun-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Fogg</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=278213.278222" />
	</analytic>
	<monogr>
		<title level="m">Center for the Study of Language and Information</title>
		<editor>
			<persName><forename type="first">Batya</forename><surname>Friedman</surname></persName>
		</editor>
		<meeting><address><addrLine>Stanford, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-08-31">1997. August 31. 2018</date>
			<biblScope unit="page" from="137" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Informing HCI design through conversation analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0020-7373(05)80150-6</idno>
		<ptr target="https://doi.org/10.1016/S0020-7373(05)80150-6" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Man-Mach. Stud</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="235" to="250" />
			<date type="published" when="1991-08">1991. August 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linguistic adaptations during spoken and multimodal error resolution</title>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Speech</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="442" />
			<date type="published" when="1998-07">1998. July 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><surname>Michael Quinn Patton</surname></persName>
		</author>
		<title level="m">Qualitative research &amp; evaluation methods: integrating theory and practice</title>
		<meeting><address><addrLine>Thousand Oaks, California</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications, Inc</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Fourth edi ed.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive Language Behavior in HCI: How Expectations and Beliefs About a System Affect Users&apos; Word Choice</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holly</forename><forename type="middle">P</forename><surname>Branigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><forename type="middle">I</forename><surname>Nass</surname></persName>
		</author>
		<idno type="DOI">10.1145/1124772.1124948</idno>
		<ptr target="https://doi.org/10.1145/1124772.1124948" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;06)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1177" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Why That Nao?: How Humans Adapt to a Conventional Humanoid Robot in Taking Turns-at-Talk</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Pelikan</surname></persName>
		</author>
		<author>
			<persName><surname>Broth</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858478</idno>
		<ptr target="https://doi.org/10.1145/2858036.2858478" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems -CHI &apos;16</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems -CHI &apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4921" to="4932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Voice-enabled smart speakers to reach 55% of U.S. households by 2022, says report. TechCrunch</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Perez</surname></persName>
		</author>
		<ptr target="http://social.techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/" />
		<imprint>
			<date type="published" when="2018-09-05">September 5, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Voice Interfaces in Everyday Life</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Porcheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">E</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Sharples</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174214</idno>
		<ptr target="https://doi.org/10.1145/3173574.3174214" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI &apos;18)</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems (CHI &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">640</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Do Animals Have Accents?&quot;: Talking with Agents in Multi-Party Conversation</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Porcheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">E</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Sharples</surname></persName>
		</author>
		<idno type="DOI">10.1145/2998181.2998298</idno>
		<ptr target="https://doi.org/10.1145/2998181.2998298" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW &apos;17)</title>
		<meeting>the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="207" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pragmatics as Social Competence</title>
		<author>
			<persName><forename type="first">Carol</forename><forename type="middle">A</forename><surname>Prutting</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshd.4702.123</idno>
		<ptr target="https://doi.org/10.1044/jshd.4702.123" />
	</analytic>
	<monogr>
		<title level="j">J. Speech Hear. Disord</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="134" />
			<date type="published" when="1982-05">1982. May 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parsing Pragmatics</title>
		<author>
			<persName><forename type="first">Kenyatta</forename><forename type="middle">O</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><forename type="middle">D</forename><surname>Hyter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenda</forename><surname>Dejarnette</surname></persName>
		</author>
		<idno type="DOI">10.1044/leader.FTR1.17132012.14</idno>
		<ptr target="https://doi.org/10.1044/leader.FTR1.17132012.14" />
	</analytic>
	<monogr>
		<title level="j">ASHA Lead</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="14" to="17" />
			<date type="published" when="2012-10">2012. October 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hey Alexa, What&apos;s Up?&quot;: A Mixed-Methods Studies of In-Home Conversational Agent Usage</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnita</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">I</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3196709.3196772</idno>
		<ptr target="https://doi.org/10.1145/3196709.3196772" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Designing Interactive Systems Conference (DIS &apos;18)</title>
		<meeting>the 2018 Designing Interactive Systems Conference (DIS &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="857" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unpacking&quot; Scaffolding: Identifying Discourse and Multimodal Strategies that Support Learning</title>
		<author>
			<persName><forename type="first">Tina</forename><surname>Sharpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Educ. Int. J</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2006-05">2006. May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Plans and situated actions: The problem of humanmachine communication</title>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">A</forename><surname>Suchman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Lori</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stevens</surname></persName>
		</author>
		<title level="m">The new coviewing: Designing for learning through joint media engagement</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Geraldine</forename><forename type="middle">P</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharine</forename><forename type="middle">G</forename><surname>Butler</surname></persName>
		</author>
		<title level="m">Language Learning Disabilities in School-Age Children and Adolescents: Some Principles and Applications</title>
		<meeting><address><addrLine>New York; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Toronto</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>edition ed.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Coordination of verbal and non-verbal actions in human-robot interaction at museums and exhibitions</title>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Burdelski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshinori</forename><surname>Kuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihoko</forename><surname>Fukushima</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.pragma.2009.12.023</idno>
		<ptr target="https://doi.org/10.1016/j.pragma.2009.12.023" />
	</analytic>
	<monogr>
		<title level="j">J. Pragmat</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2398" to="2414" />
			<date type="published" when="2010-09">2010. September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Children Asking Questions: Speech Interface Reformulations and Personification Preferences</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Yarosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stryker</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Senthilkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bernheim</surname></persName>
		</author>
		<author>
			<persName><surname>Brush</surname></persName>
		</author>
		<idno type="DOI">10.1145/3202185.3202207</idno>
		<ptr target="https://doi.org/10.1145/3202185.3202207" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Interaction Design and Children (IDC &apos;18)</title>
		<meeting>the 17th ACM Conference on Interaction Design and Children (IDC &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="300" to="312" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
