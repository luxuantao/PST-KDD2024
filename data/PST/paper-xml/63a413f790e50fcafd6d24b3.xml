<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Invariant Graph Representations for Out-of-Distribution Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Invariant Graph Representations for Out-of-Distribution Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph representation learning has shown effectiveness when testing and training graph data come from the same distribution, but most existing approaches fail to generalize under distribution shifts. Invariant learning, backed by the invariance principle from causality, can achieve guaranteed generalization under distribution shifts in theory and has shown great successes in practice. However, invariant learning for graphs under distribution shifts remains unexplored and challenging. To solve this problem, we propose Graph Invariant Learning (GIL) model capable of learning generalized graph representations under distribution shifts. Our proposed method can capture the invariant relationships between predictive graph structural information and labels in a mixture of latent environments through jointly optimizing three tailored modules. Specifically, we first design a GNN-based subgraph generator to identify invariant subgraphs. Then we use the variant subgraphs, i.e., complements of invariant subgraphs, to infer the latent environment labels. We further propose an invariant learning module to learn graph representations that can generalize to unknown test graphs. Theoretical justifications for our proposed method are also provided. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our method against state-of-the-art baselines under distribution shifts for the graph classification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph structured data is ubiquitous in the real world, e.g., social networks, biology networks, chemical molecules, etc. Graph representation learning, which encodes graphs into vectorized representations, has been the central topic in graph machine learning in the last decade. For example, graph neural networks (GNNs) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> design end-to-end learning schemes to extract useful graph information and are shown to be successful in a variety of applications.</p><p>Despite the enormous success, the existing approaches for learning graph representations heavily rely on the I.I.D. assumption, i.e., the testing and training graph data are independently drawn from an identical distribution. However, distribution shifts of graph data widely exist in real-world scenarios and are usually inevitable due to the uncontrollable underlying data generation mechanism <ref type="bibr" target="#b3">[4]</ref>. Most existing approaches fail to generalize to out-of-distribution (OOD) testing graph data. One critical bottleneck is that the existing methods ignore the invariant graph patterns and tend to rely on correlations that are variant for graphs from different environments. Therefore, it is of paramount significance to learn graph representations under distribution shifts and develop methods capable of out-of-distribution (OOD) generalization. Such studies are particularly critical for high-stake graph applications such as medical diagnosis <ref type="bibr" target="#b4">[5]</ref>, financial analysis <ref type="bibr" target="#b5">[6]</ref>, molecular prediction <ref type="bibr" target="#b6">[7]</ref>, etc.</p><p>In this work, we propose a brand new methodology to learn invariant graph representation under distribution shifts. Invariant learning, which aims to exploit the invariant relationships between features and labels across different distributions while disregarding the variant spurious correlations, can provably achieve satisfactory OOD generalization under distribution shifts <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Though invariant learning has been studied for images and texts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>, it remains largely unexplored in the literature of graph representation learning. However, invariant graph representation learning is non-trivial due to the following challenges. First, graph data usually comes from a mixture of latent environments without accurate environment labels, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Since most invariant methods require multiple training environments with explicit environment labels, these existing methods cannot be directly applied to graphs. Second, the formation process of graphs is affected by the complex interaction of both invariant and variant patterns. How to identify the invariant patterns among latent environments is even more challenging. Last but not least, even after having obtained the environmental labels, how to design a theoretically grounded learning scheme to generate graph representations capable of OOD generalization remains largely unexplored.</p><p>To tackle these challenges, in this paper, we propose Graph Invariant Learning method (GIL) which is able to capture invariant graph patterns in a mixture of latent environments and capable of OOD generalization under distribution shifts. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, our proposed method can capture the invariant relationships between predictive graph structural information and labels in a mixture of latent environments through jointly optimizing three mutually promoting modules, with each module tackling one aforementioned challenge. Specifically, in the invariant subgraph identification module, we design a GNN-based subgraph generator to identify potentially invariant subgraphs from the complex interaction between invariant and variant patterns. Then, we use the variant subgraphs, i.e., the complement of invariant subgraphs, to infer environment labels by clustering these environment-discriminative features. The variant subgraphs capture variant correlations under different distributions and therefore contain informative features to infer environment labels. Lastly, in the invariant learning module, we propose to optimize the maximal invariant subgraph generator criterion given the identified invariant subgraphs and inferred environments to generate graph representations capable of OOD generalization under distribution shifts. We theoretically show that the OOD generalization problem on graphs can be formulated as finding a maximal invariant subgraph generator of our GIL, and further prove that our GIL satisfies permutation invariance. We conduct extensive experiments on both synthetic graph datasets and real graph benchmarks for the graph classification task. The results show that the representations learned from GIL achieve substantial performance gains on the unseen OOD testing graphs compared with various state-of-the-art baselines.</p><p>Our contributions are summarized as follows.</p><p>• We propose a novel Graph Invariant Learning method (GIL) to learn invariant and OOD generalized graph representations under distribution shifts. To the best of our knowledge, we are the first to study invariant learning for graph representation learning under a mixture of latent environments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notations and Problem Formulation</head><p>Notations. Let G and Y be the graph and label space. We consider a graph dataset</p><formula xml:id="formula_0">G = {(G i , Y i )} N i=1</formula><p>where G i ∈ G and Y i ∈ Y. Following the OOD convention <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, we assume the dataset is collected from multiple training environments, i.e., G = {G e } e∈supp(Etr) , where</p><formula xml:id="formula_1">G e = {(G e i , Y e i )} Ne i=1</formula><p>denotes the dataset from environment e, supp(E tr ) is the support of the environmental variable in the training data. We use G and Y to denote the random variables of graph and label, and G e and Y e to specify random variables from environment e. The environment label for graphs is unobserved since it is prohibitively expensive to collect graph environment labels for most real scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Φ(⋅)</head><p>Cluster variant subgraphs {" -} to infer environments </p><formula xml:id="formula_2">… B$= Ladder … B%= Tree … B&amp;= Wheel Environment Inference 0 /.1 = 1 / 2 3 ⋅ 1 1 2 GNN 9</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant Subgraph Identification</head><formula xml:id="formula_3">2 4 = Top 5 (0 ⊙ 2) 2 6 = 2 -2 4</formula><p>Graphs from a mixture of latent environments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… …</head><p>Training Dataset</p><formula xml:id="formula_4">Invariant subgraphs {" 7 } Variant subgraphs {" -} Inferred environments ℰ VWXYZ</formula><p>Latent environments:</p><formula xml:id="formula_5">8 $ = Ladder, 8 ( = Tree, 8 ) = Wheel Labels: 9 = 9 = … Testing Dataset % ∘ '(⋅) Invariance regularizer Shared H ∘ J(⋅) H ∘ J(⋅) H ∘ J(⋅) Shared ∇ , ℛ -! ∇ , ℛ -" ∇,ℛ -# \trace(Var(∇ , ℛ -)) Invariant Learning … B$= Ladder … B&amp;= Wheel … B%= Tree ℛ -! ℛ -" ℛ -# Classification loss [ -(ℛ -) + = Objective function ( ) = + , = % ∘ ' ∘ Φ(,) Φ(⋅)</formula><p>Distribution shifts Problem Formulation. We formulate the generalization under distribution shifts on graphs as:</p><p>Problem 1. Let E denote the random variable on indices of all possible environments. Our goal is to find an optimal predictor f * (•) : G → Y that performs well on all environments:</p><formula xml:id="formula_6">f * (•) = arg min f sup e∈supp(E) R(f |e),<label>(1)</label></formula><p>where Note that supp(E tr ) ⊂ supp(E). Besides, distribution shifts indicate that P e (G, Y) ̸ = P e ′ (G, Y), e ∈ supp(E tr ), e ′ ∈ supp(E) \ supp(E tr ), i.e., the joint distribution of the graph and the corresponding label is different for training and testing graph data. Problem 1 is difficult to be solved since supp(E) is unobserved or latent <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. In addition, for most graph datasets, we do not have access to accurate environment labels or environment partitions. Therefore, we focus on jointly inferring the environments of the graph dataset G and achieving good OOD generalization performance under the inferred environments. The problem is formulated as: Problem 2. Given a graph dataset G collected from a mixture of latent environments but without environment labels, the task is to jointly infer graph environments E inf er , i.e., G = {G e } e∈supp(E inf er ) , and learn a graph predictor f * (•) in Problem 1 under the inferred environments E inf er to achieve good OOD generalization performance.</p><formula xml:id="formula_7">R(f |e) = E e G,Y [ℓ(f (G), Y)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce our proposed method in detail, whose framework is shown in Figure <ref type="figure" target="#fig_1">2</ref>. We first present the invariant subgraph identification module. Then, we infer environment labels by clustering the variant subgraphs. Next, we introduce the maximal invariant subgraph generator criterion to generate graph representations which can generalize to test graphs under distribution shifts. Lastly, we provide some discussions of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Invariant Subgraph Identification</head><p>We assume that each input graph G ∈ G has an invariant subgraph G I ⊂ G so that its relationship with the label is invariant across different environments. We refer to the rest of the graph, i.e., the complement of G I , as the variant subgraph and denote it as G V . G V represents the graph part whose relationship with the label is variant across different environments, e.g., spurious correlations. Therefore, the model will have a better OOD generalization ability if it can identify the invariant subgraph and only uses structural information from G I .</p><p>We denote a generator to obtain the invariant subgraph as G I = Φ(G). Following the invariant learning literature <ref type="bibr" target="#b11">[12]</ref>, we make an assumption on Φ(G) as follows: The invariance assumption means that there exists a subgraph generator such that it can generate invariant subgraphs across different environments. The sufficiency assumption means that the generated invariant subgraphs should have sufficient predictive abilities in predicting the graph labels.</p><p>Under this assumption, we instantiate Φ(•) with learnable parameters. Consider an input graph instance G with n nodes. The corresponding adjacency matrix is denoted as A = {0, 1} n×n , where A i,j = 1 represents that there exists an edge between node i and j, and A i,j = 0 otherwise. To split the input graph G into G I and G V , a common strategy is to use a binary mask matrix M = {0, 1} n×n on the adjacency matrix A. However, directly optimizing a discrete matrix M is intractable as G has exponentially many subgraph candidates <ref type="bibr" target="#b12">[13]</ref>. Besides, learning M for each graph G separately hinders the method from handling unseen test graphs <ref type="bibr" target="#b13">[14]</ref>. Therefore, we adopt a shared learnable GNN (denoted as GNN M ) to generate a soft mask matrix M = R n×n as follows:</p><formula xml:id="formula_8">Mi,j = Z (m) i ⊤ • Z (m) j , Z (m) = GNN M (G),<label>(2)</label></formula><p>where Z (m) is the node representation. Then, we obtain the invariant and variant subgraphs as:</p><formula xml:id="formula_9">AI = Top t (M ⊙ A) , AV = A − AI ,<label>(3)</label></formula><p>where A I and A V denotes the adjacency matrix of G I and G V , respectively, ⊙ means the elementwise matrix multiplication, and Top t (•) selects the top t-percentage of elements with the largest values. The parameters of GNN M are trained on all available graphs to generate the corresponding G I and G V . Using the inductive learning ability of GNNs, it can also be used to unseen test graphs, as opposed to directly optimizing mask matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Environment Inference</head><p>After obtaining the invariant and variant subgraphs, we can infer the environment label E inf er . The intuition is that since the invariant subgraph captures invariant relationships between predictive graph structural information and labels, the variant subgraphs in turn capture variant correlations under different distributions, which are environment-discriminative features. Therefore, we can use the variant subgraphs to infer the latent environments. We adopt another GNN encoder, whose parameters are also shared among different graphs, to generate the representation of the variant subgraph G V :</p><formula xml:id="formula_10">ZV = GNN V (GV ), hV = READOUT V (ZV ),<label>(4)</label></formula><p>where READOUT is a permutation-invariant readout function that aggregates node-level representation Z V into graph-level representation h V . The representation of all the variant subgraphs is denoted as</p><formula xml:id="formula_11">H = [h V1 , ..., h V N ].</formula><p>After obtaining H, we use an off-the-shelf clustering algorithm to infer the environment label E inf er . In this paper, we adopt k-means <ref type="bibr" target="#b14">[15]</ref> as our clustering algorithm:</p><formula xml:id="formula_12">E inf er = k-means(H).<label>(5)</label></formula><p>Using E inf er , we can partition the graph dataset into multiple training environments, i.e., G = {G e } e∈supp(E inf er ) . The environment inference module is purely unsupervised without needing any ground-truth environment labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Invariant Learning</head><p>After obtaining the inferred invariant subgraphs and environment labels, we propose the invariant learning module which can generate OOD generalized graph representations under distribution shifts.</p><p>Recall that both the invariant subgraph identification module and environment inference module heavily depend on the generator Φ. Therefore, we aim to learn the optimal generator Φ * in Assumption 3.1 by proposing and optimizing the maximal invariant subgraph generator criterion. First, following the invariant learning literature <ref type="bibr" target="#b8">[9]</ref>, we give the following definition. Definition 1. The invariant subgraph generator set I with respect to E is defined as:</p><formula xml:id="formula_13">IE = {Φ(•) : P e (Y|Φ(G)) = P e ′ (Y|Φ(G)), e, e ′ ∈ supp(E)}.<label>(6)</label></formula><p>Then, we show that the optimal generator Φ * satisfies the following theorem. Theorem 3.2. A generator Φ(G) is the optimal generator that satisfies Assumption 3.1 if and only if it is the maximal invariant subgraph generator, i.e.,</p><formula xml:id="formula_14">Φ * = arg max Φ∈I E I (Y; Φ(G)) ,<label>(7)</label></formula><p>where I(•; •) is the mutual information between the label and the generated subgraph.</p><p>The proof is provided in Appendix. Eq. ( <ref type="formula" target="#formula_14">7</ref>) provides us an objective function to optimize the subgraph generator. However, directly solving Eq. ( <ref type="formula" target="#formula_14">7</ref>) for a non-linear Φ is difficult <ref type="bibr" target="#b8">[9]</ref>. Following the invariant learning literature <ref type="bibr" target="#b8">[9]</ref>, we transform Eq. ( <ref type="formula" target="#formula_14">7</ref>) into an invariance regularizer:</p><formula xml:id="formula_15">E e∈supp(E inf er ) R e (f (G), Y; θ) + λtrace(VarE inf er (∇ θ R e )),<label>(8)</label></formula><p>where f (•) = w • g • Φ, E inf er is the infered environment label, and θ denotes all the learnable parameters. Recall that g(•) is the representation learning function of the invariant subgraphs and w(•) is the classifier. We instantiate g as another GNN as follows:</p><formula xml:id="formula_16">ZI = GNN I (GI ), hI = READOUT I (ZI ).<label>(9)</label></formula><p>Z I and h I are the node-level and graph-level representations of invariant subgraph G I , respectively. w(•) is instantiated as a multilayer perceptron followed by the softmax activation function. By optimizing Eq. ( <ref type="formula" target="#formula_15">8</ref>), we can get our desired generator Φ and the subgraph representation learning function g(•), which collectively serve as our representation learning method h(•), i.e., h = g • Φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussions</head><p>Training Procedure. We present the pseudocode of GIL in Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section, we theoretically analyze our GIL model by showing that the maximal invariant subgraph generator can achieve OOD optimal. The proofs are provided in Appendix. Theorem 4.1. Let Φ * be the optimal invariant subgraph generator in Assumption 3.1 and denote the complement as G\Φ * (G), i.e., the corresponding variant subgraph. Then, we can obtain the optimal predictor under distribution shifts, i.e., the solution to Problem 1, as follows:</p><formula xml:id="formula_17">arg min w,g w • g • Φ * (G) = arg min f sup e∈supp(E) R(f |e),<label>(10)</label></formula><p>if the following conditions hold: (1) Φ * (G) ⊥ G\Φ * (G); and (2) ∀Φ ∈ I E , ∃ e ′ ∈ supp(E) such that P e ′ (G, Y) = P e ′ (Φ(G), Y)P e ′ (G\Φ(G)) and P e ′ (Φ(G)) = P e (Φ(G)).</p><p>The theorem shows that we can transform the OOD generalization problem into finding the optimal invariant subgraphs while maintaining the optimality.</p><p>We also prove that our GIL satisfies permutation invariance in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate the effectiveness of our GIL on both synthetic and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Datasets. We adopt one synthetic dataset with controllable ground-truth environments and four real-world benchmark datasets for the graph classification task.</p><p>• SP-Motif: Following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, we generate a synthetic dataset where each graph consists of one variant subgraph and one invariant subgraph, i.e., motif. The variant subgraph includes Tree, Ladder, and Wheel (denoted by V = 0, 1, 2, respectively) and the invariant subgraph includes Cycle, House, and Crane (denoted by I = 0, 1, 2). The ground-truth label Y only depends on the invariant subgraph I, which is sampled uniformly. The spurious correlation between V and Y is injected by controlling the variant subgraphs distribution as:</p><formula xml:id="formula_18">P (V ) = r if V = I and P (V ) = (1 − r)/2 if V ̸ = I.</formula><p>Intuitively, r controls the strength of the spurious correlation. We set r to different values in the testing and training set to simulate the distribution shifts. • MNIST-75sp <ref type="bibr" target="#b16">[17]</ref>: The task is to classify each graph that is converted from an image in MNIST <ref type="bibr" target="#b17">[18]</ref> into the corresponding handwritten digit. Distribution shifts exist on node features by adding random noises in the testing set. • Graph-SST2 <ref type="bibr" target="#b18">[19]</ref>: Each graph is converted from a text sequence. Graphs are split into different sets based on average node degrees to create distribution shifts. • Open Graph Benchmark (OGB) <ref type="bibr" target="#b19">[20]</ref>: We consider two datasets, MOLSIDER and MOLHIV. The default split separates structurally different molecules with different scaffolds into different subsets.</p><p>Baselines. We compare our GIL with some representative state-of-the-art methods. The first group of these methods generates masks on graph structures to filter out important subgraphs using different GNNs, including Attention <ref type="bibr" target="#b1">[2]</ref>, Top-k Pool <ref type="bibr" target="#b20">[21]</ref>, SAGPool <ref type="bibr" target="#b21">[22]</ref>, and ASAP <ref type="bibr" target="#b22">[23]</ref>. The second group is invariant learning methods, including standard ERM, GroupDRO <ref type="bibr" target="#b23">[24]</ref>, IRM <ref type="bibr" target="#b7">[8]</ref>, V-REx <ref type="bibr" target="#b24">[25]</ref>, DIR <ref type="bibr" target="#b15">[16]</ref>. We also consider a recent interpretable graph learning method GSAT <ref type="bibr" target="#b25">[26]</ref>. For a fair comparison, we use the same GNN backbone as GIL for the baselines.</p><p>Optimization and Hyper-parameters. The adopted GNNs and READOUT functions including GNN M , GNN V , GNN I , READOUT V , and READOUT I are listed in Appendix. The hyperparameter λ in Eq. ( <ref type="formula" target="#formula_15">8</ref>) is chosen from {10 −5 , 10 −3 , 10 −1 }. The number of clusters in Eq. ( <ref type="formula" target="#formula_12">5</ref>) is chosen from <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. They are tuned on the validation set. We report the mean results and standard deviations of five runs. More details on the datasets, baselines and implementations are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on SP-Motif</head><p>Settings. To simulate different degrees of distribution shifts, we vary r in both the training and testing datasets. For the training set, we select r train from {1/3, 0.5, 0.6, 0.7, 0.8, 0.9}. A larger r train indicates a higher spurious correlation between Y and G V in the training set, while r train = 1/3 implies that the training set is balanced without any spurious correlation. For the testing set, we consider two settings: (1) r test = 1/3, which simulates that the invariant subgraphs and variant subgraphs are randomly attached without spurious correlations; (2) r test = 0.2, which indicates that the testing set has reversed spurious correlations and thus is more challenging.</p><p>Quantitative Results. The results are shown in Table <ref type="table" target="#tab_3">1</ref>. We have the following observations. Our proposed GIL model consistently and significantly outperforms the baselines and achieves the best performance on all settings. The results demonstrate that our proposed method can well handle graph distribution shifts and have a remarkable out-of-distribution generalization ability.  As r train grows larger, the performance of all the methods tends to decrease since there exists a larger degree of distribution shift. Nevertheless, our proposed method is able to maintain the most relatively stable performance. In fact, the performance gap between GIL and the baselines becomes more significant as the degree of distribution shift increases. For example, when r test = 1/3, the accuracy of all baselines drops by more than 7% when r train changes from 0.5 to 0.8, indicating their poor OOD generalization ability. In contrast, our method only has 3% performance drop.</p><p>When the degree of distribution shift is relatively small, GNNs with different pooling methods to generate subgraphs generally report better results. On the other hand, when the degree of distribution shift is large, invariant baselines show more stable performance. Among them, DIR, which is a recently proposed invariant method specifically designed for graphs, is one competitive baseline. Nevertheless, our proposed method outperforms DIR by more than 3% in terms of the classification accuracy in most cases. GSAT achieves promising gains over the other baselines, but our GIL still performs better than GSAT. When r test = r train = 1/3, i.e., no distribution shifts, our proposed method also achieves the best results, indicating that learning invariant subgraphs is also beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis.</head><p>To analyze whether our proposed method can accurately capture the invariant subgraph, we compare GIL with baselines that also output subgraphs using the ground-truth invariant subgraphs. The evaluation metric is Precision@5. We report the results in Figure <ref type="figure">3</ref>. The results show that GIL has a clear advantage in discovering invariant subgraphs under latent environments, while the other baselines cannot handle distribution shifts well.   Besides the quantitative evaluation, we plot a showcase from the testing set of SP-Motif (r train = 0.8 and r test = 0.2) in Figure <ref type="figure" target="#fig_5">6</ref>. The figure shows that the learned invariant subgraph of our method is more accurate than baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on Real-world Graphs</head><p>We further evaluate the effectiveness of our method on real-world graph datasets. The experimental results are presented in Table <ref type="table" target="#tab_4">2</ref>. Our GIL achieves the best performance on all four datasets, indicating that GIL can well handle distribution shifts on real-world graphs. For example, GIL increases the classification accuracy by 1.8% on MNIST-75sp and ROC-AUC by 2.0% on MOLHIV against the strongest baselines respectively. On MOLHIV, the results of most baselines are worse than ERM, indicating that they fail to achieve OOD generalization in this dataset. Besides, different datasets have different distribution shifts, e.g., Graph-SST2 has different node degrees, the distribution shift of MNIST-75sp is on node features, and OGB is split based on scaffold. Therefore, the results show that our proposed method can well handle diverse types of distribution shifts in real graph datasets. For MOLHIV, besides adopting GIN <ref type="bibr" target="#b2">[3]</ref> as backbone (shown in Table <ref type="table" target="#tab_4">2</ref>), our method is also compatible with the other popular GNNs. We try using HIG and CIN <ref type="bibr" target="#b26">[27]</ref> (Rank #2 and #8 on the MOLHIV leaderboard<ref type="foot" target="#foot_1">1</ref> ) as the backbone since these models are orthogonal to ours. Table <ref type="table" target="#tab_5">3</ref> shows that our GIL can consistently improve these models.</p><p>In addition, we present some showcases of the learned invariant subgraph of the proposed GIL on both the train and test set of Graph-SST2. This dataset consists of sentences with positive/negative sentiments and is more understandable for humans. Figure <ref type="figure" target="#fig_6">7</ref> shows that our method can learn invariant subgraphs by consistently focusing on the positive/negative words that are salient for sentiments and distinguishing invariant/variant parts under distribution shifts. For example, the subgraph "The world's best actors" identified by GIL has a predictive and invariant relationship with the positive sentiment label, while the subgraph "daniel auteuil" may reflect variant sentiments in different sentences. These results validate: (1) the invariant and variant subgraphs widely exist in real-world datasets, and (2) our GIL can well identify invariant subgraphs under distribution shifts and further make predictions with high accuracy based on the learned invariant subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis of Environment Inference</head><p>In our proposed model, all components are jointly optimized. To show that the environment inference module and invariant learning module can mutually enhance each other, we record the test accuracy and the Silhouette score <ref type="bibr" target="#b27">[28]</ref>, which is a commonly used evaluation metric for clustering, as the model is trained. The results on SP-Motif (r train = 0.8, r test = 1/3) are shown in Figure <ref type="figure">4</ref>. We can observe that the test accuracy and the clustering performance improve synchronously over training. A plausible reason is that, as the training stage progresses, the invariant subgraph generator is optimized so that it can generate more informative invariant subgraphs and therefore improve the performance on the testing set. On the other hand, accurately discovering invariant subgraphs can also promote identifying variant subgraphs, which capture the environment-discriminate features and better infer the latent environments. To verify that GIL can infer the environments accurately, we use t-SNE <ref type="bibr" target="#b28">[29]</ref> to plot the discovered environments on a 2D-plane when the optimization is finished. Figure <ref type="figure">5</ref> shows that the variant subgraphs perfectly capture the environment-discriminate features. Notice that GIL achieves such results without needing any ground-truth environment label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyper-parameter Sensitivity</head><p>We investigate the sensitivity of hyper-parameters of our method, including the number of environments |E inf er |, the invariance regularizer coefficient λ, and the size of the invariant subgraph mask t in Eq. ( <ref type="formula" target="#formula_9">3</ref>). For simplicity, we only report the results on SP-Motif (r train = 0.8 and r test = 1/3) and MNIST-75sp in Figure <ref type="figure" target="#fig_9">8</ref>, while the results on other datasets show similar patterns.</p><p>First, the number of environments has a moderate impact on the model performance.</p><p>For SP-Motif, the performance reaches a peak when |E inf er | = 3, showing that GIL achieves the best result when the number of environments matches the ground truth.   For MNIST-75sp, the best number of environments is |E inf er | = 2. A plausible reason is that a large number of environments will bring difficulty in inferring the latent environment, leading to sub-optimal performance. Second, the coefficient λ also has a slight influence on the performance, indicating that we need to properly balance the classification loss and the invariance regularizer term. Finally, a proper value of the mask size t is important. A very large t will result in too many edges in the invariant subgraph and bring in variant structures, while a small t may let the invariant subgraph become too small to capture enough structural information. Although an appropriate choice of hyper-parameters can further improve the performance, our method is not very sensitive to hyperparameters. Figure <ref type="figure" target="#fig_9">8</ref> shows that GIL can outperform the best baselines with a wide range of hyper-parameters choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Works</head><p>Graph neural networks. Recently, graph neural networks (GNNs) have shown enormous success in graph representation learning <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, demonstrating their strength in various tasks <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. GNNs generally adopt a neighborhood aggregation (message passing) paradigm, i.e., the representations of nodes are iteratively updated by aggregating representations of their neighbors. The representation of the whole graph is summarized on node representations through the readout function (i.e., pooling) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. However, most existing GNN models do not consider the out-of-distribution generalization ability <ref type="bibr" target="#b36">[37]</ref> so that their performances can drop substantially on testing graphs with distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization of GNNs.</head><p>Early works <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref> for analyzing the generalization ability of GNNs do not consider distribution shifts <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. More recently, the generalization ability of GNNs under distribution shifts starts to receive research attention <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. <ref type="bibr" target="#b46">[47]</ref> find that encoding task-specific non-linearities in the architecture or features can improve GNNs in extrapolating graph algorithmic tasks. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref> try to encourage GNNs to perform well on testing graphs with different sizes. Some works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> are proposed to deal with node-level tasks. EERM <ref type="bibr" target="#b51">[52]</ref> studies the OOD generalization in node classification. However, little attention has been paid to learning graph-level representations under distribution shifts from the invariant learning perspective. One exception is the work DIR <ref type="bibr" target="#b15">[16]</ref>, which conducts interventions on graphs to create interventional distributions. However, performing causal intervention relies on strong assumptions <ref type="bibr" target="#b52">[53]</ref> that could be violated and expensive to satisfy in practice <ref type="bibr" target="#b53">[54]</ref>. GSAT <ref type="bibr" target="#b25">[26]</ref> applies graph information bottleneck criteria for generalization, but its goal is mainly to build inherently interpretable GNNs.</p><p>Invariant Learning. Invariant learning aims to exploit the invariant relationships between features and labels across distribution shifts, while filtering out the variant spurious correlations. Backed by causal theory, invariant learning model can lead to OOD optimal models under some assumptions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. However, most existing methods heavily rely on multiple environments that have to be explicitly provided in the training dataset. Such annotation is not only prohibitively expensive for graphs, but also inherently problematic as the environment split could be inaccurate, rendering these invariant learning methods inapplicable. A few works study OOD generalization on latent environments in computer vision <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> or raw feature data <ref type="bibr" target="#b56">[57]</ref>, which cannot be directly applied to graphs. In summary, how to learn invariant graph representations without explicit environment label under distribution shits remains largely unexplored in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we propose the graph invariant learning (GIL) model to tackle the problem of learning invariant graph representations under distribution shifts. Three tailored modules are jointly optimized to encourage the graph representations to capture the invariant relationships between predictive graph structural information and labels. Theoretical analysis and extensive experiments on both synthetic and real-world datasets demonstrate the superiority of GIL. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of distribution shifts under a mixture of latent environments, which leads to poor generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of GIL model. Our proposed method jointly optimizes three modules: (1) In the invariant subgraph identification module, a GNN-based subgraph generator Φ(•) identifies the invariant subgraph G I and the variant subgraph G V . (2) The environment inference module uses the variant subgraphs {G V } to infer the latent environments by clustering the representations of {G V }. (3) The invariant learning module jointly optimizes the invariant subgraph generator Φ(•), the representation learning function g(•), and the classifier w(•). Training stage (shown by grey arrows): we back propagate with the objective function to update model parameters. Testing stage (shown by green arrows): we use the optimized model to make predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is the risk of the predictor f on the environment e, and ℓ(•, •) : Y × Y → R denotes a loss function. We further decompose f (•) = w • h, where h(•) : G → R d is the representation learning function, d is the dimensionality, and w(•) : R d → Y is the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Assumption 3 . 1 .</head><label>31</label><figDesc>Given G, there exists an optimal invariant subgraph generator Φ * (G) satisfying: a. Invariance property: ∀e, e ′ ∈ supp(E), P e (Y|Φ * (G)) = P e ′ (Y|Φ * (G)). b. Sufficiency property: Y = w * (g * (Φ * (G))) + ϵ, ϵ ⊥ G, where g * (•) denotes a representation learning function, w * is the classifier, ⊥ indicates statistical independence, and ϵ is random noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>Figure 3: The results of discovering the ground-truth invariant subgraphs on SP-Motif (r test = 0.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualizations of the learned invariant subgraph for a showcase from the testing set of SP-Motif. In Figures (a)-(e), the red lines indicate the learned invariant subgraph, and the ground-truth is shown by the black lines in Figure (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Four showcases of sentences with positive/negative sentiments of train/test sets on Graph-SST2 learned by our GIL. Blue edges indicate the learned invariant subgraphs, while the others are variant subgraphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The number of environments |E inf er |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>The invariant subgraph mask size t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The impact of different hyperparameters. Yellow and blue lines denote the results of GIL and grey dashed lines are the best results of all baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Our proposed method can automatically infer the environment label of graphs from a mixture of latent environments without supervision. • We propose maximal invariant subgraph generator criterion to learn graph representations capable of OOD generalization under distribution shifts. • We theoretically show that finding a maximal invariant subgraph generator of GIL can solve the OOD generalization problem. Extensive empirical results demonstrate the effectiveness of GIL on various synthetic and benchmark datasets under distribution shifts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Since we only need to generate mask for the existing edges in graphs, the time complexity of generating invariant and variant subgraphs and further obtaining their representations is O(|E| d + |V | d 2 ). The time complexity of environment inference is O(|B||E inf er |T d), where |B| is the batch size, T is the number of iterations for the k-means algorithm, and |E inf er | denotes the number of inferred environments. The time complexity of the invariance regularizer is O(|E inf er |d 2 ), as the number of parameters for most GNNs is O(d 2 ). Since |B|, |E inf er |, and T are small constants, the overall time complexity of GIL is O(|E| d + |V | d 2 ). In comparison, the time complexity of other GNN-based graph representation methods is also O(|E| d + |V | d 2 ). Therefore, the time complexity of our proposed GIL is on par with the existing methods.</figDesc><table /><note>Time Complexity. The time complexity of our GIL is O(|E| d + |V | d 2 ), where |V | and |E| denotes the number of nodes and edges, respectively, and d is the dimensionality of the representations. Specifically, we adopt message-passing GNNs to instantiate our GNN components, which has a complexity of O(|E| d + |V | d 2 ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The graph classification accuracy (%) on testing sets of the synthetic dataset SP-Motif. In each column, the boldfaced and the underlined score denotes the best and the second-best result, respectively. Numbers in the lower right corner denote standard deviations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Scenario 1: rtest = 1/3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Scenario 2: rtest = 0.2</cell><cell></cell><cell></cell></row><row><cell>rtrain</cell><cell>r = 1/3</cell><cell>r = 0.5</cell><cell>r = 0.6</cell><cell>r = 0.7</cell><cell>r = 0.8</cell><cell>r = 0.9</cell><cell>r = 1/3</cell><cell>r = 0.5</cell><cell>r = 0.6</cell><cell>r = 0.7</cell><cell>r = 0.8</cell><cell>r = 0.9</cell></row><row><cell>ERM</cell><cell cols="12">53.60±3.79 51.24±4.13 47.04±7.01 38.80±3.72 37.84±3.01 37.44±2.15 48.48±4.53 41.72±4.81 36.92±6.93 35.72±8,33 28.80±3.91 19.60±1.66</cell></row><row><cell>Attention</cell><cell cols="12">54.31±3.98 53.24±3.56 42.52±6.20 35.20±1.05 34.48±1.18 33.88±1.01 44.04±4.33 31.64±0.67 25.72±5.34 24.80±4.06 23.20±3.60 18.04±2.88</cell></row><row><cell cols="13">Top-k Pool 54.68±2.71 53.12±5.58 44.56±4.57 37.44±2.04 35.24±2.28 34.28±4.11 45.68±5.16 34.20±4.34 31.00±2.89 30.64±3.59 29.16±2.18 27.56±3.91</cell></row><row><cell cols="13">SAG Pool 54.08±3.66 52.60±3.52 44.68±5.25 37.68±4.03 34.28±1.82 32.72±1.83 44.36±6.09 38.64±3.02 31.36±4.40 32.84±1.86 28.72±3.11 26.60±5.37</cell></row><row><cell>ASAP</cell><cell cols="12">54.00±4.21 51.92±3.81 45.12±1.98 36.28±0.86 34.24±2.02 34.40±3.15 49.88±4.90 34.52±4.35 27.00±2.61 27.20±2.53 27.96±3.89 22.88±4.33</cell></row><row><cell cols="13">GroupDRO 53.20±4.91 51.40±4.35 48.32±5.35 39.12±4.27 38.40±2.76 37.64±1.69 52.68±4.04 43.68±4.05 31.92±6.84 34.36±8.41 28.88±5.14 20.32±1.64</cell></row><row><cell>IRM</cell><cell cols="12">52.00±2.34 50.60±3.54 47.84±6.95 38.80±3.72 39.84±3.21 39.00±3.98 50.24±6.73 41.60±4.75 35.24±5.35 34.92±8.03 29.44±5.47 21.84±3.57</cell></row><row><cell>V-REx</cell><cell cols="12">53.16±3.25 46.04±6.11 45.36±3.66 40.24±3.86 39.48±3.00 39.12±3.48 50.56±2.83 37.16±6.24 34.52±3.00 29.72±4.58 27.32±3.18 24.04±6.08</cell></row><row><cell>DIR</cell><cell cols="12">52.96±5.06 52.08±1.93 50.12±2.76 49.84±2.46 45.20±1.11 41.24±4.73 50.68±5.20 49.96±1.75 45.44±6.00 40.56±2.36 39.92±4.53 32.52±4.59</cell></row><row><cell>GSAT</cell><cell cols="12">53.67±3.65 53.34±4.08 51.54±3.78 50.12±3.29 45.83±4.01 44.22±5.57 51.36±4.21 50.48±3.98 46.93±5.03 43.55±3.67 40.35±4.21 33.87±5.19</cell></row><row><cell>GIL</cell><cell cols="12">55.44±3.11 54.56±3.02 53.60±4.82 53.12±2.18 51.24±3.88 46.04±3.51 54.80±3.93 52.48±4.41 50.08±5.47 47.44±2.87 46.36±3.80 35.80±5.03</cell></row><row><cell>0.17 0.23 0.29 Precision@5 0.35</cell><cell cols="4">1/3 0.5 0.6 0.7 0.8 0.9 r train GIL GSAT DIR SAG Pool Top-k Pool ASAP Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The graph classification results (%) on testing sets of the real-world datasets. We report the accuracy for MNIST-75sp and Graph-SST2, ROC-AUC for MOLSIDER and MOLHIV.</figDesc><table><row><cell></cell><cell cols="4">MNIST-75sp Graph-SST2 MOLSIDER MOLHIV</cell></row><row><cell>ERM</cell><cell>14.94±3.27</cell><cell>81.44±0.59</cell><cell>57.57±1.56</cell><cell>76.20±1.14</cell></row><row><cell>Attention</cell><cell>16.44±3.78</cell><cell>81.57±0.71</cell><cell>56.99±0.54</cell><cell>75.84±1.33</cell></row><row><cell>Top-k Pool</cell><cell>15.02±3.08</cell><cell>79.78±1.35</cell><cell>60.63±1.52</cell><cell>73.01±1.65</cell></row><row><cell>SAG Pool</cell><cell>19.34±1.73</cell><cell>80.24±1.72</cell><cell>61.29±1.31</cell><cell>73.26±0.84</cell></row><row><cell>ASAP</cell><cell>15.14±3.58</cell><cell>81.57±0.84</cell><cell>55.77±1.34</cell><cell>73.81±1.17</cell></row><row><cell>GroupDRO</cell><cell>15.72±4.35</cell><cell>81.29±1.44</cell><cell>56.31±1.15</cell><cell>75.44±2.70</cell></row><row><cell>IRM</cell><cell>18.74±2.43</cell><cell>81.01±1.13</cell><cell>57.10±0.92</cell><cell>74.46±2.74</cell></row><row><cell>V-REx</cell><cell>18.40±1.12</cell><cell>81.76±0.08</cell><cell>57.76±0.78</cell><cell>75.62±0.79</cell></row><row><cell>DIR</cell><cell>17.38±3.52</cell><cell>83.29±0.53</cell><cell>57.74±1.63</cell><cell>77.05±0.57</cell></row><row><cell>GSAT</cell><cell>20.12±1.35</cell><cell>82.95±0.58</cell><cell>60.82±1.36</cell><cell>76.47±1.53</cell></row><row><cell>GIL</cell><cell>21.94±0.38</cell><cell>83.44±0.37</cell><cell>63.50±0.57</cell><cell>79.08±0.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The test results with different backbones.</figDesc><table><row><cell>CIN</cell><cell>GIL</cell><cell>HIG</cell><cell>PAS+FPs</cell><cell>GIL</cell></row><row><cell cols="5">(Rank #8) (CIN Backbone) (Rank #2) (Rank #1) (HIG Backbone)</cell></row><row><cell>80.94±0.57</cell><cell>81.15±0.46</cell><cell cols="2">84.03±0.21 84.20±0.15</cell><cell>84.23±0.25</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="36" xml:id="foot_0">36th Conference on Neural Information Processing Systems (NeurIPS 2022).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">https://ogb.stanford.edu/docs/leader_graphprop/#ogbg-molhiv</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Key Research and Development Program of China No. 2020AAA0106300 and National Natural Science Foundation of China (No. 62250008, 62222209, 62102222, 62206149), China National Postdoctoral Program for Innovative Talents No. BX20220185, China Postdoctoral Science Foundation No. 2022M711813.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph neural network-based diagnosis prediction</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buyue</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="379" to="390" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using external knowledge for financial event prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2161" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization with maximal invariant predictor</title>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoichiro</forename><surname>Yamaguchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01883</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Invariance principle meets information bottleneck for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems (NeurIPS)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Invariant rationalization</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1448" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1309" to="1342" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9240</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameterized explainer for graph neural network</title>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manchek</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. series c (applied statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Yingxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4202" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Hao Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15445</idno>
		<title level="m">Explainability in graph neural networks: A taxonomic survey</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Asap: Adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">Ekagra</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpretable and generalizable graph learning via stochastic attention mechanism</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go cellular: Cw networks</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName><forename type="first">Rousseeuw</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Intention-aware sequential recommendation with structured intent transition</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Disentangled contrastive learning on graphs</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21872" to="21884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disentangled representation learning for recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fates of microscopic social ecosystems: Keep alive or dead?</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="668" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Disentangled graph contrastive learning with independence promotion</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gqnas: Graph q network for neural architecture search</title>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1288" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization on graphs: A survey</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07987</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A pac-bayesian approach to generalization bounds for graph neural networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3419" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The vapnik-chervonenkis dimension of graph and recursive neural networks</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="248" to="259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wild-time: A benchmark of in-the-wild distribution shift over time</title>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
				<meeting>the Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to solve travelling salesman problem with hardness-adaptive curriculum</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ood-gnn: Out-of-distribution generalized graph neural network</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph neural architecture search under distribution shifts</title>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18083" to="18095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic graph neural networks under spatio-temporal distribution shift</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From local structures to size generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11975" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
				<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shift-robust gnns: Overcoming the limitations of localized graph training data</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Debiased graph neural networks with agnostic label selection bias</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nian</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Hernán</surname></persName>
		</author>
		<author>
			<persName><surname>Robins</surname></persName>
		</author>
		<title level="m">Causal inference</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Causal attention for unbiased visual recognition</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3091" to="3100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11749" to="11756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Environment inference for invariant learning</title>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2189" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Heterogeneous risk minimization</title>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
