<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
							<email>hqzhou@stu.suda.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>minzhang@suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, large-scale pre-trained language models (PLMs) have made extraordinary progress in most NLP tasks. But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve stateof-the-art (SOTA) performance. The recent SOTA performance is yielded by a Guassian HMM variant proposed by <ref type="bibr" target="#b14">He et al. (2018)</ref>. However, as a generative model, HMM makes very strong independence assumptions, making it very challenging to incorporate contexualized word representations from PLMs. In this work, we for the first time propose a neural conditional random field autoencoder (CRF-AE) model for unsupervised POS tagging. The discriminative encoder of CRF-AE can straightforwardly incorporate PLM word representations. Moreover, inspired by featurerich HMM, we reintroduce hand-crafted features into the decoder of CRF-AE. Finally, experiments clearly show that our model outperforms previous state-of-the-art models by a large margin on Penn Treebank and multilingual Universal Dependencies treebank v2.0.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised learning has been an important yet challenging research direction in NLP <ref type="bibr" target="#b18">(Klein and Manning, 2004;</ref><ref type="bibr" target="#b20">Liang et al., 2006;</ref><ref type="bibr" target="#b29">Seginer, 2007)</ref>. Training models directly from unlabeled data can relieve painful data annotation and is thus especially attractive for low-resource languages <ref type="bibr" target="#b14">(He et al., 2018)</ref>. As three typical tasks related to syntactic analysis, unsupervised part-of-speech (POS) tagging (or induction), dependency parsing, and constituency parsing have attracted intensive interest during the past three decades <ref type="bibr" target="#b24">(Pereira and Schabes, 1992;</ref><ref type="bibr">Christodoulopoulos et al., 2010, inter alia)</ref>. Compared with tree-structure dependency and constituency parsing, POS tagging corresponds I looked at my watch .</p><p>PRP VBD IN PRP$ NN . to simpler sequential structure, and aims to assign a POS tag to each word, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>. Besides the alleviation of labeled data, unsupervised POS tagging is particularly valuable for child language acquisition study because every child manages to induce syntactic categories without access to labeled data <ref type="bibr" target="#b36">(Yuret et al., 2014)</ref>.</p><p>Nowadays, supervised POS tagging models trained on large-scale labeled data can already achieve extremely high accuracy, for example over 97.5% on English Penn Treebank (PTB) texts <ref type="bibr" target="#b15">(Huang et al., 2015;</ref><ref type="bibr" target="#b2">Bohnet et al., 2018;</ref><ref type="bibr" target="#b38">Zhou et al., 2020)</ref>. However, unsupervised POS tagging, though having attracted a lot of research interest <ref type="bibr" target="#b21">(Lin et al., 2015;</ref><ref type="bibr" target="#b32">Tran et al., 2016;</ref><ref type="bibr" target="#b14">He et al., 2018;</ref><ref type="bibr" target="#b30">Stratos, 2019;</ref><ref type="bibr" target="#b12">Gupta et al., 2020)</ref>, can only achieve at most 80.8% many-to-one (M-1) accuracy, where M-1 means multiple induced tags can be mapped to a single ground-truth tag when evaluating the model on the test data.</p><p>The generative Hidden Markov Models (HMMs) are the most representative and successful approach for unsupervised POS tagging <ref type="bibr" target="#b23">(Merialdo, 1994;</ref><ref type="bibr" target="#b11">Graça et al., 2009)</ref>. By treating POS tags as latent variables, a first-order HMM factorizes the joint probability of a sentence and a tag sequence p(x, y) into independent emission probabilities p(x i y i ) and transition probabilities p(y i−1 y i ).</p><p>The training objective is to maximize the marginal probability p(x), which can be solved by the EM algorithm or direct gradient descent <ref type="bibr" target="#b28">(Salakhutdinov et al., 2003)</ref>. <ref type="bibr" target="#b1">Berg-Kirkpatrick et al. (2010)</ref> propose a feature-rich HMM (FHMM), which further parameterizes p(x i y i ) with many hand-crafted morphological features, greatly boosting M-1 accuracy to 75.5 from 63.1 of the basic HMM.</p><p>In the DL era, researchers have paid a lot of attention to HMMs for unsupervised POS tagging. <ref type="bibr" target="#b21">Lin et al. (2015)</ref> propose a Gaussian HMM (GHMM), where p(x i y i ) corresponds to the probability of the pre-trained word embedding (fixed during training) of x i against the Gaussian distribution of y i . <ref type="bibr" target="#b32">Tran et al. (2016)</ref> propose a neural HMM model (NHMM), where p(x i y i ) and p(y i−1 y i ) are all computed via neural networks with POS tag and word embeddings as inputs. <ref type="bibr" target="#b14">He et al. (2018)</ref> extend the Gaussian HMM of <ref type="bibr" target="#b21">Lin et al. (2015)</ref> by introducing an invertible neural projection (INP) component for the pre-trained word embeddings, which has a similar effect of tuning word embeddings during training. Their INP Gaussian HMM (INP-GHMM) approach achieves state-of-the-art (SOTA) M-1 accuracy (80.8) on PTB so far.</p><p>The major weakness of HMMs is the strong independence assumption in emission probabilities p(x i y i ), which directly hinders the use of contextualized word representations from powerful pre-trained language models (PLMs) such as ELMo/BERT <ref type="bibr" target="#b25">(Peters et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref>. It is a pity since PLMs are able to greatly boost performance of many NLP tasks.</p><p>In this work, we for the first time propose a neural conditional random field autoencoder (CRF-AE) model for unsupervised POS tagging, inspired by <ref type="bibr" target="#b0">Ammar et al. (2014)</ref> who propose a non-neural CRF-AE model. In the discriminative encoder of CRF-AE, we straightforwardly incorporate ELMo word representations. Moreover, inspired by feature-rich HMM <ref type="bibr" target="#b1">(Berg-Kirkpatrick et al., 2010)</ref>, we reintroduce hand-crafted features into the decoder of CRF-AE. In summary, this work makes the following contributions:  <ref type="bibr" target="#b3">Cai et al. (2017)</ref> also extend the idea to non-neural unsupervised dependency parsing. The basic idea is first producing latent structures, i.e., POS tag sequences, with a discriminative CRF over the observed sentence, and then reconstructing the original sentence given each latent structure. The two steps correspond to the encoder and the decoder respectively.</p><p>Training loss. We denote a sentence as x = x 1 , x 2 , ⋯, x i , ⋯, x n , and a POS tag sequence as y = y 1 , y 2 , ⋯, y i , ⋯, y n . Given an unlabeled dataset D which does not contain any POS tag sequences, the training loss is:</p><formula xml:id="formula_0">L(D; φ, θ) = − x∈D log E y∼p(y x;φ) p(x y; θ) + λ φ 2 2 + θ 2 2 ,<label>(1)</label></formula><p>where p(y x; φ) is the CRF encoder; p(x y; θ) is the decoder; φ and θ are model parameters.</p><p>This training loss encourages the model to meet the intuition that a high-probability POS sequence should also permit reconstruction of the sentence with a high probability. <ref type="bibr" target="#b0">Ammar et al. (2014)</ref> adopt the Expectation-Maximization (EM) algorithm for training. In this work, we directly compute the training loss via the Forward algorithm. Then, we employ the powerful AutoGrad function of deep learning to compute the gradient of each parameter. Our preliminary experiments on HMM and feature-rich HMM show that this gradient-based approach is consistently superior to EM in both efficiency and performance.</p><p>Inference. During evaluation, we follow <ref type="bibr" target="#b0">Ammar et al. (2014)</ref> and use both the CRF and the reconstruction probabilities to obtain the optimal tag sequence:</p><formula xml:id="formula_1">y * = arg max y p(y x; φ)p(x y; θ),<label>(2)</label></formula><p>which can be solved by the Viterbi algorithm. CRF Encoder: p(y x; φ). As a discriminative log-linear model, the CRF encoder defines a conditional probability:</p><formula xml:id="formula_2">p(y x; φ) = exp (S(x, y; φ)) Z(x; φ) ≡ ∑ y exp(S(x, y; φ)) ,<label>(3)</label></formula><p>where Z(x) is the partition function, also known as the normalization term.</p><p>The score of y given x is decomposed into bigram scores: </p><formula xml:id="formula_3">S(x, y; φ) = n i=1 s (x, y i−1 , y i ; φ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In this work, we for the first time propose a neural CRF-AE and leverage PLM representations and hand-crafted features for unsupervised POS tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CRF Encoder w/ PLM Representations</head><p>As discussed in §2, the CRF-AE framework consists of two major components, i.e., the CRF encoder and the decoder for sentence reconstruction. We first introduce how to enhance the CRF encoder. The major challenge of the CRF encoder is how to induce latent sequences more accurately via effective contextual representations. Like most works before the DL era, <ref type="bibr" target="#b0">Ammar et al. (2014)</ref> employ manually designed features to represent contexts. One of the major advances brought by DL is the strong capability of contextual representation via neural networks like LSTM and Transformer. Furthermore, pre-trained language models, such as ELMo and BERT, greatly amplify this advantage and are shown to be able to substantially improve performance for almost all NLP tasks.</p><p>However, few works have tried to utilize such neural contextualized encoders for unsupervised POS tagging, except <ref type="bibr" target="#b32">Tran et al. (2016)</ref> and <ref type="bibr" target="#b12">Gupta et al. (2020)</ref>. Most importantly, according to our knowledge, there is no work so far that successfully employ PLMs for unsupervised POS tagging.</p><p>In this work, we propose to employ the contextual representations from PLM to enhance the CRF encoder of the CRF-AE model. Here we use ELMo <ref type="bibr" target="#b25">(Peters et al., 2018)</ref> to illustrate our method, which is the same for other PLMs like BERT.</p><p>ELMo outputs. The encoder of ELMo consists of three layers <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>. The bottom layer computes context-free word representations via word-wise character-level convolutional neural networks. The top two layers, each with two unidi-rectional LSTMs (forward and backward), obtain context-aware word representations by concatenating the forward and backward representations.</p><p>After feeding an input sentence into ELMo, each word x i has three representation vectors, i.e., (h 0 i , h<ref type="foot" target="#foot_0">1</ref> i , h 2 i ), corresponding to three encoder layers respectively. Following the standard practice, we take the weighted arithmetic mean (ScalarMix) of output vectors as the final contextualized word representation r i for x i :</p><formula xml:id="formula_4">r i = γ K−1 k=0 ω k h k i ,<label>(7)</label></formula><p>where ω k (0 ≤ k &lt; K) are softmax-normalized weights 1 and K is the layer number; γ is the scale factor of the entire contextualized word representation. In our final model, we only use h 1 i and h 2 i , since including h 0 i degrades performance (see Table <ref type="table" target="#tab_4">2</ref>).</p><p>Minus operation. Apart from specific information of the focused word x i , the contextualized word representation r i from ELMo also contains a lot of common contextual information shared by neighbour words <ref type="bibr" target="#b10">(Ethayarajh, 2019)</ref>. Therefore, inspired by previous works on constituent parsing <ref type="bibr" target="#b33">(Wang and Chang, 2016;</ref><ref type="bibr" target="#b7">Cross and Huang, 2016)</ref>, we adopt the minus operation for representations as follows:</p><formula xml:id="formula_5">m i = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ → r i ← r i ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ − ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ → r i−1 ← r i+1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ ,<label>(8)</label></formula><p>where → r i is the forward part of the final contextualized word representation r i and ← r i is backward one. m i is the word representation of x i after the minus operation.</p><p>Bottleneck MLP. The ELMo adopts large dimensions d, i.e., 1024, to encode as much information as possible. Representations from ELMo contains syntax clues and even semantic ones besides the information about the POS. Inspired by supervised dependency parsing models <ref type="bibr" target="#b9">(Dozat and Manning, 2017;</ref><ref type="bibr" target="#b19">Li and Eisner, 2019)</ref>  <ref type="table">1</ref>: Feature templates for feature-rich reconstruction. †: before extracting features, we replace continuous digits into a single "0" in each word. ‡: features appeared less then 50 times in the training data are replaced with a special UNK feature.</p><p>information will be stripped away:</p><formula xml:id="formula_6">c i = MLP ⧖ (m i ) = LeakyReLU (W ⧖ ⋅ LayerNorm(m i ) + b ⧖ ) ,<label>(9)</label></formula><p>where the bottleneck size</p><formula xml:id="formula_7">d ′ ≪ d is output dimen- sions of the bottleneck projection weight W ⧖ ∈ R d×d ′ and the bias b ⧖ ∈ R d ′ .</formula><p>Scorer. The definition of a POS tagging sequence y given x is identical to equation 4. But the definition of bigram scores is different from the vanilla CRF-AE. Here, a bigram score consists of two parts: a unigram score s (x, y i ; φ) estimated from ELMo representations and a matrix-maintained transition score t (y i−1 , y i ; φ).</p><formula xml:id="formula_8">s (x, y i−1 , y i ; φ) = s (x, y i ; φ) + t (y i−1 , y i ; φ) . (10) Specifically, s (x, y i ) is calculated as follows: s (x, y i ; φ) = LayerNorm(W s ⋅ c i + b s ) [y i ] , (11) where W s ∈ R d ′ × Y is the projection weight of scoring, b s ∈ R Y is the scoring bias, and Y is the POS tag set. [y i ] is the index selection operation.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reconstruction w/ Hand-crafted Features</head><p>In <ref type="bibr" target="#b0">Ammar et al. (2014)</ref>, the reconstruction probabilities are stored and updated as a matrix. The conditional probability p(x i y i ), i.e., generating x i given y i , is modeled at the whole-word level. This leads to the data sparseness problem. For rare words, the probabilities are usually unreliable.</p><p>Therefore, we borrow the idea of feature-rich HMM by <ref type="bibr" target="#b1">Berg-Kirkpatrick et al. (2010)</ref>. The idea is to utilize rich morphological information to learn more reliable generation probability. For example, suffixes usually provide strong clues to POS categories. In this work, we adopt the feature templates proposed by <ref type="bibr" target="#b1">Berg-Kirkpatrick et al. (2010)</ref>, as shown in Table <ref type="table">1</ref>.</p><p>With the hand-crafted features, we then parameterize tag-to-word emission probabilities as local multinomials:</p><formula xml:id="formula_9">p(x i y i ; θ) = exp (θ ⋅ f (x i , y i )) ∑ x ′ ∈V exp (θ ⋅ f (x ′ , y i )) (<label>12</label></formula><formula xml:id="formula_10">)</formula><p>where θ is the feature weight vector and V is the vocabulary set.</p><p>4 Experiments on English PTB (1) WSJ-All. Almost all previous works train and evaluate their models on the entire WSJ data. We report results on WSJ-All for comparison with previous works. However, this data setting is very unfriendly for selecting hyper-parameters, such as stopping and best epoch numbers, M-1 mappings, learning rates, network dimensions, etc. It is probable that some previous works make modeling choices by directly looking at the evaluation performance, since training loss (e.g., data likelihood) is quite loosely correlated with performance. Such details are usually omitted or only implicitly discussed in previous works.</p><p>(2) WSJ-Split. We follow the practice in unsupervised dependency parsing and divide the WSJ dataset into train (sections 02-21), dev (section 22) and test (section 23). We tune hyper-parameters and study the contributions of individual model components by referring to performance on WSJ-Dev. Moreover, we determine the best many-to-one mappings on WSJ-Dev, which are directly used to compute many-to-one accuracy (M-1) on both WSJ-Dev and WSJ-Test.</p><p>We strongly suggest that in future researchers can adopt the WSJ-Split setting. First, the WSJ-Split setting is more realistic because it is able to evaluate a model's generalization ability with out-of-vocabulary words. Second, it is more reasonable and fairer to use WSJ-dev to choose hyper-parameters and it is usually feasible to manually annotate a dev data, even if very small-scale. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Development on WSJ-Split</head><p>Using which ELMo layers. As mentioned above, ELMo produces three representation vectors for each word x, corresponding to its three encoder layers. Since the usefulness of information contained in different ELMo layers is unknown for our task at hand, we conduct experiments to study which layers to use and how to use them. Table <ref type="table" target="#tab_4">2</ref> shows the results. When using single-layer representations, it is obvious that using one of the top two layers (1/2) is superior to using the bottom 0-th layer<ref type="foot" target="#foot_1">2</ref> . This is in line with our expectation considering that the 0-th layer corresponds to contextindependent word type embeddings. The first layer is superior to the second one, which is consistent with <ref type="bibr" target="#b25">Peters et al. (2018)</ref>, who also conclude that the information contained by the first layer are more suitable for POS tagging than the second layer.</p><p>Then we try to combine multiple layers by using aforementioned ScalarMix in Equation <ref type="formula" target="#formula_4">7</ref>. It is clear that using the top two contextualized layers ({1, 2}) achieves best performance. We find that the weight contribution of layer 1 and 2 is about 92% vs. 8%, confirming again that the first contextualized layer provides the majority of syntactic information, while the second layer is more concerned with high-level semantics. We can also see that replacing ScalarMix with simple concatenation leads to large performance drop.</p><p>Comparing M-1, 1-1, VM, and LL, we can see that M-1, 1-1 and VM are highly correlated, whereas LL is quite loosely correlated with model performance, suggesting that training loss cannot be used for selecting models or tuning hyperparameters.</p><p>In the following, we try to understand the contribution of different components by removing one from the full CRF-AE model at a time. Table <ref type="table" target="#tab_5">3</ref> shows the results.</p><p>Usefulness of hand-crafted features. In order to measure the effectiveness of hand-crafted features in the reconstruction part, we revert to the vanilla matrix-maintained version. We can see that rich hand-crafted features are critical and not using them leads to the largest performance drop.</p><p>Usefulness of PLMs. We first replace pretrained ELMo with a conventional three-layer BiL-STM encoder that is trained from scratch. We use pre-trained word embeddings of <ref type="bibr" target="#b14">He et al. (2018)</ref>  very useful information. We have also tried to replace ELMo with BERT without much hyperparameter tuning, as shown in the bottom row, but found that the performance decreases. The results are similar on the multilingual UD data in Table <ref type="table" target="#tab_9">6</ref>. We suspect the reasons are two-fold. First, we did not carefully tune the hyper-parameters for using BERT due to time and resource limitation. Second, we suspect the ELMo word representations suffice and are even more suitable for unsupervised POS tagging. The POS tag of a word usually heavily depends on neighboring words within a small window, which makes the BiLSTM encoder superior to Transformer. The latter is more powerful to capture long-distance dependencies.</p><p>Usefulness of minus operation. Besides the minus operation in Equation <ref type="formula" target="#formula_5">8</ref>, the default choice is directly using the ELMo output, i.e., r i . As shown in the fourth row, models without the minus operation are inferior to the models with the minus operation. We believe it is because the original ELMo representations have a lot of common contextual information shared by neighbour words, and the minus operation can remove them.</p><p>Usefulness of the three-stage training procedure. To find out the effect of our three-step progressive training procedure, we randomly initialize model parameters. The result shows that the randomly initialization decreases model performances substantially. It proves that the three-stage training procedure helps models find relatively good initial parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on WSJ-Test</head><p>We report results on WSJ-Test in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison on WSJ-All</head><p>In order to compare with previous works, we report results on WSJ-All in Table <ref type="table">5</ref>. We directly use all hyper-parameters obtained from WSJ-Split. We can see that our proposed model outperforms all previous works by large margin. The INP-GHMM model <ref type="bibr" target="#b14">(He et al., 2018)</ref> achieves the previous best performance on WSJ-All. Our model outperforms theirs by 2.41 and 3.54 on M-1 and VM, respectively.</p><p>5 Experiments on Multilingual UD 5.1 Settings Data. For more thorough comparison with previous works, we also report results<ref type="foot" target="#foot_3">4</ref> on the Multilingual Universal Dependencies treebank v2.0 (UD), consisting of 10 languages <ref type="bibr" target="#b22">(McDonald et al., 2013)</ref>. Similar to experiments on English PTB, we adopt two settings for the UD data, i.e., UD-Split and UD-All. For UD-Split, we adopt the default partition of the UD data.</p><p>Hyper-parameters. We directly adopt most hyper-parameters obtained for PTB with three important exceptions. First, The number of predicted POS tags is changed to 12. Second, since the scale of data for each language diverge a lot, we adjust the feature cutoff threshold to be proportional to the token number against English partition. For example, the "de" data contains about 293k tokens,  <ref type="bibr" target="#b30">Stratos (2019)</ref>, and, G'20 for <ref type="bibr" target="#b12">Gupta et al. (2020)</ref>.</p><p>which is about 28% of that of "en" (1M), and therefore we set the threshold to 14 (28% × 50). Third, we adjust the hand-crafted features to accommodate the 12-tag UD standard and characteristics of different languages, detailed in the following.</p><p>Modifications on hand-crafted features. The fine-grained 45-tag WSJ standard is greatly different from the coarse-grained 12-tag UD standard adopted by the multilingual UD datasets <ref type="bibr" target="#b26">(Petrov et al., 2012)</ref>. Therefore, we start from the features of <ref type="bibr" target="#b1">Berg-Kirkpatrick et al. (2010)</ref> in Table <ref type="table">1</ref> as the base, and make adjustments from two aspects.</p><p>(1) Adjustments for UD. We remove the "Capitalized" feature, which is originally purposed to distinguish proper and common nouns which correspond to a single UD tag. Moreover, we replace all punctuation marks with a special "PUNCT" word form, add a new feature template "is-Punctuation", as UD uses a single tag for punctuation marks.</p><p>(2) Adjustments for specific languages<ref type="foot" target="#foot_4">5</ref> . The UD tag set doesn't distinguish inflections such as numbers, tenses, and genders. We find this can be accommodated by customizing suffix uni/bi/trigram features. We simply remove a certain number of ending characters (related to inflectional affixes)  for a word form before extracting suffix features. We remove the last character for "it", and the last two characters for "de". For "fr", "es", and "pt-br", we remove last two characters if the word ends with "s", and the last one otherwise. For "en", we only remove the last "s" letter if applicable.</p><formula xml:id="formula_11">±2.2 - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on UD-Split</head><p>Table <ref type="table" target="#tab_9">6</ref> shows the M-1 results. For 1-1 and VMresults, please refer to Table <ref type="table">10</ref> and Table <ref type="table">9</ref> in the Appendix.</p><p>We perform ablation study on UD-Dev. Most of the results show the same trend as on WSJ-Dev. In particular, we find that our two adjustment strategies for the UD data are very helpful, and the UD adjustment is more helpful. After observation, we find that without UD-specific adjustments, punctuation marks are more likely to be divided into multiple tags. For example, models may assign three different tags to periods, commas, and quotation marks. Moreover, with the removal of the "Capitalized" feature, which is one of the UD adjustments, the models no longer distinguish common and proper nouns and assign one tag to them. <ref type="foot" target="#foot_5">6</ref>with GHMM, and the simple GHMM is already more superior to our model. Therefore, we replace FHMM with GHMM in the first stage of our training procedure. Results show that our models are substantially improved in "de". However, we still do not understand the reason behind these results, which we leave for future investigation due to time limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Comparison on UD-All</head><p>To compare with previous works, we report results on UD-All in Table <ref type="table">11</ref> in the Appendix. For thorough comparison, we also re-run GHMM and INP-GHMM on UD-All. The results show identical trends as those on UD-Split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Works</head><p>Unsupervised POS tagging. In addition to HMMs and the CRF-AE, other approaches for unsupervised POS tagging are as follows.</p><p>(1) Clustering. The clustering approach, as a mainstream unsupervised learning technique, is also investigated for unsupervised POS tagging <ref type="bibr" target="#b35">Yatbaz et al. (2012)</ref>; <ref type="bibr" target="#b36">Yuret et al. (2014)</ref>; <ref type="bibr" target="#b12">Gupta et al. (2020)</ref>. All these works adopt the k-means algorithm to divide word tokens into different groups. The main difference among them is how to represent words. <ref type="bibr" target="#b35">Yatbaz et al. (2012)</ref> propose to learn context-free word embeddings by minimizing the distance between each word and its substituted words. Substituted words are selected according to a n-gram language model. <ref type="bibr" target="#b36">Yuret et al. (2014)</ref> extend their previous work to produce contextsensitive word embeddings. <ref type="bibr" target="#b12">Gupta et al. (2020)</ref> adopt a deep clustering approach that uses a feedforward neural network to transform word representations from mBERT into a lower-dimension clustering-friendly space. Transformation with reconstruction loss and clustering are jointly trained. Unfortunately, all three works have not released their source code.</p><p>(2) Mutual information maximization. The mutual information maximization approach is proposed by <ref type="bibr" target="#b30">Stratos (2019)</ref>. The idea is that we can predict POS tags in two ways (using the words themselves or their context), and predictions from these two ways should agree as more as possible.</p><p>Utilizing PLMs for unsupervised tagging or parsing. As discussed earlier, SyntDEC <ref type="bibr" target="#b12">(Gupta et al., 2020)</ref> is the only work that employs PLMs for unsupervised POS tagging based on deep clustering. As for unsupervised parsing, <ref type="bibr" target="#b34">Wu et al. (2020)</ref> propose a perturbed masking technique to estimate inter-word correlations and then induce syntax trees from those correlations. <ref type="bibr" target="#b17">Kim et al. (2020)</ref> extract constituency trees from the PLMs through capturing syntactical proximity between representations of two adjacent words (or subwords). If the proximity is loose, then it is likely that the middle position of the two words corresponds to some constituent boundary. <ref type="bibr" target="#b4">Cao et al. (2020)</ref> successfully exploit PLMs for unsupervised constituency parsing based on constituency test, achieving SOTA performance.</p><p>Utilizing CRF-AE. <ref type="bibr" target="#b3">Cai et al. (2017)</ref> apply CRF-AE to unsupervised dependency parsing. They use the encoder to generate a most likely dependency tree and then force the decoder to reconstruct the input sentence from the tree. <ref type="bibr" target="#b37">Zhang et al. (2017)</ref> propose a neural CRF-AE for semi-supervised learning on sequence labeling problems (including POS tagging) and <ref type="bibr" target="#b16">Jia et al. (2020)</ref> adopt a neural CRF-AE for semi-supervised semantic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This work bridges PLMs and hand-crafted features for unsupervised POS tagging. Based on the CRF-AE framework, we employ powerful contextualized representations from PLMs in the CRF encoder, and incorporate rich morphological features for better reconstruction. Our proposed approach achieves new SOTA on 45-tag English PTB and 12tag multilingual UD datasets, outperforming previous results by large margin. Experiments and analysis show that rich features and PLM representations are critical for the superior performance of our model. Meanwhile, simple adjustments of hand-crafted features are key for the success of our model on languages other than English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of Evaluation Metrics</head><p>The core issue of the unsupervised POS tagging evaluation is that we can not directly compute the tagging accuracy since the correspondence between ground truth tags and predicted tag indexes (indexto-tag mapping) is unknown and varies from model to model. The different evaluation metrics handle this issue in different way.</p><p>A.1 Many-to-One Accuracy (M-1) M-1 is the most commonly used evaluation metric. It addresses the problem of correspondence by assigning each predicted tag index j ∈ P to its most frequent co-occurring ground truth tag g i ∈ G:</p><formula xml:id="formula_12">M-1(A) = j max g i A g i ,j ,<label>(13)</label></formula><p>where A ∈ R n×n is contingency matrix and the matrix item A g i ,j is the number of words which are annotated as a g i and predicted as a j by the model to be evaluated. This metric, obviously, allows different predicted indexes to map to the same ground truth tag<ref type="foot" target="#foot_6">7</ref> .</p><p>A.2 One-to-One Accuracy (1-1)</p><p>Different from M-1 that we allows a ground truth tag g i corresponding to multiple predicted indexes, 1-1 only allows one predicted index can be assigned to a ground truth tag, and vice versa. Calculating 1-1 is a typical assignment problem that finding a optimal bijection function f ∶ P → G that maximums the correct matching count from all possible bijection functions F:</p><formula xml:id="formula_13">1-1(A) = max f ∈F j A f (j),j .<label>(14)</label></formula><p>In this paper we solve this assignment problem with the Hungarian algorithm<ref type="foot" target="#foot_7">8</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Validity-Measure (VM)</head><p>VM <ref type="bibr" target="#b27">(Rosenberg and Hirschberg, 2007</ref>) is an entropy-based measure, which do not require the index-to-tag mapping and considers two criteria: homogeneity h and completeness c. The homogeneity of a predicted index indicates the purity of its co-occurring ground truth tags. The predicted index j results the highest homogeneity when it only co-occur with g i , i.e., A g i ,j = ∑ g i ′ A g i ′ ,j , and has a low homogeneity it appears with different ground truth tags randomly. The homogeneity of a model is the simply the sum of the homogeneity of all index indicates. The completeness is symmetrical to homogeneity, merely exchanging the position of predicted indexes and ground truth tags. VM employs the conditional entropy to measure the value of homogeneity and completeness:</p><formula xml:id="formula_14">H(G P, A) = − j i A g i ,j N log A g i ,j ∑ g i ′ A g i ′ ,j ,<label>(15)</label></formula><formula xml:id="formula_15">H(P G, A) = − i j A g i ,j N log A g i ,j ∑ j ′ A g i ,j ′ ,<label>(16)</label></formula><p>where A ∈ R n×n is contingency matrix and the matrix item A g i ,j is the number of words which are annotated as a g i and predicted as a j.</p><p>To alleviate the impact of the size of the dataset and the numbers of the POS class, the conditional entropy is normalized by the entropy of ground truth POS tag H(G, A) and H(P, A) for homogeneity and completeness, respectively:</p><formula xml:id="formula_16">h(A) = 1 − H(G P, A) H(G, A) ,<label>(17)</label></formula><formula xml:id="formula_17">c(A) = 1 − H(P G, A) H(P, A) ,<label>(18)</label></formula><p>where</p><formula xml:id="formula_18">H(G, A) = − i ∑ j A g i ,j N log ∑ j A g i ,j N ,<label>(19)</label></formula><formula xml:id="formula_19">H(P, A) = − j ∑ g i A g i ,j N log ∑ g i A g i ,j N .<label>(20)</label></formula><p>Completeness is symmetrical to homogeneity, merely exchanging G and P in the formulas.</p><p>In order to balance the significance between homogeneity and completeness, VM is defined as the weighted harmonic mean of homogeneity and completeness:</p><formula xml:id="formula_20">VM(A) = (1 + β)h(A)c(A) βh(A) + c(A) ,<label>(21)</label></formula><p>where β are set to 1 in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Hyper-parameters B.1 Model</head><p>The number of predicted POS tags is 45 for experiments on WSJ and 12 for Multilingual experiments. The ELMo parameters we use for experiments on WSJ are "Original (5.5B)"<ref type="foot" target="#foot_8">9</ref> from AllenNLP. The parameters for Multilingual are from "ELMoForManyLangs"<ref type="foot" target="#foot_9">10</ref> ( <ref type="bibr" target="#b5">Che et al., 2018)</ref>. We use "bert-base-cased" (BERT) and "bert-base-multilingual-cased" (mBERT) <ref type="foot" target="#foot_10">11</ref> for the ablation study of PLMs on WSJ and UD respectively. We do not fine-tune ELMo parameters. The dropout value is uniformly set to 0.33, and the negative slope of the activation function Leaky-ReLU is set to 1 × 10 −2 . The seeds we selected for experiments are 0, 1, 2, 3, 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Feature</head><p>We set the feature cutoff threshold to 50, which means that all features that appear in the training data less than 50 times are replaced with a special "UNK" feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Training</head><p>We use a mini-batch update strategy with a batch size of 5000 words and optimize models with Adam. The learning rate used in the training of the FHMM in the first step is 0.5. The CRF encoder is then trained on pseudo-labeled data for 5 epochs with a learning rate of 2 × 10 −3 in the subsequent pre-training step. In the final step, the CRF encoder has a learning rate of 1 × 10 −2 , and we set the reconstruction learning rate to 2 × 10 −1 . Other hyperparameters are identical among all three steps in training procedure, The β 1 and β 2 are both 0.9. The learning rate decay is 0.75 per 45 epochs, the gradient clipping value is 5, and the weight decay value λ is 1 × 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Re-run INP-GHMM Hyper-parameters</head><p>We use the word-wise character-level convolutional layer (0-th layer) of ELMo to extract word embeddings. We use 8 coupling layers. To accelerate the training of INP-GHMM, we increase the batch sizes from 32 to 512 sentences. We also decrease the learning rate from 1 × 10 −3 to 5 × 10 −4 , as we found that high learning rates lead to performance decreases as training progresses.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Explanation of adjustments for specific languages on UD</head><p>Most of European languages are inflected languages. Some words are inflected for number, gender, tense, aspect and so on. For example in English nouns are inflected for number (singular or plural); verbs for tense. A major way to inflect words is adding inflectional suffixes to the end of words, e.g., English nouns inflected for number with suffix "s" ("museum" → "museums"). Therefore, in some languages suffixes is more closely related to inflections than coarse-grained POS. For instance, as shown in Table <ref type="table" target="#tab_10">7</ref>, the last letter of Italian words is highly corresponding to gender and number, and haves little connection to coarse-grained POS. In this work, we simply remove a certain number of ending characters for a word form before extracting suffix features, as shown in Table <ref type="table" target="#tab_11">8</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of POS tagging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>=Figure 3 :</head><label>3</label><figDesc>Figure 3: Model architecture of proposed model. x is the "CRF encoder w/ ELMo representations" and y is the "reconstruction w/ hand-crafted features".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc></figDesc><table><row><cell>Feature</cell><cell>John</cell><cell>75th</cell><cell>two-tiered</cell></row><row><cell>Word</cell><cell>John</cell><cell>0th  †</cell><cell>UNK  ‡</cell></row><row><cell cols="2">Uni-gram Suffix n</cell><cell>h</cell><cell>d</cell></row><row><cell>Bi-gram Suffix</cell><cell>hn</cell><cell>th</cell><cell>ed</cell></row><row><cell>Tri-gram Suffix</cell><cell>ohn</cell><cell>0th</cell><cell>red</cell></row><row><cell>Has Digit</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Has Hyphen</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Capitalized</cell><cell></cell><cell></cell><cell></cell></row></table><note>, we adopt a bottleneck MLP (MLP ⧖ ), whose output vector has a very low dimension. Because of the low dimension of the MLP output, redundant and irrelevant</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results of utilizing different layers of the ELMo on WSJ-Dev. ⊕ means directly concatenating the representation vectors of different layers.</figDesc><table><row><cell>Layer</cell><cell>M-1</cell><cell>1-1</cell><cell>VM</cell><cell>LL</cell></row><row><cell>0</cell><cell>79.98 ±0.3</cell><cell>59.13 ±3.4</cell><cell>73.06 ±0.9</cell><cell>-73.06 ±1.4</cell></row><row><cell>1</cell><cell>82.61 ±0.8</cell><cell>63.03 ±5.0</cell><cell>76.98 ±1.2</cell><cell>-79.52 ±0.8</cell></row><row><cell>2</cell><cell>82.45 ±1.0</cell><cell>60.29 ±3.2</cell><cell>76.27 ±1.0</cell><cell>-83.35 ±0.5</cell></row><row><cell>{0, 1, 2}</cell><cell>81.53 ±0.3</cell><cell>64.03 ±4.1</cell><cell>76.21 ±0.7</cell><cell>-76.89 ±0.4</cell></row><row><cell>{1, 2} ⊕</cell><cell>82.28 ±1.3</cell><cell>63.54 ±4.5</cell><cell>76.91 ±1.3</cell><cell>-78.96 ±0.4</cell></row><row><cell>{1, 2}</cell><cell>83.20 ±0.7</cell><cell>65.17 ±2.3</cell><cell>77.69 ±0.7</cell><cell>-80.49 ±0.5</cell></row></table><note>Evaluation metrics. Following previous works, we mainly adopt many-to-one accuracy, and also report one-to-one accuracy (1-1) and validitymeasure (VM) values for better comparison. To reduce the effect of performance vibration, we follow previous works, run each model for five times with different seeds, and report the mean and standard deviation. Please see Appendix A for details.Hyper-parameters. We set the number of predicted POS tags to 45 and the output dimensions of MLP ⧖ to 5. We train each model on the training data for at most 50 epochs, and select the best epoch based on data log-likelihood (LL). Please see Appendix B for full details of hyper-parameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>as encoder inputs. As expected, performance also declines a lot. It shows that ELMo does provide The contribution of different components on WSJ-Dev by removing one component at a time.</figDesc><table><row><cell>Model</cell><cell>M-1</cell><cell>1-1</cell><cell>VM</cell></row><row><cell>Full CRF-AE</cell><cell>83.20 ±0.7</cell><cell>65.17 ±2.3</cell><cell>77.69 ±0.7</cell></row><row><cell>w/o Features</cell><cell>76.74 ±1.4</cell><cell>61.34 ±3.5</cell><cell>73.55 ±1.0</cell></row><row><cell>w/o PLM Repr.</cell><cell>78.40 ±0.9</cell><cell>61.31 ±4.9</cell><cell>72.55 ±1.6</cell></row><row><cell>w/o Minus Op.</cell><cell>81.28 ±1.5</cell><cell>63.07 ±2.8</cell><cell>76.04 ±1.1</cell></row><row><cell cols="2">w/o 3-stage Train 80.21 ±3.4</cell><cell>59.50 ±2.9</cell><cell>75.70 ±1.2</cell></row><row><cell>ELMo → BERT</cell><cell>82.30 ±1.0</cell><cell>62.78 ±5.8</cell><cell>76.13 ±1.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>M-1 accuracy on UD-Split. Upper Part: The contribution of different components on UD-Dev by removing one component at a time. * means adopting the language-specific suffix features for this language. " " means the result is identical to that of Full CRF-AE. Lower Part: Performance comparison on UD-Test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Examples of inflections of Italian adjective, pronoun, and noun. "M." means the gender Masculine and "F." means Feminine.</figDesc><table><row><cell></cell><cell cols="3">Gender Singular Plural</cell><cell>Gloss</cell></row><row><cell>Adj.</cell><cell>M. F.</cell><cell>rosso rossa</cell><cell>rossi rosse</cell><cell>red</cell></row><row><cell>Pron.</cell><cell>M. F.</cell><cell>lo la</cell><cell>li le</cell><cell>him/ her/them</cell></row><row><cell>Noun</cell><cell>M. F.</cell><cell cols="3">bambino bambini boy/girl bambina bambine</cell></row><row><cell cols="4">Langs. Uni-gram Bi-gram</cell><cell>Tri-gram</cell></row><row><cell>it</cell><cell cols="2">museo musei</cell><cell>museo musei</cell><cell>museo musei</cell></row><row><cell>de</cell><cell cols="2">museum museen</cell><cell>museum museen</cell><cell>museum museen</cell></row><row><cell>fr</cell><cell cols="2">musée musées</cell><cell>musée musées</cell><cell>musée musées</cell></row><row><cell>es</cell><cell cols="2">museo museos</cell><cell>museo museos</cell><cell>museo museos</cell></row><row><cell>pt-br</cell><cell cols="2">museu museus</cell><cell>museu museus</cell><cell>museu museus</cell></row><row><cell>en</cell><cell cols="4">museum museums museums museums museum museum</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Language-specific suffix features for the UD datasets. The underlined characters represent extracted suffix features.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The weights are trained only in the second stage of our training method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Minus operations do not apply to vectors at the 0-th layer, i.e., context-independent word type embeddings, which are directly used as mi.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/jxhe/struct-learning-with-flow</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We run each model for five times with different random seeds.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">We only adopt language-specific adjustments for "de", "en", "es", "fr", "it" and "pt-br".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">However, we find that some models still divide nouns into multiple tags by some unknown criteria.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">In the WSJ-Split data setting, the index-to-tag mapping of metrics for WSJ-Dev and WSJ-Test are both observed from WSJ-Dev.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://en.wikipedia.org/wiki/Hungar ian_algorithm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">https://allennlp.org/elmo</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9">https://github.com/HIT-SCIR/ELMoForM anyLangs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10">https://github.com/google-research/b ert</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for the helpful comments. We are very grateful to Wei Jiang for his early-stage exploration on unsupervised POS tagging. We also thank Chen Gong, Yu Zhang, Ying Li, Qingrong Xia, Yahui Liu, and Tong Zhu for their help in paper writing and polishing. This work was supported by National Natural Science Foundation of China (Grant No.  62176173, 61876116)  and a Project Funded by the Priority Academic Program Development (PAPD) of Jiangsu Higher Education Institutions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Without language-specific adjustments, highly inflected languages, e.g., Italian (it) and Brazilian Portuguese (pt-br), are more likely to distinguish words by their number or gender rather than part-ofspeech. For example, in English, models without making language-specific adjustments will tend to split nouns into two classes: single nouns and plural nouns ending with "s".</p><p>We report the M-1 results on UD-Test in Table <ref type="table">6</ref>. We run our implemented vanilla HMM and featurerich HMM, and the latter adopt the same features after UD and linguage adjustments. Unfortunately, we are unable to re-run SyntDEC, the current SOTA on UD-All, since its authors <ref type="bibr" target="#b12">(Gupta et al., 2020)</ref> have not yet released their code. We also re-run INP-GHMM <ref type="bibr" target="#b14">(He et al., 2018)</ref> with their released code, which is the current SOTA on WSJ-All. We take context-free word representations (0-th layer) of ELMo as inputs of INP-GHMM, which should be better than Skip-Gram embeddings. Please see Appendix C for details of hyper-parameters.</p><p>Results show that our models achieve the highest M-1 accuracy on 9 out of 10 languages, except "de". After investigation on why our models fail to outperform INP-GHMM on "de", we find that the direct reason is that INP-GHMM is initialized 76.17   <ref type="bibr" target="#b30">Stratos (2019)</ref>, and G'20 for <ref type="bibr" target="#b12">Gupta et al. (2020)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional random field autoencoders for unsupervised structured prediction</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
				<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3311" to="3319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Morphosyntactic tagging with a meta-BiLSTM model over context sensitive token encodings</title>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonçalo</forename><surname>Simões</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2642" to="2652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CRF autoencoder for unsupervised dependency parsing</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1638" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised parsing via constituency tests</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.389</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4798" to="4808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards better UD parsing: Deep contextualized word embeddings, ensemble, and treebank concatenation</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-2005</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
				<meeting>of CoNLL<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two decades of unsupervised POS induction: How far have we come?</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1001</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings</title>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1006</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Posterior vs parameter sparsity in latent variable models</title>
		<author>
			<persName><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
				<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="664" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Clustering contextualized representations of text for unsupervised syntax induction</title>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<idno>abs/2010.12784</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dependency grammar induction with neural lexicalization and big training data</title>
		<author>
			<persName><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1176</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1683" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of syntactic structure with invertible neural projections</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1292" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic dependency parsing using CRF autoencoders</title>
		<author>
			<persName><forename type="first">Zixia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youmi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.607</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6795" to="6805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction</title>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Edmiston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanggoo</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/1218955.1219016</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Specializing word embeddings (for parsing) by information bottleneck</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1276</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2744" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised POS induction with word embeddings</title>
		<author>
			<persName><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1144</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1311" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal Dependency annotation for multilingual parsing</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Quirmbach-Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Bedini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="92" to="97" />
		</imprint>
		<respStmt>
			<orgName>Núria Bertomeu Castelló, and Jungmee Lee</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tagging English text with a probabilistic model</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="171" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Insideoutside reestimation from partially bracketed corpora</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop</title>
				<meeting><address><addrLine>Harriman, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-02-23">1992. February 23-26, 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
				<meeting>of LREC<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2089" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vmeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimization with EM and expectation-conjugate-gradient</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="672" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast unsupervised incremental parsing</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Seginer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="384" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mutual information maximization for simple and accurate part-of-speech induction</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1113</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging with anchor hidden Markov models</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00096</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="245" to="257" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised neural hidden Markov models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-5907</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Structured Prediction for NLP</title>
				<meeting>of the Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional LSTM</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1218</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2306" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perturbed masking: Parameter-free probing for analyzing and interpreting BERT</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.383</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4166" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning syntactic categories using paradigmatic representations of word context</title>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Ali Yatbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enis</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="940" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised instance-based part of speech induction using probable substitutes</title>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Ali Yatbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enis</forename><surname>Sert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
				<meeting>of COLING<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2303" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised structured prediction with neural CRF autoencoder</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1701" to="1711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Is POS tagging necessary or even helpful for neural dependency parsing?</title>
		<author>
			<persName><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60450-9_15</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NLPCC</title>
				<meeting>of NLPCC</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="179" to="191" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
