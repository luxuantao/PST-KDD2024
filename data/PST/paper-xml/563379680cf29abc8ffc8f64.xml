<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>110819</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Synthetical Automation for Process Industries</orgName>
								<address>
									<postCode>110819</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">052DF16CC9652C19316C6C8D683B2675</idno>
					<idno type="DOI">10.1109/TNNLS.2015.2485259</idno>
					<note type="submission">received April 15, 2015; revised September 25, 2015; accepted September 28, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-This paper is concerned with the exponential stability and stabilization of memristive neural networks (MNNs) with delays. First, we present some generalized double-integral inequalities, which include some existing inequalities as their special cases. Second, combining with quadratic convex combination method, these double-integral inequalities are employed to formulate a delay-dependent stability condition for MNNs with delays. Third, a state-dependent switching control law is obtained for MNNs with delays based on the proposed stability conditions. The desired feedback gain matrices are accomplished by solving a set of linear matrix inequalities. Finally, the feasibility and effectiveness of the proposed results are tested by two numerical examples.</p><p>Index Terms-Exponential stability, generalized doubleintegral inequality, memristive neural networks (MNNs), quadratic convex combination, stabilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M EMRISTOR, as the fourth fundamental passive circuit, was originally postulated by Chua in 1971, and was first demonstrated by Hewlett-Packard research team in 2008. This new circuit element shares many properties of resistors and shares the same unit of measurement (i.e., ohm) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. One of the most remarkable characteristics of memristor is that it can memorize the passed quantity of electric charge by supplying a voltage or current. The other noticeable property of memristor is its nanometer dimension. Memristor devices can be scaled down to 10 nm even blow, and memristive memories can achieve a very high integration density of 100 Gb/cm 2 , a few times higher than the advanced flash memory technologies. These unique properties make them an attractive candidate device for massively parallel, large-scale neuromorphic systems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Recently, various memristor-based networks have been established by means of the memristive circuits. For example, in <ref type="bibr" target="#b4">[5]</ref>, a memristor discrete-time cellular neural network was designed, which could perform a number of applications, such as logical operations, higher brain functions, and RSA algorithm (named after its inventors, Rivest, Shamir and Adleman), and so on. In <ref type="bibr" target="#b5">[6]</ref>, a neural network hardware architecture using the memristor bridge synapse was proposed to solve the problem of nonvolatile weight storage. In <ref type="bibr" target="#b6">[7]</ref>, a novel memristor-based cellular neural network was designed, which could be used in image processing field.</p><p>In addition, it is well known that pinched hysteresis loop is a fingerprint of the memristor <ref type="bibr" target="#b7">[8]</ref>. This means that a lag occurs between the application and the removal of a field and its subsequent effect, which is the same as the neurons in the human brain <ref type="bibr" target="#b8">[9]</ref>. Thus, one can build a new model of neural networks [i.e., memristive neural networks (MNNs)] to emulate the human brain by replacing the resistors in the primitive neural networks with memristors <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b32">[33]</ref>. In recent years, MNNs have emerged as a hot subject of research due to their immense potential applications in many fields, such as brain emulation, combinatorial optimization, pattern recognition, image processing, memory storage, and secure communication <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b19">[20]</ref>. In some of these applications, the global asymptotic/exponential stability of a unique equilibrium point is of primary importance from a theoretical and application point of view <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b37">[38]</ref>. Time delay is an inevitable phenomenon in dynamical systems, and it is also one of the main sources leading to instability or other poor performance of a system <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b44">[45]</ref>. Therefore, the stability analysis for MNNs with delays has attracted a great deal of interest (see <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref> and the references therein). In the field of stability analysis of neural networks, Jensen inequality or some double-integral inequalities are the popular tools to deal with the integral terms of the derivative of Lyapunov functionals <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>. However, such kinds of tools themselves are conservative to some degree, and there is still some space to be improved. It is well known that an excellent magnifying method of inequalities shall significantly improve the stability results. Thus, how to reduce such gap is an important and interesting research topic.</p><p>In many practical applications, if the neural network is unstable for some choices of neural model parameters, or the stability performance of neural system is devastated by the ineluctable time delays, one usually has to consider the stabilization control of neural networks <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b48">[49]</ref>. These phenomena shall also occur in the application of MNNs.</p><p>On the other hand, the stabilization control for MNNs has many potential applications in the field of neurobiology (e.g., memristive physiological states <ref type="bibr" target="#b26">[27]</ref>). Hence, the stabilization issue for MNNs is of both theoretical and practical importance <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b31">[32]</ref>. By using the theories of differential inclusions and set-valued maps, the exponential stabilization problem of MNNs was primarily studied in <ref type="bibr" target="#b26">[27]</ref>, in which the optimal control problem was also considered. In <ref type="bibr" target="#b27">[28]</ref>, intermittent controller was adopted to stabilize the chaotic MNNs. In <ref type="bibr" target="#b28">[29]</ref>, the finite-time stabilizability and instabilizability problems for delayed MNNs were first investigated by building a nonlinear discontinuous controller. The designed nonlinear controller was general, and by choosing different parameters of the controller, different results concerning the finite-time stabilizability and the instabilizability of MNNs could be obtained. In <ref type="bibr" target="#b29">[30]</ref>, M-matrix technique was used to verify the exponential stabilization of a class of MNNs models. One of the similarities of the used technique in <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref> is that they all ignore the neuron's excitatory and inhibitory effects on neural networks owing to the use of the absolute values. In <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref>, by employing differential inclusions theory and taking D ∈ [D, D], A ∈ [A, A], and B ∈ [B, B], some stabilization criteria for MNNs had been investigated. In theory, however, the matrix inequalities in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref> are parameterized linear matrix inequalities (LMIs) <ref type="bibr" target="#b45">[46]</ref> rather than LMIs, because these parameters are measurable functions with respect to the neuron states and are time varying in essence. Therefore, it is not easy to check the feasibility of the inequalities in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref>.</p><p>Motivated by the aforementioned analysis, we intend to investigate both the stability and stabilization problems for MNNs with time-varying delays. In this paper, the main contribution is as follows. Some generalized double-integral inequalities, which improve some existing ones, are established. These inequalities contain some free weight matrices, and thus, their flexibilities are improved. Combining these generalized inequalities and quadratic convex combination technique, a novel delay-dependent criterion is derived to ensure the exponential stability of the considered MNNs. Meanwhile, the stability condition is extended to settle the stabilization problem of MNNs. Different from <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>, in which the authors are only concerned with a centralized control law, we keep a watchful eye on the state-dependent switching or the so-called distributed control law. Comparing with the methods used in <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>, this paper fully considers the switched structure characteristic of memristive synaptic weights.</p><p>The rest of this paper is organized as follows. Some preliminaries are presented in Section II, in which some generalized double-integral inequalities are given. In Section III, a novel LMI-based exponential stability result for MNNs is established by binding the generalized double-integral inequalities and quadratic convex combination method together. Based on the obtained stability result, the stabilization issue for MNNs is considered, and a state-dependent switching/distributed control law is designed accordingly. Section IV provides two numerical examples to show the effectiveness of the obtained results. Finally, the conclusion is drawn in Section V.</p><p>Notation: Throughout this paper, R n is the n-dimensional Euclidean space, and R k×n denotes the set of k ×n real matrix. X ≥ 0 (respectively, X &gt; 0) means that the matrix X is a real symmetric positive semidefinite (respectively, positive definite) matrix. X T represents the transpose of a matrix X. Sys{X} is defined as Sys{X} = X T + X. diag{•} denotes a diagonal matrix. I and 0 denote the identity matrix and zero matrix with appropriate dimensions, respectively. * in a matrix represents the elements below the main diagonal of a symmetric matrix. ⊗ denotes the Kronecker product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM DESCRIPTION AND PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Description</head><p>Consider the following MNNs with time-varying delay:</p><formula xml:id="formula_0">ẋ(t) = -D(x(t))x(t) + A(x(t)) f (x(t)) + B(x(t)) f (x(t -τ (t))) + u(t)<label>(1)</label></formula><p>where x(t) = (x 1 (t), x 2 (t), . . . , x n (t)) T is the state vector; τ (t) is the time-varying delay;</p><formula xml:id="formula_1">f (x(•)) = ( f 1 (x 1 (•)), f 2 (x 2 (•)), . . . , f n (x n (•))) T ∈ R n denotes the neuron activa- tion function with f (0) = 0; u(t) = (u 1 (t), u 2 (t), . . . , u n (t)) T is the control input vector; D(x(t)) = diag(d 1 (x 1 (t)), d 2 (x 2 (t)), . . . , d n (x n (t))), A(x(t)) = (a i j (x i (t))) n×n , and B(x(t)) = (b i j (x i (t))</formula><p>) n×n represent the memristive synaptic weights, respectively. The MNNs (1) can be implemented by very-large-scale integration circuits, as shown in Fig. 1 <ref type="bibr" target="#b27">[28]</ref>.</p><p>Referring to the work in <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and <ref type="bibr" target="#b31">[32]</ref>, we suppose the state-dependent parameters in (1) satisfy the following conditions: </p><formula xml:id="formula_2">d i (x i (t)) = d * i , |x i (t)| ≤ T i d * * i , |x i (t)| &gt; T i a i j (x i (t)) = a * i j , |x i (t)| ≤ T i a * * i j , |x i (t)| &gt; T i b i j (x i (t)) = b * i j , |x i (t)| ≤ T i b * * i j , |x i (t)| &gt; T i in which switching jumps T i &gt; 0, d * i &gt; 0, d * * i &gt; 0, a * i j , a * * i j , b * i j ,</formula><formula xml:id="formula_3">i (x i (t)) is d * i , a i j (x i (t)</formula><p>) and b i j (x i (t)) should be a * i j and b * i j , respectively. Hence, the combination number of the possible forms of D(x(t)), A(x(t)), and B(x(t)) is 2 n . We order the 2 n cases in the following way:</p><formula xml:id="formula_4">(D 1 , A 1 , B 1 ), (D 2 , A 2 , B 2 ), . . . , (D 2 n , A 2 n , B 2 n ).</formula><p>Therefore, at any fixed time t ≥ 0, the form of D(x(t)), A(x(t)), and B(x(t)) must be one of the 2 n cases, i.e., there exists some i 0 ∈ {1, 2, . . . , 2 n } such that D(x(t)) = D i 0 , Fig. <ref type="figure">1</ref>. Circuit of MNNs <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A(x(t))</head><p>= A i 0 , and B(x(t)) = B i 0 . Thus, for example, at the fixed time t, MNNs (1) have the following form:</p><formula xml:id="formula_5">ẋ(t) = -D i 0 x(t) + A i 0 f (x(t)) + B i 0 f (x(t -τ (t))) + u(t). (2)</formula><p>Next, for i = 1, 2, . . . , 2 n , we define the characteristic function of D i , A i , and B i at any fixed time t as follows:</p><formula xml:id="formula_6">π i (t) = ⎧ ⎪ ⎨ ⎪ ⎩ 1, D(x(t)) = D i , A(x(t)) = A i and B(x(t)) = B i 0, otherwise.<label>(3)</label></formula><p>Obviously, 2 n i=1 π i (t) = 1. So, MNNs (1) can be expressed as</p><formula xml:id="formula_7">ẋ(t) = 2 n i=1 π i (t)[-D i x(t) + A i f (x(t)) + B i f (x(t -τ (t))) + u(t)] = -D(t)x(t) + A(t) f (x(t))+ B(t) f (x(t -τ (t)))+u(t) (4)</formula><p>where</p><formula xml:id="formula_8">D(t) = 2 n i=1 π i (t)D i , A(t) = 2 n i=1 π i (t)A i , and B(t) = 2 n</formula><p>i=1 π i (t)B i . Throughout this paper, we shall use the following assumptions.</p><p>Assumption 1: There exist matrices</p><formula xml:id="formula_9">F 1 = diag(F - 1 , F - 2 , . . . , F - n ) and F 2 = diag(F + 1 , F + 2 , . . . , F + n )</formula><p>such that the activation functions f j (•) satisfy the following condition:</p><formula xml:id="formula_10">F - j ≤ f j (a) -f j (b) a -b ≤ F + j (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where a, b ∈ R, a = b, F - j , and F + j are the known real scalars.</p><p>Assumption 2: The time-varying delay τ (t) satisfies the following conditions:</p><formula xml:id="formula_12">0 ≤ τ 1 ≤ τ (t) ≤ τ 2 , μ 1 ≤ τ (t) ≤ μ 2<label>(6)</label></formula><p>where τ 1 , τ 2 , μ 1 , and μ 2 are the known constants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Existence of a Solution</head><p>The initial value of system (1) is</p><formula xml:id="formula_13">x(θ ) = φ(θ), θ ∈ [-τ 2 , 0]</formula><p>. Next, we shall show that system (1) has at least one solution for any initial condition.</p><p>Suppose that t k , k = 1, 2, . . . , is the switching jump time with 0 &lt; t k &lt; t k+1 .</p><p>Step 1: Consider the following initial value problem:</p><formula xml:id="formula_14">⎧ ⎪ ⎨ ⎪ ⎩ ẋ(t) = -D(x(t))x(t) + A(x(t)) f (x(t)) + B(x(t)) f (x(t -τ (t))) + u(t), t ∈ [0, t 1 ] x(θ ) = φ(θ), θ ∈ [-τ 2 , 0].<label>(7)</label></formula><p>Notice that system <ref type="bibr" target="#b6">(7)</ref>  Step 2: If t 1 &gt; τ 2 , consider the following initial value problem:</p><formula xml:id="formula_15">⎧ ⎪ ⎨ ⎪ ⎩ ẋ(t) = -D(x(t))x(t) + A(x(t)) f (x(t)) + B(x(t)) f (x(t -τ (t))) + u(t), t ∈ [t 1 , t 2 ] x(θ ) = x 0 (θ ), θ ∈ [t 1 -τ 2 , t 1 ].<label>(8)</label></formula><p>Argued as in Step 1, (8) has a solution on</p><formula xml:id="formula_16">[t 1 , t 2 ]. If t 1 ≤ τ 2 ,</formula><p>then, by replacing the second equation in <ref type="bibr" target="#b7">(8)</ref> with</p><formula xml:id="formula_17">x(θ ) = φ(θ), θ ∈ [t 1 -τ 2 , 0] x 0 (θ ), θ ∈ (0, t 1 ].</formula><p>We can also show the existence of a solution of ( <ref type="formula" target="#formula_15">8</ref>)</p><formula xml:id="formula_18">on [t 1 , t 2 ].</formula><p>We denote this solution by</p><formula xml:id="formula_19">x 1 (t), t ∈ [t 1 , t 2 ].</formula><p>Step 3: By using inductive method, for</p><formula xml:id="formula_20">t m -t m-1 &gt; τ 2 , m = 2, 3, . . . , we can get that ⎧ ⎪ ⎨ ⎪ ⎩ ẋ(t) = -D(x(t))x(t) + A(x(t)) f (x(t)) + B(x(t)) f (x(t -τ (t))) + u(t), t ∈ [t m , t m+1 ] x(θ ) = x m-1 (θ ), θ ∈ [t m -τ 2 , t m ]<label>(9)</label></formula><p>has a solution on</p><formula xml:id="formula_21">[t m , t m+1 ]. If t m -t m-1 ≤ τ 2 ,</formula><p>then, by replacing the second equation in <ref type="bibr" target="#b8">(9)</ref> with</p><formula xml:id="formula_22">x(θ ) = x * (θ ), θ ∈ [t m -τ 2 , t m-1 ] x m-1 (θ ), θ ∈ (t m-1 , t m ]</formula><p>where x * (t) is the solution of the initial value problem defined on [t m -τ 2 , t m-1 ], we can also show the existence of a solution of ( <ref type="formula" target="#formula_20">9</ref>) on [t m , t m+1 ]. We denote this solution by</p><formula xml:id="formula_23">x m (t), t ∈ [t m , t m+1 ]. Define x(t, φ) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ x 0 (t), t ∈ (0, t 1 ] x 1 (t), t ∈ (t 1 , t 2 ] . . . x m (t), t ∈ (t m , t m+1 ] . . . . (<label>10</label></formula><formula xml:id="formula_24">)</formula><p>It is easy to verify that x(t, φ) is a solution of system (1) with the initial value x(θ</p><formula xml:id="formula_25">) = φ(θ), θ ∈ [-τ 2 , 0].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Definitions and Lemmas</head><p>We are now in a position to introduce the notions of exponential stability and stabilization.</p><p>Definition 1 <ref type="bibr" target="#b26">[27]</ref>: Given ε &gt; 0, the zero solution of system (1) with u(t) ≡ 0 is said to be ε-exponentially stable if there exists scalar χ &gt; 0, such that</p><formula xml:id="formula_26">x(t) ≤ χ x t c e -εt ∀t ≥ 0 where x t c = sup -τ 2 ≤θ≤0 { x(θ ) , ẋ(θ ) }.</formula><p>Definition 2 <ref type="bibr" target="#b26">[27]</ref>: Given ε &gt; 0, system (1) is ε-exponentially stabilized if there exists a suitable feedback control law u(t), such that the closed-loop system (1) is ε-exponentially stable, where ε is called the decay rate.</p><p>The following lemmas are critical to derive our main results.</p><p>Lemma 1 <ref type="bibr" target="#b50">[51]</ref>: Let α and β be real column vectors with dimensions of n 1 and n 2 , respectively. For given real symmetric positive definite matrices</p><formula xml:id="formula_27">R 1 ∈ R n 1 ×n 1 and R 2 ∈ R n 2 ×n 2 , if R 1 S * R 2</formula><p>≥ 0, then the following inequality holds for any scalar κ &gt; 0 and S ∈ R n 1 ×n 2 :</p><formula xml:id="formula_28">-2α T Sβ ≤ κα T R 1 α + κ -1 β T R 2 β.</formula><p>Lemma 2 (Quadratic Convex Combination <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b50">[51]</ref>): Let X 0 , X 1 , and X 2 be n × n real symmetric matrices and a continuous function τ (t) satisfies 0 ≤ τ 1 ≤ τ (t) ≤ τ 2 , where τ 1 and τ 2 are the constants. If X 0 ≥ 0, then</p><formula xml:id="formula_29">τ 2 (t)X 0 + τ (t)X 1 + X 2 &lt; 0(≤ 0) ∀τ (t) ∈ [τ 1 , τ 2 ] is equivalent to τ 2 i X 0 + τ i X 1 + X 2 &lt; 0(≤ 0), (i = 1, 2). Lemma 3 (Generalized Double-Integral Inequality): For any matrices M 1 , M 2 , M 3 ∈ R k×n , symmetric positive definite matrix R ∈ R n×n , two functions τ 1 (t) and τ 2 (t) satisfying 0 ≤ τ 1 ≤ τ 1 (t) ≤ τ 2 (t) ≤ τ 2 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and vector function</head><formula xml:id="formula_30">β : [τ 1 , τ 2 ] → R n such that τ 2 (t ) τ 1 (t ) β(s)ds = ζ T 1 ϕ 1 (t), τ 2 (t ) τ 1 (t ) τ 2 (t ) θ β(s)dsdθ = ζ T 2 ϕ 2 (t), τ 2 (t ) τ 1 (t ) θ τ 1 (t ) β(s)dsdθ = ζ T 3 ϕ 3 (t), where ζ 1 , ζ 2 , ζ 3 ∈ R k×n , and ϕ 1 (t), ϕ 2 (t), ϕ 3 (t) ∈ R k .</formula><p>Then, the following inequalities hold:</p><formula xml:id="formula_31">ϕ T 1 (t) M 1 ζ T 1 +ζ 1 M T 1 -(τ 2 (t) -τ 1 (t))M 1 R -1 M T 1 ϕ 1 (t) ≤ τ 2 (t ) τ 1 (t ) β T (s)Rβ(s)ds (11) ϕ T 2 (t) M 2 ζ T 2 +ζ 2 M T 2 - (τ 2 (t) -τ 1 (t)) 2 2 M 2 R -1 M T 2 ϕ 2 (t) ≤ τ 2 (t ) τ 1 (t ) τ 2 (t ) θ β T (s)Rβ(s)dsdθ (12) ϕ T 3 (t) M 3 ζ T 3 + ζ 3 M T 3 - (τ 2 (t) -τ 1 (t)) 2 2 M 3 R -1 M T 3 ϕ 3 (t) ≤ τ 2 (t ) τ 1 (t ) θ τ 1 (t ) β T (s)Rβ(s)dsdθ. (<label>13</label></formula><formula xml:id="formula_32">)</formula><p>Proof: The proof of inequality <ref type="bibr" target="#b10">(11)</ref> can be done in a similar way to that in <ref type="bibr" target="#b51">[52]</ref> and is omitted here. The proofs of the generalized double-integral inequalities ( <ref type="formula">12</ref>) and ( <ref type="formula" target="#formula_31">13</ref>) are given in Appendix A.</p><p>Remark 1: <ref type="formula">11</ref>)- <ref type="bibr" target="#b12">(13)</ref> in Lemma 3 are reduced to the following inequalities, respectively:</p><formula xml:id="formula_33">Let τ 1 (t) = a, τ 2 (t) = b, M 1 = (1/b -a)ζ 1 R, M 2 = (1/b -a)ζ 2 R, and M 3 = (1/b -a)ζ 3 R, then inequalities (</formula><formula xml:id="formula_34">(b -a) b a β T (s)Rβ(s)ds ≥ ϕ T 1 (t)ζ 1 Rζ T 1 ϕ 1 (t) (14) (b -a) 2 2 b a b θ β T (s)Rβ(s)dsdθ ≥ ϕ T 2 (t)ζ 2 Rζ T 2 ϕ 2 (t) (15) (b -a) 2 2 b a θ a β T (s)Rβ(s)dsdθ ≥ ϕ T 3 (t)ζ 3 Rζ T 3 ϕ 3 (t). (<label>16</label></formula><formula xml:id="formula_35">)</formula><p>Obviously, inequality ( <ref type="formula">14</ref>) is the well-known Jensen inequality <ref type="bibr" target="#b52">[53]</ref>. Inequalities ( <ref type="formula">15</ref>) and ( <ref type="formula" target="#formula_34">16</ref>) are the descriptions of the</p><formula xml:id="formula_36">⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ (i, τ l , μ r ) √ h Z l hW l h O l √ h i Y h i R 1 h i R 2 * -e 2ετ 2 Y 0 0 0 0 0 * * -2e 2ετ 2 R 1 0 0 0 0 * * * -2e 2ετ 2 R 2 0 0 0 * * * * -Y 0 0 * * * * * -2R 1 0 * * * * * * -2R 2 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ &lt; 0 (17) X S 1 * X &gt; 0 (18) R 1 S 2 * R 2 &gt; 0 (<label>19</label></formula><formula xml:id="formula_37">)</formula><p>last two inequalities of [1, Lemma 1] and [33, <ref type="bibr">Lemma 4]</ref>, respectively. If β(s) = ẋ(s), <ref type="bibr" target="#b14">(15)</ref> is the form of the last one inequality of [44, <ref type="bibr">Lemma 1]</ref>. Thus, the proposed inequalities in this paper are more relaxed. What is more, these inequalities will be used together with quadratic convex combination technique to present a stability criterion for system (1) in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MAIN RESULTS</head><p>In this section, we shall establish our main results based on LMI approach. One is for the exponential stability of system (1), and the other is for the stabilization of system (1) by designing a state-dependent switching control law. For the sake of simplicity in matrix representation, we define e i ∈ R 10n×n (i = 1, . . . , 10) as block entry matrices, e.g., e T 2 = [0 I n 0 . . . 0] n×10n . We also define two functions, 1 (s) and 2 (s), as follows:</p><formula xml:id="formula_38">1 (s) = e -2ετ 1 , s ≤ 1 e -2ετ 2 , s &gt; 1 2 (s) = e -2ετ 2 , s ≤ 1 e -2ετ 1 , s &gt; 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Exponential Stability Analysis</head><p>To facilitate the derivation of the stabilization criterion, the stability analysis problem is first discussed for the system (1) with u(t) ≡ 0.</p><p>Theorem 1: Assume that Assumptions 1 and 2 hold. For a given constant ε &gt; 0, the system (1) is ε-exponentially stable if there exist symmetric positive definite matrices P,</p><formula xml:id="formula_39">H 1 , H 2 , H 3 ∈ R 3n×3n , symmetric positive definite matrices X, Y , R 1 , R 2 ∈ R n×n , positive definite diagonal matrices j , pq ∈ R n×n ( j = 1, . . . , 4, p = 1, 2, 3, and q = 2, 3, 4), any matrices Z 1 , Z 2 , W 1 , W 2 , O 1 , O 2 ∈ R 10n×n</formula><p>, and any matrices S 1 , S 2 ∈ R n×n such that for i = 1, 2, . . . , 2 n , l = 1, 2, and r = 1, 2, the LMIs ( <ref type="formula">17</ref>)- <ref type="bibr" target="#b18">(19)</ref>, as shown at the top of this page, hold, where</p><formula xml:id="formula_40">(i, τ l , μ r ) = 1 + 2 (i, μ r ) + 3 (τ l ) 1 = h 2 e 1 Xe T</formula><p>1 -e -2ετ 2 e 9 Xe T 9 -e -2ετ 2 Sys e 9 S 1 e T 10 e -2ετ 2 e 10 Xe T 10 - </p><formula xml:id="formula_41">4 j =1 j (I 2 ⊗ j )ϒ T j - 3 p=1 4 q=2,q&gt; p pq (I 2 ⊗ pq )ϒ T pq 2 (i, μ r ) = 2ε 5 P T 5 + Sys 5 P T 6 (i, μ r ) + 7 H 1 T 7 -e -2ετ 1 8 H 1 T 8 + e -2ετ 1 8 H 2 T 8 -(1 -μ 2 ) 2 (μ 2 ) 9 H 2 T 9 + (1-μ 1 ) 1 (μ 1 ) 9 H 3 T 9 -e -2ετ 2 10 H 3 T 10 3 (τ l ) = e -2ετ 2 Sys 1 S 2 T 2 + Z 1 T 1 + Z 2 T 2 + W 1 T 3 (τ l ) + W 2 T 4 (τ l ) + O 1 T 5 (τ l ) + O 2 T 6 (τ l ) j = [e j e j</formula><formula xml:id="formula_42">ϒ = F 1 F 2 - F 1 + F 2 2 * I T i = [-D i 0 n×3n A i 0 B i 0 n×3n ] h = τ 2 -τ 1 .</formula><p>Proof: For the detailed proofs, see Appendix B. Remark 2: In <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref>, some algebraic stability criteria for MNNs were presented. Those criteria were derived by using the upper bound of the absolute value of the memristive synaptic weights, so that the inhibitory effects of neurons are ignored, which may lead to a certain degree of conservativeness. Different from the used methods in <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref>, in this paper, we divide MNNs (1) into 2 n cases based on the switched structure of the memristive synaptic weights. For the 2 n cases, a set of new stability conditions is established by constructing a uniform Lyapunov functional. More weights is utilized in Theorem 1, especially, the inhibitory effects of neurons on MNNs are considered. Therefore, The-IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</p><formula xml:id="formula_43">K (t) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ K 1 , |x 1 (t)| &gt; T 1 , |x 2 (t)| &gt; T 2 , |x 3 (t)| &gt; T 3 , . . . , |x n (t)| &gt; T n K 2 , |x 1 (t)| &gt; T 1 , |x 2 (t)| ≤ T 2 , |x 3 (t)| &gt; T 3 , . . . , |x n (t)| &gt; T n . . . , . . . K 2 n , |x 1 (t)| ≤ T 1 , |x 2 (t)| ≤ T 2 , |x 3 (t)| ≤ T 3 , . . . , |x n (t)| ≤ T n (21) ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ (i, τ l , μ r ) √ h Z l hW l h O l √ hϒ i hϒ i hϒ i * -e 2ετ 2 Y 0 0 0 0 0 * * -2e 2ετ 2 R 1 0 0 0 0 * * * -2e 2ετ 2 R 2 0 0 0 * * * * -2P + Y 0 0 * * * * * -4P + 2R 1 0 * * * * * * -4P + 2R 2 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ &lt; 0 (<label>23</label></formula><formula xml:id="formula_44">)</formula><p>orem 1 is an improvement of some existing results in the literature.</p><p>Remark 3: It should be noted that inequalities ( <ref type="formula">14</ref>)-( <ref type="formula" target="#formula_34">16</ref>) are usually used to estimate the upper bound of the integral terms - <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b43">[44]</ref>). In that way, the derived stability results seem to be tight owing to the inherent conservatism of inequalities ( <ref type="formula">14</ref>)-( <ref type="formula" target="#formula_34">16</ref>). In the proof of Theorem 1, we estimate these integral terms by combining the proposed generalized double-integral inequality (see Lemma 3) with quadratic convex combination together. One of the advantages of this method is that some free weight matrices (i.e., Z l , W l , and O l , l = 1, 2) are introduced to reduce the conservatism of the stability criterion [see inequalities <ref type="bibr" target="#b36">(37)</ref>- <ref type="bibr" target="#b41">(42)</ref> for details].</p><formula xml:id="formula_45">t -τ 1 t -τ 2 ẋ T (z)Y ẋ(z)dz, - t -τ 1 t -τ 2 t -τ 1 θ ẋ T (z)R 1 ẋ(z)dzdθ , - t -τ 1 t -τ 2 θ t -τ 2 ẋ T (z)R 2 ẋ(z)dzdθ (see</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Controller Design</head><p>Stability analysis is the foundation of the control synthesis. Next, we will explore the stabilization problem of system (4) based on the obtained stability result.</p><p>By the analysis in Section II-A, we can regard the MNNs (1) as a class of switched neural networks with 2 n subsystems, and each subsystem can be viewed as a mode. It is clear that the modes may switch from one to another, and the switching between two different modes is governed by the neuron states themselves. In this section, our goal is to design a state-dependent switching control law u(t), just as one usually designs a mode-dependent controller in Markovian systems (see <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, and references therein). Here, the considered control law is given in the following form:</p><formula xml:id="formula_46">u(t) = 2 n i=1 π i (t)K i x(t) = K (t)x(t). (<label>20</label></formula><formula xml:id="formula_47">)</formula><p>According to the definition of π i (t) in (3), K (t) can be detailedly expressed as <ref type="bibr" target="#b20">(21)</ref> shown at the top of this page.</p><p>Under the state-dependent switching control law <ref type="bibr" target="#b19">(20)</ref>, the closed-loop system (4) can be recasted into</p><formula xml:id="formula_48">ẋ(t) = -(D(t) -K (t))x(t) + A(t) f (x(t)) + B(t) f (x(t -τ (t))). (<label>22</label></formula><formula xml:id="formula_49">)</formula><p>Theorem 2: Assume that Assumption 1 and Assumption 2 hold. For a given constant ε &gt; 0, the closed-loop system ( <ref type="formula" target="#formula_48">22</ref>) is ε-exponentially stabilized under state-dependent switching control law <ref type="bibr" target="#b19">(20)</ref> if there exist symmetric positive definite matrices P, H 1 , H 2 , H 3 ∈ R 3n×3n , symmetric positive definite matrices X, Y , R 1 , R 2 ∈ R n×n , positive definite diagonal matrices j , pq ∈ R n×n ( j = 1, . . . , 4, p = 1, 2, 3, and q = 2, 3, 4), any matrices</p><formula xml:id="formula_50">Z 1 , Z 2 , W 1 , W 2 , O 1 , O 2 ∈ R 10n×n ,</formula><p>and any matrices S 1 , S 2 , M i ∈ R n×n , such that for i = 1, 2, . . . , 2 n , l = 1, 2, and r = 1, 2, inequalities ( <ref type="formula">18</ref>), <ref type="bibr" target="#b18">(19)</ref>, and the LMI <ref type="bibr" target="#b22">(23)</ref>, as shown at the top of this page, hold, where</p><formula xml:id="formula_51">(i, τ l , μ r ) = (i, τ l , μ r ) + e 1 Sys{M i }e T 1 ϒ i = i P T + i , T i = [M i 0 n×9n ] P = E T 3 P E 3 , E T 3 = [I<label>0</label></formula><p>0] and (i, τ l , μ r ) and i follow the same definitions as those in Theorem 1. Moreover, the stabilized control gain matrices in <ref type="bibr" target="#b19">(20)</ref> are given by K i = P -1 M i .</p><p>Proof: For detailed proof, see Appendix C. Remark 4: It should be pointed out that the gain matrices K i , i = 1, 2, . . . , 2 n , can be easily obtained by solving a finite number of LMIs ( <ref type="formula">18</ref>), <ref type="bibr" target="#b18">(19)</ref>, and (23) numerically using the interior-point algorithm. The designed state-dependent switching control law <ref type="bibr" target="#b19">(20)</ref> can also be seen as a series of distributed ones. It is fairly similar to the fuzzy state feedback controller via parallel distributed compensation (PDC) scheme in fuzzy systems <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Controller gain matrix (21) can be viewed as a succinct fuzzy control query table. In a practical application, we can type the information of (21) into computer beforehand; computer can sample the input and compare it with the query table to realize the automatic control.</p><p>Remark 5:</p><formula xml:id="formula_52">Let M = M 1 = M 2 = • • • = M 2 n in</formula><p>Theorem 2, and take K = P -1 M, then the state-dependent switching/distributed control law <ref type="bibr" target="#b19">(20)</ref> is reduced to the memoryless state feedback/centralized control law u(t) = K x(t), as shown in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>.</p><p>Remark 6: By using M-matrix theory and algebraic inequality technique, memoryless state feedback control law, intermittent controller, and nonlinear discontinuous controller were designed to stabilize the considered MNNs model in <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>, respectively. Although some compendious criteria for the exponential stabilization in terms of LMIs were established in <ref type="bibr" target="#b26">[27]</ref>, they strictly depended upon the upper bound of the absolute value of the memristive synaptic weights. Compared with those results in <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>, Theorem 2 has three conspicuous advantages.</p><p>1) The excitatory and inhibitory effects of neurons on neural networks are considered, because we do not take the maximal absolute value operation for a i j (x i (t)) and b i j (x i (t)). 2) Generalized double-integral inequality (i.e., <ref type="bibr">Lemma 3)</ref> and quadratic convex approach are used to bound parts of the integral terms, for example, -</p><formula xml:id="formula_53">t -τ 1 t -τ 2 ẋ T (z)Y ẋ(z)dz, - t -τ 1 t -τ 2 t -τ 1 θ ẋ T (z)R 1 ẋ(z)dzdθ , and - t -τ 1 t -τ 2 θ t -τ 2 ẋ T (z)R 2 ẋ(z)dzdθ .</formula><p>Therefore, the conservativeness of the inequality zoom is further reduced, and the corresponding stability criterion becomes more efficient.</p><p>3) By fully considering the switched structure characteristic of memristive synaptic weights, our goal is to design a state-dependent switching/distributed control law instead of centralized one. The introducing of any matrices M i , i = 1, 2, . . . , 2 n (rather than single M) improves the feasible range of the obtained LMIs. As a result, Theorem 2 is less conservative than those in <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 7: By employing D ∈ [D, D], A ∈ [A, A], and B ∈ [B, B]</head><p>, some stabilization criteria for MNNs were investigated in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref>. However, from the point of differential inclusions, the essences of those parameters are abstract and measurable functions with respect to the neuron states. Thus, the results in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref> may contain an infinite number of LMIs. It is a formidable task to check them. On the contrary, the LMI-based conditions in Theorem 2 are established by using the boundaries of memristive synaptic weights D(x(t)), A(x(t)), and B(x(t)), and can be finitely verified by LMI toolbox of MATLAB.</p><p>Remark 8: LMI-based approach, as one of the most efficient ways to solve the stability problem of time-delay systems, has two marvelous advantages. One is that the LMI approach can easily incorporate free variables into stability criteria and decrease the conservatism. The other is that more inhibitory information on the system can be contained in LMI-based results than the algebraic inequality and M-matrix approaches. However, LMI approach also has its own obvious shortcoming. For example, there will be a computational complexity problem as the increasing of the dimension of LMI. Thus, it is essential to keep a reasonable and low computational complexity for LMI with larger dimension. Certainly, with the rapid development of high-performance computer, the calculation of the complex LMIs will become easier and easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NUMERICAL EXAMPLES</head><p>In this section, we will give two examples. The first one is used to illustrate the effectiveness of the stability result in Theorem 1, and the second one is given to show the feasibility of stabilization controller design strategy in Theorem 2. Example 1: In <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref>, a class of stability conditions for MNNs were derived. Here, we only take <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Th. 3.1]</ref>, listed as follows, as a comparison object, since these results are fairly similar.</p><p>Proposition 1 <ref type="bibr" target="#b25">[26]</ref>: Assume the activation functions f j are monotonically nondecreasing and satisfy the condition ( <ref type="formula" target="#formula_10">5</ref>), then the origin of system ( <ref type="formula" target="#formula_0">1</ref>) is globally exponentially stable if the following inequality holds:</p><formula xml:id="formula_54">= - 1 2 min 1≤i≤n {d i } + n j =1 i=1 1 d j â2 j i + b2 j i F + i 2 &lt; 0 (<label>24</label></formula><formula xml:id="formula_55">)</formula><p>where</p><formula xml:id="formula_56">d i = min{d * i , d * * i }, â j i = max{|a * j i |, |a * * j i |}, and b j i = max{|b * j i |, |b * * j i |} i, j = 1, 2 . . . , n. Let u(t) ≡ 0.</formula><p>Consider the two-neuron MNNs (1) with the following memristive synaptic weights:</p><formula xml:id="formula_57">D(x(t)) = d 1 (x 1 (t)) 0 0 d 2 (x 2 (t)) A(x(t)) = a 11 (x 1 (t)) 0.6 0.5 a 22 (x 2 (t)) B(x(t)) = -0.1 b 12 (x 1 (t)) b 21 (x 2 (t)) -0.2</formula><p>where</p><formula xml:id="formula_58">d 1 (x 1 (t)) = 1, |x 1 (t)| ≤ 0.5 0.9, |x 1 (t)| &gt; 0.5 d 2 (x 2 (t)) = 0.9, |x 2 (t)| ≤ 0.5 1, |x 2 (t)| &gt; 0.5 a 11 (x 1 (t)) = 0, |x 1 (t)| ≤ 0.5 -0.1, |x 1 (t)| &gt; 0.5 a 22 (x 2 (t)) = -0.1, |x 2 (t)| ≤ 0.5 0, |x 2 (t)| &gt; 0.5 b 12 (x 1 (t)) = 0.6, |x 1 (t)| ≤ 0.5 0.1, |x 1 (t)| &gt; 0.5 b 21 (x 2 (t)) = 0.3, |x 2 (t)| ≤ 0.5 0.2, |x 2 (t)| &gt; 0.5.</formula><p>Consider the activation function f (x j ) = tanh(x j ), j = 1, 2. Obviously, the activation functions are monotonically nondecreasing and satisfy Assumption 1 with F + j = 1 and F - j = 0. Examining Theorem 1 by LMI toolbox of MATLAB, we obtain Table <ref type="table" target="#tab_3">I</ref>, which lists the obtained upper bounds of τ 2 for different values of τ 1 , μ 1 , μ 2 , and ε. However, according to Proposition 1, one has = 0.8056 &gt; 0, Hence, Proposition 1 fails to work here. Similarly, the results in <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref> are not IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig. <ref type="figure">2</ref>. Trajectory of neuron state x(t) with u(t) ≡ 0 in Example 1. true, either. Therefore, pertaining to this example, we can conclude that Theorem 1 is less conservative than the results in <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref>.</p><p>For numerical simulation, we take τ (t) = 3 + cos(0.8t). Figs. <ref type="figure">2</ref> and<ref type="figure" target="#fig_1">3</ref> show the time responses of variables x 1 (t) and x 2 (t) with 15 random initial values, respectively. It is clear that x 1 (t) and x 2 (t) converge exponentially to zeros.</p><p>Example 2: Consider the two-neuron MNNs (1) with f (x j ) = (1/2)(|x j + 1| -|x j -1|), j = 1, 2, and memristive synaptic weights</p><formula xml:id="formula_59">D(x(t)) = d 1 (x 1 (t)) 0 0 d 2 (x 2 (t)) A(x(t)) = a 11 (x 1 (t)) a 12 (x 1 (t)) a 21 (x 2 (t)) a 22 (x 2 (t)) B(x(t)) = b 11 (x 1 (t)) b 12 (x 1 (t)) b 21 (x 2 (t)) b 22 (x 2 (t))</formula><p>where </p><formula xml:id="formula_60">d 1 (x 1 (t)) = 1, |x 1 (t)| ≤ 1 0.5, |x 1 (t)| &gt; 1 d 2 (x 2 (t)) = 0.5, |x 1 (t)| ≤ 1 1, |x 1 (t)| &gt; 1 a 11 (x 1 (t)) = 0, |x 1 (t)| ≤ 1 -0.1, |x 1 (t)| &gt; 1</formula><formula xml:id="formula_61">a 12 (x 1 (t)) = 1.5, |x 1 (t)| ≤ 1 0.25, |x 1 (t)| &gt; 1 a 21 (x 2 (t)) = 0.5, |x 2 (t)| ≤ 1 1.5, |x 2 (t)| &gt; 1 a 22 (x 2 (t)) = -0.1, |x 2 (t)| ≤ 1 0, |x 2 (t)| &gt; 1 b 11 (x 1 (t)) = -0.25, |x 1 (t)| ≤ 1 0, |x 1 (t)| &gt; 1 b 12 (x 1 (t)) = 2, |x 1 (t)| ≤ 1 1, |x 1 (t)| &gt; 1 b 21 (x 2 (t)) = 0.25, |x 2 (t)| ≤ 1 1.75, |x 2 (t)| &gt; 1 b 22 (x 2 (t)) = -0.4, |x 2 (t)| ≤ 1 -0.1, |x 2 (t)| &gt; 1.</formula><p>It is easy to see that Assumption 1 is satisfied with</p><formula xml:id="formula_62">F - j = 0, F + j = 1, j = 1, 2. For comparison, we set μ 2 = -μ 1 = 1.2 &gt; 1.</formula><p>In this case, the results in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> fail to calculate the admissible maximum upper bounds as well as the control gains. However, applying Theorem 2 in this paper, it yields the upper bounds of τ 2 for different values of τ 1 and ε, which are listed in Table <ref type="table" target="#tab_4">II</ref>. Hence, in some sense, Theorem 2 of this paper improves the outcomes in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>.</p><p>Choosing τ (t) = 1.15 + 0.15 sin(8t). It is clear that τ 1 = 1, τ 2 = 1.3, and μ 2 = -μ 1 = 1.2. For this system without external controller, Fig. <ref type="figure">4</ref> shows the time responses of x 1 (t) and x 2 (t) with 15 random initial values. Obviously, the concerned system without external controller is not convergent. By taking Fig. <ref type="figure">5</ref>.</p><p>Trajectory of neuron state x(t) with external controller in Example 2. ε = 0.1 and solving Theorem 2, the feedback control gains can be obtained as follows: Under the obtained control gains shown as above, Fig. <ref type="figure">5</ref> shows the responses of variables x 1 (t) and x 2 (t), respectively. It is clear that both x 1 (t) and x 2 (t) with ε = 0.1 converge exponentially to zeros. This is in accordance with the result in Theorem 2 of this paper.</p><formula xml:id="formula_63">K 1 = -2.</formula><p>V. CONCLUSION Both issues of stability and stabilization for MNNs have intrigued increasing interests from different fields. However, few papers consider the stabilization problem by designing a state-dependent switching control law. In this paper, first, by combining some generalized double-integral inequalities and quadratic convex combination technique, a new delaydependent exponential stability criterion has been established. Afterward, based on the stability condition, we have derived a sufficient condition on the existence of state-dependent switching control law for the considered closed-loop MNNs. The designed control law possesses the same property of the PDC controller as that in fuzzy systems. It has been shown that the gain matrices can be determined by solving a set of LMIs. Finally, two illustrative examples are provided to show the effectiveness of the proposed methods.</p><p>Although the stability and stabilization issues for MNNs have been addressed in this paper, the Lyapunov matrices in Lyapunov functional are uniform for each subsystems. This inevitably brings some conservatism. Thus, how to construct new Lyapunov functionals with distinct Lyapunov matrices for different subsystems is our future research topic.</p><p>Meanwhile, it is also promising to extend the results developed in this paper to H ∞ control owing to the inevitable noise disturbance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF LEMMA 3</head><p>It is clear that the inequalities ( <ref type="formula">12</ref>) and ( <ref type="formula" target="#formula_31">13</ref>) are true for case τ 1 (t) = τ 2 (t). Now, suppose that τ 1 (t) &lt; τ 2 (t). Applying Schur complement <ref type="bibr" target="#b55">[56]</ref>, we have</p><formula xml:id="formula_64">β T (s)Rβ(s) β T (s) β(s) R -1 ≥ 0. (<label>25</label></formula><formula xml:id="formula_65">)</formula><p>Integrating ( <ref type="formula" target="#formula_64">25</ref>) from θ to τ 2 (t), we have</p><formula xml:id="formula_66">⎡ ⎢ ⎢ ⎣ τ 2 (t ) θ β T (s)Rβ(s)ds τ 2 (t ) θ β T (s)ds τ 2 (t ) θ β(s)ds (τ 2 (t) -θ)R -1 ⎤ ⎥ ⎥ ⎦ ≥ 0. (<label>26</label></formula><formula xml:id="formula_67">)</formula><p>Integrating ( <ref type="formula" target="#formula_66">26</ref>) from τ 1 (t) to τ 2 (t) and letting</p><formula xml:id="formula_68">h(t) = ((τ 2 (t) -τ 1 (t)) 2 /2), we have ⎡ ⎢ ⎢ ⎣ τ 2 (t ) τ 1 (t ) τ 2 (t ) θ β T (s)Rβ(s)dsdθ τ 2 (t ) τ 1 (t ) τ 2 (t ) θ β T (s)dsdθ τ 2 (t ) τ 1 (t ) τ 2 (t ) θ β(s)dsdθ h(t)R -1 ⎤ ⎥ ⎥ ⎦ ≥ 0.<label>(27)</label></formula><p>Applying Schur complement to (27) yields</p><formula xml:id="formula_69">τ 2 (t ) τ 1 (t ) τ 2 (t ) θ β T (s)Rβ(s)dsdθ ≥ 1 h(t) ϕ T 2 (t)ζ 2 Rζ T 2 ϕ 2 (t). (<label>28</label></formula><formula xml:id="formula_70">)</formula><p>Since R &gt; 0, for any constant matrix M 2 ∈ R k×n , the following inequality holds:</p><formula xml:id="formula_71">(ζ 2 R -h(t)M 2 )R -1 (ζ 2 R -h(t)M 2 ) T ≥ 0<label>(29)</label></formula><p>from which we obtain 1 h(t)</p><formula xml:id="formula_72">ζ 2 Rζ T 2 ≥ ζ 2 M T 2 + M 2 ζ T 2 -h(t)M 2 R -1 M T 2 . (<label>30</label></formula><formula xml:id="formula_73">)</formula><p>Combining ( <ref type="formula" target="#formula_69">28</ref>) and ( <ref type="formula" target="#formula_72">30</ref>), we conclude that (12) holds. In a similar way, we can conclude that (13) also holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B PROOF OF THEOREM 1</head><p>First, let us define</p><formula xml:id="formula_74">α(t) = x T (t) x T (t -τ 1 ) x T (t -τ (t)) x T (t -τ 2 ) f T (x(t)) f T (x(t -τ 1 )) f T (x(t -τ (t))) f T (x(t)) t -τ 1 t -τ (t ) x T (z)dz t -τ (t ) t -τ 2 x T (z)dz T T (t) = [-D(t) 0 n×3n A(t) 0 B(t) 0 n×3n ].</formula><p>Then, it is clear that (t) = 2 n i=1 π i (t) i , where i is defined in Theorem 1. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Consider the following Lyapunov functional candidate for neural network (4):</p><formula xml:id="formula_75">V (t) = V 1 (t) + V 2 (t) + V 3 (t) + V 4 (t)<label>(31)</label></formula><p>where</p><formula xml:id="formula_76">V 1 (t) = ξ T (t)Pξ(t) V 2 (t) = t t -τ 1 e 2ε(z-t ) η T (z)H 1 η(z)dz + t -τ 1 t -τ (t ) e 2ε(z-t ) η T (z)H 2 η(z)dz + t -τ (t ) t -τ 2 e 2ε(z-t ) η T (z)H 3 η(z)dz V 3 (t) = h t -τ 1 t -τ 2 t θ e 2ε(z-t ) x T (z)X x(z)dzdθ + t -τ 1 t -τ 2 t θ e 2ε(z-t ) ẋ T (z)Y ẋ(z)dzdθ V 4 (t) = t -τ 1 t -τ 2 t -τ 1 θ t λ e 2ε(z-t ) ẋ T (z)R 1 ẋ(z)dzdλdθ + t -τ 1 t -τ 2 θ t -τ 2 t λ e 2ε(z-t ) ẋ T (z)R 2 ẋ(z)dzdλdθ with ξ(t) = col{x(t), t -τ (t ) t -τ 2 x(z)dz, t -τ 1 t -τ (t ) x(z)dz} and η(z) = col{x(z), f (x(z)) -F 1 x(z), F 2 x(z) -f (x(z))}.</formula><p>Calculating the derivative of V (t) along the solution of system (4), it yields </p><formula xml:id="formula_77">V1 (t) = 2ξ T (t)P ξ (t) + 2εξ T (t)Pξ(t) -2εV 1 (t) = α T (t)</formula><formula xml:id="formula_78">V2 (t) = η T (t)H 1 η(t) -e -2ετ 1 η T (t -τ 1 )H 1 η(t -τ 1 ) + e -2ετ 1 η T (t -τ 1 )H 2 η(t -τ 1 ) -(1 -τ (t))e -2ετ (t ) η T (t -τ (t))H 2 η(t -τ (t)) + (1 -τ (t))e -2ετ (t ) η T (t -τ (t))H 3 η(t -τ (t)) -η T (t -τ 2 )e -2ετ 2 H 3 η(t -τ 2 ) -2εV 2 (t) ≤ α T (t) 7 H 1 T 7 -e -2ετ 1 8 H 1 T 8 + e -2ετ 1 8 H 2 T 8 -(1 -μ 2 ) 2 (μ 2 ) × 9 H 2 T 9 + (1 -μ 1 ) 1 (μ 1 ) 9 H 3 T 9 -e -2ετ 2 10 H 3 T 10 α(t) -2εV 2 (t) (<label>33</label></formula><formula xml:id="formula_79">)</formula><formula xml:id="formula_80">V3 (t) = h 2 x T (t)X x(t) -h t -τ 1 t -τ 2 e 2ε(z-t ) x T (z)X x(z)dz + h ẋ T (t)Y ẋ(t) - t -τ 1 t -τ 2 e 2ε(z-t ) ẋ T (z)Y ẋ(z)dz -2εV 3 (t) ≤ h 2 x T (t)X x(t) -he -2ετ 2 t -τ 1 t -τ 2</formula><p>x T (z)X x(z)dz</p><formula xml:id="formula_81">+ h ẋ T (t)Y ẋ(t) -e -2ετ 2 t -τ 1 t -τ 2 ẋ T (z)Y ẋ(z)dz -2εV 3 (t) = h 2 x T (t)X x(t) -he -2ετ 2 t -τ (t ) t -τ 2 x T (z)X x(z)dz -he -2ετ 2 t -τ 1 t -τ (t )</formula><p>x T (z)X x(z)dz</p><formula xml:id="formula_82">+ h ẋ T (t)Y ẋ(t) -e -2ετ 2 t -τ (t ) t -τ 2 ẋ T (z)Y ẋ(z)dz -e -2ετ 2 t -τ 1 t -τ (t ) ẋ T (z)Y ẋ(z)dz -2εV 3 (t) (34) V4 (t) ≤ h 2 2 ẋ T (t)R 1 ẋ(t) + h 2 2 ẋ T (t)R 2 ẋ(t) -2εV 4 (t) -e -2ετ 2 t -τ 1 t -τ 2 t -τ 1 θ ẋ T (z)R 1 ẋ(z)dzdθ -e -2ετ 2 t -τ 1 t -τ 2 θ t -τ 2 ẋ T (z)R 2 ẋ(z)dzdθ = h 2 2 ẋ T (t)R 1 ẋ(t) + h 2 2 ẋ T (t)R 2 ẋ(t) -2εV 4 (t) -e -2ετ 2 t -τ (t ) t -τ 2 t -τ (t ) θ ẋ T (z)R 1 ẋ(z)dzdθ -e -2ετ 2 (τ 2 -τ (t)) t -τ 1 t -τ (t ) ẋ T (z)R 1 ẋ(z)dz -e -2ετ 2 t -τ 1 t -τ (t ) t -τ 1 θ ẋ T (z)R 1 ẋ(z)dzdθ -e -2ετ 2 t -τ (t ) t -τ 2 θ t -τ 2 ẋ T (z)R 2 ẋ(z)dzdθ -e -2ετ 2 (τ (t) -τ 1 ) t -τ (t ) t -τ 2 ẋ T (z)R 2 ẋ(z)dz -e -2ετ 2 t -τ 1 t -τ (t ) θ t -τ (t ) ẋ T (z)R 2 ẋ(z)dzdθ.<label>(35)</label></formula><p>By applying reciprocally convex approach <ref type="bibr" target="#b56">[57]</ref>, we can obtain that if there exists a matrix S 1 such that (18) holds, then one has</p><formula xml:id="formula_83">-h t -τ (t ) t -τ 2 x T (z)X x(z)dz -h t -τ 1 t -τ (t ) x T (z)X x(z)dz ≤ - h τ 2 -τ (t) t -τ (t ) t -τ 2 x T (z)dz X t -τ (t ) t -τ 2 x(z)dz - h τ (t) -τ 1 t -τ 1 t -τ (t ) x T (z)dz X t -τ 1 t -τ (t ) x(z)dz ≤ - ⎡ ⎢ ⎢ ⎢ ⎣ t -τ 1 t -τ (t ) x(z)dz t -τ (t ) t -τ 2 x(z)dz ⎤ ⎥ ⎥ ⎥ ⎦ T X S 1 * X ⎡ ⎢ ⎢ ⎣ t -τ 1 t -τ (t ) x(z)dz t -τ (t ) t -τ 2 x(z)dz ⎤ ⎥ ⎥ ⎦ . (36) Noticing that t -τ (t ) t -τ 2 ẋ(z)dz = x(t -τ (t)) -x(t -τ 2 ) = T 1 α(t).</formula><p>From Lemma 3, we can deduce that there exists a matrix</p><formula xml:id="formula_84">Z 1 ∈ R 10n×n such that - t -τ (t ) t -τ 2 ẋ T (z)Y ẋ(z)dz ≤ α T (t) Z 1 T 1 + 1 Z T 1 + (τ 2 -τ (t))Z 1 Y -1 Z T 1 α(t).<label>(37)</label></formula><p>Similarly, there exist matrices Z 2 , W 1 , W 2 , O 1 , and</p><formula xml:id="formula_85">O 2 ∈ R 10n×n such that - t -τ 1 t -τ (t ) ẋ T (z)Y ẋ(z)dz ≤ α T (t) Z 2 T 2 + 2 Z T 2 + (τ (t) -τ 1 )Z 2 Y -1 Z T 2 α(t) (<label>38</label></formula><formula xml:id="formula_86">) - t -τ (t ) t -τ 2 t -τ (t ) θ ẋ T (z)R 1 ẋ(z)dzdθ ≤ α T (t) W 1 T 3 (τ (t)) + 3 (τ (t))W T 1 + (τ 2 -τ (t)) 2 2 W 1 R -1 1 W T 1 α(t) (<label>39</label></formula><formula xml:id="formula_87">) - t -τ 1 t -τ (t ) t -τ 1 θ ẋ T (z)R 1 ẋ(z)dzdθ ≤ α T (t) W 2 T 4 (τ (t)) + 4 (τ (t))W T 2 + (τ (t) -τ 1 ) 2 2 W 2 R -1 1 W T 2 α(t)<label>(40)</label></formula><p>-</p><formula xml:id="formula_88">t -τ (t ) t -τ 2 θ t -τ 2 ẋ T (z)R 2 ẋ(z)dzdθ ≤ α T (t) O 1 T 5 (τ (t)) + 5 (τ (t))O T 1 + (τ 2 -τ (t)) 2 2 O 1 R -1 2 O T 1 α(t) (<label>41</label></formula><formula xml:id="formula_89">) - t -τ 1 t -τ (t ) θ t -τ (t ) ẋ T (z)R 2 ẋ(z)dzdθ ≤ α T (t) O 2 T 6 (τ (t)) + 6 (τ (t))O T 2 + (τ (t) -τ 1 ) 2 2 O 2 R -1 2 O T 2 α(t)<label>(42)</label></formula><p>where 1 and 2 are defined in Theorem 1, and In addition, by applying Jesen inequality and Lemma 1, we obtain</p><formula xml:id="formula_90">-(τ 2 -τ (t)) t -τ 1 t -τ (t ) ẋ T (z)R 1 ẋ(z)dz -(τ (t) -τ 1 ) t -τ (t ) t -τ 2 ẋ T (z)R 2 ẋ(z)dz ≤ - τ 2 -τ (t) τ (t) -τ 1 t -τ 1 t -τ (t ) ẋ T (z)dz R 1 t -τ 1 t -τ (t ) ẋ(z)dz - τ (t) -τ 1 τ 2 -τ (t) t -τ (t ) t -τ 2 ẋ T (z)dz R 2 t -τ (t ) t -τ 2 ẋ(z)dz = -α T (t) τ 2 -τ (t) τ (t) -τ 1 2 R 1 T 2 + τ (t) -τ 1 τ 2 -τ (t) 1 R 2 T 1 α(t) ≤ 2α T (t) 1 S 2 T 2 α(t). (<label>43</label></formula><formula xml:id="formula_91">)</formula><p>For any positive diagonal matrix = diag(λ 1 , λ 2 , . . . , λ n ), we can get from Assumption 1 that 0</p><formula xml:id="formula_92">≤ - x(ϑ) f (x(ϑ)) T F 1 F 2 - F 1 + F 2 2 * x(ϑ) f (x(ϑ)) = - x(ϑ) f (x(ϑ)) T (I 2 ⊗ )ϒ x(ϑ) f (x(ϑ))</formula><p>.</p><p>Let ϑ be t, t -τ 1 , t -τ (t), and t -τ 2 , and replace with 1 , 2 , 3 , and 4 , respectively. Then, one has for j = 1, 2, 3, 4</p><formula xml:id="formula_93">0 ≤ -α T (t) j (I 2 ⊗ j )ϒ T j α(t) thus 0 ≤ -α T (t) 4 j =1 j (I 2 ⊗ j )ϒ T j α(t). (<label>44</label></formula><formula xml:id="formula_94">)</formula><p>From Assumption 1, we can also get</p><formula xml:id="formula_95">0 ≤ - x(ϑ 1 ) -x(ϑ 2 ) f (x(ϑ 1 )) -f (x(ϑ 2 )) T ×(I 2 ⊗ )ϒ x(ϑ 1 ) -x(ϑ 2 ) f (x(ϑ 1 )) -f (x(ϑ 2 ))</formula><p>.</p><p>Let ϑ 1 and ϑ 2 take values in {t, t -τ 1 , t -τ (t), t -τ 2 } and replace with pq . Then, one has for p = 1, 2, 3 and q = 2, 3, 4, with q &gt; p 0   For <ref type="bibr" target="#b45">(46)</ref>, if (t, τ (t), τ (t)) ≤ 0, one has</p><formula xml:id="formula_96">≤ -α T (t) pq (I 2 ⊗ pq )ϒ T pq α(t) thus 0 ≤ -α T (t)</formula><formula xml:id="formula_97">1 (t) = (t) hY + h 2 2 R 1 + h 2 2 R 2 T (t) 2 (τ (t)) = e -2ετ 2 (τ 2 -τ (t)) 2 2 W 1 K -1 1 W T 1 + O 1 R -1 2 O T 1 + (τ (t) -τ 1 ) 2 2 W 2 R -1 1 W T 2 + O 2 R -1 2 O T 2 . ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ (t,</formula><formula xml:id="formula_98">V (t) ≤ V (0)e -2εt , t ≥ 0.<label>(47)</label></formula><p>Due to the fact that P &gt; 0, we can obtain that there exists a small scalar δ &gt; 0 such that</p><formula xml:id="formula_99">V (t) ≥ δ x(t) 2 . (<label>48</label></formula><formula xml:id="formula_100">)</formula><p>Combing <ref type="bibr" target="#b30">(31)</ref>, <ref type="bibr" target="#b46">(47)</ref>, and (48), we can get that there exists a scalar ρ such that δ x(t) 2 ≤ V (0)e -2εt ≤ ρ x t </p><p>Applying Schur complement lemma <ref type="bibr" target="#b55">[56]</ref> to (51), we have <ref type="bibr" target="#b51">(52)</ref>, as shown at the top of this page, for l = 1, 2 and r = 1, 2.</p><p>Recalling that (t) =</p><p>2 n i=1 π i (t) i , it is easy to verify that ( <ref type="formula">52</ref>) is true if the LMI <ref type="bibr" target="#b16">(17)</ref> holds. This completes the proof of Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C PROOF OF THEOREM 2</head><p>It follows from Theorem 1 that the closed-loop system ( <ref type="formula" target="#formula_48">22</ref>) is ε-exponentially stable if conditions ( <ref type="formula">18</ref>), <ref type="bibr" target="#b18">(19)</ref>, and the LMI <ref type="bibr" target="#b52">(53)</ref>, as shown at the top of this page, hold, where ˆ (i, τ l , μ r ) = (i, τ l , μ r ) + Sys 5 P ¯ T 6 ¯ 6 = [ ¯ i 0 10n×n 0 10n×n ] ˆ i = i + ¯ i , ¯ T i = [K i 0 n×9n ]. By premultiplying and postmultiplying <ref type="bibr" target="#b52">(53)</ref>, respectively, by diag{I 10n×10n , I, I, I, PY -1 , PR -1 1 , PR -1 2 } and its transpose, and noting the fact -PY -1 P ≤ -2P + Y , -PR -1 1 P ≤ -2P + R 1 , and -PP -1 2 P ≤ -2P + R 2 , one can deduce that after some manipulations, ( <ref type="formula">53</ref>) is ensured by the LMI <ref type="bibr" target="#b53">(54)</ref>, as shown at the top of this page.</p><p>Let M i = PK i , then one can verify that 5 P ¯ T 6 = e 1 M i e T  1 . Furthermore, we can verify that ( <ref type="formula">54</ref>) is equivalent to <ref type="bibr" target="#b22">(23)</ref>. This completes the proof of Theorem 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>+4 ], j = 1, 2, 3, 4 pq = [e pe q e p+4e q+4 ] p = 1, 2, 3, q = 2, 3, 4, q &gt; p 5 = [e 1 e 10 e 9 ] 6 (i, μ r ) = [ i (1 -μ r )e 3e 4 e 2 -(1 -μ r )e 3 ] 7 = [e 1 e 5e 1 F 1 e 1 F 2e 5 ] 8 = [e 2 e 6e 2 F 1 e 2 F 2e 6 ] 9 = [e 3 e 7e 3 F 1 e 3 F 2e 7 ] 10 = [e 4 e 8e 4 F 1 e 4 F 2e 8 ] 1 = e 3e 4 , 2 = e 2e 3 3 (τ l ) = (τ 2 -τ l )e 3e 10 , 4 (τ l ) = (τ l -τ 1 )e 2e 9 5 (τ l ) = e 10 -(τ 2 -τ l )e 4 , 6 (τ l ) = e 9 -(τ l -τ 1 )e 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. 3-D trajectory of neuron state x(t) with u(t) ≡ 0 in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>T 9 - 3 T 10 3 2 T 2 + Z 1 T 1 + Z 2 T 2 + W 1 T 3 2 T 4 1 T 5 2 T 6</head><label>931022112213241526</label><figDesc>e -2ετ 2 10 H (τ (t)) = e -2ετ 2 Sys 1 S (τ (t)) + W (τ (t)) + O (τ (t)) + O (τ (t))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,139.43,58.61,332.78,323.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Exponential Stability and Stabilization of Delayed Memristive Neural Networks Based on Quadratic Convex Combination Method Zhanshan Wang, Member, IEEE, Sanbo Ding, Zhanjun Huang, and Huaguang Zhang, Fellow, IEEE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I OBTAINED</head><label>I</label><figDesc>UPPER BOUNDS OF τ 2 FOR VARIOUS τ 1 , μ 1 , μ 2 , AND ε BY THEOREM 1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II OBTAINED</head><label>II</label><figDesc>UPPER BOUNDS OF τ 2 FOR μ 2 = -μ 1 = 1.2 AND VARIOUS τ 1 , ε</figDesc><table><row><cell>Fig. 4.</cell><cell>Trajectory of neuron state x(t) without external controller</cell></row><row><cell cols="2">in Example 2.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>τ l , μ r )</figDesc><table><row><cell></cell><cell>*   *   *   *   *</cell><cell cols="3">√ h Z l -e 2ετ 2 Y  *   *   *   *</cell><cell cols="3">hW l 0 -2e 2ετ 2 R 1  *   *   *</cell><cell cols="3">h O l 0 0 -2e 2ετ 2 R 2  *   *</cell><cell cols="3">√ h (t)Y h (t)R 1 h (t)R 2 0 0 0 0 0 0 0 0 0 -Y 0 0  *  -2R 1 0</cell><cell>⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎥ ⎥</cell><cell>&lt; 0</cell><cell>(52)</cell></row><row><cell></cell><cell>*  ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣</cell><cell>ˆ (i, τ l , μ r )  *   *   *   *   *</cell><cell>*</cell><cell cols="2">√ h Z l -e 2ετ 2 Y  *   *   *   *</cell><cell>*</cell><cell cols="2">hW l 0 -2e 2ετ 2 R 1  *   *   *</cell><cell>*</cell><cell cols="2">h O l 0 0 -2e 2ετ 2 R 2  *   *</cell><cell>*</cell><cell>*  h ˆ i Y h ˆ i R 1 h ˆ i R 2 -2R 2 √ 0 0 0 0 0 0 0 0 0 -Y 0 0  *  -2R 1 0</cell><cell>⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎥ ⎥</cell><cell>&lt; 0</cell><cell>(53)</cell></row><row><cell>⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣</cell><cell>ˆ (i, τ l , μ r )  *   *   *   *   *</cell><cell>*  h Z l √ -e 2ετ 2 Y  *   *   *   *</cell><cell cols="3">*  hW l 0 -2e 2ετ 2 R 1  *   *   *</cell><cell cols="3">*  h O l 0 0 -2e 2ετ 2 R 2  *   *</cell><cell cols="3">*  h ˆ i P T √ 0 0 0 -2P + Y  *</cell><cell cols="2">*  h ˆ i P T 0 0 0 0 -4P + 2R 1</cell><cell>*</cell><cell>-2R 2 h ˆ i P T 0 0 0 0 0</cell><cell>⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎥ ⎥</cell><cell>&lt; 0</cell><cell>(54)</cell></row><row><cell></cell><cell>*</cell><cell>*</cell><cell></cell><cell>*</cell><cell></cell><cell></cell><cell>*</cell><cell></cell><cell></cell><cell>*</cell><cell></cell><cell></cell><cell>*</cell><cell>-4P + 2R 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>On the other hand, it is clear that (t, τ (t), τ (t)) is a quadratic convex combination<ref type="bibr" target="#b44">[45]</ref>,<ref type="bibr" target="#b50">[51]</ref> with respect to τ (t) ∈ [τ 1 , τ 2 ],and (t, τ (t), τ (t)) is also a convex combination with respect to τ (t) ∈ [μ 1 , μ 2 ], respectively. Therefore,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>2 c e -2εt .</cell><cell>(49)</cell></row><row><cell>Thus</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>x(t) ≤</cell><cell>ρ δ</cell><cell>x t c e -εt , t ≥ 0</cell><cell>(50)</cell></row><row><cell cols="5">which means that the system (4) is exponentially stable if</cell></row><row><cell cols="2">(t, τ (t), τ (t)) ≤ 0.</cell><cell></cell><cell></cell></row><row><cell cols="4">(t, τ (t), τ (t)) ≤ 0, if the following inequalities hold:</cell></row><row><cell>⎧ ⎪ ⎪ ⎪ ⎨</cell><cell cols="3">(t, τ (t), τ (t))| τ (t )=τ 1 , τ (t )=μ 1 &lt; 0 (t, τ (t), τ (t))| τ (t )=τ 1 , τ (t )=μ 2 &lt; 0</cell></row><row><cell>⎪ ⎪ ⎪ ⎩</cell><cell cols="2">(t, τ (t), τ (t))|</cell><cell></cell></row></table><note><p>τ (t )=τ 2 , τ (t )=μ 1 &lt; 0 (t, τ (t), τ (t))| τ (t )=τ 2 , τ (t )=μ 2 &lt; 0.</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61473070 and Grant 61433004, in part by the Fundamental Research Funds through the Central Universities under Grant N140406001, Grant N130504002, and Grant N130104001, and in part by the SAPI Fundamental Research Funds under Grant 2013ZCX01.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Passivity and passification of memristor-based recurrent neural networks with additive timevarying delays</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rakkiyappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2043" to="2057" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finite-time synchronization control of a class of memristor-based recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="133" to="140" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memristor crossbar-based neuromorphic computing system: A case study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1864" to="1878" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamical properties and design analysis for nonvolatile memristor memories</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="724" to="736" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memristor cellular automata and memristor discrete-time cellular neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Bifurcation Chaos</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3605" to="3656" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memristor bridge synapse-based neural network and its learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1426" to="1435" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memristor-based cellular nonlinear/neural network: Design, analysis, and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1202" to="1213" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Three fingerprints of memristor</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synchronization control of a class of memristor-based recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="116" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weak, modified and function projective synchronization of chaotic memristive neural networks with time delays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synchronization of a class of memristive neural networks with time delays via sampleddata control</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="373" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive finite-time complete periodic synchronization of memristive neural networks with time delays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11063-014-9373-6</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Complete periodic adaptive antisynchronization of memristor-based neural networks with mixed time-varying delays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Phys</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1337" to="1349" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic exponential synchronization control of memristive neural networks with multiple time-varying delays</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="16" to="25" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive synchronization of memristor-based neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2033" to="2042" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synchronization for delayed memristive BAM neural networks using impulsive control with random nonlinearities</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mathiyalagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakthivel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page" from="967" to="979" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">State estimation of memristor-based recurrent neural networks with timevarying delays based on passivity theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rakkiyappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laksmanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="32" to="43" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Passivity and passification of memristorbased recurrent neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2099" to="2109" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exponential adaptive lag synchronization of memristive neural networks via fuzzy method and applications in pseudorandom number generators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1704" to="1713" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Circuit design and exponential stabilization of memristive neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attractivity analysis of memristor-based cellular neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="704" to="717" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">New global exponential stability results for a memristive neural system with time-varying delays</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="553" to="559" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exponential stability of stochastic memristor-based recurrent neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="92" to="98" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stability of delayed memristive neural networks with time-varying impulses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Neurodyn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="429" to="436" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Global exponential periodicity and stability of a class of memristor-based recurrent neural networks with multiple delays</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page" from="386" to="396" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global exponential stability of a class of memristor-based recurrent neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="149" to="154" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exponential stabilization of memristive neural networks with time delays</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1919" to="1929" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exponential stabilization of memristor-based chaotic neural networks with time-varying delays via intermittent control</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1431" to="1441" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finite-time stabilizability and instabilizability of delayed memristive neural networks with nonlinear discontinuous controller</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2015.2460239</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Global exponential dissipativity and stabilization of memristor-based recurrent neural networks with timevarying delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Delay-dependent robust stability and stabilization of uncertain memristive delay neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="155" to="161" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reliable stabilization for memristor-based recurrent neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mathiyalagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anbuvithya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakthivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="140" to="147" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Synchronization of memristor-based recurrent neural networks with two delay components based on second-order reciprocally convex approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rakkiyappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="79" to="93" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LMI-based approach for global asymptotic stability analysis of recurrent neural networks with various delays and structures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1032" to="1045" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comprehensive review of stability analysis of continuous-time recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1229" to="1261" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stability criteria for recurrent neural networks with time-varying delay based on secondary delay partitioning method</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2589" to="2595" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stability analysis for periodic solution of BAM neural networks with discontinuous neuron activations and impulses</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Model</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2564" to="2574" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stability analysis for neural networks with inverse Lipschitzian neuron activations and impulses</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Model</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2347" to="2359" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Design of state estimator for bidirectional associative memory neural networks with leakage delays</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sakthivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vadivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mathiyalagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sivachitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Asymptotic stability of stochastic delayed recurrent neural networks with impulsive effects</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sakthivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samidurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Anthoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exponential H ∞ filtering for discrete-time switched neural networks with random delays</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mathiyalagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakthivel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="676" to="687" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stability and stabilization of Markovian jump systems with time delay via new Lyapunov functionals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2413" to="2421" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On stabilization of stochastic Cohen-Grossberg neural networks with mode-dependent mixed time-delays and Markovian switching</title>
		<author>
			<persName><forename type="first">C.-D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="800" to="811" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stability for neural networks with time-varying delays via some new approaches</title>
		<author>
			<persName><forename type="first">O.-M</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="193" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stability analysis for neural networks with time-varying delay based on quadratic convex combination</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="521" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stability and stabilization of discrete-time T-S fuzzy systems with stochastic perturbation and timevarying delay</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="124" to="138" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">State feedback stabilization of stochastic feedforward nonlinear systems with input time-delay</title>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-R</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Autom. Sinica</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2972" to="2976" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A new Lyapunov based robust control for uncertain mechanical systems</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Autom. Sinica</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="875" to="882" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Data-driven robust approximate optimal tracking control for unknown general nonlinear systems using adaptive dynamic programming method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2226" to="2236" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Theory of Ordinary Differential Equations</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Coddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Levinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955">1955</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Global asymptotic stability analysis for delayed neural networks using a matrix-based quadratic convex approach</title>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-L</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="57" to="69" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Network-based synchronization of delayed neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-L</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="676" to="689" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Stability of Time-Delay Systems</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Birkhäuser</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Nonfragile control with guaranteed cost of T-S fuzzy singular systems based on parallel distributed compensation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1183" to="1196" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">New approach to delay-dependent stability analysis and stabilization for continuous-time fuzzy systems with timevarying delay</title>
		<author>
			<persName><forename type="first">H.-N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="482" to="493" />
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Feron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Balakrishnan</surname></persName>
		</author>
		<title level="m">Linear Matrix Inequalities in System and Control Theory</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Reciprocally convex approach to stability of systems with time-varying delays</title>
		<author>
			<persName><forename type="first">P</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="238" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
