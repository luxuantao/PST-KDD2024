<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Allocation and Scheduling of Precedence-Related Periodic Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krithi</forename><surname>Ramamritham</surname></persName>
						</author>
						<title level="a" type="main">Allocation and Scheduling of Precedence-Related Periodic Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">65128E89909F65F3D7CEDA661F68421F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Real-time systems</term>
					<term>task allocation</term>
					<term>scheduling</term>
					<term>periodic tasks</term>
					<term>precedence constraints</term>
					<term>distributed systems. &apos; A round denotes the rime interval during which every processor can goals [11-[51</term>
					<term>[81</term>
					<term>[lo]</term>
					<term>[121</term>
					<term>[ W</term>
					<term>[15l</term>
					<term>[181</term>
					<term>[19l</term>
					<term>[211</term>
					<term>[221</term>
					<term>communicate with all others in the system 111</term>
					<term>[2]</term>
					<term>[6]</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses a static algorithm for allocating and scheduling components of periodic tasks across sites in distributed systems. Besides dealing with the periodicity constraints, (which have been the sole concern of many previous algorithms), this algorithm handles precedence, communication, as well as replication requirements of subtasks of the tasks. The algorithm determines the allocation of subtasks of periodic tasks to sites, the scheduled start times of subtasks allocated to a site, and the schedule for communication along the communication channel(s). Simulation results show that the heuristics and search techniques incorporated in the algorithm are very effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Safety-critical tasks in real-time systems must meet their deadlines under all circumstances, otherwise the result could be catastrophic. Resources needed to meet the deadlines of safety-critical tasks are typically preallocated. Also, these tasks are usually statically scheduled such that their deadlines will be met even under worst-case conditions. The algorithm described in this paper is most suitable for the static allocation and scheduling of safety-critical periodic tasks. It is a static algorithm in that the decisions concerning the allocation and scheduling of components of a task across sites in a distributed system as well as the scheduling of communication among these components are made prior to the beginning of system execution. Besides periodicity constraints, tasks handled by the algorithm can have resource requirements and can possess precedence, communication, as well as replication constraints. Such tasks occur in current applications such as robotics but are of special significance for next generation real-time systems, such as the space station, automated factories, and advanced command and control systems.</p><p>The motivation for considering such tasks is that once there is a way to deal with tasks having precedence structures and communication constraints, real-time systems can be conveniently programmed as communicating modules, each with its own specifications. Communication between modules can take place via shared resources or via communication ports. The modules may have producer-consumer relationships andor may synchronize their activities. A complex program with communicating modules can be translated into a task composed of a set of communicating subtasks where each subtask has resource requirements, involves the execution of sequential code, and has communication as well as precedence constraints with other subtasks. Such subtasks can be scheduled using algorithms similar Manuscript received February 11, 1992; revised September 1994. This work was supported in part by the National Science Foundation under Grants CDA-8922572 and IRI-9208920, and by the Office of Naval Research under Contract "14-92-5-1048.</p><p>The author is with the Department of Computer and Information Science, University of Massachusetts, Amherst, MA 01003 USA. He is currently on sabbatical with the Department of Computer Science and Engineering, Indian Institute of Technology, Madras 600036 India.</p><p>IEEE Log Number 9409328. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>! i 3</head><p>Periodic Task 1: Subtasks = 1.1, 1.2. 1.3. and 1.4 Period = 50 Subtask 1.2 IS required to be e x e ~ cuted in triplicate; Subtask 1.4 votes on the results communicated by the three copies of 1.2.</p><p>Fig. to the one discussed here. Also, a task, for example, one requiring access to many resources, can now be (correctly) handled by breaking it up into multiple subtasks related by precedence constraints where each subtask requires a subset of the resources.</p><p>Suppose we have a program with four modules, A, B, C, and D.</p><p>Module A gets sensory information from the environment, preprocesses it and sends part of the processed information to B and the rest to C. B and C further process the information sent to them and send the results to D which displays the results. Since the processing done by B is crucial to the functioning of the system, B is required to be replicated. This set of four modules can be translated into a task composed of four subtasks with precedence, communication and replication constraints as shown by periodic task 1 of Fig. <ref type="figure">1</ref>. Note that we have assumed that each subtask executes sequential code and hence when we refer to the communication between two subtasks, we mean the communication that occurs when one completes execution and sends its results to the other before the latter begins execution. Periodic task 2 of Fig. <ref type="figure">1</ref> is another example of a task with three subtasks related by simple precedence.</p><p>The algorithm consists of two parts. The first part decides whether a cluster of communicating subtasks of a task should be assigned to the same site. This decision is based on the computation times of the subtasks in a cluster and the amount of communication between them. This part is heuristic in nature and we compare the performance of different heuristics. Given the clustering done in the first part, the second part assigns the clusters of subtasks to the sites in a system and also determines a feasible schedule, if possible, for the subtasks as well as the communication between them. This is done using a search driven by task characteristics, where, at each point in the search, subtasks eligible for execution are considered in accordance with task characteristics such as latest-start-times and precedence constraints. Since the first part of the algorithm eliminates some of the communication (by deciding that certain subtasks should be assigned to the same site), the search space in the second part is considerably reduced.</p><p>A majority of current allocation and scheduling work dealing with periodic tasks assume that tasks have only periodicity constraints 1045-9219/95$04.00 0 1995 IEEE <ref type="bibr">[8]</ref>, <ref type="bibr">[16]</ref>. Constraints such as those that arise from resource sharing and synchronization are beginning to be considered [ 171. Resource constrained scheduling is also investigated in [20] but they deal with tasks that execute on a single site in a distributed system. Precedence constraints and exclusion constraints, resulting for example from resource usage considerations, are considered in <ref type="bibr">[ 191,</ref><ref type="bibr">but delays due to communication are not. References [l] and [6]</ref> deal with the allocation and scheduling of simple periodic tasks having fault tolerance requirements on a multiprocessor.</p><p>The work described in [ l l ] comes closest to ours but differs in a number of ways: For practical reasons, we use heuristicsdirected search; Their's is a pure branch and bound search and its practicality is yet to be demonstrated. Our algorithm handles replication constraints; They have not considered replication issues. Finally, unlike [l 11, our algorithm allows subtasks of a task to execute on different sites. This is essential when certain subtasks of a task have replication requirements and hence replicates of a subtask have to be executed at different sites. Further, the total computational requirements of subtasks of a task may be such that a single site may not be able to execute all of them within the period of the task. However, by distributing the subtasks, in particular, by exploiting the parallelism within a task, it may be possible to meet the periodicity requirements. Also, all resources needed by all the subtasks may not be available on any one site. Allowing subtasks of a task to execute at different sites provides the flexibility to deal with this situation.</p><p>The rest of the paper is structured as follows: In Section 11, the characteristics of the periodic tasks considered by the scheduling algorithm are discussed. The details of the algorithm are provided in Section 111. Results of an evaluation of its performance for different types of tasks are discussed in Section IV. Section V concludes the paper by summarizing the important characteristics of the algorithm and discussing its applicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">TASK AND SYSTEM CHARACTERISTICS</head><p>The assumption is made that each site in the distributed system has one processing element and a given set of (passive) resources. The sites are connected by a multiple-access network. The algorithm is designed to work with communication media and protocols wherein knowing the arrival time and characteristics of a message, we can predict when the message will be delivered. For instance, point-to-point networks or multi-access networks employing a timedivision multiple-access (TDMA) protocol have such predictability. Communication from one site to another occurs at prespecified times, as per the schedule generated. Since the scheduler preschedules the communication on the network, no contention occurs for the multipleaccess network at run time. A reader may desire to examine Fig. <ref type="figure" target="#fig_2">3</ref> to get some idea about what such a schedule looks like.</p><p>Specifications of periodic tasks include the following (see Fig. <ref type="figure">1</ref> for the specification of two sample periodic tasks).</p><p>1) The period of the task. The semantics assumed is that one instance of all subtasks of a task should be executed every period.</p><p>2) The precedence relationship among subtasks of the task. This is expressed as a graph wherein the nodes represent subtasks and a directed arc exists from a subtask to its successor. (Here we assume that communication or precedence relationships do not exist between subtasks of difSerent periodic tasks. In <ref type="bibr">[13]</ref> we show how this assumption can be relaxed.) 3 ) Computation times of subtasks are expressed via values attached to each graph node. These represent worst-case computation times. (It is assumed that the execution of each subtask cannot be preempted <ref type="bibr">[18]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>The maximum amount of information communicated from a subtask to its successor is expressed via a value attached to the corresponding arc. This is used to determine the communication delays incurred due to information transfer from a subtask to its successor if they are scheduled on different nodes in a distributed system. This information is also used to schedule the communication between communicating subtasks placed on different sites. (Communication within a site is assumed to incur zero delay.) We assume that the value associated with arcs in the graph are the communication times for the corresponding messages. Such a view simplifies subsequent discussion. In Section V we discuss how other communication mechanisms can be accommodated. 5) The replication requirements of subtasks is specified by a value (given by RR) attached to each subtask indicating the number of replicates needed for the subtask. (Thus, it is assumed that fault tolerance is achieved via replication. The results of the replicates of a subtask are sent to each successor of the subtask which, depending on the fault model assumed, may vote on the results to determine the valid input. If voting is done, it is assumed that the voting overheads incurred by the successor are either negligible or are already accounted for in the computation time of the successor.) 6) Resource constraints attached to each subtask express any specific resources needed by that subtask. These include, the CPU, sensors, I/O devices, data structures, files, and data bases.</p><p>The resource constraints restrict the sites to which a subtask can be assigned: These sites should have the resources required by the subtask. For simplicity, it is assumed that all the specified resources are needed by the subtask throughout its execution and that resources allocated to a subtask are released at the end of its execution. (With some minor changes, the algorithm can be made to handle situations where these assumptions are relaxed.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">CONSTRAINED SEARCH FOR A FEASIBLE SCHEDULE</head><p>The purpose of our algorithm is to allocate subtasks of a set of tasks across sites in a distributed system and to schedule the subtasks such that the tasks meet their periodicity requirements. It is well-known that even some of the simplest scheduling problems are NP-hard in the strong sense <ref type="bibr">[3]</ref> and hence in practice it is not possible to determine optimal schedules efficiently. When allocating and scheduling subtasks that communicate, the following issues have to be dealt with in conjunction. 1) Given a set of communicating subtasks, should they be placed on the same site? 2) Which site should a subtask be allocated to and when should it begin execution? An optimal solution should consider the cross product of the solution space of these queries. As we will show, this is impractical for nontrivial distributed systems and for periodic tasks with complex characteristics.</p><p>Let us consider the first issue first. Ideally, we should cluster subtasks of a task such that a) the cost of communication among subtasks within a cluster is higher than that between subtasks in different clusters and b) the cost of communication among subtasks within a cluster negate the advantages of the parallel execution of the subtasks at different sites. Subtasks belonging to the same cluster should be allocated to the same site. The basic idea then is to cluster together subtasks that have "substantial" amounts of communication among them. This strategy attempts to eliminate the large communication costs. While more elaborate clustering mechanisms are possible, in this paper, we present the results for clusters of size two. In this case, suppose we have n pairs of communicating subtasks. If an optimal solution has to be found, the 2" different clustering possibilities that exist must be examined. In our experiments we consider task sets that have over 70 subtasks with around half as many pairs of communicating subtasks. Even assuming it takes one ps to examine each possibility, it will take more than a day to exhaust all of them.</p><p>The complexity of finding an optimal solution poses a further practical problem when the second issue is also considered. Hence our algorithm addresses the two issues raised above in separate phases and utilizes heuristic solutions. In addition, it provides a way by which it is possible to iterate over the allocation and scheduling decisions to find a feasible solution. Our experiments show that this approach as well as the adopted heuristics are extremely effective. Specifically, they show that if a task set can be feasibly allocated and scheduled, very likely, the algorithm will find it without any backtracking during the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview of the Algorithm</head><p>Given a set of periodic tasks, the algorithm attempts to assign subtasks of the tasks to sites in a distributed system and to construct a schedule of length L where L is the least common multiple of the task periods. A real-time system with the given set of tasks then repeatedly executes its tasks according to this schedule every L units of time.</p><p>Given the graph depicting each task, Step-I constructs the comprehensive graph containing all instances of the tasks that will execute in an interval of length L. The comprehensive graph includes the replicates of the subtasks that have replication requirements.</p><p>Step-I1 involves clustering subtasks in the comprehensive graph. Specifically, based on the amount of communication involved between a pair of communicating subtasks and the computation time of the subtasks, a decision i s made as to whether the two subtasks should be assigned to the same site, thereby eliminating the communication costs involved. The algorithm makes its decision based on whether the fraction sum of the computation time of the two subtasks cost of communication is lower than a tunable parameter called communication factor, C F .</p><p>Applying the above scheme to every pair of communicating subtasks in the comprehensive graph derived in Step-I, a communication graph is generated with the current value of C F .</p><p>Step-111 allocates the subtasks to sites in the system, schedules these subtasks as well as communication, and if possible, determines a feasible schedule. This is done using a heuristic search technique that takes into account the various task characteristics, in particular, subtask computation times, communication costs, deadlines, and precedence constraints. It allocates a subtask to a site, determines the order in which each site processes its subtasks, and schedules communication. The allocation and scheduling decisions are made in conjunction. Specifically, allocation and scheduling decisions about a subtask are made only after all its predecessors have been allocated and scheduled. These decisions take into account the communication and computational needs of the subtasks that follow.</p><p>If at the end of Step-111, a feasible allocation and schedule is not possible, the value of CF is altered, and Steps I1 and 111 are repeated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of the Algorithm</head><p>We now give details of the three steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">) Construction of the Comprehensive Graph:</head><p>The following semantics is associated with periodic tasks. All subtasks of the jth instance of a periodic task with period P should be completed between ( P x ( J -1)) and ( P x J ) . Given these semantics, the algorithm attempts to construct a feasible schedule for all task instances that should execute within the interval (0, L ) where L is the Least Common Multiple of the periods of all the periodic tasks involved. The comprehensive graph is composed of N , = instances of the zth periodic task (with period Pc). The first instance df each periodic task is ready to begin execution at time 0. In addition to the other constraints, the Jth instance of task z, for j = 1 , 2 , . . .,N,, will have a start time constraint whereby its first subtask(s), i.e., those that have no predecessors, cannot be scheduled to begin before ( ( 3 -1) x P,). Also, the last subtask(s) of the the gth instance of task 2, i.e., those which have no successors, will have a deadline of The comprehensive graph is then modified to consider replication requirements of subtasks. The additional replicates of a subtask added to the graph are endowed with the same specifications as the original subtask.</p><p>2) Constructing the Communication Graph: As mentioned earlier, the primary considerations in determining whether two subtasks of a task should be placed on the same site are the computation times of the subtasks and the amount of communication between them. If the two subtasks are placed on the same site, the costs of communication between the two subtasks are avoided and hence the time required to complete this pair of subtasks is reduced [2]. However, this has a number of implications. First, if a subtask has two successors (for example, consider subtask 1.1. of Fig. <ref type="figure">1</ref>) and if the subtask and both its successors are assigned to the same site, the potential for parallel execution of the successors is not exploited. Second, the load on the site to which both subtasks are allocated increases which can prevent the site from taking on a subtask (of another task) whose characteristics make this site more suitable for its execution. In general, requiring that some of the subtasks be assigned to the same site reduces the options for the remaining subtasks. Clearly, the scheme that decides whether communicating subtasks must be assigned to the same site must be flexible enough to take the specific characteristics of the tasks being allocated.</p><p>Suppose we had two pairs of subtasks where the computational characteristics of one pair are the same as the other. Then, it is better to assign subtasks with the higher communication costs to the same site. This i s the basis for our scheme. Specifically, two subtasks with computation times C, and C, where the communication from the first subtask to the second takes Comm,, units of time will be placed on the same site if the following holds: (C, + C,) &lt; (CF x <ref type="bibr">Comm,,)</ref> where CF is a tunable parameter called communication factor. For a given value of C F , this scheme tends to assign the pair of subtasks with higher communication costs to the same site.</p><p>It should be clear that the maximum value of CF that needs to be considered is</p><formula xml:id="formula_0">m a x v i , 1, + 6 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comm,,</head><p>for all communicating subtasks i and J . Let us call this m a x c f . (t has some positive nonzero value. It is set to 1 in the experiments of Section IV.) Assume that Comm,, for every pair of communicating subtasks i and J is nonzero. This i s true in practice since completion of a subtask's execution has to be notified to the successor, say via an "enabling signal." Thus, assigning a value of m a x c f to C F will force all communicating subtasks to be allocated to the same site. More and more pairs of communicating subtasks will be separated as the value of CF is decreased from maxcf. A CF value of 0 will force communicating subtasks to be allocated to different sites. By making Comm,, infinity (zero) between a pair of subtasks z and j , even if it is not, the subtasks involved can be forced to be be allocated to the same (different) site. C F values can be chosen in different ways. Our experimentation was done starting with a CF value of mazc f. For a given value of CF if a feasible schedule is not anived at, the scheduling algorithm reattempts by considering a lower C F value, one given by decrementing the current C F value by e where n is a constant.</p><p>Given a comprehensive graph, traversing the graph top-down and left to right, the "pairwise heuristic" is applied to every pair of communicating subtasks, to determine whether the subtasks should be assigned to the same site. In addition, in order to produce consistent clustering decisions, a pair of communicating subtasks, say, tl and its successor t2, may have to be placed on different sites as follows.</p><p>Suppose t 2 has a predecessor t o , in addition to t l . If prior decisions require that t o and t2 must be allocated to the same site and that t o and t 1 must be on different sites, then, clearly, t1 and t2 have to be allocated to different sites also. One specific instance of the above is the case where t o and tl are replicates with a successor t 2 , and t 2 and to are assigned to the same site.</p><p>For the general case of replicates, consider a subtask t l that is replicated. At most one of the replicates oft1 can be assigned to the same site as t l 's predecessor. Similarly, at most one of the replicates of tl can be assigned to the same site as t l ' s successor.</p><p>Suppose subtasks t l and t 2 have a common predecessor l o . If prior decisions require that t o and t l must be on the same site, and that t o and 12 must be on different sites, then t l and t 2 must also be on different sites. The graph that results from applying the pairwise heuristic as well as the above rules is called the communication graph. Fig. <ref type="figure" target="#fig_1">2</ref> shows the communication graph of the graph in Figure <ref type="figure">1</ref> for C F = maxcf.</p><p>Subtasks which are connected by arcs that do not have any associated times are the ones that must be allocated to the same site. In this case, since we are considering C F = maxcf, two subtasks that communicate are assigned to the same site. When applied to every pair of communicating subtasks, this scheme eliminates all communication except those to and from the replicates of a subtask.</p><p>In Section IV, we evaluate the performance of the pairwise heuristic and compare it with a "random" scheme where whether or not to retain a communication link is decided by the toss of a coin.</p><p>Once the communication graph has been derived, we determine the latest start time of each subtask t in the communication graph. Assume t is a subtask of periodic task (instance) T with deadline D . Let us define the length of a path between two subtasks in the communication graph to be the sum of the computation times of all the subtasks, including the subtasks in consideration, plus the sum of the communication times, if any, associated with the arcs that lie along the path. Let LP be the length of the longest path from t up to and including the last subtask of T . The latest start time of t is defined to be D -L P . Latest start times of tasks are used in ordering tasks for consideration during scheduling. The rectangle attached to each subtask in Fig. <ref type="figure" target="#fig_1">2</ref> indicates the "latest start time" of the subtask.</p><p>As the term "latest start time" implies, if a subtask is started any later than this time, the task it belongs to will definitely miss its deadline. However, as the following example indicates, this is an optimistic latest start time. Consider periodic task I of Fig. <ref type="figure">I</ref> without the replication requirement on subtask 1.2. Assume that all the communication subtasks of this task have been eliminated, i.e., all the subtasks must be scheduled on the same site. Suppose the task has a deadline of D . Then subtask 1.1 should start by in order for the task to meet its deadline. This is because all four subtasks need to be completed by time D on the same site. Clearly, LST' is less than the optimistic latest start time of 1.1. But, if subtasks 1.2 and 1.3 can execute on different sites, use of the optimistic start time is appropriate. Since this is the more general case, our algorithm uses this latest start time. However, to accommodate cases illustrated by the above example, additional checks are done while making allocation and scheduling decisions to make sure that a subtask starts early enough. These are described under "testing for infeasibility" in Section 111x4).</p><p>To ease further discussions, we refer to the communications that have to be scheduled as communication subtasks and the subtasks that must be allocated to sites as CPU subtasks. This nomenclature recognizes the additional resource constraint imposed by subtasks: Whereas CPU subtasks are allocated and scheduled on (processing) sites, communication subtasks are scheduled on the (communication) network. Viewing it in this fashion allows us to uniformly deal with all (types of) subtasks, given that the algorithm takes resource constraints into account.</p><p>3) Making Allocation and Scheduling Decisions: A subtask becomes enabled only when all its predecessors have completed execution. Thus, at any given time, only some of the subtasks are eligible for consideration. A subtask becomes ready only if it is enabled and when its start-time constraint is met. Given a list of ready subtasks, the order in which we consider them for allocation and scheduling will determine whether or not a feasible schedule is derived. For example, assume that at a given site two subtasks are enabled at time 20. Each subtask has computation time 10; the first has a deadline of 30 and the second 40. In this case, if we do not consider the subtasks according to their latest start times or deadlines, a feasible schedule can not be arrived at. In general, delaying the execution of the subtask with the least latest start time will delay the overall completion time of the set of tasks. Hence, the ready list is ordered according to increasing latest start time of the subtasks in the list. If there is a tie, the subtask with more number of successors is placed first. This is equivalent to using the LSTlMISF (Latest Start Timemaximum Immediate Successors First) heuristic during the search for a feasible allocation and schedule.</p><p>Since we are assuming that each site has only one processor, when the processor is busy (idle), the site is busy (idle). Some sites may be busy executing previously scheduled CPU subtasks that are yet to finish. Clearly, since CPU subtasks are assumed to be nonpreemptable, ready subtasks can be scheduled only on currently idle sites. For uniformity, we do not allow communication subtasks to be preempted either. Thus, a communication subtask can be scheduled only if the communication channel is idle. When making allocation and scheduling decisions, we refer to an idle site or an idle communication channel as a schedulable resource.</p><p>After initializing the ready list with the root node of the communication graph, search proceeds as follows. At each search point, the algorithm first checks if the allocation and scheduling decisions made thus far will not lead to a feasible schedule. These checks are discussed under "testing for infeasibility."</p><p>If the checks indicate that a feasible schedule is likely, then the subtasks in the ready list are mapped to schedulable resources.</p><p>Obviously, there are a number of possible mappings and they are generated and considered in order (as discussed under "Systematic Generation of Mappings").</p><p>If the current mapping is valid, i.e., meets certain requirements (discussed under "Testing the Validity of a Mapping"), the search path is extended by one more level and the search proceeds.</p><p>The "time" corresponding to the new level is set to be the smaller of min (earliest start time of currently enabled tasks) and min (earliest completion time of subtasks currently occupying resources). Subtasks that were in the previous ready list but not scheduled are placed in the new ready list. Tasks that have just become ready are added to the new ready list.</p><p>If the current mapping is invalid, the next mapping is generated and its validity determined. If no more valid mappings exist at the current point of search the algorithm discards the current search point. Once this occurs, if the algorithm is allowed to backtrack, it backtracks to the previous search point. On going back to the previous search point, the next valid mapping, if any, at that point is pursued.</p><p>If it is found that the current set of allocation and scheduling decisions will not lead to a feasible schedule, the current search point is "bound," i.e., is discarded. Here again, if backtracking is allowed, the algorithm backtracks to the previous search point and proceeds (if possible) with the next valid mapping at that point. Experimental results show that the LSTlMISF based ordering of the ready list works effectively in conjunction with the systematic generation of mappings, the tests used to validate a given mapping, and the checks used to determine whether the current search path will lead to a feasible schedule. Hence we discuss these now.</p><p>a ) Systematic Generation of Mappings: Given subtasks in the ready list, a mapping defines the assignment of subtasks to a schedulable resource. To simplify the generation of mappings, we introduce the notion of "idle subtasks." Sometimes, resources may remain idle because none of the ready tasks require it. Also, because of the characteristics of subtasks that become enabled or become ready at a future time, at times, it may be better to allow a resource to remain idle even if a currently ready task can be scheduled on it. For instance, a task that is yet to become ready may have an earlier latest start time than another that is ready. Further, if the resource requirements of the former conflict with the latter, then immediately scheduling the latter may affect the schedulability of the former. Thus, we have to consider "scheduling" not only the subtasks, but also the idle subtasks. Idle subtasks represent time slots during which one or more schedulable resources are allowed to remain idle. The notion of idle subtasks allows us to treat resource assignment uniformly: Some subtask is always assigned to a schedulable resource; if it happens to be the idle subtask, the resource remains idle. To facilitate this scheme, a number of idle subtasks, equal to the number of sites idle at this point, are appended to the ready list.</p><p>Suppose there are n subtasks (excluding idle tasks) in the ready list and k idle schedulable resources at a certain point in the search. Considering idle subtasks, the effective size of the ready list is ( n + k ) . There are O ( ( n + l ~) ~) possible mappings from subtasks to schedulable resources, considering idle subtasks as well.</p><p>A mapping ( m l , m2,. . . m k ) represents the assignment of the m,th subtask in the ready list to the zth idle schedulable resource. If the possible mappings from subtasks to schedulable resources are generated systematically, then given a certain ready list and a particular mapping, the next possible mapping can be determined without any other information. This scheme, inspired by the one used in [4], considerably reduces the amount of information that has to be maintained during search: only the most recently used mapping at this point in the search needs to be kept as part of the search structure.</p><p>For example, suppose subtasks ti and t2 are ready, t l has a lower latest start time, and schedulable resources SI and s2 are available. Then n = 2 and k = 2. The mappings from the ready list ( t l , t 2 , idle, idle) to the list of idle schedulable resources ( s l , s2) will be generated in the following sequence: This is a lexicographically ordered sequence where each mapping has k elements and each element in a mapping is between 1 and n + I;. Mappings that have the same effect as a previously generated mapping are not generated. The above sequence corresponds to the following sequence of mappings:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(ti , b ) , ( t i , i d k ) , ( h , tI),(t2, idle),(idle, tl),(idle, tz),(idle, i d l e ) .</head><p>Observe that mapping (2,4) (corresponding to ( t 2 , idle)) is not generated since it has the same effect as (2,3). For similar reasons, (4,1), (4,2), and (4,3) are also not generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b ) Testing the Validity of a Mapping:</head><p>The following conditions will have to be met for a mapping to be valid.</p><p>As noted earlier, schedulable resources can be allowed to remain idle. However,</p><p>-when there 'are ready tasks, not all resources in the system can remain idle. if the subtask with the lowest latest start time among subtasks yet to be scheduled is ready and is schedulable, it has to be scheduled.</p><p>-Resource constraints must be met:</p><p>-When a CPU subtask is mapped to a site, the resources needed by the subtask should be available in that site. Communication subtasks can be allocated only to the communication channels.</p><p>-Two subtasks must be scheduled on different sites if and only if the two subtasks are separated by a communication subtask.</p><p>Replicates of a subtask (if any) should be scheduled on different sites. Subtasks tl and t2 should be assigned to the same site if they have a CPU subtask t as their common successor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c ) Testing for Infeasibility:</head><p>The current search point will not lead to a feasible schedule, i.e., a schedule that meets all the timing constraints, if one of the following hold. First, time at the current search point is greater than the latest start time of a task in the ready list. Second, the total time available on a particular resource between --the current time and L is less than that required by the subtasks that will execute between now and L and require that resource. For example, assume that communication is via a multiple-access network and the current time is t . If the time needed for all the communication subtasks that have not yet been scheduled at this point is greater than ( Lt ) , then the current search will not lead to a feasible schedule.</p><p>In addition to these two cases, by "looking ahead," a potentially infeasible schedule can be detected sooner. Once again, consider periodic task 1 of Fig. <ref type="figure">1</ref> without the replication requirement on subtask 1.2. Assume that all the communication subtasks of this task have been eliminated, i.e., all the CPU subtasks must be scheduled on the same site.</p><p>Suppose subtasks 1.2 and 1.3 are currently on the ready list. Then the following condition should hold:</p><formula xml:id="formula_1">( c u r r e n t tame + C12 + CI 3 5 LST1 4 )</formula><p>where C, is the computation time of subtask a and LST, is the latest start time of a. The reason for the above condition should be obvious: Since both subtasks must execute on one site, they should complete execution before the latest start time of the successor task. If the above condition does not hold, the current partial schedule will not lead to a feasible schedule.</p><p>Suppose instead that subtask 1.1 is on the ready list. Then the following condition should be satisfied for the search to proceed:</p><p>( c u r r e n t t i m e + C1.1 + C1.z + c 1 . 3 5 LST1.4) Clearly, the above specific cases can be generalized. However, overheads are involved in detecting whether the conditions for the "look ahead" apply and hence all our experiments (see Section IV) exploit just these simple situations. In general, the sooner we determine that the search path being pursued will not lead to a feasible schedule the less time and resources will be wasted on scheduling. That is, the more search points we are able to "bound" and the more mappings we are able to identify as being invalid, the faster we can determine a feasible schedule. Thus, it is important to constrain the search as much as possible.</p><p>Our experiments indicate that the LSTMISF heuristic in conjunction with the mapping generation and validation scheme as well as the search path bounding scheme determine the initial search path so effectively that if the initial path does not lead to a feasible schedule for a given set of tasks, chances are high that we have an infeasible task set. This is attested by our test results which show that even a large number of additional backtracks produces only a small marginal improvement in performance. In any case, the algorithm produces monotonically better solutions as the number of allowed backtracks is increased. Specifically, the percentage of task sets for which feasible solutions can be found either stays the same or displays slight improvement with an increase in the number of backtracks.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows the schedule that results when the algorithm is applied to the communication graph of Fig. <ref type="figure" target="#fig_1">2</ref>. It shows the system resource to which each subtask is allocated along with the scheduled'start time of the subtask. The first three lines correspond to the schedule for sites 1, 2, and 3 respectively, and the last is the schedule for the network. Here all subtasks of Task 1 execute on site 1, all subtasks of Task 2 execute on site 0, the redundant copies of subtask 1.2.1.1 execute on sites 0 and 2. Derivation of this schedule required no backtracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iv. EVALUATION OF THE ALGORITHM</head><p>We have implemented the above algorithm in C++ and have tested it with many periodic tasks. Whereas we studied the algorithm under various parameter settings and different task types, due to space limitations, here we show only some of the salient results. Observations that can be made from these test cases were corroborated by results not reported here but can be found in <ref type="bibr">[13]</ref>. All our periodic tasks have precedence constraints that can be represented as general graphs <ref type="bibr">[lo]</ref> and have the following characteristics.</p><p>The computation time of each subtask is uniformly distributed between C,,, and C,,, set to 50 and 100 time units, respectively. The communication cost attached to an arc in the precedence graph lies between ( c o m m r a t i o x C,,,) and ( c o m m r a t i o x Cmax). Experiments were conducted for comm-ratio values between 0.1 and 0.4. Recall that some of the subtasks in a task are replicated. In the experiments, with a probability of redundancy-ratio, each subtask of a task has redundancyno of redundant subtasks (i.e. a total of 1 + redundancyno copies of a subtask). In the experiments, redundancy-ratio = 0.1 and redundancyno = 1. To exercise the algorithm under different periodicity constraints, the following scheme was devised. A parameter, Zaxityfactor was used to set the period, P, of the first task as follows:</p><formula xml:id="formula_2">P = C x t a s k s i z e</formula><p>x (1 + ( r e d u n d a n c y r a t i o x redundancy-no))</p><p>x laxity-factor Note that this formula is based on just the computational requirements of the tasks. The computational requirements of a task increases with the number of subtasks in a task, given by tasksize, with the average computation time of a subtask, C , with the redundancy-ratio, and with the redundancyno. Under the above formula, given a task with certain computational requirements, the larger the laxityfactor, the larger the period of the task. Even though we have conducted experiments with larger task sets[l3], all the results shown here are for task sets with three periodic tasks. The first periodic task has four subtasks (i.e., its tasksize=4), the second has eight, and the third has twelve.</p><p>Given the above formula for P, the period of the second task is twice that of the first; the period of the third is three times that of the first. Thus the length L of the schedule generated = the least common multiple of the three periods = (6 x P).</p><p>Even though we generated 100 task sets to obtain each point in the graphs, we removed from consideration task sets that were definitely infeasible. This was detected by determining the latest start time of the root subtask of a periodic task, ignoring all communication. If the latest start time is less than 0 the task set is not considered further. (In no case did we find more than 25 out of the 100 tasks generated to be definitely infeasible, thus leaving at least 75 tasks for exercising the algorithm.) Obviously, this does not eliminate all infeasible task sets since the presence of communication costs can make some task sets infeasible. However, as mentioned earlier, the problem of determining if a given set of periodic tasks is feasibly schedulable is a computationally intractable problem. Because of this, when a heuristic algorithm does not succeed in determining a feasible schedule, it could be due to the infeasibility of the task set. Still, if one heuristic scheme or one combination of parameter settings for an algorithm is able to feasibly schedule a periodic task set while another is not, we can clearly conclude the superiority of the first. Hence, the metric chosen to compare the performance of different algorithms or different parameter settings is the Success Ratio. If an algorithm is able to find feasible schedules for x of the given y task sets, (where none of the y task sets are definitely infeasible) its success ratio is said to be (xly). We express this typically as a percentage.</p><p>The tests involved a system with six sites connected by a multipleaccess network. Resources other than the CPU and the communication network were not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Effect of Different Strategies for Determining Communication Graphs</head><p>We tested the performance of the technique used for allocating a pair of communicating subtasks to the same site based on the communication costs, the computation times of the subtasks, and the communication factor ( C F ) .</p><p>Figs. 4 and 5 show the results for communication ratio values of 0.1 and 0.4 respectively. The results shown are for the case where the algorithm is not allowed to backtrack upon failure. Each figure has four graphs, for four values of CF. The CF=O graph shows the performance when two communicating subtasks are assigned to dzfferent sites irrespective of the communication costs or computation times. The CF=maxcf graph shows the performance when two communicating subtasks are assigned to the same site irrespective of the communication costs or computation times. (Thus, all subtasks of a task, except redundant subtasks, are assigned to the same site.) The CF=increment graph shows the performance when communication graphs are generated for different values of C F starting with m a x c f down to 0 in increments of m a x c f l l 0 . If a feasible schedule is generated for any of these communication graphs, the algorithm is considered to have succeeded with the given task set. The CF=random graph shows the result when communication graphs are generated via coin tosses, one for each pair of communicating subtasks.</p><p>The two figures show the drop in performance with increasing values of communication ratio and the improvement in performance with increasing task laxities. The results show that when communication ratio = 0.4, CF=maxcf performs better than CF=O. The opposite is true when communication ratio = 0.1. The reason should be clear. When communication costs are high (low), it is better to place communicating subtasks on the same site (on different sites). CF=increment performs better than CF=O and CF=maxcf for both values of communication ratio. This is to be expected since in this case the performance is improved by considering a number of CF values. The poor results with CF=random shows that it is important to make "informed decisions" that make use of subtask characteristics. This is obvious since the CF=random case generally performs worse than the others for communication ratio = 0.1 and worse than CF=maxcf and CF=increment when the ratio is 0.4.</p><p>Clearly, these results show that it is better to try different communication graphs for different values of C F , especially when where there is a performance increase when we go from increments of maxcf15 to m a x c f l l 0 , it is a very small (less than 3%). There is no noticeable further improvement when we go to increments of maxcfl20. Because of these observations, we worked with increments of m a x c f I10 for subsequent studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Costs of Scheduling and Allocation</head><p>We do not take into account any program or compiler optimizations.)</p><p>The numbers reported here are for a laxity factor of 1.5 and a communication ratio of 0.4. We also examined the costs in terms of the number of search points visited by the scheduling and allocation algorithm. The figures can be found in <ref type="bibr">[13]</ref>.</p><p>The periodic tasks considered produced comprehensive graphs with around 75 nodes and depending on the communication factor <ref type="bibr">( C F )</ref> and the comm-ratio, the communication graphs had between 85 and 300 nodes. The construction of these communication graphs took between 500 millisecs and 750 millisecs.</p><p>From Fig. <ref type="figure">5</ref>, we note that for C F = m u x c f, 83% of the task sets are successfully handled. The number of nodes in the communication graphs for these successful cases ranged from 85 to 240. It took between 510 ms and 12 s to schedule these communication graphs. 11% of the remaining task sets were successfully handled when different values of CF were considered. The average number of nodes in each of the communication graphs for these task sets ranged from 130 to 230 and the costs of determining the feasible schedule ranged from 3.75 to 12.4 s. The increase in costs are attributable to the fact that several communication graphs are examined before a feasible schedule is found. Finally, the communication graphs in the failure cases (6%) had between 175 and 300 nodes and in all but one of the failure cases (both for CF=maxcf and CF=increment), the costs of attempting to produce a feasible schedule ranged from 30 ms to 4 s. This suggests that, in most cases, a failure is detected early in the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of Deadline-Driven Search</head><p>Suppose we take an approach to allocation and scheduling where deadlines are not considered by the algorithm. Specifically, suppose we use an algorithm, such as in <ref type="bibr">[9]</ref> or [4], to construct a schedule that meets communication, precedence, and replication constraints and finally test the schedule with respect to the deadlines. To simulate such an algorithm, we removed, from our algorithm, the deadlinerelated conditions used in the check for a valid mapping, and in the check for feasibility. Fig. <ref type="figure" target="#fig_4">6</ref> shows the drop in the success ratio when these checks are removed. Four graphs are shown, for communication ratio values of 0.1, 0.2, 0.3, and 0.4. The significance of the drops, some as high as 30%, under higher values of communication ratio, indicates that it is important to consider task deadlines during the search as opposed to finding a schedule that meets other constraints and then testing it with respect to task deadlines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect of Backtracking During Search</head><p>Whereas the results shown thus far were obtained when the algorithm is not allowed to backtrack, we examined the effect of allowing a limited number of backtracks. The studies show that, for the task sets considered, there is only a small improvement of less that 3.5% even at low laxities (i.e., tight situations) when we allow 100 backtracks. When even more backtracks (upto 1000) are allowed, no noticeable improvement in performance is observed. However, a very large price (as much as a four fold increase in costs for successful cases and a thirty fold increase for the failure cases) is paid for obtaining even this small marginal increase in performance. Overall, the tests indicate that if the initial path does not lead to a feasible schedule, it is highly likely that the task set is infeasible. This is because, a very large number of backtracks results only in a very small improvement in performance and at a very high marginal increase in cost. These results show the effectiveness of the schemes used in determining the initial search path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>The algorithm discussed in this paper for scheduling safety-critical periodic tasks provides a framework for allocating and scheduling periodic tasks in distributed and multiprocessor systems. It allocates and schedules subtasks whose execution is constrained by precedence constraints, communication requirements, and replication requirements. The algorithm is based on a search technique that takes into account the various task characteristics. Its decisions relate to the site assignment of each subtask of a task, the order in which each site processes its subtasks, and how the communication medium schedules message transmissions.</p><p>Periodicity constraints of tasks are translated into deadline constraints of subtasks of these periodic tasks. Determining a schedule that meets these deadlines while satisfying the other constraints is the goal of the algorithm. This is a fundamental difference between this (deadline-driven) algorithm and other similar ones (e.g., <ref type="bibr">[4]</ref>, <ref type="bibr">[9]</ref>) that deal with precedence constrained scheduling aimed at producing least length schedules.</p><p>We now discuss some of the possible extensions to the algorithm. When a system goes through different modes of operation, it is the case that not all periodic tasks occur in all the modes. In this case, we can design a set of schedules, each applicable to a specific mode; schedules are switched when a mode change occurs. In this case, further constraints will be imposed on the schedules constructed upon mode changes. These will typically be placement constraints arising from the need to maintain continuity (with respect to the schedule adopted in the previous mode) of allocation of a subtask to a specific site. The algorithm can be extended to schedule aperiodic tasks that execute on a single site <ref type="bibr">[14]</ref>.</p><p>Finally, the algorithm can be tailored to apply to systems which use communication schemes that are more complicated than the one assumed in Section 111. For instance, it can be easily applied when sites are connected by a point-to-point communication network. In this case, individual communication links have to be scheduled. Some real-time systems intended for safety-critical applications (e.g., <ref type="bibr">MARS [5]</ref>), make use of the TDMA protocol. Here each site gets access rights (say, for w units of time) once during every cycle (say, of length c). Hence with TDMA, the minimum worst-case delay is at least c. In general, the worst-case delay is some multiple of c depending on whether the information from a subtask to its successor Byzantine Agreement in a Generalized Connected Network S. C. Wang, Y. H. Chin, and K. Q. Yan is transferable in time w. In any case, such delays are computable and used in the construction of communication graphs. The scheduling of the (multiple-access) network is also not too much more complicated than the simple scheme: A TDMA gives the impression of each site possessing an exclusive communication channel (where each can be used between specific time intervals only -depending on the phasing of the sites). In deterministic protocols using dynamic token passing schemes <ref type="bibr">[12] [7]</ref> the maximum communication delays can be determined and can be used in the the construction of communication graphs. However, in this case, the algorithm will not be involved in scheduling communication subtasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Communication graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Assignment and schedule for the two periodic tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. nication ratio = 0.1.Effect of strategies for determining communication graphs: commu-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Drop in performance if time constraints are not used during search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1. Structure of two periodic tasks.</figDesc><table><row><cell>Periodic Task 2</cell></row><row><cell>Subtaks = 2.1, 2.2. and 2.3</cell></row><row><cell>Period = 15</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Traditionally, the Byzantine Agreement (BA) problem is studied either in a fully connected network or in a broadcast network. A generalized network model for BA is proposed in this paper. A fullyconnected network or a broadcast network is a special case of the new network architecture. Under the new generalized network model, the BA problem is reexamined with the assumption of malicious faults on both processors and transmission medium (TM), as opposed to previous studies which consider malicious faults on processors only. The proposed algorithm uses the minimum number of message exchanges, and can tolerate the maximum number of allowable faulty components to make each healthy processor reach a common agreement for the cases of processor failures, TM failures, or processorlTM failures. The results can also be used to solve the interactive consistency problem and the consensus problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In many cases, a healthy processor in a distributed system must reach a common agreement with other healthy processors before doing some special task. Examples include the two-phase commit in a distributed database system [l 11, the placement of a replicated file in a distributed environment [9], [12], <ref type="bibr">[14]</ref>, and a landing task controlled by cooperating processors in a flight control system <ref type="bibr">[20]</ref>.</p><p>One well-known unanimity problem studied by Lamport <ref type="bibr">[131, [151, [18]</ref>, [19], [21], <ref type="bibr">[22]</ref>. The problem assumes: 1) there are 12 processors of which at most f, processors could fail (may change its received messages to mislead others to reach BA) without causing a breakdown of a workable network; 2 ) the processors can directly communicate with each other through message exchange in a Fully Connected Network (FCN); 3) the message's sender is always identifiable by the receiver; 4) an arbitrary processor is chosen as a source and its initial value u s is broadcasted to other processors and itself at the start of the algorithm execution and 5) the only faulty components considered are processors. Based on these assumptions, various algorithms for the BA problem have been developed in order to meet the following ( B A I ) : Every healthy processor computes a common value w which is used to determine the agreement;</p><p>(BA2): If the source is healthy, then the common value w should be the source's initial value vs.</p><p>Based on these goals, the aim of solving a BA problem is to develop an optimum algorithm such that the algorithm can use the minimum number of rounds' to reach an agreement and can tolerate the maximum number of faulty components which may change its received messages.</p><p>With the conditions stated above, the BA problem was first proposed and solved by Lamport </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task allocation in fault-tolerant distributed systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bannister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acta Infomrica</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Heuristic models of task assignment scheduling in distributed systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Efe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982-06">June 1982</date>
			<publisher>IEEE Comput</publisher>
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Strong NP-completeness results: Motivation, examples, and implications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="499" to="508" />
			<date type="published" when="1978-07">July 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical multiprocessor scheduling algorithms for efficient parallel processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1023" to="1029" />
			<date type="published" when="1984-11">Nov. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed fault tolerant real-time systems: The Mars approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kopetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mulozzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Compur</title>
		<imprint>
			<biblScope unit="page" from="448" to="155" />
			<date type="published" when="1986">2 5 4 0 , 1989. May 1986</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
	<note>On scheduling tasks with a quick recovery from failure</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The 802.3D protocol: A variation on the IEEE 802.3 standard for real-time LAN&apos;s</title>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Lann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA, Tech. Rep</title>
		<imprint>
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scheduling algorithms for multiprogramming in a hard real-time environment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Layland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A task allocation model for distributed computing systems</title>
		<author>
			<persName><forename type="first">P.-Y</forename><forename type="middle">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsuchiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Molesky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Massachusetts, Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="1982-09">1982. Sept. 1989</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Compur.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Static allocation of periodic tasks with precedence constraints in distributed real-time systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int</title>
		<meeting>9th Int</meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Channel characteristics in local area hard real-time systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamritham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISDN and Compur. Networks</title>
		<imprint>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Univ</orgName>
		</respStmt>
	</monogr>
	<note>Allocation and scheduling of complex periodic tasks</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Massachusetts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-10">Oct. 1989; revised Jan. 1992</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 90-01</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Providing for dynamic arrivals during the static allocation and scheduling of complex periodic tasks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamritham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Adan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Massachusetts, Tech. Rep</title>
		<imprint>
			<date type="published" when="1990-10">Oct. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient scheduling algorithms for real-time multiprocessor systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamritham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stankovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel and Distribut. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Solutions for some practical problems in prioritized preemptive scheduling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lehoczky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Real-7ime Sysr. Symp</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scheduling processes with release times, deadlines, precedence constraints, and exclusion relations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Stankovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamritham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Parnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="36" to="369" />
			<date type="published" when="1990-05">1990. May 1991. Mar. 1990</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Comput.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and integrated heuristic algorithms for scheduling tasks with time and resource constraints</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamritham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. SystSojiware</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
