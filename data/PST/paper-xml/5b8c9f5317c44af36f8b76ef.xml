<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Decision Boundary of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-01-01">1 Jan 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
							<email>yu.li@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">KAUST CEMSE</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lizhong</forename><surname>Ding</surname></persName>
							<email>lizhong.ding@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">KAUST CEMSE</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
							<email>xin.gao@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">KAUST CEMSE</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the Decision Boundary of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-01-01">1 Jan 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1808.05385v3[cs.NE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep learning models and techniques have achieved great empirical success, our understanding of the source of success in many aspects remains very limited. In an attempt to bridge the gap, we investigate the decision boundary of a production deep learning architecture with weak assumptions on both the training data and the model. We demonstrate, both theoretically and empirically, that the last weight layer of a neural network converges to a linear SVM trained on the output of the last hidden layer, for both the binary case and the multi-class case with the commonly used cross-entropy loss. Furthermore, we show empirically that training a neural network as a whole, instead of only fine-tuning the last weight layer, may result in better bias constant for the last weight layer, which is important for generalization. In addition to facilitating the understanding of deep learning, our result can be helpful for solving a broad range of practical problems of deep learning, such as catastrophic forgetting and adversarial attacking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, deep learning has achieved impressive success in various fields <ref type="bibr" target="#b16">[17]</ref>. Not only has it boosted the performance of the state-of-the-art methods in various areas, such as computer vision <ref type="bibr" target="#b15">[16]</ref> and natural language processing <ref type="bibr" target="#b7">[8]</ref>, it has also enabled machines to achieve human level intelligence in specific tasks <ref type="bibr" target="#b24">[25]</ref>. Despite its great empirical success, deep learning is often criticized for being used as a black box <ref type="bibr" target="#b4">[5]</ref>, which refers to the well-known gap between its empirical power and the theoretical understanding of it <ref type="bibr" target="#b23">[24]</ref>.</p><p>As suggested by <ref type="bibr" target="#b23">[24]</ref>, a satisfactory theoretical understanding of deep learning should cover three aspects: 1) representation power, 2) optimization characteristics, and 3) generalization property. The representation power of deep learning has been extensively and rigorously discussed in <ref type="bibr" target="#b28">[29]</ref>. In terms of the second aspect, that is, the convergence analysis of stochastic gradient descent (SGD) and the property of the minima obtained, numerous recent studies have endued promising answers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. For example, <ref type="bibr" target="#b11">[12]</ref> proves the conjecture of <ref type="bibr" target="#b1">[2]</ref>, extending the result to deep nonlinear neural networks and showing the nonexistence of poor local minima. <ref type="bibr" target="#b21">[22]</ref> also shows that all local minima are globally optimal, given reasonable assumptions. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> prove the convergence of SGD given assumptions of the input distribution.</p><p>As for the generalization mystery, the studies are still in the early stage. Through systematic experiments, <ref type="bibr" target="#b28">[29]</ref> suggests that although the explicit regularization, such as weight decay and dropout, may be helpful, the implicit regularization of SGD may be the key for generalization. Following that direction, <ref type="bibr" target="#b3">[4]</ref> provides the generalization guarantee for over-parameterized networks on linearly separable data, which are trained by SGD. <ref type="bibr" target="#b26">[27]</ref> shows that, for linearly separable data, gradient descent (GD) on an unregularized logistic regression problem results in the max-margin (hard margin SVM) solution. On the other hand, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> try to demystify the generalization property via deriving the generalization bounds. In this paper, we follow the direction of <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27]</ref>, investigating the implicit bias of GD and SGD. Unlike the previous studies, we do not oversimplify the model architecture. In fact, the architecture, which is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, is a productive one, which can reach the state-of-the-art performance on CIFAR-10 if we use DenseNet <ref type="bibr" target="#b10">[11]</ref> as the transformation function. Moreover, we have little requirement for the input data distribution, only assuming that the loss converges to zero. In the Main Result section and Experiments section, we show that the direction of the neural network's last weight layer converges to that of the SVM solution trained on the transformed data in the transformed space both theoretically and empirically. In addition, we also show that the decision boundary of the last layer is closer to the SVM decision boundary if we train the whole network, instead of only fine-tuning the last layer, in the Experiments part. We extend our result to multi-class classification problem with cross-entropy loss, which is the most common scenario in practice, on the MNIST and CIFAR10. Our study bridges the gap between the purely theoretical side, which investigates the over-simplified models and has strict requirements for the input distribution, and the practical usage of complex deep learning models. In practice, people usually owe the superior performance of deep learning to the model's ability of learning representation and classifier simultaneously. We demystify the relationship between the learned representation and the classifier, and characterize the learned classifier in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>Unlike the setting of previous studies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>, which assume the training data is linearly separable or follows a certain distribution, we do not have such a requirement. Formally, for binary classification, we consider a dataset {x n , y n } N n=1 , with x n ∈ R d , and binary labels y n ∈ {−1, 1}. We use X ∈ R d * N to denote the data matrix. For multi-class classification, we have y n ∈ [K] := {1, 2, . . . , K} and K is the number of classes.</p><p>Regarding the neural network model, we do not restrict to any specific the architecture neither. Consider a neural network with the architecture shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which is basically a production network with practical usage. We divide the neural network into four components. The original space and label space are the training interface. The transformation function combined with the transformed space (the output of the last hidden layer) is one of the reasons why the deep learning's performance is being continuously improved. For the sake of analysis, we take the transformed space as an independent component which is fully connected with the label space. Formally, we denote the output of the last hidden layer on example x n as δ n , with δ n ∈ R t .</p><p>We denote the entire parameter set of the network as θ. The network defines a function f (x; θ) : R d → {−1, 1} for the binary case. The transformation function is δ n = h(x n ; φ), where φ is the parameter set of the transformation function. Notice that from δ n to the final output, the last weight layer defines a linear transformation, which has the following form:</p><formula xml:id="formula_0">g(δ n ; W) = Wδ n ,<label>(1)</label></formula><p>where W ∈ R t * k is the weight vector of the last layer (notice that for the binary case, k = 1). We use W i ∈ R t * 1 to denote the i-th row of it. So, we have θ = (φ, W).</p><p>In general, the empirical loss over the training dataset has the following form:</p><formula xml:id="formula_1">L(θ) = N n=1 l(f (x n ; θ), y n ),<label>(2)</label></formula><p>where l is the specified loss function (e.g., exponential loss, cross-entropy, . . . ). For example, with the exponential loss, l(t, y n ) = e −ynt , the empirical risk is given by</p><formula xml:id="formula_2">L exp (θ) = N n=1 e −ynf (xn;θ) ; L exp (W, φ) = N n=1 e −ynWδn(xn;φ) ,<label>(3)</label></formula><p>where the second expression emphasizes the last weight layer.</p><p>For multi-class classification, the commonly used loss function is cross-entropy loss:</p><formula xml:id="formula_3">L cross−entropy (W, φ) = − N n=1 log exp(W yn δ n (x n ; φ)) K l=1 exp(W l δ n (x n ; φ)) ,<label>(4)</label></formula><p>where W l is the l-th component of W, which is the weight for a certain class l; W yn is the component of W for the class represented by y n .</p><p>The goal of performing optimization is to find: arg min θ L(θ).</p><p>In the following, we focus on minimizing Equation (3) using GD algorithm with a constant learning rate η for the binary case and Equation ( <ref type="formula" target="#formula_3">4</ref>) for the multi-class case. At iteration t, the update rule has the following form:</p><formula xml:id="formula_4">θ t = θ t−1 − η∇L(θ t−1 ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Result</head><p>In this section, we start with the result in <ref type="bibr" target="#b26">[27]</ref> for linearly separable data in logistic regression and then obtain the result for the neural network in Fig. <ref type="figure" target="#fig_0">1</ref>. Finally, we extend the result from the binary case to the multi-class case.</p><p>In <ref type="bibr" target="#b26">[27]</ref>, the authors investigate the following problem. Definition 1. For a logistic regression problem, whose weight vector is w ∈ R d , the loss has the following form:</p><formula xml:id="formula_5">L logistic (w) = N n=1 l(y n w x n ).</formula><p>For this binary case, assuming all the labels are positive ∀n : y n = 1 (we can re-define y n x n as x n ), we have the GD update for that loss function at iteration t having the following form:</p><formula xml:id="formula_6">w t = w t−1 − η∇L logistic (w t−1 ) = w t−1 − η N n=1 l (w t−1 x n )x n .</formula><p>The authors show that w t finally diverges <ref type="bibr" target="#b26">[27]</ref>: Lemma 1. Let w t be the iterates of gradient descent in Definition 1 with η &lt; 2β −1 σ −2 max (X), where β is the smoothness of l and σ max (X) is the maximal singular value of the data matrix X ∈ R d * N and any starting point w 0 . For linearly separable data and β-smooth decreasing loss function, we have: (1) lim t→∞ L logistic (w t ) = 0, (2) lim t→∞ w t = ∞ and (3) ∀n : lim t→∞ w t x n = ∞.</p><p>But the direction of the above solution converges to that of the hard margin SVM solution <ref type="bibr" target="#b26">[27]</ref>. Lemma 2. For any dataset which is linearly separable, any β-smooth decreasing loss function with an exponential tail (the loss function tail is bounded by two exponential functions), any step size η &lt; 2β −1 σ −2 max (X) and any starting point w 0 , the gradient descent iterations will behave as:</p><formula xml:id="formula_7">w t = ŵ log t + ρ t , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where ŵ is the L 2 max margin vector:</p><formula xml:id="formula_9">ŵ = arg min w∈R d w 2 subject to w x n ≥ 1,<label>(7)</label></formula><p>and the residual grows at most as ρ t = O(log log(t)), and so</p><formula xml:id="formula_10">lim t→∞ w t w t = ŵ ŵ .</formula><p>Furthermore, except for measuring zero, the residual ρ t is bounded.</p><p>As for our problem, we have the following assumption: Assumption 1. The loss in Equation ( <ref type="formula" target="#formula_1">2</ref>) converges to zero:</p><formula xml:id="formula_11">lim t→∞ L(θ t ) = 0.</formula><p>This assumption is a reasonable assumption. It could be satisfied as long as the data is linearly or non-linearly separable, with no wrongly labeled data points and the model has enough capacity, which is usually the case for deep learning models. Based on Assumption 1, we have the following lemma: Lemma 3. Under Assumption 1, for the neural network with architecture as in Fig. <ref type="figure" target="#fig_0">1</ref>, even if the dataset {x n , y n } N n=1 is not linearly separable, the transformed dataset {δ n , y n } N n=1 is linearly separable: ∃W * such that ∀n : y n W * δ n &gt; 0.</p><p>In fact, since the last weight layer is a linear transformation, if {δ n , y n } N n=1 is not linearly separable, the classification error can never reach zero, let alone the loss. Following Definition 1, let us re-define y n δ n as δ n , Based on Lemma 2 and Lemma 3, we obtain the first main result: Theorem 1. For any neural network for binary classification, any β-smooth decreasing loss function with an exponential tail, small enough step size η &lt; 2β −1 σ −2 max (X) and any start point W 0 , as long as lim t→∞ L(θ t ) = 0, the direction of the neural network's last weight layer converges:</p><formula xml:id="formula_12">lim t→∞ W t W t = Ŵ Ŵ , (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>where Ŵ is the L 2 max margin vector:</p><formula xml:id="formula_14">Ŵ = arg min W∈R t * 1 W 2 subject to Wδ n ≥ 1,</formula><p>in which δ n is the re-defined input of the last weight layer.</p><p>It is true that the convergence of the transformation function can also affect the last layer decision boundary. However, since the loss converges to zero, the variance of the transformation function is bounded after long enough training time, which makes the theorem hold.</p><p>As for the multi-class classification problem, we have the following lemma from <ref type="bibr" target="#b26">[27]</ref>:</p><p>Lemma 4. For a logistic regression problem in which we learn a predictor w k for each class k ∈ [K] in a linearly separable multi-class dataset, any starting point w k,0 and any small enough step size, under most circumstances (i.e., except for a measure zero), the iterates of gradient descent on the cross-entropy loss will behave as:</p><formula xml:id="formula_15">w k,t = ŵk log(t) + ρ k,t ,<label>(9)</label></formula><p>where the residual ρ k,t is bounded and ŵk is the solution of the K-class SVM:  Similar to Theorem 1, we can derive the following result for the multi-class case with cross-entropy loss. Theorem 2. For any neural network, small enough step size η and any starting point W 0 , as long as the dataset makes lim t→∞ L(θ t ) = 0, the iterates of gradient descent on the cross-entropy loss of the last weight layer W will behave as:</p><formula xml:id="formula_16">W k,t = Ŵk log(t) + ρ k,t ,<label>(11)</label></formula><p>where the residual ρ k,t is bounded, W k,t is the weight for class k at iteration t and Ŵk is the solution of the K-class SVM:</p><formula xml:id="formula_17">arg min W1,...,W k K k=1 W k 2 subject to ∀n, ∀k = y n : W yn δ n ≥ W k δ n + 1,</formula><p>and so:</p><formula xml:id="formula_18">lim t→∞ W t,k W t,k = Ŵk Ŵk .<label>(12)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting</head><p>There are seven datasets in our experiments, including five simulated 2D datasets and two real datasets. The five simulated datasets can be referred to Fig. <ref type="figure" target="#fig_6">2 (A1-A5</ref>). The first three (Plate, Blob, and Sector) are linearly separable. The last two (Sector not separable and Moon) are non-linearly separable. There are 5000 points within each simulated dataset. The two real datasets are MNIST <ref type="bibr" target="#b14">[15]</ref> and CIFAR-10 <ref type="bibr" target="#b17">[18]</ref>. Since MNIST and CIFAR-10 are multi-class datasets, we randomly chose two classes out of the 10 classes for each one for the binary classification case. We used the network architecture in Fig. <ref type="figure" target="#fig_0">1</ref> for all the experiments. The only difference is the transformation function. We used a fully connected layer with 2000 nodes as the transformation function for the five simulated datasets; ResNet <ref type="bibr" target="#b9">[10]</ref> for MNIST; and DenseNet <ref type="bibr" target="#b10">[11]</ref> for CIFAR-10. For visualization purpose, we set t as 2. We used cross-entropy loss as the loss function and ReLU as the activation function. For multi-class classification problem, we set the number of nodes in the output layer the same as the number of classes. We used GD for the simulated datasets and SGD for MNIST and CIFAR-10. We turned off all the commonly used explicit regularizers, such as weight decay and dropout, for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simulated datasets</head><p>The results are summarized in Fig. <ref type="figure" target="#fig_2">2</ref> (additional results can be found in the Appendices). The decision boundary of neural networks in the original input space can be referred to Fig. <ref type="figure" target="#fig_2">2</ref> (B1-B5). The green and black dots are the training data points. We sampled test data points uniformly across the whole space so that we can visualize the decision boundary of the trained neural networks. The blue points are the ones predicted by the model with the same label as the black training data while the red points are the ones predicted with the same label as the green training data. The curve that separates the blue points and red points can be considered as the decision boundary of the network. Although it is difficult to gain insight from the original space, as suggested by the analysis in the Main Result section, the transformed space is more interesting. Fig. <ref type="figure" target="#fig_2">2</ref> (D1-D5) shows the training data and testing data in the transformed space. As a comparison, we trained a linear SVM with the transformed training data and labeled the same testing data points with the SVM classifier, whose results are shown in Fig. <ref type="figure" target="#fig_2">2</ref> (C1-C5). As shown in the figure, the direction of the neural network's last layer decision boundary trained with GD converges to that of the linear SVM solution, which verifies Theorem 1. Furthermore, the two kinds of decision boundaries are very close to each other, not only in the direction but also in the constant bias term. We further discuss this phenomenon in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MNIST binary</head><p>After training a residual network with the MNIST data, we mapped the data into the transformed space. Within that space, we sampled test data uniformly and labeled those test data points using the last layer of the network in Fig. <ref type="figure" target="#fig_0">1</ref>, which results in the decision boundary in Fig. <ref type="figure" target="#fig_3">3 (A</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CIFAR-10 binary</head><p>We trained a model with DenseNet transformation function on the CIFAR-10 dataset. The decision boundary results of this dataset could be referred to Fig. <ref type="figure" target="#fig_5">4</ref>. As shown in the figure, similar to the result on MNIST, the directions of those two boundaries are very close to each other, which further supports Theorem 1. Furthermore, in addition to being close in terms of direction, the neural network boundary is very close to the midpoint of the two clusters, if it does not cross the midpoint, where the SVM boundary should pass theoretically. This phenomenon is consistent with the result of the simulated datasets and the MNIST dataset, suggesting that training the whole neural network using GD or SGD may result in a decision boundary with good bias constant. In terms of the trained model's generalization property, although we turned off explicit regularizers, the model can still have 92.6% testing accuracy for this CIFAR-10 dataset, which is within the performance range of a productive deep learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Multi-class classification</head><p>In practice, deep learning is usually used for multi-class classification with cross-entropy loss. We investigated the multi-class classification case in this section. We performed experiments on a simulated three class Blob dataset. The neural network decision boundary in the original space and the transformed space can be referred to Fig. <ref type="figure" target="#fig_6">5 (A,B</ref>), respectively. As a comparison, the SVM decision boundary on the transformed data in the transformed space is shown in Fig. <ref type="figure" target="#fig_6">5</ref> (C). Those results, which show the decision boundary direction of the neural network last weight layer converges to that of SVM, verify Theorem 2. We also performed such experiment on the MNIST data with DenseNet transformation function. During the training, we also tried other optimizers other than just SGD, such as Momentum. The results are shown in Fig. <ref type="figure" target="#fig_6">5 (D,E</ref>). From the two figures, we can find that the corresponding decision boundary directions of the neural network last layer and SVM are very close to each other. Besides, similar to the previous result, the decision boundary of neural network is very close to the midpoint between different clusters. Those experiments further support Theorem 2, which also shows that our hypothesis may be generalized to other optimizers, such as Momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Real task: Fashion MNIST</head><p>We also investigated the decision boundary of the DenseNet's last layer, which is used to perform 10-class classification on Fashion MNIST. We used the same architecture from <ref type="bibr" target="#b10">[11]</ref>, except for that we added an additional layer to make the last hidden layer in 2D space for visualization purpose. We turned off the commonly used techniques for improving performance, such as data augmentation and dropout. We deployed Momentum as the optimizer. After the model being trained for 1,000 epoches, the loss oscillated around 6 * 10 −4 . The testing accuracy is around 91.8%, which is within the known performance range of the deep learning model on this dataset. We show the decision boundary comparison of the network's last layer and the multi-class linear SVM solution in Fig. <ref type="figure" target="#fig_7">6</ref>.</p><p>As shown in the figure, although the experiment setting is not exactly the same as the assumptions in our main result, the decision boundary of the trained neural network still worths investigating. In fact, the decision boundary shown on the up-left of Fig. <ref type="figure" target="#fig_7">6</ref> (A) is very similar to that of Fig. <ref type="figure" target="#fig_7">6</ref> (B). On the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The result of this paper can be useful for solving several practical problems related to deep learning, such as catastrophic forgetting <ref type="bibr" target="#b13">[14]</ref> and the data-hungry challenge <ref type="bibr" target="#b5">[6]</ref>. We take these two as examples.</p><p>On the other hand, we believe that investigating the transformation function would be helpful for solving adversarial attacking <ref type="bibr" target="#b22">[23]</ref> and studying the last layer can push out new ways of introducing uncertainty into supervised deep learning <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Catastrophic forgetting</head><p>Catastrophic forgetting <ref type="bibr" target="#b13">[14]</ref>, which means the neural network does not have the ability of learning new knowledge without forgetting the learned knowledge, is one of the bottlenecks of deep learning. Recently, a rehearsal framework, called SupportNet <ref type="bibr" target="#b18">[19]</ref>, was proposed to deal with catastrophic forgetting when performing class incremental learning. In short, it maintains a subset of the old data, which is chosen based on the support vector information obtained by using SVM to approximate the last layer, and feeds the subset together with the new data to the model when incorporating the new classes into the model. Despite the lack of theoretical analysis in the paper, the framework works quite well in practice, even achieving nearly optimal performance on some datasets. In fact, according to Lemma 1 and Theorem 2, we can write W k,t = c(t) Ŵk + ρ k,t such that c(t) → ∞ and ρ k,t is bounded. The gradient of the exponential loss for W k,t can then be formulated as:</p><formula xml:id="formula_19">−∇L exp (W k,t ) = N n=1 exp(−W k,t δ n )δ n = N n=1 exp(−c(t) Ŵk δ n ) exp(−ρ k,t δ n )δ n ,<label>(13)</label></formula><p>when the model converges and c(t) → ∞ , only those data with the largest exponents, that is, Ŵk δ n should be the smallest, will contribute to the gradients. Those samples are exactly the support vectors of the SVM trained on the transformed data, which are selected by SupportNet. Using those data for future tuning, the model is very likely to learn the same boundary for the old classes. Our results partially explains why that rehearsal method works very well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reducing the training data size and transfer learning</head><p>It is always desirable to reduce the training data size for the data-hungry deep learning method, without too much performance compromise. In practice, especially in the computer vision field, when the data size is not large enough, people usually take advantage of transfer learning <ref type="bibr" target="#b27">[28]</ref>, fine-tuning the last one or two layers of a pre-trained model with the training data. In fact, based on our result in the Main Result section and the analysis in the previous subsection, it is not data-hungry from the transformed space to the label space since only the support vector samples matter, which means the drawback property of deep learning comes from the transformation function component. The transfer learning technique, taking advantage of an existing transformation function and avoiding the data size requirement of that component, can thus learn a useful model with limited data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Bridging the gap between the theoretical research and the practical power of deep learning is a fascinating research direction. In this paper, we investigate the decision boundary of a productive deep learning architecture with weak assumption on both the training data and the model. Through comprehensive theoretical analysis and experiments, we show that the direction of the neural network's last weight layer converges to that of a linear SVM trained on the transformed data if the loss converges to zero, for both the binary case and the multi-class case with the commonly used cross-entropy loss. In addition, we show it empirically that training a neural network as a whole may result in better bias constant for the last weight layer, which is important for the generalization property of deep learning models. In addition to facilitating the understanding of deep learning and thus further improving its performance, our result can be useful for solving a broad range of practical problems in the deep learning field, such as catastrophic forgetting, reducing the data size requirement of deep learning, adversarial attacking, and introducing uncertainty into deep learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Additional results of binary classfication</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Network architecture. We do not oversimplify the network, with the only assumption being that the last hidden layer and the output layer are fully connected, which is practical and the common case. The transformation function can be any kind of deep learning architecture, including the legend fully connected layers or the commonly used ResNet or DenseNet et al. Here we show the fully connected layer for simplicity.</figDesc><graphic url="image-1.png" coords="2,178.44,72.00,255.12,217.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>∀n, ∀k = y n : w yn x n ≥ w k x n + 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results on simulated datasets. The five columns are five datasets. The first row is the training datasets in the original input space. In the last three rows, red points are random testing points classified with the same label as the green training data points and blue points are random testing points classified with the same label as the black training data points. The interface between red dots and blue dots is the decision boundary. The second row figures show the decision boundary of a trained neural network in the original space. The third and the fourth rows are in the transformed space. The third row shows the decision boundary of linear SVM trained with the transformed data in the transformed space. The last row shows the decision boundary of the neural network's last weight layer.</figDesc><graphic url="image-2.png" coords="5,50.88,257.03,510.23,322.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MNIST binary classification decision boundary. We randomly chose two classes from the MNIST dataset ("0" and "1" for the above figures) and trained a neural network with ResNet as the transformation function in Fig. 1. We set t as 2 for visualization purpose. The above figures show the decision boundary of SVM trained with transformed data (B) and the last weight layer of the neural network (A) in the transformed space. After model converged, we reinitialized the last weight layer and retrained the model with the weights in the transformation function being fixed, resulting in (C).</figDesc><graphic url="image-3.png" coords="6,107.58,72.00,396.83,92.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>). Utilizing the training data in the transformed space, we trained a linear SVM classifier and plotted out the decision boundary of that classifier in Fig.3 (B). As shown in the figures, after mapping the data into the transformed space, the direction of the first decision boundary is very close to that of the second decision boundary, which further supports Theorem 1. Furthermore, with the transformation function fixed, we reinitialized the last layer and retrained the last layer, whose result is shown in Fig.3 (C). It suggests that our result still holds. On the other hand, the original boundary obtained by training the network as a whole is closer to the SVM boundary in terms of the bias constant, which suggests the whole network training may have better initialization for the last layer and thus make the model generalize better. Notice that although we turned off dropout and the model had been trained for a very long time to make it completely fit to the training data, the trained model still has very impressive generalization property with the testing accuracy being as high as 99.7%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CIFAR-10 binary decision boundary result. We randomly chose two classes from the CIFAR-10 dataset and trained a neural network with DenseNet as the transformation function in Fig. 1, setting t as 2. (A) shows the decision boundary of the last weight layer in the transformed space and (B) shows the decision boundary of the linear SVM trained with the transformed data in the transformed space.</figDesc><graphic url="image-4.png" coords="7,178.44,466.16,255.12,88.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The multi-class experiment result. (A,B,C) show the decision boundary results on a simulated 3-class Blob dataset. (D,E) show the 3-class MNIST ("0", "1" and "2" for the above figures) result trained with DenseNet and Momentum.</figDesc><graphic url="image-5.png" coords="8,107.58,72.00,396.84,196.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The real task result. We trained a DenseNet for the 10-class Fashion MNIST classification using Momentum. After 1,000 epoches, the loss is around 6 * 10 −4 . (A) shows the decision boundary of the neural network's last layer. (B) shows the decision boundary of SVM trained with the transformed dataset.</figDesc><graphic url="image-6.png" coords="9,178.44,72.00,255.11,91.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Additional results of the simulated data, whose data range is much small than that in the main text. The four rows have the same meanings as the figure in the main text. The first three columns are the results of the neural networks with ReLU activation function on three linearly separable datasets. The last column is the result of the neural network with square activation function trained on Blob dataset.</figDesc><graphic url="image-7.png" coords="12,93.40,147.45,425.19,342.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The decision boundary of the neural network with square activation function trained on the Blob dataset in the original space in different scales.</figDesc><graphic url="image-8.png" coords="12,93.40,578.79,425.20,112.41" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We first want to clarify why in the main text, we chose the data range of the simulated linearly separable datasets to be relatively large, from 0 to around 100 or 400. Here we provide the results of the small-range linearly separable datasets (from 0 to around 1), which can be referred to Fig. <ref type="figure">7</ref>. Intuitively, the decision boundary in the original space (Fig. <ref type="figure">7</ref> (B1-B3)) is very surprising because the highly over-parameterized multi-layer neural network seems to learn a linear decision boundary. We argue that it is because of the small range of the datasets and also the shape of the activation function. As we know, a very large part of the ReLU activation function is linear. If the data range is very small, it is very likely that during training, the nonlinear part of the activation function is not used. As a result, the whole network becomes a linear classifier, which makes the decision boundary linear. We demonstrate that by performing an additional experiment on the small-range Blob dataset with the neural network having the following square activation function:</p><p>Within this function, there is no linear part. So even the data range is small, the decision boundary of the neural network should still be nonlinear. The experimental results of this setting are shown in the last column of Fig. <ref type="figure">7</ref>. From Fig. <ref type="figure">7</ref> (B4), we can see that the decision boundary in the original space is a nonlinear one, which is as expected. On the other hand, we also show the decision boundary in the original space in different scales in Fig. <ref type="figure">8</ref>. As shown in Fig. <ref type="figure">8 (A, B</ref>), although the boundary is nonlinear globally (Fig. <ref type="figure">8 (C</ref>)), it is very similar to a linear boundary if we only consider its local shape (i.e. from −1 to 2), which supports our assumption, that is, if the data range is small, the nonlinear power of the activation function is used limitedly. This experiment demonstrates that the data range combining with the activation function can have a significant impact on the decision boundary in the original space. To eliminate the potential misunderstanding and misleading results caused by the datasets and emphasize the main results, we chose the large-range datasets in the main text.</p><p>On the other hand, if we investigate the results of the neural network (Fig. <ref type="figure">7</ref> (D1-D4)) and the linear SVM (Fig. <ref type="figure">7</ref> (C1-C4)) in the tranformed space on those small-range datasets, we can find that the results are similar to those on the large-range datasets in the main text, which further supports our main results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05296</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Globally optimal gradient descent for a convnet with gaussian inputs</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07966</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10174</idno>
		<title level="m">Sgd learns over-parameterized networks that provably generalize on linearly separable data</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can we open the black box of ai?</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Castelvecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7623</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Big data deep learning: challenges and perspectives</title>
		<author>
			<persName><forename type="first">Xue-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="514" to="525" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="192" to="204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Identity mappings in deep residual networks. European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Supportnet: solving catastrophic forgetting in class incremental learning with support data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02942</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convergence analysis of two-layer neural networks with relu activation</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="597" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Theory of deep learning ii: Landscape of the empirical risk in deep learning</title>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09833</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Quynh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08045</idno>
		<title level="m">The loss surface of deep and wide neural networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Symposium on</title>
				<imprint>
			<publisher>Security and Privacy</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brando</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hidary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrushikesh</forename><surname>Mhaskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00173</idno>
		<title level="m">Theory of deep learning iii: explaining the non-overfitting puzzle</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exponentially vanishing sub-optimal local minima in multilayer neural networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05777</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10345</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recovery guarantees for one-hidden-layer neural networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03175</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
