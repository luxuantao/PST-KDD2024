<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Fusion with Co-Attention Networks for Fake News Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengwei</forename><surname>Zhan</surname></persName>
							<email>zhanpengwei@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunjian</forename><surname>Zhang</surname></persName>
							<email>zhangyunjian@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liming</forename><surname>Wang</surname></persName>
							<email>wangliming@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Xu</surname></persName>
							<email>xuzhen@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Fusion with Co-Attention Networks for Fake News Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fake news with textual and visual contents has a better story-telling ability than text-only contents, and can be spread quickly with social media. People can be easily deceived by such fake news, and traditional expert identification is labor-intensive. Therefore, automatic detection of multimodal fake news has become a new hot-spot issue. A shortcoming of existing approaches is their inability to fuse multimodality features effectively. They simply concatenate unimodal features without considering inter-modality relations. Inspired by the way people read news with image and text, we propose a novel Multimodal Co-Attention Networks (MCAN) to better fuse textual and visual features for fake news detection. Extensive experiments conducted on two realworld datasets demonstrate that MCAN can learn inter-dependencies among multimodal features and outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rapid growth of social media has created fertile soil for the emergence and fast spread of fake news <ref type="bibr" target="#b32">(Zhao et al., 2015)</ref>, resulting in serious consequences. For example, during the U.S. 2016 presidential election, the most popular fake news was more widely spread than the most popular authentic news on Facebook, which confused people and broke the authenticity balance of the news ecosystem <ref type="bibr" target="#b22">(Shu et al., 2017)</ref>. To mitigate the negative effects caused by fake news, it is crucial to detect fake news on social media automatically.</p><p>Tweets with images are getting popular on social media recently, which have richer information and attract more viewers than tweets with only texts <ref type="bibr" target="#b8">(Jin et al., 2017)</ref>. Fake news also makes full use of this advantage to draw and mislead readers. Figure <ref type="figure" target="#fig_0">1</ref> shows three examples of fake news from Twitter. In the left example, both text and image indicate it is likely to be fake. The text of the middle one provides little evidence that it is fake news, but the image is obviously forged. In the right example, the image seems normal, while the textual contents indicate that it is probably fake. A hypothesis drawn from these examples is that combining text and the attached image is more conducive to detecting fake news.</p><p>Recent works have a growing interest in using multimodal (text + image) information to detect fake news. <ref type="bibr" target="#b8">Jin et al.(2017)</ref> utilize local attention mechanisms to fuse features of image, text, and social context. Some studies explore to learn the joint representations of text and image, based on auxiliary adversarial networks <ref type="bibr" target="#b28">(Wang et al., 2018)</ref> and variational autoencoders <ref type="bibr" target="#b10">(Khattar et al., 2019)</ref>. Nevertheless, they are not fine-grained enough in feature extraction and feature fusion. First, some studies require labor-intensive extra information, such as social context <ref type="bibr" target="#b8">(Jin et al., 2017)</ref> and event category <ref type="bibr" target="#b28">(Wang et al., 2018)</ref>, to help detect fake news, which increases the cost of the detection. Second, except for texts in tweets, the methods mentioned above all focus on characteristics of images at the semantic level (e.g., emotional provocations), which can be reflected in the spatial domain. However, these methods ignore the individual information of fake images at the physical level, e.g., re-compression artifacts, which is reflected in the frequency domain <ref type="bibr" target="#b20">(Qi et al., 2019)</ref>. Third, some models <ref type="bibr" target="#b28">(Wang et al., 2018;</ref><ref type="bibr" target="#b10">Khattar et al., 2019)</ref> obtain fused representations by simply concatenating multi-modality features. Although leverages local attention mechanism, the attention values of att-RNN <ref type="bibr" target="#b8">(Jin et al., 2017)</ref> are only obtained from joint textual-social representations, which cannot reflect the similarity between textual-social representations and visual representations. Intuitively, when people judge news credibility with text and image, they often observe image first and then read text <ref type="bibr" target="#b29">(Wang et al., 2020)</ref>. This process may be repeated several times. In this process, people understand image according to the textual information, and understand text according to the associated image information. So the information of one modality is conditionally fused with that of another modality for once or multiple times. Intuitively, there are inter-modality attention relations between image and text. However, existing state-of-the-art methods are weak to fuse multimodal features due to their neglect of inter-modality interactions.</p><p>To address the aforementioned challenges, we propose the Multimodal Co-Attention Networks (MCAN) for fake news detection by considering multimodal features. In our proposed model, we first extract spatial-domain features and frequencydomain features from image, as well as textual features from text. Then we develop a novel fusion approach with multiple co-attention layers to learn inter-modality relations, which fuses visual features first, and then the textual features. The fused representation obtained from the last co-attention layer is used for fake news detection.</p><p>The contributions of this paper can be summarized as follows: (1) We propose a novel end-to-end approach to detect fake news on social media only using the text and the attached image, without any extra information and auxiliary tasks. (2) The proposed MCAN model stacks multiple co-attention layers to fuse the multimodal features, which can learn inter-dependencies among them. (3) Our MCAN model is a general framework for fake news detection, and the components of MCAN are flexible. The sub-networks used to extract multimodal features can be replaced by different models. Moreover, the modular fusion process of MCAN allows our model to handle more modalities conveniently. (4) We evaluate MCAN on two large scale realworld datasets. The results demonstrate that our model outperforms the state-of-the-art models.</p><p>The rest of the paper is organized as follows:</p><p>In Section 2, we summary previous related work on fake news detection. In Section 3, we detail our proposed model. The datasets, baselines, and experiment results are presented in Section 4. We conclude the study in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Following the previous work <ref type="bibr" target="#b21">(Ruchansky et al., 2017;</ref><ref type="bibr" target="#b22">Shu et al., 2017)</ref>, we specify that fake news is the news that is intentionally fabricated and can be verified as false. Existing methods for fake news detection can be divided into unimodal approaches and multimodal approaches.</p><p>2.1 Unimodal Fake News Detection.</p><p>Textual features are extracted from text content, including statistical features, such as the number of paragraphs in the text <ref type="bibr" target="#b27">(Volkova et al., 2017)</ref>, the percentage of negative words <ref type="bibr" target="#b19">(Potthast et al., 2017;</ref><ref type="bibr" target="#b2">Bond et al., 2017)</ref>, the number of punctuation and emojis <ref type="bibr" target="#b3">(Castillo et al., 2011)</ref>, and semantic features, such as writing styles <ref type="bibr" target="#b4">(Chen et al., 2015)</ref> and language styles <ref type="bibr" target="#b6">(Feng et al., 2012)</ref>. However, these features are hand-designed, bringing bias and design difficulty. To address this problem, many studies use deep learning technologies, such as RNN <ref type="bibr" target="#b14">(Ma et al., 2016)</ref>, CNN <ref type="bibr" target="#b31">(Yu et al., 2017)</ref>, and GAN <ref type="bibr" target="#b16">(Ma et al., 2019)</ref>, to identify fake news. Their results show that deep learning methods perform better.</p><p>Visual features are important for news verification <ref type="bibr" target="#b9">(Jin et al., 2016;</ref><ref type="bibr" target="#b22">Shu et al., 2017)</ref>, such as clarity score <ref type="bibr" target="#b9">(Jin et al., 2016)</ref>, the number of images <ref type="bibr" target="#b30">(Wu et al., 2015;</ref><ref type="bibr" target="#b9">Jin et al., 2016)</ref>. However, these features are manually crafted and just learn simple patterns, hardly applying to real images. <ref type="bibr" target="#b20">Qi et al. (2019)</ref> design a CNN-based model to capture image patterns, but their model only works in the case of large samples. So the applicable scope is very limited.</p><p>Social context features are born in the social connection between users and tweets, such as user profile and the number of posts. <ref type="bibr" target="#b12">Liu et al. (2018)</ref> use user profiles on the news propagation path to determine the truth of the news. Some other works model propagation patterns as tree structures based on kernel methods <ref type="bibr" target="#b30">(Wu et al., 2015;</ref><ref type="bibr" target="#b15">Ma et al., 2017)</ref>. However, social context features are handcrafted, incomplete, and unstructured.</p><p>The above work embodies the limitations of unimodal features in detecting fake news. In this paper, we consider multiple modalities simultaneously when detecting fake news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Fake News Detection.</head><p>Recent works explore to fuse multimodal features. <ref type="bibr" target="#b8">Jin et al. (2017)</ref> use local attention mechanism to fuse textual, visual, and social context features. <ref type="bibr" target="#b28">Wang et al. (2018)</ref> learn event-invariant features by an aided adversarial network. <ref type="bibr" target="#b10">Khattar et al. (2019)</ref> utilize autoencoders coupled with a detector to learn the shared representation of the text and the attached image. However, they ignore the characteristics of fake images at physical level (e.g., recompression artifacts), and the fused features they learned lack correlations across multiple modalities.</p><p>To overcome the limitations of existing works, we propose MCAN to learn inter-dependencies among modalities. We extract spatial-domain and frequency-domain features of image, and textual features. Then we fuse them through a deep coattention model inspired by a realistic scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>Our model aims to learn multimodal fusion representations by considering dependencies across the modalities. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the proposed model has three major procedures: feature extraction, feature fusion, and fake news detection.</p><p>Given news with text and image, we first utilize three different sub-models to extract features from spatial domain, frequency domain, and text. Then the multi-modality features are fused through a deep co-attention model, which consists of multiple co-attention layers. At last, the output of the coattention model is used for judging the truth of the input news. Spatial-Domain Feature. To learn the semanticlevel characteristics of the given image, we employ the VGG-19 network <ref type="bibr" target="#b23">(Simonyan and Zisserman, 2014)</ref> to extract visual features from spatial domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction</head><p>After the second of the last layer of VGG-19, we add a fully connected layer (denoted as "s-fc" in Figure <ref type="figure" target="#fig_1">2</ref>) with ReLU activation function to generate a d × 1 dimensional feature representation of the input image in spatial domain, which is denoted as</p><formula xml:id="formula_0">R S ∈ R d×1 .</formula><p>Frequency-Domain Feature. Fake-news images are often re-compressed images or tampered images that show periodicity in frequency domain <ref type="bibr" target="#b20">(Qi et al., 2019)</ref>, which can be easily captured by CNNs. Thus we design a CNN-based sub-network to extract features from frequency domain, as in Figure <ref type="figure" target="#fig_2">3</ref>. The image is transformed from spatial domain to frequency domain through discrete cosine transform (DCT) as in <ref type="bibr" target="#b20">Qi et al. (2019)</ref>. After that, we obtain 64 vectors H 0 , H 1 , . . . , H 63 , which are then sampled to the fixed size 250. For parallel computation, we pick 64 250-dimensional vectors into a matrix H F ∈ R (64×250) , which is fed to the CNN-based network later. The CNN-based subnetwork consists of a major network along with multi-branch networks. The earlier parts of the major network have three convolutional blocks and a max-pooling layer. The multi-branch networks are the same as architectures in Inception V3 <ref type="bibr" target="#b24">(Szegedy et al., 2016)</ref>. The last parts of the major network are a max-pooling layer followed by a convolutional block. Each convolutional block is comprised of a two-dimensional convolutional layer with batch normalization and ReLU activation function. After adding a fully connected layer with ReLU activation function (denoted as "f-fc" in Figure <ref type="figure" target="#fig_1">2</ref>), we obtain the feature representation of the image in frequency domain R F ∈ R d×1 .</p><p>Textual Feature. The text content of the tweet is a sequential list of words denoted as</p><formula xml:id="formula_1">T = [T 1 , T 2 , . . . , T n ],</formula><p>where n is the number of words in a tweet, and each word T i ∈ T is tokenized by a pre-prepared vocabulary <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>. Recently, the BERT model <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> which is pre-trained on a large language corpus, has been proven to perform very well in multiple natural language processing tasks. Thus we utilize BERT to obtain the aggregate sequence representation as textual features we desired. The textual feature is then resized to be a d × 1 dimensional representation (denoted as R T ) by a fully connected layer with ReLU activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Fusion</head><p>Intuitively, people often look at the image first and then read the text when reading the news with image and text. This process may be repeated several times, continuously fusing image and text information. Therefore, we develop a novel fusion approach to simulate this process. Before presenting the fusing process, we first introduce its basic unit, the co-attention (CA) block. We achieve feature fusion by cascaded stacking multiple CA layers, which consists of two parallelly connected CA blocks.</p><p>Co-attention block. Co-attention block <ref type="bibr" target="#b13">(Lu et al., 2019)</ref> is a variant of the standard multi-head self-attention (MSA) block <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>, which can capture global dependencies of all positions in a sequence and is widely used in NLP and VQA tasks <ref type="bibr" target="#b18">(Nguyen and Okatani, 2018;</ref><ref type="bibr" target="#b7">Gao et al., 2019)</ref>. The MSA block showed in Figure <ref type="figure" target="#fig_3">4</ref>(a) consists of a multi-headed self-attention function and a fully connected feed-forward network, both wrapped a residual connection followed by layer normalization. The input of MSA is first used to compute (d × 1)-dimensional queries, keys, and values packed into matrixes Q, K, V , respectively. The similarity of the dot product between Q and K determines the attention distribution on the V . Multi-head attention function with m heads has m self-attention functions in parallel. For the i-th head, the inputs are transformed from Q, K, and V as follow:</p><formula xml:id="formula_2">Q i = QW Q i , K i = KW K i , V i = V W V i (1)</formula><p>where</p><formula xml:id="formula_3">W Q i , W K i , W V i ∈ R 1×d h are</formula><p>the projection matrices for the i-th head, d h = d/m is the dimensionality of the output feature of each head.</p><p>The calculation process of multi-head selfattention function can be presented as follows:</p><formula xml:id="formula_4">MA(Q, K, V ) = h W O (2) h = h 1 ⊕ h 2 ⊕... ⊕ h m h i = A(Q i , K i , V i ) = softmax( Q i K T i √ d h )V i where W O ∈ R md h ×1 , ⊕ denotes concatenation of vectors.</formula><p>The fully connected feed-forward network consists of two linear transformations with a ReLU activation function in between,</p><formula xml:id="formula_5">FFN(x) = max(0, xW 1 )W 2 (3)</formula><p>where the dimensionality of input and output is d × 1, and the inner-layer dimensionality is d f f .</p><p>The co-attention block (denoted as "Co-Attn") is extended from the MSA block, as shown in Figure <ref type="figure" target="#fig_3">4</ref>(b). For a Co-Attn block, the queries are from one modality while keys and values are from another modality. Especially, the query matrice is used as a residual item after the multi-head attention sublayer. The rest architectures are the same as MSA. The Co-Attn block produces an attention-pooled feature for one modality conditioned on another modality. If Q comes from text and k and V come from the attached image, the attention value calculated using Q and K can be used as a measure of the similarity between the text and image, and then weights the image. Just like humans, after reading the text, they will pay more attention to the areas in the image that are similar to the text. We believe that co-attention can simulate this process and learn inter-dependencies between different features.</p><p>Co-attention layer. We obtain a CA layer by connecting two Co-Attn blocks in parallel, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Giving two Co-Attn blocks different features, the CA layer computes queries, keys, and values for each Co-Attn block as in a MSA block. Then the keys and values of one Co-Attn block are passed as input to another Co-Attn block. The outputs of two Co-Attn blocks are concatenated together and then fed into a fully connected layer to get the fused representation. The CA layer models dense interactions between input modalities by exchanging their information.</p><p>Multiple co-attention stacking. In order to fuse multimodal features deeply, we stack 4 CA layers in depth. The fusion process is progressive, and the output of each CA layer is one of the inputs of the next layer (see Figure <ref type="figure" target="#fig_1">2</ref>). We first fuse spatialdomain representation R S and frequency-domain representation R F in first CA layer and obtain R C , respectively. The output vector of each CA layer is d-dimensional.The calculation processes are formulated as follows. Due to the page limit, we only show the calculation processes in the first CA layer and skip the repeated calcula-tion details of other layers.</p><formula xml:id="formula_6">R C S←F = R S + MA(R S , R F , R F ) (4) R C S←F = R C S←F + FFN(R C S←F )) (5) R C F ←S = R F + MA(R F , R S , R S ) (6) R C F ←S = R C F ←S + FFN(R C F ←S ) (7) R (1) C = (R C S←F ⊕ R C F ←S )W (1) C (8)</formula><p>where R C S←F ∈ R d is the attention-pooled feature for spatial domain conditioned on frequency domain, R C F ←S ∈ R d is the attention-pooled feature for frequency domain conditioned on spatial domain, and W</p><p>(1)</p><formula xml:id="formula_7">C ∈ R 2d×d is the projection matrice of the first CA layer. R (1)</formula><p>C is transformed to be a (d × 1)-dimensional representation before being input to the next CA layer. Specifically, the first and the third CA layers share parameters, and the second and the fourth CA layers share parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Learning</head><p>We have obtained the multimodal feature representation R C , which is used to predict. The output of the proposed MCAN is the probability of a tweet being fake:</p><formula xml:id="formula_8">ŷ = softmax (max(0, f W f )W s ) (9)</formula><p>where W f is parameters of the fully connected layer, and W s is parameters of the linear layer in the softmax layer. The loss function is devised to minimize the cross-entropy value:</p><formula xml:id="formula_9">L (Θ) = −y log (ŷ) − (1 − y) log (1 − ŷ) (10)</formula><p>where y is the ground truth, with 1 representing fake news and 0 representing real news, and Θ denotes all learnable parameters in the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments 4.1 Datasets</head><p>To evaluate the effectiveness of the proposed MCAN, we conduct experiments on two public real-world datasets, which are collected from Twitter and Weibo, respectively. The Twitter dataset was released for Verifying Multimedia Use task at MediaEval <ref type="bibr" target="#b1">(Boididou et al., 2016)</ref>. The Weibo dataset is collected by <ref type="bibr" target="#b8">Jin et al. (2017)</ref>. In the Weibo dataset, the real news is verified by an authoritative news agency in China, Xinhua News Agency. The fake news is verified by the official rumor debunking system of Weibo. The tweets in each dataset contain texts, attached images/videos, and social context information. In this work, we focus on text and image information. So we remove the tweets with videos and the tweets without texts or images. In Twitter dataset, 512 images are shared by the remaining data. When preprocessing the Weibo dataset, the steps we used are similar to that in the work <ref type="bibr" target="#b8">(Jin et al., 2017)</ref>. We keep the same data split scheme as the benchmark on these two datasets. The detailed statistics of the two datasets are listed in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>The max length of the text is 25 on Twitter and 160 on Weibo. The hidden size of "s-fc", "f-fc" and "t-fc" are 256. We set d=256, m = 4, and d f f = 512. The hidden size of "p-fc" is 35. The parameters of VGG-19 and BERT are frozen when training on Twitter dataset due to overfitting, but not on Weibo dataset. The BERT model used on Twitter dataset is multilingual cased BERT-based model and the one used on Weibo dataset is Chinese BERT-based model. Our proposed model is trained for 100 epochs with early stopping. We use Adam (Kingma and Ba, 2014) and AdaBelief <ref type="bibr" target="#b33">(Zhuang et al., 2020)</ref> as optimizers on Twitter and Weibo datasets, respectively, to seek the optimal parameters of our model. The optimal hyperparameters of our model are determined by grid searching, and the selection criterion is accuracy. The hyperparameters of baselines are the same as those in respective studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>To validate the effectiveness of MCAN, we choose two categories of baseline models: unimodal models and multimodal models, which are listed as follows:</p><p>(1) Text: a BERT model coupled with the decision network in MCAN, using textual information. ( <ref type="formula">2</ref>  <ref type="formula">7</ref>) EANN <ref type="bibr" target="#b28">(Wang et al., 2018)</ref>: A neural network based on the adversarial idea to remove the event-specific features. In EANN, event identification is an auxiliary task, and event labels are not in original datasets. For a fair comparison, we removed the event discriminator. ( <ref type="formula">8</ref>) MVAE <ref type="bibr" target="#b10">(Khattar et al., 2019)</ref>: MVAE learns shared representations of text and image using a variational autoencoder coupled with a binary classifier. We use the same model as in the original work <ref type="bibr" target="#b10">(Khattar et al., 2019)</ref>. ( <ref type="formula">9</ref>) MCAN-A: MCAN without the part of fusing multimodal features. Spatial-domain features, frequency-domain features, and textual features are simply concatenated for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison</head><p>Table <ref type="table" target="#tab_1">2</ref> shows the results of baselines and our proposed model on two datasets. We can observe that the proposed MCAN outperforms all the baselines over all metrics across two datasets.</p><p>There are many similar trends on the two datasets. MCAN-A performs better than unimodal models, which indicates that adding features usually improves model performance, but it is not always positively correlated. For example, Text on Weibo dataset is better than MCAN-A. After adding the process of multimodal fusion, our proposed MCAN beats MCAN-A and other multimodal models, which embodies our proposed feature fusion method is indeed better than the simple concatenation method.</p><p>There are also some differences on the two datasets. The performance of Text (BERT) and Spatial (VGG-19) on Weibo dataset is much better than that on Twitter dataset. The reason is related to the dataset itself. On Weibo dataset, the average length of a tweet is about 10 times of that of a tweet on Twitter dataset, which probably makes BERT perform better on Weibo dataset. Moreover, more than 70% of tweets on Twitter dataset are related to a single event. Thus, the training samples of BERT and VGG-19 are too similar, resulting in poor performance of model generalization. This is the reason why we fine-tuned BERT and VGG-19 on Weibo dataset but not on Twitter dataset. They are easy to overfit on Twitter dataset. But Weibo dataset has no such imbalanced issue.</p><p>On Weibo dataset, the accuracy of fine-tuned BERT and VGG-19 all exceed 85%. In this case, our proposed MCAN further improves the accuracy to close to 90% with the help of cascaded way of stacking CA layers. Comparing with the situation on Twitter dataset, we can find that our model performs better in the face of weak unimodal features. In our MCAN model, the representation ability of features can be greatly improved by effectively fusing other features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Analysis</head><p>Quantitative Analysis. To evaluate the effectiveness of each component of the proposed MCAN, we remove each one from the entire model for comparison. "ALL" denotes the entire model MCAN with all components, including spatialdomain representation (S), textual representation (T), frequency-domain representation (F), and coattention layers (A). After removing each one of them, we obtain the sub-models "-S", "-T", "-F" and "-A", respectively. "-F-A" denotes the reduced MCAN without both frequency-domain representation and co-attention layers. The results are exhibited in Figure <ref type="figure" target="#fig_7">5</ref>. We can see that every component plays a significant role in improving the performance of MCAN. MCAN beats MCAN-F, which reveals that the frequency domain information is indeed helpful to detect fake news. On Twitter dataset, the contribution of textual representations to the entire model is less than that of visual representations, while the situation on Weibo dataset is opposite. This is still due to the imbalanced issue and the less average length of a tweet on Twitter dataset, which decrease the performance of the textual represen-tation. Besides, on Weibo dataset, removing one or two components, the performance of MCAN does not drop significantly as on Twitter dataset. This benefits from balanced data distribution and the stability of fine-tuned BERT and VGG-19, as mentioned in Section 4.4. From Figure <ref type="figure" target="#fig_8">6</ref>, we can observe that the separability of the feature representation learned by MCAN is much better than its reduced model MCAN-A. MCAN-A can learn discriminable features, but many features are still easily misclassified, showing in Figure <ref type="figure" target="#fig_8">6</ref>(a). On the contrary, the features learned by MCAN are more discriminable with a more significant segregated area between two types of samples, as exhibited in Figure <ref type="figure" target="#fig_8">6(b)</ref>. This is attributed to the cascaded way of stacking co-attention layers in MCAN, which fuses the characteristics of multiple modalities deeply and boosts to distinguish fake news and real news.</p><p>From the above phenomena, we can conclude that the proposed method MCAN learns better and more distinctive feature representations with the coattention layers, thus achieving better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Studies</head><p>To further illustrate the importance of multimodal features for fake news detection, we compare the results reported by MCAN and unimodal models (Text and Spatial) and exhibit some fake news correctly captured by MCAN but missed by unimodal models.</p><p>Before washed away by flood, an Indian man calmly gave the last gesture to a photographer.</p><p>A group of dolphins brought a dog that fell into a canal to safe area.   In Figure <ref type="figure" target="#fig_11">8</ref>, the two examples are detected by MCAN but missed by Spatial. The attached images in two examples look normal. However, the words in the tweet seem exaggerated and unbelievable. It is challenging for spatial-domain-only MCAN to detect, but with multimodal features, our MCAN model identifies them correctly.</p><p>These comparative cases prove that when a single-modal model, whether a text-based model or an image-based model, cannot correctly distinguish fake news, the proposed MCAN using multimodal features can give high confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose a novel Multimodal Co-Networks (MCAN) to tackle the challenge of fusing multimodal (textual and visual) features for fake news detection. We utilize three different sub-networks to extract features from text, spatial domain, and frequency domain, respectively. Then the three features are deeply fused by stacking co-attention layers, which is inspired by human behavior. When people read news with image, image and text are read once or multiple times, and continuously fused in brain. Experiments on two public benchmark datasets for fake news detection validate the effectiveness of MCAN, and the results show that MCAN outperforms the current state-of-the-art methods. In the future, we plan to extend the co-attention based fusion approach in MCAN to other fake news research, such as fake news diffusion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Some fake news from Twitter.</figDesc><graphic url="image-1.png" coords="1,307.56,204.54,70.21,70.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of our MCAN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The detailed architecture of feature extraction in frequency domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the self-attention block and the co-attention block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the third and fourth layers, the inputs are the output of the previous layer and text representation R T , and outputs are R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>C</head><label></label><figDesc>fused features of text, spatial domain, and frequency domain. Let f = R (4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) Spatial: a model consists of a VGG-19 model and the decision network of MCAN, utilizing image information in spatial domain. (3) Freq: proposed MCAN only has the part of dealing with frequency-domain features. (4) VQA<ref type="bibr" target="#b0">(Antol et al., 2015)</ref>: a model aims to answer questions according to the given images. For fair comparisons, we use a one-layer LSTM. (5) NeuralTalk (Vinyals et al., 2014): a deep recurrent framework for image caption. The joint representation of image and text is obtained by averaging the output of RNN at each timestep. (6) att-RNN (Jin et al., 2017): att-RNN utilizes local attention to fuse textual, visual, and social context features. For a fair comparison, we remove the part dealing with social context information. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: MCAN ablation analysis in Accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualizations of learned latent feature representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Some fake news detected by MCAN but missed by Text on the Weibo dataset.</figDesc><graphic url="image-9.png" coords="8,418.67,346.41,127.23,106.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7</head><label>7</label><figDesc>Figure7shows two top-confident tweets cessfully detected by MCAN but missed by textonly MCAN. The textual contents of the two examples can provide little evidence that it is fake news. However, the two attached images seem forged pictures.</figDesc><graphic url="image-10.png" coords="8,315.13,610.04,82.65,136.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Some fake news detected by MCAN but missed by Spatial on the Weibo dataset.</figDesc><graphic url="image-11.png" coords="8,406.30,610.04,129.13,88.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of two datasets.</figDesc><table><row><cell></cell><cell cols="2">Twitter Weibo</cell></row><row><cell># of fake news</cell><cell>8199</cell><cell>4211</cell></row><row><cell># of real news</cell><cell>6681</cell><cell>3639</cell></row><row><cell># of images</cell><cell>512</cell><cell>7850</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results of different methods on two datasets</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by National Research and Development Program of China (No.2017YFB1010004).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Verifying multimedia use at mediaeval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boididou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Rebecka</forename><forename type="middle">D</forename><surname>Gary D Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie-Ann L</forename><surname>Holman</surname></persName>
		</author>
		<author>
			<persName><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><forename type="middle">N</forename><surname>Lassiter F Speller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><forename type="middle">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kohlby</forename><forename type="middle">W</forename><surname>Mejia</surname></persName>
		</author>
		<author>
			<persName><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Eleny C Ceniceros</surname></persName>
		</author>
		<author>
			<persName><surname>Rustige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="668" to="677" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>crooked hillary&apos;, and &apos;deceptive donald&apos;: Language of lies in the 2016 us presidential debates</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information credibility on twitter</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
				<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Misleading online content: recognizing clickbait as&quot; false news</title>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niall</forename><forename type="middle">J</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on workshop on multimodal deception detection</title>
				<meeting>the 2015 ACM on workshop on multimodal deception detection</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="15" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Syntactic stylometry for deception detection</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritwik</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal fusion with recurrent neural networks for rumor detection on microblogs</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
				<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="795" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Novel visual and statistical image features for microblogs news verification</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="598" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mvae: Multimodal variational autoencoder for fake news detection</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Khattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaipal</forename><surname>Singh Goud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Early detection of fake news on social media through propagation path classification with recurrent and convolutional networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Fang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Detecting rumors from microblogs with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sejeong</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meeyoung</forename><surname>Cha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Detect rumors in microblog posts using propagation structure via kernel learning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detect rumors on twitter by promoting information campaigns with generative adversarial learning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3049" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">2008. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering</title>
		<author>
			<persName><forename type="first">Duy-Kien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6087" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Reinartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05638</idno>
		<title level="m">A stylometric inquiry into hyperpartisan and fake news</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting multi-domain visual information for fake news detection</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="518" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Csi: A hybrid deep model for fake news detection</title>
		<author>
			<persName><forename type="first">Natali</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fake news detection on social media: A data mining perspective</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Separating facts from fiction: Linguistic models to classify suspicious and trusted news posts on twitter</title>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Yea</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Hodas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="647" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eann: Event adversarial neural networks for multi-modal fake news detection</title>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishlay</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th acm sigkdd international conference on knowledge discovery &amp; data mining</title>
				<meeting>the 24th acm sigkdd international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="849" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fake news detection via knowledge-driven multimodal graph convolutional networks</title>
		<author>
			<persName><forename type="first">Youze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengsheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3372278.3390713</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="540" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">False rumors detection on sina weibo by propagation structures</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 31st international conference on data engineering</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="651" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A convolutional approach for misinformation identification</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Enquiring minds: Early detection of rumors in social media from enquiry posts</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2736277.2741637</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1395" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommy</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sekhar</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicha</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xenophon</forename><surname>Papademetris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<title level="m">Adabelief optimizer: Adapting stepsizes by the belief in observed gradients</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
