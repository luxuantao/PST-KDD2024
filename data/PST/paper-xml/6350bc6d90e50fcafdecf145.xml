<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering</title>
				<funder ref="#_sXhgSHX">
					<orgName type="full">Key Research and Development Program of Shaanxi</orgName>
				</funder>
				<funder ref="#_YnUhHmz">
					<orgName type="full">Innovation Capability Support Program of Shaanxi</orgName>
				</funder>
				<funder ref="#_3S2QWFV #_dqBu92U #_nbKb6RV #_KwA9mae #_hUVdsg3 #_z8Vd9kC #_ATTKQsQ #_TVmwSDU #_nTcJnSF">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-19">19 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaming</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
							<email>zyguan@</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<email>zwang@stu.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
							<email>ywzhao@mail.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cai</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weigang</forename><surname>Lu</surname></persName>
							<email>wglu@stu.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianbin</forename><surname>Huang</surname></persName>
							<email>jbhuang@xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-19">19 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.10462v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent self-supervised pre-training methods on Heterogeneous Information Networks (HINs) have shown promising competitiveness over traditional semisupervised Heterogeneous Graph Neural Networks (HGNNs). Unfortunately, their performance heavily depends on careful customization of various strategies for generating high-quality positive examples and negative examples, which notably limits their flexibility and generalization ability. In this work, we present SHGP, a novel Self-supervised Heterogeneous Graph Pre-training approach, which does not need to generate any positive examples or negative examples. It consists of two modules that share the same attention-aggregation scheme. In each iteration, the Att-LPA module produces pseudo-labels through structural clustering, which serve as the self-supervision signals to guide the Att-HGNN module to learn object embeddings and attention coefficients. The two modules can effectively utilize and enhance each other, promoting the model to learn discriminative embeddings. Extensive experiments on four real-world datasets demonstrate the superior effectiveness of SHGP against state-of-the-art unsupervised baselines and even semi-supervised baselines. We release our source code at: https://github.com/kepsail/SHGP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, various semi-supervised graph neural networks (GNNs) have been proposed to learn graph embeddings. They have achieved remarkable success in many graph analytic tasks. This success, however, comes at the cost of a heavy reliance on high-quality supervision labels. In real-world scenarios, labels are usually expensive to acquire, and sometimes even impossible due to privacy concerns.</p><p>To relieve the label scarcity issue in (semi-) supervised learning, and take full advantage of a large amount of easily available unlabeled data, the self-supervised learning (SSL) paradigm has recently drawn considerable research interest in the computer vision research community. It leverages the supervision signal from the data itself to learn generalizable embeddings, which are then transferred to various downstream tasks with only a few task-specific labels. One of the most common SSL paradigms is contrastive learning, which learns representations by estimating and maximizing the mutual information between the input and the output of a deep neural network encoder <ref type="bibr" target="#b8">[8]</ref>.</p><p>For graphs, some recent graph contrastive learning methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref> have shown promising competitiveness compared with semi-supervised GNNs. They usually require three typical steps: <ref type="bibr" target="#b1">(1)</ref> constructing positive examples (semantically correlated structural instances) by strategies such as node dropping, edge perturbation, and negative examples (uncorrelated instances) by strategies such as feature shuffling, mini-batch sampling; <ref type="bibr" target="#b2">(2)</ref> encoding these examples through graph encoders such as GCN <ref type="bibr" target="#b16">[16]</ref>; <ref type="bibr" target="#b3">(3)</ref> maximizing/minimizing the similarity between these positive/negative examples. Nevertheless, in the real world, graphs often contain multiple types of objects and multiple types of relationships between them, which are called heterogeneous graphs, or heterogeneous information networks (HINs) <ref type="bibr" target="#b27">[27]</ref>. Due to the challenges caused by the heterogeneity, existing SSL methods on homogeneous graphs cannot be straightforwardly applied to HINs. Very recently, several works have made some efforts to conduct SSL on HINs <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b10">10]</ref>. In comparison with SSL methods on homogeneous graphs, the key difference is that they usually have different example generation strategies, so as to capture the heterogeneous structural properties in HINs.</p><p>The strategies of generating high-quality positive/negative examples are critical to the performance of existing methods <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b36">36]</ref>. Unfortunately, whether for homogeneous graphs or heterogeneous graphs, the example generation strategies are dataset-specific, and may not be applicable to all scenarios. This is because real-world graphs are abstractions of things from various domains, e.g., social networks, citation networks, etc. They usually have significantly different structural properties and semantics. Previous works have systematically studied this and found that different strategies are good at capturing different structural semantics. For example, study <ref type="bibr" target="#b41">[41]</ref> observed that edge perturbation benefits social networks but hurts some biochemical networks, and study <ref type="bibr" target="#b36">[36]</ref> observed that negative examples benefit sparser graphs. Consequently, in practice, the example generation strategies have to be empirically constructed and investigated through either trial-and-error or rules of thumb. This significantly limits the practicality and general applicability of existing methods.</p><p>In this work, we focus on HINs which are more challenging, and propose a novel SSL approach, named SHGP. Different from existing methods, SHGP requires neither positive examples nor negative examples, thus circumventing the above issues. Specifically, SHGP adopts any HGNN model that is based on attention-aggregation scheme as the base encoder, which is termed as the module Att-HGNN. The attention coefficients in Att-HGNN are particularly used to combine with the structural clustering method LPA (label propagation algorithm) <ref type="bibr" target="#b21">[21]</ref>, as the module Att-LPA. Through performing structural clustering on HINs, Att-LPA is able to produce clustering labels, which are treated as pseudo-labels. In turn, these pseudo-labels serve as guidance signals to help Att-HGNN learn better embeddings as well as better attention coefficients. Thus, the two modules are able to exploit and enhance each other, finally leading the model to learn discriminative and informative embeddings. In summary, we have three main contributions as follows:</p><p>? We propose a novel SSL method on HINs, SHGP. It innovatively consists of the Att-LPA module and the Att-HGNN module. The two modules can effectively enhance each other, facilitating the model to learn effective embeddings. ? To the best of our knowledge, SHGP is the first attempt to perform SSL on HINs without any positive or negative examples. Therefore, it can directly avoid the laborious investigation of example generation strategies, improving the model's generalization ability and flexibility. ? We transfer the object embeddings learned by SHGP to various downstream tasks. The experimental results show that SHGP can outperform state-of-the-art baselines, even including some semisupervised baselines, demonstrating its superior effectiveness.</p><p>2 Related work SSL on HINs. There are several existing methods <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b10">10]</ref> that conduct SSL on HINs. Determined by their contrastive loss functions, all these methods require high-quality positive and negative examples to effectively learn embeddings. Thus, their effectiveness and performance hinge on the specific strategies of generating positive examples and negative examples, which limits their flexibility and generalization ability.</p><p>SSL on homogeneous graphs. Existing SSL methods on homogeneous graphs <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b30">30]</ref>  GNN+LPA methods. There exist several methods <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b24">24]</ref> that combine LPA <ref type="bibr" target="#b21">[21]</ref> with GNNs. However, they are all supervised learning methods, and only deal with homogeneous graphs. In this work, we study SSL on HINs.</p><p>Others. DeepCluster <ref type="bibr" target="#b2">[2]</ref> uses K-means to perform clustering in the embedding space. Differently, our SHGP directly performs structural clustering in the graph space. JOAO <ref type="bibr" target="#b40">[40]</ref> explores the automatic selection of positive example generation strategies, which is still not fully automatic.</p><p>HuBERT <ref type="bibr" target="#b9">[9]</ref> is an SSL approach for speech representation learning. GIANT <ref type="bibr" target="#b3">[3]</ref> leverages graphstructured self-supervision to extract numerical node features from raw data. MARU <ref type="bibr" target="#b12">[12]</ref> learns object embeddings by exploiting meta-contexts in random walks. Different from them, in this work, we study how to effectively conduct SSL on HINs. Graph pooling methods e.g. <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b39">39]</ref> learn soft cluster assignment to coarsen graph topology in each model layer. Differently, our method propagates integer (hard) cluster labels in each layer to perform structural clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We first briefly introduce some concepts about HINs, and then formally describe the problem we study in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heterogeneous Information</head><p>Network. An HIN is defined as: G = (V, E, A, R, ?, ?), where V is the set of objects, E is the set of links, ? : V ? A and ? : E ? R are respectively the object type mapping function and the link type mapping function, A denotes the set of object types, and R denotes the set of relations (link types), where |A| + |R| &gt; 2. Let X = {X 1 , ..., X |A| } denote the set containing all the feature matrices associated with each type of objects. A meta-path P of length l is defined in the form of</p><formula xml:id="formula_0">A 1 R1 --? A 2 R2 --? ? ? ? R l -? A l+1 (abbreviated as A 1 A 2 ? ? ? A l+1 ), which describes a composite relation R = R 1 ? R 2 ? ? ? ? ? R l between object types A 1 and A l+1</formula><p>, where ? denotes the composition operator on relations.</p><p>We show a toy HIN in the left part of Figure <ref type="figure">1</ref>. It contains four object types: "Paper" (P ), "Author" (A), "Conference" (C) and "Term" (T ), and three relations: "Publish" between P and C, "Write" between P and A, and "Contain" between P and T . AP C is a meta-path of length two, and a 1 p 2 c 2 is such a path instance, which means that author a 1 has published paper p 2 in conference c 2 . SSL on HINs. Given an HIN G, the problem is to learn an embedding vector h i ? R d for each object i ? V, in a self-supervised manner, i.e., without using any task-specific labels. The pre-trained embeddings are expected to capture the general-purpose information contained in G, and can be easily transferred to various unseen downstream tasks with only a few task-specific labels. The overall architecture of SHGP. Given an HIN, in each iteration, we use Att-HGNN to produce embeddings and predictions, and use Att-LPA to produce pseudo-labels. The loss is computed as the cross-entropy between the predictions and the pseudo-labels. The attention coefficients (and other parameters) are optimized via gradient descent, which serve as the new attention-aggregation weights of Att-HGCN and Att-LPA in the next iteration, promoting them to produce better embeddings and predictions, as well as better pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we present the proposed method SHGP, which consists of two key modules. The Att-HGNN module is instantiated as any HGNN model that is based on attention-aggregation scheme. The Att-LPA module combines the structural clustering method LPA <ref type="bibr" target="#b21">[21]</ref> with the attention-aggregation scheme used in Att-HGNN. The overall model architecture is shown in Figure <ref type="figure">1</ref> and explained in the figure caption. In the following, we describe the procedure of SHGP in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initialization</head><p>At the beginning, we randomly initialize all the model parameters in Att-HGNN by the Xavier uniform distribution <ref type="bibr" target="#b6">[6]</ref>. To obtain initial pseudo-labels, we use the original LPA <ref type="bibr" target="#b21">[21]</ref> to perform a thorough structural clustering on the input HIN G. Specifically, LPA randomly associates each object with a unique integer as its initial label, and lets them iteratively propagate along the links in G. In each iteration, each object updates its label to the label that appears most frequently in its neighborhood. After convergence, the final label indicates the cluster to which each object belongs. We treat these clustering labels returned by LPA as the initial pseudo-labels, and re-organize them as a one-hot label matrix Y [0] ? R |V|?K , where K denotes the cluster size, which is not a hyper-parameter but depends on the uniqueness of these labels. Thus, in the subsequent steps, the propagation of the pseudo-labels can be easily implemented via the matrix multiplication operation.</p><p>Under the guidance of these obtained initial pseudo-labels, we first train Att-HGNN for several epochs as the model "warm-up", to learn the initial meaningful attention coefficients as well as other model parameters. We use W [0] to denote all these initial parameters. Then, we proceed with the following iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Iteration</head><p>In the t-th iteration, we compute the object embeddings H [t] by the Att-HGNN module. Att-HGNN is parameterized by W [t-1] , and its inputs include: the HIN topology G, the object features X . This is formulated as follows:</p><formula xml:id="formula_1">H [t] = Att-HGNN W [t-1] , G, X<label>(1)</label></formula><p>Meanwhile, we update the pseudo-labels by the creatively proposed Att-LPA module. In comparison with Att-HGNN, Att-LPA does not input X , but instead inputs Y [t-1] , i.e., the pseudo-labels of the previous iteration. This is formulated as follows:</p><formula xml:id="formula_2">Y [t] = Att-LPA W [t-1] , G, Y [t-1]</formula><p>(2) Att-HGNN and Att-LPA perform one forward pass of attention-based aggregation in the same way. The only difference between them is that Att-HGNN aggregates the (projected) features of neighbors while Att-LPA aggregates the pseudo-labels of neighbors produced in the previous iteration, both weighted by exactly the same attention coefficients. Now, we input H [t] into a softmax classifier which is built on the top layer of the Att-HGNN to make predictions P [t] . The loss is computed as the cross-entropy between the predictions P [t] and the pseudo-labels Y [t] , as follows:</p><formula xml:id="formula_3">P [t] =softmax(H [t] ? C [t-1] ) L [t] = - i?V K c=1 Y [t] i,c ln P [t] i,c<label>(3)</label></formula><p>where 1] denotes the parameter matrix of the classifier.</p><formula xml:id="formula_4">C [t-1] ? W [t-</formula><p>Having the loss, we can then optimize all the model parameters by gradient descent, as follows:</p><formula xml:id="formula_5">W [t] = W [t-1] -? ? ? W L [t]</formula><p>(4) where ? denotes the learning rate. As the optimization proceeds, the model will learn better model parameters (including attention coefficients). This leads Att-HGNN and Att-LPA to respectively produce better embeddings (and predictions) and better pseudo-labels in the next iteration, which, in turn, promotes the model to learn further better parameters. Thus, the two processes are able to closely interact with each other, as well as enhance each other, finally resulting in discriminative and informative embeddings. The overall procedure is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Encoder of Att-HGNN</head><p>The Att-HGNN module can be specifically instantiated as any attention-based HGNN encoders. Existing possible choices include: HAN <ref type="bibr" target="#b32">[32]</ref>, HGT <ref type="bibr" target="#b11">[11]</ref>, GTN <ref type="bibr" target="#b42">[42]</ref>, and ie-HGCN <ref type="bibr" target="#b38">[38]</ref>. Among them, Algorithm 1 The overall procedure of SHGP Input: An HIN G Output: Object embeddings 1: Perform a thorough LPA process to get initial pseudo-labels. 2: Guided by the initial pseudo-labels, warm-up Att-HGNN for several epochs. Perform one forward pass of Att-HGNN module to compute embeddings by Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Perform one forward pass of Att-LPA module to update pseudo-labels by Eq. ( <ref type="formula">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute cross-entropy loss by Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Optimize all the model parameters (including attention coefficients) by Eq. ( <ref type="formula">4</ref>). 8: end while our previously proposed ie-HGCN <ref type="bibr" target="#b38">[38]</ref> is simple and efficient. It has shown superior performance over the other three models. Therefore, in this work, we adopt it as the base encoder of Att-HGNN, which is in detail described as follows.</p><p>Let N i denote the set of object i's neighbors. Specially, object i itself is also added to N i . Accordingly, an edge (i, i) is added to E, and a dummy self-relation ?(i, i) is added to R. In each model layer, object i's new representation h i is computed as follows:</p><formula xml:id="formula_6">h i = ? j?Ni ? ?(i,j) i ? a ?(i,j) i,j ? h j ? W ?(i,j) (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where ? is the non-linear activation function, and h j is neighbor j's current representation. W ?(i,j) is the projection parameter matrix, a</p><formula xml:id="formula_8">?(i,j) i,j</formula><p>is the normalized link weight ( a</p><formula xml:id="formula_9">?(i,i) i,i<label>= 1)</label></formula><p>, and ? ?(i,j) i is the normalized attention coefficient, all of which are specific to relation ?(i, j).</p><p>With Att-HGNN, we can formulate our Att-LPA in a similar way. Specifically, in each layer, object i's pseudo-label is updated as follows:</p><formula xml:id="formula_10">y i = one-hot argmax j?Ni ? ?(i,j) i ? a ?(i,j) i,j ? y j (6)</formula><p>where y j is neighbor j's pseudo-label vector in the one-hot form. Object i first aggregates its neighbors' pseudo-labels according to the same normalized link weights and attention coefficients as in Eq. ( <ref type="formula" target="#formula_6">5</ref>). Then, its new pseudo-label vector y i is obtained through the argmax operator and the one-hot encoding function.</p><p>The above two equations describe the computational details of one model layer, where the iteration indices and the layer indices are omitted for notation brevity. Here, we can further use the superscript [x, y] to index a symbol with respect to x-th iteration and y-th layer. Assume the model has N layers. In the t-th iteration, for the input layer of Att-HGNN, we set: h [t,0] i = x i (i.e., the object feature vector), while for Att-LPA, we set: y</p><formula xml:id="formula_11">[t,0] i = y [t-1,N ] i</formula><p>. At the end of the t-th iteration, we properly organize all the h [t,N ] i and y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[t,N ] i</head><p>as H [t] in Eq. ( <ref type="formula" target="#formula_1">1</ref>) and Y [t] in Eq. ( <ref type="formula">2</ref>) respectively. In this way, based on the ie-HGCN encoder, we establish our Att-HGNN module and Att-LPA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Time complexity</head><p>As analyzed in the literature, both LPA <ref type="bibr" target="#b21">[21]</ref> and ie-HGCN <ref type="bibr" target="#b38">[38]</ref> have quasi-linear time complexity. Therefore, our SHGP also has quasi-linear time complexity, i.e., O(|V| + |E|). Please see Appendix A.2 for the efficiency study of SHGP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Label consistency</head><p>For SHGP, in the initialization phase, we perform the thorough LPA process until convergence to obtain the initial pseudo-labels, and warm-up Att-HGNN module to learn initial meaningful attention coefficients. In each subsequent iteration, we no longer need to perform LPA from scratch, but instead perform one forward pass of Att-LPA. Therefore, the clustering results are basically consistent with the results of the previous iteration. Thus, our SHGP does not need to perform alignment between clustering labels and predictions. This can avoid the issue in DeepCluster <ref type="bibr" target="#b2">[2]</ref> that there is no correspondence between two consecutive clustering assignments, and can also avoid the alignment process in M3S <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Class space</head><p>In this work, we cluster all types of objects into one common class space. We note that in most existing HIN datasets, various types of objects can indeed share the same class space. They typically show a "star" network schema <ref type="bibr" target="#b28">[28]</ref>, since they are usually constructed according to one "hub" type of objects. The other types of objects are actually the attributes of the "hub" objects. For example, on DBLP, the "hub" objects are paper (P ) objects. These papers and their associated authors, conferences, and etc. can all be categorized into four research areas: DM, IR, DB, and AI.</p><p>In the following extensive experiments, we demonstrate that this strategy works well on existing widely used HIN benchmark datasets. In future work, we will further investigate the problem of clustering different types of objects into different class spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we verify the generalization ability of the proposed SHGP by transferring the pre-trained object embeddings to various downstream tasks including object classification, object clustering, and embedding visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>In the experiments, we use four publicly available HIN benchmark datasets, which are widely used in previous related works <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b33">33]</ref>. Their statistics are summarized in Table <ref type="table" target="#tab_1">1</ref>. Please see Appendix A.1 for more details of these datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare our SHGP with seven HIN-oriented baselines, including two semi-supervised methods, and five unsupervised (self-supervised) methods:</p><p>? Semi-supervised: HAN <ref type="bibr" target="#b32">[32]</ref>, and ie-HGCN (abbreviated as HGCN) <ref type="bibr" target="#b38">[38]</ref>.</p><p>? Unsupervised: metapath2vec (abbreviated as M2V) <ref type="bibr" target="#b5">[5]</ref>, DMGI <ref type="bibr" target="#b18">[18]</ref>, HDGI <ref type="bibr" target="#b23">[23]</ref>, HeCo <ref type="bibr" target="#b33">[33]</ref>, and HGCN-DC (abbreviated as H-DC).</p><p>Here, HAN and HGCN are semi-supervised HGNN methods. M2V is a traditional unsupervised HIN embedding method. DMGI, HDGI, and HeCo are state-of-the-art SSL methods on HINs. Note that, in the experiments, we adopt the baseline HGCN's encoder as the base encoder of our Att-HGNN. H-DC is a variant of our SHGP. It also uses HGCN as the base encoder, but unlike SHGP which performs structural clustering in the graph space, H-DC uses K-means to perform clustering in the embedding space, like DeepCluster <ref type="bibr" target="#b2">[2]</ref>. We can use baselines HGCN and H-DC to investigate the effectiveness of the proposed self-supervised pre-training scheme based on structural clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation details</head><p>For the proposed SHGP, in all the experiments, we use two HGCN layers as the Att-HGNN encoder, and search the dimensionalities of the hidden layers in the set {64, 128, 256, 512}. All the model parameters are initialized by the Xavier uniform distribution <ref type="bibr" target="#b6">[6]</ref>, and they are optimized through the Adam optimizer. The learning rate and weight decay are searched from 1e-4 to 1e-2. For the number of warm-up epochs, we search its best value in the set {5, 10, 20, 30, 40, 50}. We pre-train SHGP with up to 100 epochs and select the model with the lowest validation loss as the pre-trained model. Then, we freeze the model and transfer the learned object embeddings to various downstream tasks. For baselines, we reproduce their experimental results on our datasets. Their hyper-parameters are searched based on their papers and their official source codes. Some of the baseline methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b33">33]</ref> need users to input several meta-paths, which are specified in Appendix A.1 in detail. For all the methods, we randomly repeat all the evaluation tasks for ten times and report the average results. All the experiments are conducted on an NVIDIA GTX 1080Ti GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Object classification</head><p>We conduct object classification on the object embeddings learned by all the semi-supervised baselines and unsupervised baselines. On each dataset, for the objects that have ground-truth labels, we randomly select {4%, 6%, 8%} objects as the training set. The others are divided equally as the validation set and the test set. For all the unsupervised methods, they don't use any class labels during learning object embeddings. After finishing the pre-training, the output object embeddings and their corresponding labels are used to train a linear logistic regression classifier. For all the semi-supervised methods, we directly report the classification results output by their own classifiers. We adopt Micro-F1 and Macro-F1 as evaluation metrics. The results are reported in Table <ref type="table" target="#tab_2">2</ref>.</p><p>We can see that our proposed SHGP achieves the best overall performance, even exceeding several semi-supervised learning methods, indicating its superior effectiveness. On MAG, SHGP achieves very high performance, i.e., over 98% Micro-F1/Macro-F1 scores, which are close to saturation.</p><p>Recall that SHGP adopts HGCN as the base encoder, and here, SHGP achieves better performance than HGCN. This indicates the effectiveness of the proposed strategy, i.e., pre-training HGNNs in a self-supervised manner based on structural clustering. SHGP also performs better than H-DC. This is reasonable because H-DC uses K-means to perform clustering in the embedding space to obtain pseudo-labels, and thus it cannot effectively exploit the structural information which naturally resides in the graph space.</p><p>Most unsupervised pre-training methods outperform traditional semi-supervised methods. This verifies the superiority of the recent advances in the self-supervised pre-training paradigm. Among the unsupervised methods, M2V performs much worse than the others. This is because M2V can only exploit a single meta-path, unlike DMGI, HDGI and HeCo which can well fuse the information conveyed by multiple meta-paths through regularization or attention mechanism. Among the semisupervised methods, HGCN outperforms HAN, probably because HAN can only exploit user-specified meta-paths while HGCN can automatically discover and exploit the most useful meta-paths <ref type="bibr" target="#b38">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Object clustering</head><p>We evaluate the embeddings through object clustering. In this task, we only consider the unsupervised methods, and the semi-supervised methods are excluded because they have exploited class labels during learning embeddings. On each dataset, we use K-means to cluster the embeddings of the labeled objects, and report normalized mutual information (NMI) and adjusted rand index (ARI) to quantitatively assess the clustering quality. As shown in Table <ref type="table" target="#tab_3">3</ref>, our SHGP achieves the best overall performance in this task. Especially, on MAG, SHGP outperforms other baselines by a large margin. This demonstrates its superior competitiveness against other baselines in terms of learning discriminative embeddings. On IMDB, all the methods have shown limited performance. This may be because the class labels and the structural information on this dataset are not significantly correlated. M2V shows the worst performance again, which may be due to its limitation as we analyzed in the previous experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Visualization</head><p>For a more intuitive comparison, on MAG, we visualize the paper objects' embeddings pre-trained by three unsupervised methods, i.e., DMGI, HeCo, and our SHGP. The embeddings are further embedded into the 2-dimensional Euclidean space by the t-SNE algorithm <ref type="bibr" target="#b17">[17]</ref>, and they are colored according to their ground-truth labels. The results are shown in Figure <ref type="figure" target="#fig_3">2</ref>. We can see that DMGI shows blurred boundaries between different classes. HeCo performs relatively better than DMGI, but the green (left) class objects and the blue (bottom) class objects are still mixed to some extent. Our SHGP shows the best within-class compactness and the clearest between-class boundaries, which demonstrates its superior effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Hyper-parameter study</head><p>In this subsection, we investigate the sensitivity of the warm-up epochs, which is the main hyperparameter that we have introduced in Section 4.1. Specifically, on all the datasets, we explore the change of object classification performance as the number of warm-up epochs gradually increases.</p><p>The results are shown in Figure <ref type="figure" target="#fig_1">3</ref>. As shown, overall, this hyper-parameter is not very sensitive. The general pattern is that the performance increases first, and then declines gradually. This is because, at the beginning, the model has not made full use of the information contained in the initial pseudo-labels yet. After that, the model may gradually overfit these initial pseudo-labels, which prevents the model from continuously improving its performance in the iterations. The overall inflection points are 10, 20, 20, and 20 for MAG, ACM, DBLP, and IMDB respectively, which are the default values in the other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel self-supervised pre-training method on HINs, named SHGP. It consists of two key modules which share the same attention-aggregation scheme. The two modules are able to utilize and enhance each other, promoting the model to effectively learn informative embeddings. Different from existing SSL methods on HINs, our SHGP does not require any positive examples or negative examples, thereby enjoying a high degree of flexibility and ease of use. We conduct extensive experiments to demonstrate the superior effectiveness of SHGP against state-ofthe-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Societal impact</head><p>Heterogeneous Information Networks (HINs) widely exist in our society, such as social networks, power networks, virus networks, etc. In this work, we develop a method to learn the representations for objects in HINs. The learned representations can be used for various analytical tasks such as object classification, object clustering, link prediction, etc. We believe that our method contributes to our society in many aspects. However, there is also some risk that our method could be abused illegally or unethically for some undesirable purposes. We hope those bad things would not happen.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1: The overall architecture of SHGP. Given an HIN, in each iteration, we use Att-HGNN to produce embeddings and predictions, and use Att-LPA to produce pseudo-labels. The loss is computed as the cross-entropy between the predictions and the pseudo-labels. The attention coefficients (and other parameters) are optimized via gradient descent, which serve as the new attention-aggregation weights of Att-HGCN and Att-LPA in the next iteration, promoting them to produce better embeddings and predictions, as well as better pseudo-labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 :</head><label>3</label><figDesc>while Not Converged do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3328), A (42553), U (2103), D (2016) M A, M U , M D ? MAG is a subset of Microsoft Academic Graph. It contains four object types: Paper (P ), Author (A), Institution (I) and Field (F ), and eight relations between them. Paper objects are labeled as four classes according to their published venues: IEEE Journal of Photovoltaics, Astrophysics, Low Temperature Physics, and Journal of Applied Meteorology and Climatology. ? ACM is extracted from ACM digital library. It contains three object types: Paper (P ), Author (A) and Subject (S), and four relations between them. Paper objects are divided into three classes: Data Mining, Database, and Computer Network. ? DBLP is extracted from DBLP bibliography. It contains four object types: Author (A), Paper (P ), Conference (C) and Term (T ), and six relations between them. Author (A) objects are labeled according to their four research areas: Data Mining, Information Retrieval, Database, and Artificial Intelligence. ? IMDB is extracted from the online movie rating website IMDB. It contains four object types: Movie (M ), Actor (A), User (U ) and Director (D), and six relations between them. Movie (M ) objects are categorized into four classes according to their genres: Comedy, Documentary, Drama, and Horror.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of the pre-trained embeddings of paper objects on MAG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analysis of the number of warm-up epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Datasets</cell><cell>Objects (number)</cell><cell></cell><cell></cell><cell cols="2">Relations</cell><cell></cell></row><row><cell>MAG</cell><cell>P (4017), A (15383), I (1480), F (5454)</cell><cell>P</cell><cell>P , P</cell><cell></cell><cell>F , P</cell><cell cols="2">A, A</cell><cell>I</cell></row><row><cell>ACM</cell><cell>P (4025), A (7167), S (60)</cell><cell></cell><cell>P</cell><cell cols="2">A, P</cell><cell>S</cell></row><row><cell>DBLP</cell><cell>A (4057), P (14328), C (20), T (8898)</cell><cell>P</cell><cell cols="2">A, P</cell><cell cols="2">C, P</cell><cell>T</cell></row><row><cell>IMDB</cell><cell>M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Object classification results (%).</figDesc><table><row><cell>Datasets Metrics Train HAN HGCN M2V DMGI HDGI HeCo H-DC SHGP</cell></row><row><cell>Mic-F1</cell></row><row><cell>MAG</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Object clustering results (%).</figDesc><table><row><cell></cell><cell cols="2">MAG</cell><cell cols="2">ACM</cell><cell cols="2">DBLP</cell><cell>IMDB</cell></row><row><cell></cell><cell>NMI</cell><cell>ARI</cell><cell>NMI</cell><cell>ARI</cell><cell>NMI</cell><cell>ARI</cell><cell>NMI ARI</cell></row><row><cell>M2V</cell><cell cols="6">39.67 43.75 32.53 28.49 49.50 56.73</cell><cell>1.43</cell><cell>1.03</cell></row><row><cell cols="7">DMGI 70.89 73.51 38.45 32.46 65.17 67.23</cell><cell>3.49</cell><cell>2.65</cell></row><row><cell>HDGI</cell><cell cols="6">73.96 77.15 39.13 32.34 59.98 62.33</cell><cell>4.15</cell><cell>2.96</cell></row><row><cell>HeCo</cell><cell cols="6">79.33 83.16 39.06 32.69 68.81 74.05</cell><cell>5.69</cell><cell>2.32</cell></row><row><cell>H-DC</cell><cell cols="6">42.75 49.01 18.60 19.75 47.15 53.15</cell><cell>1.57</cell><cell>1.12</cell></row><row><cell cols="7">SHGP 90.65 93.00 39.42 32.63 73.30 77.31</cell><cell>6.33</cell><cell>3.10</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><head>Funding transparency statement</head><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">62133012</rs>, <rs type="grantNumber">61936006</rs>, <rs type="grantNumber">62103314</rs>, <rs type="grantNumber">62203354</rs>, <rs type="grantNumber">61876144</rs>, <rs type="grantNumber">61876145</rs>, <rs type="grantNumber">62073255</rs>, <rs type="grantNumber">61876138</rs> and <rs type="grantNumber">62002255</rs>, the <rs type="funder">Key Research and Development Program of Shaanxi</rs> under Grant <rs type="grantNumber">2020ZDLGY04-07</rs>, and the <rs type="funder">Innovation Capability Support Program of Shaanxi</rs> under Grant <rs type="grantNumber">2021TD-05</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3S2QWFV">
					<idno type="grant-number">62133012</idno>
				</org>
				<org type="funding" xml:id="_dqBu92U">
					<idno type="grant-number">61936006</idno>
				</org>
				<org type="funding" xml:id="_nbKb6RV">
					<idno type="grant-number">62103314</idno>
				</org>
				<org type="funding" xml:id="_KwA9mae">
					<idno type="grant-number">62203354</idno>
				</org>
				<org type="funding" xml:id="_hUVdsg3">
					<idno type="grant-number">61876144</idno>
				</org>
				<org type="funding" xml:id="_z8Vd9kC">
					<idno type="grant-number">61876145</idno>
				</org>
				<org type="funding" xml:id="_ATTKQsQ">
					<idno type="grant-number">62073255</idno>
				</org>
				<org type="funding" xml:id="_TVmwSDU">
					<idno type="grant-number">61876138</idno>
				</org>
				<org type="funding" xml:id="_nTcJnSF">
					<idno type="grant-number">62002255</idno>
				</org>
				<org type="funding" xml:id="_sXhgSHX">
					<idno type="grant-number">2020ZDLGY04-07</idno>
				</org>
				<org type="funding" xml:id="_YnUhHmz">
					<idno type="grant-number">2021TD-05</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Label-gcn: An effective method for adding label propagation to graph convolutional networks</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Bellei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussain</forename><surname>Alattas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesrine</forename><surname>Kaaniche</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02153</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Node feature extraction by self-supervised multi-scale neighborhood prediction</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00064</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08171</idno>
		<title level="m">Evaluating modules in graph contrastive learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Yao-Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Hubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3451" to="3460" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maru: Meta-context aware random walks for heterogeneous network representation learning</title>
		<author>
			<persName><forename type="first">Jyun-Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J-T</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pre-training on large-scale heterogeneous graph</title>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianrui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="756" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive pre-training of gnns on heterogeneous graphs</title>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="803" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hdmi: High-order deep multiplex infomax</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2414" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised attributed multiplex network embedding</title>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5371" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Near linear time algorithm to detect community structures in large-scale networks</title>
		<author>
			<persName><forename type="first">Usha</forename><surname>Nandini Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?ka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soundar</forename><surname>Kumara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Asap: Adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">Ekagra</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08538</idno>
		<title level="m">Heterogeneous deep graph infomax</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labeled nodes</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5892" to="5899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based topk similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="992" to="1003" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ranking-based clustering of heterogeneous information networks with star network schema</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yintao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Augmentation-free graph contrastive learning</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04874</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06755</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised heterogeneous graph neural network with co-contrastive learning</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1726" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised learning on graphs: Contrastive, generative, or predictive</title>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Simgrace: A simple framework for graph contrastive learning without data augmentation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03104</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Infogcl: Information-aware graph contrastive learning</title>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15438</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Xiaochun Cao, and Yuanfang Guo. Heterogeneous graph information bottleneck</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingxin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhua</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1638" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interpretable and efficient heterogeneous graph convolutional network</title>
		<author>
			<persName><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07594</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11960" to="11970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
