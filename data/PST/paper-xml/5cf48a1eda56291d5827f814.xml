<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Invariant and Equivariant Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
							<email>nicolas.keriven@ens.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">École Normale Supérieure Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
							<email>gabriel.peyre@ens.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">École Normale Supérieure Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Universal Invariant and Equivariant Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNN) come in many flavors, but should always be either invariant (permutation of the nodes of the input graph does not affect the output) or equivariant (permutation of the input permutes the output). In this paper, we consider a specific class of invariant and equivariant networks, for which we prove new universality theorems. More precisely, we consider networks with a single hidden layer, obtained by summing channels formed by applying an equivariant linear operator, a pointwise non-linearity, and either an invariant or equivariant linear output layer. Recently, <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> showed that by allowing higherorder tensorization inside the network, universal invariant GNNs can be obtained. As a first contribution, we propose an alternative proof of this result, which relies on the Stone-Weierstrass theorem for algebra of real-valued functions. Our main contribution is then an extension of this result to the equivariant case, which appears in many practical applications but has been less studied from a theoretical point of view. The proof relies on a new generalized Stone-Weierstrass theorem for algebra of equivariant functions, which is of independent interest. Additionally, unlike many previous works that consider a fixed number of nodes, our results show that a GNN defined by a single set of parameters can approximate uniformly well a function defined on graphs of varying size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Designing Neural Networks (NN) to exhibit some invariance or equivariance to group operations is a central problem in machine learning <ref type="bibr" target="#b28">(Shawe-Taylor, 1993)</ref>. Among these, Graph Neural Networks (GNN) are primary examples that have gathered a lot of attention for a large range of applications. Indeed, since a graph is not changed by permutation of its nodes, GNNs must be either invariant to permutation, if they return a result that must not depend on the representation of the input, or equivariant to permutation, if the output must be permuted when the input is permuted, for instance when the network returns a signal over the nodes of the input graph. In this paper, we examine universal approximation theorems for invariant and equivariant GNNs.</p><p>From a theoretical point of view, invariant GNNs have been much more studied than their equivariant counterpart (see the following subsection). However, many practical applications deal with equivariance instead, such as community detection <ref type="bibr" target="#b4">(Chen et al., 2019)</ref>, recommender systems <ref type="bibr" target="#b32">(Ying et al., 2018)</ref>, interaction networks of physical systems <ref type="bibr" target="#b0">(Battaglia et al., 2016)</ref>, state prediction <ref type="bibr" target="#b24">(Sanchez-Gonzalez et al., 2018)</ref>, protein interface prediction <ref type="bibr" target="#b9">(Fout et al., 2017)</ref>, among many others. See <ref type="bibr" target="#b34">(Zhou et al., 2018;</ref><ref type="bibr" target="#b1">Bronstein et al., 2017)</ref> for thorough reviews. It is therefore of great interest to increase our understanding of equivariant networks, in particular, by extending arguably one of the most classical result on neural networks, namely the universal approximation theorem for multi-layers perceptron (MLP) with a single hidden layer <ref type="bibr" target="#b6">(Cybenko, 1989;</ref><ref type="bibr" target="#b13">Hornik et al., 1989;</ref><ref type="bibr" target="#b20">Pinkus, 1999)</ref>. <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> recently proved that certain invariant GNNs were universal approximators of invariant continuous functions on graphs. The main goal of this paper is to extend this result to the equivariant case, for similar architectures.</p><p>Outline and contribution. The outline of our paper is as follows. After reviewing previous works and notations in the rest of the introduction, in Section 2 we provide an alternative proof of the result of <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref> for invariant GNNs (Theorem 1), which will serve as a basis for the equivariant case. It relies on a non-trivial application of the classical Stone-Weierstrass theorem for algebras of real-valued functions (recalled in Theorem 2). Then, as our main contribution, in Section 3 we prove this result for the equivariant case (Theorem 3), which to the best of our knowledge was not known before. The proof relies on a new version of Stone-Weierstrass theorem (Theorem 4). Unlike many works that consider a fixed number of nodes n, in both cases we will prove that a GNN described by a single set of parameters can approximate uniformly well a function that acts on graphs of varying size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous works</head><p>The design of neural network architectures which are equivariant or invariant under group actions is an active area of research, see for instance <ref type="bibr" target="#b22">(Ravanbakhsh et al., 2017;</ref><ref type="bibr" target="#b10">Gens and Domingos, 2014;</ref><ref type="bibr" target="#b5">Cohen and Welling, 2016)</ref> for finite groups and <ref type="bibr" target="#b29">(Wood and Shawe-Taylor, 1996;</ref><ref type="bibr">Kondor and Trivedi, 2018)</ref> for infinite groups. We focus here our attention to discrete groups acting on the coordinates of the features, and more specifically to the action of the full set of permutations on tensors (order-1 tensors corresponding to sets, order-2 to graphs, order-3 to triangulations, etc).</p><p>Convolutional GNN. The most appealing construction of GNN architectures is through the use of local operators acting on vectors indexed by the vertices. Early definitions of these "message passing" architectures rely on fixed point iterations <ref type="bibr" target="#b27">(Scarselli et al., 2009)</ref>, while more recent constructions make use of non-linear functions of the adjacency matrix, for instance using spectral decompositions <ref type="bibr" target="#b3">(Bruna et al., 2014)</ref> or polynomials <ref type="bibr" target="#b8">(Defferrard et al., 2016)</ref>. We refer to <ref type="bibr" target="#b1">(Bronstein et al., 2017;</ref><ref type="bibr" target="#b30">Xu et al., 2019)</ref> for recent reviews. For regular-grid graphs, they match classical convolutional networks <ref type="bibr" target="#b17">(LeCun et al., 1989)</ref> which by design can only approximate translation-invariant or equivariant functions <ref type="bibr" target="#b31">(Yarotsky, 2018)</ref>. It thus comes at no surprise that these convolutional GNN are not universal approximators <ref type="bibr" target="#b30">(Xu et al., 2019)</ref> of permutation-invariant functions.</p><p>Fully-invariant GNN. Designing Graph (and their higher-dimensional generalizations) NN which are equivariant or invariant to the whole permutation group (as opposed to e.g. only translations) requires the use of a small sub-space of linear operators, which is identified in <ref type="bibr" target="#b18">(Maron et al., 2019a)</ref>. This generalizes several previous constructions, for instance for sets <ref type="bibr" target="#b33">(Zaheer et al., 2017;</ref><ref type="bibr" target="#b12">Hartford et al., 2018)</ref> and points clouds <ref type="bibr" target="#b21">(Qi et al., 2017)</ref>. Universality results are known to hold in the special cases of sets, point clouds <ref type="bibr" target="#b21">(Qi et al., 2017)</ref> and discrete measures <ref type="bibr" target="#b7">(de Bie et al., 2019)</ref> networks.</p><p>In the invariant GNN case, the universality of architectures built using a single hidden layer of such equivariant operators followed by an invariant layer is proved in <ref type="bibr" target="#b19">(Maron et al., 2019b</ref>) (see also <ref type="bibr">(Kondor et al., 2018)</ref>). This is the closest work from our, and we will provide an alternative proof of this result in Section 2, as a basis for our main result in Section 3.</p><p>Universality in the equivariant case has been less studied. Most of the literature focuses on equivariance to translation and its relation to convolutions <ref type="bibr">(Kondor et al., 2018;</ref><ref type="bibr" target="#b5">Cohen and Welling, 2016)</ref>, which are ubiquitous in image processing. In this context, <ref type="bibr" target="#b31">Yarotsky (2018)</ref> proved the universality of some translation-equivariant networks. Closer to our work, universality of NNs equivariant to permutations acting on point clouds has been recently proven in <ref type="bibr" target="#b26">(Sannai et al., 2019)</ref>, however their theorem does not allow for high-order inputs like graphs. It is the purpose of our paper to fill this missing piece and prove the universality of a class of equivariant GNNs for high-order inputs such as (hyper-)graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Notations and definitions</head><p>Graphs. In this paper, (hyper-)graphs with n nodes are represented by tensors G ∈ R n d indexed by 1 i 1 , . . . , i d n. For instance, "classical" graphs are represented by edge weight matrices (d = 2), and hyper-graphs by high-order tensors of "multi-edges" connecting more than two nodes. Note that we do not impose G to be symmetric, or to contain only non-negative elements. In the rest of the paper, we fix some d 1 for the order of the inputs, however we allow n to vary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Permutations. Let [n]</head><p>def.</p><p>= {1, . . . , n}. The set of permutations σ : [n] → [n] (bijections from [n] to itself) is denoted by O n , or simply O when there is no ambiguity. Given a permutation σ and an order-k tensor G ∈ R n k , a "permutation of nodes" on G is denoted by σ G and defined as</p><formula xml:id="formula_0">(σ G) σ(i1),...,σ(i k ) = G i1,...,i k .</formula><p>We denote by P σ ∈ {0, 1} n×n the permutation matrix corresponding to σ, or simply P when there is no ambiguity. For instance, for G ∈ R n 2 we have σ G = P GP .</p><p>Two graphs G 1 , G 2 are said isomorphic if there is a permutation σ such that = {σ G ; σ ∈ O} the orbit of all the permuted versions of G.</p><formula xml:id="formula_1">G 1 = σ G 2 . If G = σ G, we say that σ is a self-isomorphism of G.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant and equivariant linear operators. A function</head><formula xml:id="formula_2">f : R n k → R is said to be invariant if f (σ G) = f (G) for every permutation σ. A function f : R n k → R n is said to be equivariant if f (σ G) = σ f (G).</formula><p>Our construction of GNNs alternates between linear operators that are invariant or equivariant to permutations, and non-linearities. <ref type="bibr" target="#b18">Maron et al. (2019a)</ref> elegantly characterize all such linear functions, and prove that they live in vector spaces of dimension, respectively, exactly b(k) and b(k + ), where b(i) is the i th Bell number. An important corollary of this result is that the dimension of this space does not depend on the number of nodes n, but only on the order of the input and output tensors. Therefore one can parameterize linearly for all n such an operator by the same set of coefficients. For instance, a linear equivariant operator F : R n 2 → R n 2 from matrices to matrices is formed by a linear combination of b(4) = 15 basic operators such as "sum of rows replicated on the diagonal", "sum of columns replicated on the rows", and so on. The 15 coefficients used in this linear combination define the "same" linear operator for every n.</p><p>Invariant and equivariant Graph Neural Nets. As noted by <ref type="bibr" target="#b31">Yarotsky (2018)</ref>, it is in fact trivial to build invariant universal networks for finite groups of symmetry: just take a non-invariant universal architecture, and perform a group averaging. However, this holds little interest in practice, since the group of permutation is of size n!. Instead, researchers use architectures for which invariance is hard-coded into the construction of the network itself. The same remark holds for equivariance.</p><p>In this paper, we consider one-layer GNNs of the form:</p><formula xml:id="formula_3">f (G) = S s=1 H s ρ(F s [G] + B s ) + b,<label>(1)</label></formula><p>where F s : R n d → R n ks are linear equivariant functions that yield k s -tensors (i.e. they potentially increase or decrease the order of the input tensor), and H s are invariant linear operators H s : R n ks → R (resp. equivariant linear operators H s : R n ks → R n ), such that the GNN is globally invariant (resp. equivariant). The invariant case is studied in Section 2, and the equivariant in Section 3. The bias terms B s ∈ R n ks are equivariant, so that B s = σ B s for all σ. They are also characterized by <ref type="bibr" target="#b18">Maron et al. (2019a)</ref> and belong to a linear space of dimension b(k s ). We illustrate this simple architecture in Fig. <ref type="figure">1</ref>.</p><p>In light of the characterization by <ref type="bibr" target="#b18">Maron et al. (2019a)</ref> of linear invariant and equivariant operators described in the previous paragraph, a GNN of the form (1) is described by</p><formula xml:id="formula_4">1+ S s=1 b(k s +d)+2b(k s ) parameters in the invariant case and 1 + S s=1 b(k s + d) + b(k s + 1) + b(k s ) in the equivariant.</formula><p>As mentioned earlier, this number of parameters does not depend on the number of nodes n, and a GNN described by a single set of parameters can be applied to graphs of any size. In particular, we are going to show that a GNN approximates uniformly well a continuous function for several n at once. The function ρ is any locally Lipschitz pointwise non-linearity for which the Universal Approximation Theorem for MLP applies. We denote their set F MLP . This includes in particular any continuous function that is not a polynomial <ref type="bibr" target="#b20">(Pinkus, 1999)</ref>. Among these, we denote the sigmoid ρ sig (x) = e x /(1 + e x ).</p><formula xml:id="formula_5">R n d R n k 1 R n k 2 . . . R n k S F1 F2 FS R n k 1 R n k 2 . . . R n k S ρ(• + B1) ρ(• + B2) ρ(• + BS) H1 H2 HS y ∈ R (invariant) y ∈ R n (equivariant)</formula><p>Figure <ref type="figure">1</ref>: The model of GNNs studied in this paper. For each channel s S, the input tensor is passed through an equivariant operator Fs : R n d → R n ks , a non-linearity with some added equivariant bias Bs, and a final operator Hs that is either invariant (Section 2) or equivariant (Section 3). These GNNs are universal approximators of invariant or equivariant continuous functions (Theorems 1 and 3).</p><p>We denote by N inv. (ρ) (resp. N eq. (ρ)) the class of invariant (resp. equivariant) 1-layer networks of the form (1) (with S and k s being arbitrarily large). Our contributions show that they are dense in the spaces of continuous invariant (resp. equivariant) functions.</p><p>2 The case of invariant functions <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> recently proved that invariant GNNs similar to (1) are universal approximators of continuous invariant functions. As a warm-up, we propose an alternative proof of (a variant of) this result, that will serve as a basis for our main contribution, the equivariant case (Section 3).</p><p>Edit distance. For invariant functions, isomorphic graphs are undistinguishable, and therefore we work with a set of equivalence classes of graphs, where two graphs are equivalent if isomorphic. We define such a set for any number n n max of nodes and bounded G</p><formula xml:id="formula_6">G inv. def. = O (G) ; G ∈ R n d with n n max , G R ,</formula><p>where we recall that O (G) = {σ G ; σ ∈ O} is the set of every permuted versions of G, here seen as an equivalence class.</p><p>We need to equip this set with a metric that takes into account graphs with different number of nodes.</p><p>A distance often used in the literature is the graph edit distance <ref type="bibr" target="#b25">(Sanfeliu and Fu, 1983)</ref>. It relies on defining a set of elementary operations o and a cost c(o) associated to each of them, here we consider node addition and edge weight modification. The distance is then defined as</p><formula xml:id="formula_7">d edit (O (G 1 ) , O (G 2 )) def. = min (o1,...,o k )∈P(G1,G2) k i=1 c(o i )<label>(2)</label></formula><p>where</p><formula xml:id="formula_8">P(G 1 , G 2 ) contains every sequence of operation to transform G 1 into a graph isomor- phic to G 2 , or G 2 into G 1 .</formula><p>Here we consider c(node_addition) = c for some constant c &gt; 0, c(edge_weight_change) = |w − w | where the weight change is from w to w , and "edge" refers to any element of the tensor</p><formula xml:id="formula_9">G ∈ R n d . Note that, if we have d edit (O (G 1 ) , O (G 2 )) &lt; c</formula><p>, then G 1 and G 2 have the same number of nodes, and in that case</p><formula xml:id="formula_10">d edit (O (G 1 ) , O (G 2 )) = min σ∈On G 1 − σ G 2 1</formula><p>, where • 1 is the element-wise 1 norm, since each edge must be transformed into another.</p><p>We denote by C(G inv. , d edit ) the space of real-valued functions on G inv. that are continuous with respect to d edit , equipped with the infinity norm of uniform convergence. We then have the following result. Theorem 1. For any ρ ∈ F MLP , N inv. (ρ) is dense in C(G inv. , d edit ).</p><p>Comparison with <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref>. The two proofs are however different: their proof relies on the construction of a basis of invariant polynomials and on classical universality of MLPs, while our proof is a direct application of Stone-Weierstrass theorem for algebras of real-valued functions. See the next subsection for details.</p><p>One improvement of our result with respect to the one of <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref> is that it can handle graphs of varying sizes. As mentioned in the introduction, a single set of parameters defines a GNN that can be applied to graphs of any size. Theorem 1 shows that any continuous invariant function is uniformly well approximated by a GNN on the whole set G inv. , that is, for all numbers of nodes n n max simultaneously. On the contrary, <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> work with a fixed n, and it does not seem that their proof can extend easily to encompass several n at once. A weakness of our proof is that it does not provide an upper bound on the order of tensorization k s . Indeed, through Noether's theorem on polynomials, the proof of <ref type="bibr" target="#b19">Maron et al. (2019b)</ref> shows that k s n d (n d − 1)/2 is sufficient for universality, which we cannot seem to deduce from our proof. Moreover, they provide a lower-bound k s n d below which universality cannot be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sketch of proof of Theorem 1</head><p>The proof for the invariant case will serve as a basis for the equivariant case in the Section 3. It relies on Stone-Weierstrass theorem, which we recall below. Theorem 2 (Stone-Weierstrass <ref type="bibr" target="#b23">(Rudin (1991)</ref>, Thm. 5.7)). Suppose X is a compact Hausdorff space and A is a subalgebra of the space of continuous real-valued functions C(X) which contains a non-zero constant function. Then A is dense in C(X) if and only if it separates points, that is, for all x = y in X there exists f ∈ A such that f (x) = f (y).</p><p>We will construct a class of GNNs that satisfy all these properties in G inv. . As we will see, unlike classical applications of this theorem to e.g. polynomials, the main difficulty here will be to prove the separation of points. We start by observing that G inv. is indeed a compact set for d edit .</p><p>Properties of (G inv. , d edit ). Let us first note that the metric space (G inv. , d edit ) is Hausdorff (i.e. separable, all metric spaces are). For each</p><formula xml:id="formula_11">O (G 1 ) , O (G 2 ) ∈ G inv. we have: if d edit (O (G 1 ) , O (G 2 )) &lt; c,</formula><p>then the graphs have the same number of nodes, and in that case</p><formula xml:id="formula_12">d edit (O (G 1 ) O (G 2 )) G 1 − G 2 1 . Therefore, the embedding G → O (G) is continuous (locally Lipschitz). As the continuous image of the compact nmax n=1 G ∈ R n d ; G R , the set G inv. is indeed compact.</formula><p>Algebra of invariant GNNs. Unfortunately, N inv. (ρ) is not a subalgebra. Following <ref type="bibr" target="#b13">Hornik et al. (1989)</ref>, we first need to extend it to be closed under multiplication. We do that by allowing Kronecker products inside the invariant functions:</p><formula xml:id="formula_13">f (G) = S s=1 H s ρ (F s1 [G] + B s1 ) ⊗ . . . ⊗ ρ (F sTs [G] + B sTs ) + b<label>(3)</label></formula><p>where F st yields k st -tensors, H s : R n t k st → R are invariant, and B st are equivariant bias. By (σ G) ⊗ (σ G ) = σ (G ⊗ G ), they are indeed invariant. We denote by N ⊗ inv. (ρ) the set of all GNNs of this form, with S, T s , k st arbitrarily large. Lemma 1. For any locally Lipschitz ρ,</p><formula xml:id="formula_14">N ⊗ inv. (ρ) is a subalgebra in C(G inv. , d edit ).</formula><p>The proof, presented in Appendix A.1.1 follows from manipulations of Kronecker products.</p><p>Separability. The main difficulty in applying Stone-Weierstrass theorem is the separation of points, which we prove in the next Lemma. Lemma 2. N ⊗ inv. (ρ sig ) separates points.</p><p>The proof, presented in Appendix A.1.2, proceeds by contradiction: we show that two graphs G, G that coincides for every GNNs are necessarily permutation of each other. Applying Stone-Weierstrass theorem, we have thus proved that N ⊗ inv. (ρ sig ) is dense in C(G inv. , d edit ). Then, following <ref type="bibr" target="#b13">Hornik et al. (1989)</ref>, we go back to the original class N inv. (ρ), by applying: (i) a Fourier approximation of ρ sig , (ii) the fact that a product of cos is also a sum of cos, and (iii) an approximation of cos by any other non-linearity. The following Lemma is proved in Appendix A.1.3, and concludes the proof of Thm 1. Lemma 3. We have the following:</p><formula xml:id="formula_15">(i) N ⊗ inv. (cos) is dense in N ⊗ inv. (ρ sig ); (ii) N ⊗ inv. (cos) = N inv. (cos); (iii) for any ρ ∈ F MLP , N inv. (ρ) is dense in N inv. (cos).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The case of equivariant functions</head><p>This section contains our main contribution. We examine the case of equivariant functions that return a vector f (G) ∈ R n when G has n nodes, such that f (σ G) = σ f (G). In that case, isomorphic graphs are not equivalent anymore. Hence we consider a compact set of graphs</p><formula xml:id="formula_16">G eq. def. = G ∈ R n d ; n n max , G R ,</formula><p>Like the invariant case, we consider several numbers of nodes n n max and will prove uniform approximation over them. We do not use the edit distance but a simpler metric:</p><formula xml:id="formula_17">d(G, G ) = G − G if G and G have the same number of nodes, ∞ otherwise.</formula><p>for any norm • on R n d .</p><p>The set of equivariant continuous functions is denoted by C eq. (G eq. , d), equipped with the infinity norm f ∞ = sup G∈Geq. f (G) ∞ . We recall that N eq. (ρ) ⊂ C eq. (G eq. , d) denotes one-layer GNNs of the form (1), with equivariant output operators H s . Our main result is the following. Theorem 3. For any ρ ∈ F MLP , N eq. (ρ) is dense in C eq. (G eq. , d).</p><p>The proof, detailed in the next section, follows closely the previous proof for invariant functions, but is significantly more involved. Indeed, the classical version of Stone-Weierstrass only provides density of a subalgebra of functions in the whole space of continuous functions, while in this case C eq. (G eq. , d) is already a particular subset of continuous functions. On the other hand, it seems difficult to make use of fully general versions of Stone-Weierstrass theorem, for which some questions are still open <ref type="bibr" target="#b11">(Glimm, 1960)</ref>. Hence we prove a new, specialized Stone-Weierstrass theorem for equivariant functions (Theorem 4), obtained with a non-trivial adaptation of the constructive proof by <ref type="bibr" target="#b2">Brosowski and Deutsch (1981)</ref>.</p><p>Like the invariant case, our theorem proves uniform approximation for all numbers of nodes n n max at once by a single GNN. As is detailed in the next subsection, our proof of the generalized Stone-Weierstrass theorem relies on being able to sort the coordinates of the output space R n , and therefore our current proof technique does not extend to high-order output R n (graph to graph mappings), which we leave for future work. For the same reason, while the previous invariant case could be easily extended to invariance to subgroups of O n , as is done by <ref type="bibr" target="#b19">Maron et al. (2019b)</ref>, for the equivariant case our theorem only applies when considering the full permuation group O n . Nevertheless, our generalized Stone-Weierstrass theorem may be applicable in other contexts where equivariance to permutation is a desirable property.</p><p>Comparison with <ref type="bibr" target="#b26">(Sannai et al., 2019)</ref>. <ref type="bibr" target="#b26">Sannai et al. (2019)</ref> recently proved that equivariant NNs acting on point clouds are universal, that is, for d = 1 in our notations. Despite the apparent similarity with our result, there is a fundamental obstruction to extending their proof to high-order input tensors like graphs. Indeed, it strongly relies on Theorem 2 of <ref type="bibr" target="#b33">(Zaheer et al., 2017)</ref> that characterizes invariant functions R n → R, which is no longer valid for high-order inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sketch of proof of Theorem 3: an equivariant version of Stone-Weierstrass theorem</head><p>We first need to introduce a few more notations. For a subset I ⊂ [n], we define O I def.</p><p>= {σ ∈ O n ; ∃i ∈ I, j ∈ I c , σ(i) = j or σ(j) = i} the set of permutations that exchange at least one index between I and I c . Indexing of vectors (or multivariate functions) is denoted by brackets, e.g.</p><p>[x] I or [f ] I , and inequalities x a are to be understood element-wise.</p><p>A new Stone-Weierstrass theorem. We define the "multiplication" of two multivariate functions using the Hadamard product , i.e. the component-wise multiplication. Since (σ x) (σ x ) = σ (x x ), it is easy to see that C eq. (G eq. , d) is closed under multiplication, and is therefore a (strict) subalgebra of the set of all continuous functions that return a vector in R n for an input graph with n nodes. As mentioned before, because of this last fact we cannot directly apply Stone-Weierstrass theorem. We therefore prove a new generalized version. Theorem 4 (Stone-Weierstrass for equivariant functions). Let A be a subalgebra of C eq. (G eq. , d), such that A contains the constant function 1 and:</p><p>-(Separability) for all G, G ∈ G eq. with number of nodes respectively n and n such that</p><formula xml:id="formula_18">G / ∈ O(G ), for any k ∈ [n], k ∈ [n ], there exists f ∈ A such that [f (G)] k = [f (G )] k ;</formula><p>-("Self"-separability) for all number of nodes n n max , I ⊂ [n], G ∈ G eq. with n nodes that has no self-isomorphism in O I , and</p><formula xml:id="formula_19">k ∈ I, ∈ I c , there is f ∈ A such that [f (G)] k = [f (G)] .</formula><p>Then A is dense in C eq. (G eq. , d).</p><p>In addition to a "separability" hypothesis, which is similar to the classical one, Theorem 4 requires a "self"-separability condition, which guarantees that f (G) can have different values on its coordinates under appropriate assumptions on G. We give below an overview of the proof of Theorem 4, the full details can be found in Appendix B.</p><p>Our proof is inspired by the one for the classical Stone-Weierstrass theorem (Thm. 2) of <ref type="bibr" target="#b2">Brosowski and Deutsch (1981)</ref>. Let us first give a bit of intuition on this earlier proof. It relies on the explicit construction of "step"-functions: given two disjoint closed sets A and B, they show that A contains functions that are approximately 0 on A and approximately 1 on B. Then, given a function f : X → R (non-negative w.l.o.g.) that we are trying to approximate and ε &gt; 0, they define A k = {x ; f (x) (k − 1/3)ε} and B k = {x ; f (x) (k + 1/3)ε} as the lower (resp. upper) level sets of f for a grid of values with precision ε. Then, taking the step-functions f k between A k and B k , it is easy to prove that f is well-approximated by g = ε k f k , since for each x only the right number of f k is close to 1, the others are close to 0.</p><p>The situation is more complicated in our case. Given a function f ∈ C eq. (G eq. , d) that we want to approximate, we work in the compact subset of G eq. where the coordinates of f are ordered, since by permutation it covers every case:</p><formula xml:id="formula_20">G f def. = G ∈ G eq. ; if G ∈ R n d : [f (G)] 1 [f (G)] 2 . . . [f (G)] n .</formula><p>Then, we will prove the existence of step-functions such that: when A and B satisfy some appropriate hypotheses, the step-function is close to 0 on A, and only the first coordinates are close to 1 on B, the others are close to 0. Indeed, by combining such functions, we can approximate a vector of ordered coordinates (Fig. <ref type="figure" target="#fig_1">2</ref>). The construction of such step-functions is done in Lemma 7. Finally, we consider modified level-sets</p><formula xml:id="formula_21">A n, k def. = G ∈ G f ∩ R n d ; [f (G)] − [f (G)] +1 (k − 1/3)ε ∪ n =n G f ∩ R (n ) d , B n, k def. = G ∈ G f ∩ R n d ; [f (G)] − [f (G)] +1 (k + 1/3)ε</formula><p>that distinguish "jumps" between (ordered) coordinates. We define the associated step-functions f n, k , and show that g = ε k,n, f n, k is a valid approximation of f .</p><p>End of the proof. The rest of the proof of Theorem 3 is similar to the invariant case. We first build an algebra of GNNs, again by considering nets of the form (3), where we replace the H s 's by equivariant linear operators in this case. We denote this space by N ⊗ eq. (ρ). Lemma 4. N ⊗ eq. (ρ) is a subalgebra of C eq. (G eq. , d).</p><p>The proof, presented in Appendix A.2.1, is very similar to that of Lemma 1. Then we show the two separation conditions for equivariant GNNs. Lemma 5. N ⊗ eq. (ρ sig. ) satisfies both the separability and self-separability conditions. The proof is presented in Appendix A.2.2. The "normal" separability is in fact equivalent to the previous one (Lemma 2), since we can construct an equivariant network by simply stacking an invariant network on every coordinate. The self-separability condition is proved in a similar way. Finally we go back to N eq. (ρ) in exactly the same way. The proof of Lemma 6 is exactly similar to that of Lemma 3 and is omitted. Lemma 6. We have the following: (i) N ⊗ eq. (cos) is dense in N ⊗ eq. (ρ sig ); (ii) N ⊗ eq. (cos) = N eq. (cos); (iii) for any ρ ∈ F MLP , N eq. (ρ) is dense in N eq. (cos). This section provides numerical illustrations of our findings on simple synthetic examples. The goal is to examine the impact of the tensorization orders k s and the width S. The code is available at https://github.com/nkeriven/univgnn. We emphasize that the contribution of the present paper is first and foremost theoretical, and that, like MLPs with a single hidden layer, we cannot expect the shallow GNNs (1) to be state-of-theart and compete with deep models, despite their universality. A benchmarking of deep GNNs that use invariant and equivariant linear operators is done in <ref type="bibr" target="#b18">(Maron et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical illustrations</head><p>We consider graphs, represented using their adjacency matrices (i.e. 2-ways tensor, so that d = 2). The synthetic graphs are drawn uniformly among 5 graph topologies (complete graph, star, cycle, path or wheel) with edge weights drawn independently as the absolute value of a centered Gaussian variable. Since our approximation results are valid for several graph sizes simultaneously, both training and testing datasets contain 1.4 • 10 4 graphs, half with 5 nodes and half with 10 nodes.</p><p>The training is performed by minimizing a square Euclidean loss (MSE) on the training dataset. The minimization is performed by stochastic gradient descent using the ADAM optimizer <ref type="bibr" target="#b14">(Kingma and Ba, 2014)</ref>. We consider two different regression tasks: (i) in the invariant case, the scalar to predict is the geodesic diameter of the graph, (ii) in the equivariant case, the vector to predict assigns to each node the length of the longest shortest-path emanating from it. While these functions can be computed using polynomial time all-pairs shortest paths algorithms, they are highly non-local, and are thus challenging to learn using neural network architectures. The GNNs (1) are implemented with a fixed tensorization order k s = k ∈ {1, 2, 3} and ρ = ρ sig. .</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows that, on these two cases, when increasing the width S, the out-of-sample prediction error quickly stagnates (and sometime increasing too much S can slightly degrade performances by making the training harder). In sharp contrast, increasing the tensorization order k has a significant impact and lowers this optimal error value. This support the fact that universality relies on the use of higher tensorization order. This is a promising direction of research to integrate higher order tensors withing deeper architecture to better capture complex functions on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proved the universality of a class of one hidden layer equivariant networks. Handling this vector-valued setting required to extend the classical Stone-Weierstrass theorem.</p><p>It remains an open problem to extend this technique of proof for more general equivariant networks whose outputs are graph-valued, which are useful for instance to model dynamic graphs using recurrent architectures <ref type="bibr" target="#b0">(Battaglia et al., 2016)</ref>. Another outstanding open question, formulated in <ref type="bibr" target="#b19">(Maron et al., 2019b)</ref>, is the characterization of the approximation power of networks whose tensorization orders k s inside the layers are bounded, since they are much more likely to be implemented on large graphs in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Finally, we denote by O(G)def.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of strategy of proof for the equivariant Stone-Weierstrass theorem (Theorem 4). Considering a function f that we are trying to approximate and a graph G for which the coordinates of f (G) are sorted by decreasing order, we approximate f (G) by summing step-functions fi, whose first coordinates are close to 1, and otherwise close to 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MSE results after 150 epochs, in the invariant (top) and equivariant (bottom) cases, averaged over 5 experiments. Dashed lines represent the testing error.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interaction Networks for Learning about Objects, Relations and Physics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information and Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4509" to="4517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning: Going beyond Euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An elementary proof of the Stone-Weierstrass Theorem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brosowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Mathematical Society</title>
				<meeting>the American Mathematical Society</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Supervised Community Detection with Line Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mahematics of Control, Signals, and Systems</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic deep networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2019</title>
				<meeting>ICML 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information and Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Protein Interface Prediction using Graph Convolutional Networks. Nips</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6512" to="6521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep symmetry networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2537" to="2545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Stone-Weierstrass Theorem for C * -Algebras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Glimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="244" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02879</idno>
		<title level="m">Deep models of interactions across sets</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilayer Feedforward Networks are Universal Approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the generalization of equivariance and convolution in neural networks to the action of compact groups</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03690</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Invariant and Equivariant Graph Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the Universality of Invariant Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Approximation theory of the MLP model in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="195" />
			<date type="published" when="1999-05">May. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Equivariance through parameter-sharing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2892" to="2901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<title level="m">Functional Analysis</title>
				<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>arxiv:1806.01242</idno>
		<title level="m">Graph networks as learnable physics engines for inference and control</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A distance measure between attributed relational graphs for pattern recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on systems, man, and cybernetics</title>
				<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Universal approximations of permutation invariant/equivariant functions by deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sannai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cordonnier</surname></persName>
		</author>
		<idno>ArXiv: 1903.01939</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Symmetries and discriminability in feedforward network architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="816" to="826" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation theory and invariant neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete applied mathematics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="33" to="60" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Universal approximations of invariant maps by neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
		<idno>ArXiv: 1804.10306</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno>ArXiv: 1812.08434</idno>
		<title level="m">Graph Neural Networks: A Review of Methods and Applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
