<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Practical Temporal Prefetching With Compressed On-Chip Metadata</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Krishnendra</forename><surname>Nathella</surname></persName>
							<email>krishnendra.nathella@arm.com</email>
							<idno type="ORCID">0000-0002-8091-2389</idno>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Pabst</surname></persName>
							<email>pabstmatthew@gmail.com</email>
							<idno type="ORCID">0000-0003-2456-9763</idno>
						</author>
						<author>
							<persName><forename type="first">Dam</forename><surname>Sunwoo</surname></persName>
							<email>dam.sunwoo@arm.com</email>
							<idno type="ORCID">0000-0002-7291-0340</idno>
						</author>
						<author>
							<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
							<email>akanksha@cs.utexas</email>
							<idno type="ORCID">0000-0003-3317-5037</idno>
						</author>
						<author>
							<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
							<email>lin@cs.utexas</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78712-1139</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>78735</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Practical Temporal Prefetching With Compressed On-Chip Metadata</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TC.2021.3065909</idno>
					<note type="submission">received 23 July 2020; revised 26 January 2021; accepted 7 February 2021. Date of publication 12 March 2021; date of current version 10 October 2022.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Memory systems</term>
					<term>prefetching</term>
					<term>temporal prefetching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal prefetchers are powerful because they can prefetch irregular sequences of memory accesses, but temporal prefetchers are commercially infeasible because they store large amounts of metadata in DRAM. This article presents Triage, the first temporal data prefetcher that does not require off-chip metadata. Triage builds on two insights: (1) Metadata are not equally useful, so the less useful metadata need not be saved, and (2) for irregular workloads, it is more profitable to use portions of the LLC to store metadata than data. We also introduce novel schemes to identify useful metadata, to compress metadata, and to determine the fraction of the LLC to dedicate for metadata. Using an industrial-strength simulator running irregular workloads on a single-core system, we show that at a prefetch degree of 4, Triage improves performance by 41.1 percent compared to a baseline with no prefetching, whereas BO, a state-of-the-art prefetcher that uses only on-chip metadata, sees only 10.9 percent improvement. Compared with MISB, a temporal prefetcher that uses off-chip metadata, Triage provides a design alternative that reduces memory traffic by an order of magnitude (260.8 percent extra traffic for MISB at degree 1 versus 56.9 percent for Triage), while reducing coverage by 20 percent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D ATA prefetchers are important mechanisms for hiding the long latency of DRAM accesses. Most commercial prefetchers perform some form of strided or spatial prefetching, because such prefetchers provide significant benefits while incurring low implementation costs. By contrast, temporal prefetchers are alluring because they can learn arbitrary sequences of repeated memory references, so they are effective for workloads with irregular memory accesses, including those that arise from pointer-based data structures. But because these arbitrary sequences have to be memorized, temporal prefetchers require megabytes of metadata that must be stored in DRAM. Of course, going to DRAM to retrieve metadata incurs high latency, consumes DRAM bandwidth, and increases energy consumption, so the primary challenge has been to effectively manage this off-chip metadata.</p><p>Early solutions amortized the cost of off-chip metadata accesses across multiple prefetches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. In 2013, the Irregular Stream Buffer (ISB) represented the metadata as an address mapping, which allowed portions of the metadata to be cached on chip, with the contents of this metadata cache synchronized with the contents of the TLB. More recently, the Managed ISB (MISB) extended the ISB by introducing a more efficient fine-grained metadata management scheme that includes a metadata prefetcher.</p><p>Unfortunately, all of these solutions still have important limitations: <ref type="bibr" target="#b0">(1)</ref> Even MISB has metadata traffic of 260.8 percent, so its performance suffers in bandwidth-constrained environments, and it incurs high energy costs in any environment; (2) they add hardware complexity because they require changes to the memory interface and communication with the operating system.</p><p>In this paper, we present Triage, 1 the first temporal prefetcher that requires no off-chip metadata. Our solution builds on three observations. 1) For most workloads, the vast majority of prefetches use just a small fraction of the metadata, so only the most frequently used metadata needs to be stored. 2) For many workloads, the last-level cache (LLC) typically has lower utility than an effective prefetcher, so for irregular workloads, portions of the LLC can be more profitably used to store temporal prefetcher metadata instead of storing data. For example, for the irregular subset of SPEC2006, reducing the LLC by 1 MB reduces performance by 7.4 percent, but a state-of-the-art irregular prefetcher with unlimited resources can improve performance by 34.0 percent. Therefore, if we can distinguish the important metadata from the unimportant metadata, we can profitably use portions of the LLC to store important prefetcher metadata. 3) For regular accesses, temporal prefetchers store metadata highly inefficiently, so further compression is possible through the use of a novel metadata representation. For example, a temporal prefetcher would represent a regular sequence of n addresses as n pairs of correlated addresses, which could be represented more compactly as a 3-tuple, (address, stride, length). Thus, our Triage prefetcher re-purposes a portion of the LLC as a metadata store, and any metadata that cannot be kept in the metadata store is simply discarded. To identify important metadata, Triage uses the Hawkeye replacement policy <ref type="bibr" target="#b3">[4]</ref>, which provides significant performance benefits for small metadata stores, as it identifies frequently accessed metadata over a long history of time. Of course, the ideal size of this metadata store varies by workload, so we also introduce a dynamic cache partitioning scheme that determines the amount of the LLC cache that should be provisioned for metadata entries. Finally, we introduce a new metadata representation that compactly represents strided access patterns to significantly compress the metadata for regular workloads.</p><p>By forsaking off-chip metadata and by intelligently managing on-chip metadata, Triage offers different tradeoffs than state-of-the-art temporal prefetchers. For example, Triage reduces off-chip traffic overhead to 59.3 percent (compared with 260.8 percent for MISB), it reduces energy consumption for metadata accesses by 4-22Â, and it offers a much simpler hardware design. In Section 4, we show that in bandwidth-rich environments these benefits come at the cost of lower performance (due to limited metadata), but in bandwidth-constrained environments, they translate to significantly better performance.</p><p>This paper makes the following contributions: 2   We introduce Triage, the first PC-localized 3 temporal data prefetcher that does not use off-chip metadata.</p><p>Triage reuses a portion of the LLC for storing prefetcher metadata, and it includes an adaptive policy for dynamically provisioning the size of the metadata store at a fine granularity. We explore metadata organizations that are best suited for storing metadata in the LLC, and we find that in this new setting, tables <ref type="bibr" target="#b4">[5]</ref> are the most compact data structure for tracking correlated addresses. The use of tables represents a return to simplicity for temporal prefetchers as recent solutions propose increasingly complex metadata representations to manage off-chip metadata. We also introduce a compressed representation for regular accesses to reduce the metadata table's footprint. We evaluate Triage using a highly accurate proprietary simulator for single-core simulations and the ChampSim simulator for multi-core simulations.  <ref type="bibr" target="#b5">[6]</ref>). On a 16-core system running multiprogrammed irregular SPEC workloads, where bandwidth is more precious, Triage's speedup is 6.9 percent, compared to 4.9 percent for MISB. -Triage also works well as part of a hybrid prefetcher that combines a regular prefetcher with a temporal prefetchers (27.8 percent for Triage+BO versus 4.3 percent for BO alone on single-core systems, and 10.9 percent for BO+Triage versus 4.7 percent for BO alone on 4-core systems). -On a 4-core system running CloudSuite server benchmarks, BO+Triage improves performance by 9.7 percent, compared to 3.8 percent for BO alone. We outline and analyze the design space for temporal prefetchers along three dimensions, namely, on-chip storage, off-chip traffic, and overall performance, and we show that Triage provides an attractive design point with previously unexplored tradeoffs. This paper is organized as follows. Section 2 places our work in the context of prior work. Section 3 then describes our solution, and Section 4 then presents our empirical evaluation. Finally, we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We now discuss related work in data prefetching. We start by contrasting our work with other temporal prefetchers before briefly discussing other classes of prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal Prefetching</head><p>Temporal prefetchers are quite general because they learn address correlations, that is, correlations between consecutive memory accesses. Chilimbi et al., report that temporal streams, which are sequences of correlated address pairs, are found extensively in both scientific and commercial workloads <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Unfortunately, considerable state is required to memorize correlations among addresses.</p><p>One class of irregular prefetchers reduce this metadata requirement by forgoing address correlation to learn weaker forms of correlation, such as delta correlation <ref type="bibr" target="#b8">[9]</ref>, tag correlation <ref type="bibr" target="#b9">[10]</ref>, or context-address pair correlation <ref type="bibr" target="#b10">[11]</ref>. However, these simplifications limit the scope of memory access patterns that can be learned.</p><p>A second class of prefetchers exploit address correlation by storing metadata in off-chip memory <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>. These prefetchers use novel ways to reduce the overhead of off-2. This paper improves upon the original Triage design <ref type="bibr" target="#b2">[3]</ref> in two ways. First, it employs a new dynamic partitioning scheme that is simpler and that allows the LLC to be partitioned between data and at metadata at a finer granularity. Second, it reduces the metadata table's footprint by employing a novel compressed representation for regular accesses.</p><p>3. PC localization is a method of creating more predictable reference streams by separating streams according to the address of the instruction that issued the load. chip memory accesses, but the use of off-chip metadata has limited the commercial viability of such prefetchers.</p><p>Triage is the first data prefetcher that reaps the benefit of PC-localized address correlation-the most powerful form of temporal prefetching <ref type="bibr" target="#b11">[12]</ref>-without using any off-chip metadata. While elements of Triage borrow from temporal prefetchers that use off-chip metadata, Triage is designed with a completely different design goal, which is to prioritize the efficiency of the on-chip metadata store. Therefore, Triage rethinks many design decisions for the on-chip setting, removing complexity where possible and adding new design components where necessary.</p><p>We now provide more details about how Triage's design differs from temporal prefetchers that use off-chip metadata. We classify existing temporal prefetchers into three categories based on their off-chip metadata organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Table-Based Temporal Prefetchers</head><p>Joseph and Grunwald introduced the idea of prefetching correlated addresses in 1997 with their Markov Prefetcher <ref type="bibr" target="#b4">[5]</ref>. The Markov Prefetcher uses a table to record multiple possible successors for each address, along with a probability for each successor, but unfortunately, it was too large to be stored on-chip despite optimizations that reduced the size of the table <ref type="bibr" target="#b9">[10]</ref>.</p><p>Therefore, early temporal prefetchers explore designs that reduce the traffic and latency costs of accessing an offchip Markov table <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. For example, Solihin et al., redundantly store a chain of successors in each off-chip table entry, which increases table size but amortizes the cost of fetching metadata for temporal streams by grouping them in a single off-chip access. Triage also uses a tablebased organization, but there are two main differences. First, Triage uses PC-localization, which improves coverage and accuracy and eliminates the need to track multiple successors in each table entry, reducing table sizes by 2Â to 4 Â . Second, Triage uses a customized replacement policy that identifies the most useful table entries, removing the need for off-chip metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">GHB-Based Temporal Prefetchers</head><p>Wenisch et al. find that tables are not ideal for organizing off-chip metadata because temporal streams can have highly variable lengths <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Their STMS prefetcher <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> instead uses a global history buffer to record a history of past memory accesses in an off-chip circular buffer. The GHB reduces the latency of off-chip metadata accesses by amortizing the cost of off-chip metadata lookup over long temporal streams, and it reduces metadata traffic by probabilistically updating the off-chip structures. Somogyi et al., build on STMS to combine spatial and temporal streams with their STeMs prefetcher <ref type="bibr" target="#b14">[15]</ref>.</p><p>While the GHB improves significantly over table-based solutions, it suffers from three drawbacks that are addressed by Triage: <ref type="bibr" target="#b0">(1)</ref> The GHB makes it infeasible to combine address correlation with PC-localization, which is a technique to improve predictability by correlating addresses that belong to the same PC, (2) its metadata cannot be cached because it is organized as a FIFO buffer, and (3) it incurs metadata traffic overhead of 200-400 percent.</p><p>Furthermore, Triage's integrated stream representation differs from STeMs as it is more flexible-spatial streams in Triage's integrated representation can be of any length, whereas STeMs aligns them at page boundaries-and it is simpler because it does not need to maintain timing metadata to orchestrate prefetches from off-chip metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Irregular Stream Buffer</head><p>The Irregular Stream Buffer (ISB) combines address correlation with PC-localization by proposing a new off-chip metadata organization <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In particular, ISB maps PClocalized correlated address pairs to consecutive addresses in a new address space, called the structural address space. Furthermore, ISB caches a portion of the physical-to-structural address mappings on chip by synchronizing the contents of the on-chip metadata cache with the TLB and by hiding the latency of off-chip metadata accesses during TLB misses. While ISB significantly improves coverage and accuracy of temporal prefetchers, it still incurs metadata traffic overheads of 200-400 percent, and its metadata cache utilization is quite poor due to the absence of spatial locality in the metadata cache.</p><p>MISB <ref type="bibr" target="#b5">[6]</ref> addresses these issues by divorcing ISB's metadata cache from the TLB. In particular, MISB manages the metadata cache at a fine granularity, and it hides the latency of off-chip metadata accesses by employing highly accurate metadata prefetching. As a result, MISB reduces the traffic overhead of temporal prefetchers to 260.8 percent. Like MISB, Triage uses fine-grained metadata caching, but there are several differences between MISB and Triage: (1) Triage uses a space-efficient table-based organization that is more suited for on-chip metadata (MISB's metadata footprint is 2Â larger than Triage's metadata footprint because it tracks each correlation in two entries, a physical to structural address mapping, and a structural to physical address mapping), (2) Triage uses a smart metadata management policy for its on-chip metadata, and (3) Triage has no metadata traffic overhead.</p><p>Finally, some prefetchers do store their metadata in onchip caches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, but the metadata storage requirements for these prefetchers is relatively small (hundreds of KB), so there was no question that they would be stored somewhere on chip. By contrast, Triage shows how prefetchers whose metadata are too large to fit on chip can avoid storing off-chip metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-Temporal Prefetching</head><p>Many prefetchers predict sequential <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> and strided <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref> accesses, and while this class of prefetchers has enjoyed commercial success due to their extremely compact metadata, their benefits are limited to regular memory accesses. Some irregular memory accesses can be prefetched by exploiting spatial locality <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, such that recurring spatial patterns can be prefetched across different regions in memory. For example, the SMS prefetcher <ref type="bibr" target="#b16">[17]</ref> uses on-chip tables to correlate spatial footprints with the program counter that first accessed a memory region. These spatial locality-based prefetchers tend to be highly aggressive, issuing prefetches for many lines in a region at once. More importantly, they are limited to a very special class of irregular accesses that does not include access to pointerbased data structures, such as trees and graphs.</p><p>Other prefetchers directly target pointers by either using compiler hints or hardware structures to detect pointers <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>. For example, Content Directed Prefetching <ref type="bibr" target="#b35">[35]</ref> searches the content of cache lines for pointer addresses and eagerly issues prefetches for all pointers. Such prefetchers waste bandwidth as they prefetch many pointers that will not be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR SOLUTION</head><p>Triage repurposes on-chip cache space to store prefetcher metadata, and each metadata entry records PC-localized correlated address pairs. To effectively utilize valuable onchip cache space, Triage considers the following design questions:</p><p>How should metadata be represented to maximize space efficiency? Which metadata entries are likely to be the most useful? How much of the last-level cache should be dedicated to the metadata store?</p><p>We now explain how we address these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metadata Representation</head><p>Triage's metadata representation has two key features. First, unlike state-of-the-art temporal prefetchers, it uses a table to record address pairs. Second, for spatio-temporal streamsstreams which have a mix of spatial and temporal accesses-we employ an Integrated Stream Representation to compactly represent the spatial components.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> explains Triage's table-based organization for a purely temporal stream. In particular, the top side of Fig. <ref type="figure" target="#fig_0">1</ref> shows a stream of memory references that is segregated into two PC-localized streams, and the bottom side shows the conceptual organization of metadata where each entry maps an address to its PC-localized neighbor.</p><p>While tables are a poor choice for organizing off-chip metadata (see Section 2), their space efficiency makes them an ideal choice for organizing on-chip metadata. In particular, compared to other metadata organizations <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, our table-based organization avoids metadata redundancy by representing each correlated address pair only once.</p><p>For spatio-temporal streams, address pairs are an inefficient way to represent the spatial components. For example, the text at the top of Fig. <ref type="figure" target="#fig_1">2</ref> shows a spatio-temporal stream for which a naı ¨ve temporal-only representation results in 6 table entries (one entry for each address pair). This representation is wasteful for the spatial sub-stream B; B þ 1; B þ 2; B þ 3, which can be simply represented by tracking a stride of 1 and a stream length of 3.</p><p>To represent such spatio-temporal streams compactly, we modify Triage to additionally track a stride and stream length in each entry. The table at the bottom of Fig. <ref type="figure" target="#fig_1">2</ref> shows this new integrated representation: Each table entry tracks a PC-localized neighbor, a stride, and a stream length. The spatial sub-stream B; B þ 1; B þ 2; B þ 3 is thus recorded in the first table entry with a stride of 1 and a stream length of 3; temporal sub-streams are marked with a stride of 0 and stream length of 1. This results in a solution with just 3 table entries to represent the entire stream. We refer to this version of Triage as Triage-ISR, where ISR stands for Integrated Stream Representation.</p><p>Of course, Triage-ISR's compression benefits vary with the amount of spatial locality that the stream exhibits. For purely spatial streams, Triage-ISR can reduce metadata requirements by an order of magnitude, whereas for purely temporal streams, Triage-ISR offers no benefits. Section 3.4 provides more details about how this integrated representation is trained and used for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metadata Replacement</head><p>Triage's metadata replacement policy manages the contents of its on-chip metadata store. We build Triage's metadata replacement policy on three observations. First, most metadata reuse can be attributed to a few metadata entries (see Fig. <ref type="figure" target="#fig_2">3</ref>). Second, even among the metadata entries that are frequently reused, fewer still account for prefetches that are not redundant, that is, prefetch requests that do not hit in the cache. Finally, metadata should be managed and evicted at a fine granularity because Triage targets irregular memory accesses, which exhibit poor spatial locality.</p><p>To accomplish these goals, we modify Hawkeye <ref type="bibr" target="#b3">[4]</ref>, a state-of-the-art cache replacement policy, which learns from the optimal solution for past memory references. To emulate the optimal policy for past memory references, Hawkeye examines a long history of past cache accesses (8Â the size of the cache), and it uses a highly efficient algorithm to  reproduce the optimal solution. Fig. <ref type="figure" target="#fig_3">4</ref> shows a high-level overview of Hawkeye, where OPTgen is used to train a PCbased predictor; the predictor learns whether loads by a given load instruction (PC) are likely to hit or miss with the optimal solution. On new cache accesses, the predictor informs the cache whether the line should be inserted with high priority or low priority.</p><p>Because Hawkeye can capture long-term reuse, it is a good fit for Triage, where the replacement policy must not be overwhelmed by the many useless metadata entries. We modify Hawkeye so that the policy is trained positively only when the metadata yields a prefetch that misses in the cache. We accomplish this by delaying Hawkeye's training when the prefetch request associated with a metadata entry is actually issued to memory. If the prefetch request hits in the cache, then the metadata reuse is ignored and is not seen by any component of the Hawkeye policy.</p><p>In Section 3.5, we explain how Triage is able to manage metadata at a finer granularity than the line size of the lastlevel cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adjusting the Size of the Metadata Store</head><p>To avoid interference between application data and metadata, we partition the last-level cache by assigning separate ways to data and metadata. Since different applications require different metadata store sizes, our solution dynamically determines the number of ways that should be allocated to metadata. The partitioning is done at a way granularity, so if the cache has 16 ways, the metadata store can use 0, 1, 2, 3, 4, 5, 6, 7 or 8 ways.</p><p>To determine the number of ways to assign for metadata storage we employ a Bloom Filter <ref type="bibr" target="#b37">[37]</ref>. The Bloom Filter records all the unique metadata entries encountered. In particular, when we add a new entry to the metadata store, we add its trigger address to the Bloom Filter. If the address is not already present in the Bloom Filter-a Bloom Filter Miss-we increment a global counter. We then calculate the number of ways to allocate for metadata storage using the following simple heuristic:</p><formula xml:id="formula_0">No: of metadata ways ¼ ceil</formula><p>Global Counter Number of sets in LLC :</p><p>For example, if there are 32K sets in LLC and the global counter value is 100K, we assign 4 ways for metadata storage. Since Bloom Filters can yield false positives on a query, this scheme can underestimate the number of unique metadata entries. To accommodate false positives, we randomly increase the global counter on Bloom Filter hits with a probablity matching the false positive rate of the bloom filter. The false positive rate can be computed as</p><formula xml:id="formula_1">fp rate ¼ ð1 À e kn=m Þ k ;</formula><p>where k is the number of hash functions in the Bloom filter, n is the number of unique entries and m is the number of bits in the Bloom filter. On each hit in the Bloom filter, we generate a random value r. If r &lt; fp rate, we also increment the global counter. This modification simulates the false positives in the Bloom filter.</p><p>The bloom filter is reset periodically. We find that the reset period does not have a major impact on the effectiveness of this scheme. For our evulation we reset the bloom filter every 30M instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall Operation</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> shows the overall design of Triage, where we see that a portion of the LLC is re-purposed for Triage's metadata store. On every LLC access, the metadata portion of the LLC is probed with the incoming address to check for a possible metadata cache hit 1 . If the metadata entry is found, it is read to generate a prefetch request 2 . Regardless of whether the load resulted in a metadata hit or miss, the Training Unit is updated (as explained below), and the newly trained metadata entry is added (or updated) in the metadata store 3 . The metadata replacement state is updated on metadata misses and metadata hits that generate a successful prefetch 4 , and a bloom filter periodically recomputes the amount of LLC that should be used as a metadata cache 5 . We now explain these operations in more detail.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Training</head><p>The Training Unit (TU) keeps the most recently accessed address for each PC. When a new access B arrives for a given PC, the Training Unit is queried for the last accessed address A by the same PC. Addresses A and B are then considered to be correlated. If subsequent addresses by the same PC exhibit spatial locality-for example, B þ 1; B þ 2; B þ 3-then the stride and stream lengths are updated appropriately in the training unit. The Training Unit entry is transferred to Triage's metadata store when the spatial stream ends; the metadata store is indexed by the first address in the pair (A in this example).</p><p>More concretely, Table <ref type="table" target="#tab_1">1</ref> shows the actions that are performed to train Triage, given an entry A; B; stride; len in the Training Unit. For new entries, stride is initialized to 0, and len is initialized to 1. If the new address is same as the last address-B in this case-then no action is taken. If the new address begins a spatial stream or continues a previously detected spatial stream, then the len parameter is incremented by 1. The beginning of a spatial stream is detected if stride is currently set to 0 and if the stride between the new address and the last address is less than a constant (64 in our case). In all other cases, the Training Unit entry is written out to the metadata store, and it is re-initialized.</p><p>To avoid changing entries due to noisy data, each mapping in Triage's metadata store has an additional 1-bit confidence counter. If the Training Unit determines that A's neighbor differs from the value in the metadata store, then the confidence counter is decremented. If the Training Unit determines that A's neighbor matches the value in the metadata store, then the confidence counter is incremented. The neighbor is changed only when the confidence counter drops to 0. When the neighbor is changed, the corresponding stride and length fields are also updated corresponding to the new neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Prediction</head><p>Upon arrival of a new address A, Triage indexes the metadata by address A to find any available metadata entry. If an entry (say ðA; B; stride; lenÞ) is found, Triage puts all addresses that belong to the stream ðB; stride; lenÞ into a prefetch queue. Future accesses from the same PC trigger the issue of a prefetch request from the head of this prefetch queue. If an entry is not found in the metadata store, nothing is added to the prefetch queue, but Triage still prefetches addresses from the prefetch queue until it is empty.</p><p>For higher degree prefetching-say a degree of d-Triage issues d prefetch requests from the prefetch queue, and it ensures that at least d candidates are appended onto the prefetch queue. For example, if Triage retrieves the entry A; B; stride ¼ 1; length ¼ 2 from the metadata store, it appends, B and B þ 1 on the prefetch queue irrespective of the prefetch degree. However, this entry results in only 2 additions to the prefetch queue, for degrees greater than 2, Triage further retrieves the entry corresponding o B þ 1 from the metadata store to generate additional prefetch requests.</p><p>One drawback of our table-based organization is that higher degree prefetching requires multiple metadata lookups, but this penalty is significantly lower when the metadata resides completely on chip ($20 cycles for accessing each LLC-resident metadata entry versus 150-400 cycles for accessing each off-chip metadata entry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Metadata Replacement Updates</head><p>Our metadata replacement is based on the Hawkeye policy <ref type="bibr" target="#b3">[4]</ref>, and like the Hawkeye policy, our metadata replacement policy is trained on the behavior of a few sampled sets. The metadata replacement predictors are trained on all metadata accesses, except those, that result in redundant prefetches. The replacement predictors are probed on all metadata accesses, including hits and misses, to update the per-metadata-entry replacement state. For more details on how the Hawkeye policy works, we refer the reader to the original paper <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Metadata Partition Updates</head><p>Triage partitions the cache between data and metadata by using way partitioning. The partitions are recomputed every 50,000 metadata accesses. If Triage decides to increase the amount of metadata store, dirty lines are flushed and the newly allocated/deallocated portion of the cache is marked invalid immediately. If Triage decides to decrease the amount of metadata store, lines with metadata entries are marked invalid.</p><p>For shared caches, Triage computes the metadata allocation for each core individually and allots the corresponding portion for each core's metadata. For example, if two cores are sharing a 4MB cache, and if cores 0 and 1 want 768KB and 256KB of metadata, respectively, then Triage allocates 1MB of the shared LLC for metadata, and it partitions the metadata store in a 3:1 ratio among the two cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Hardware Design</head><p>We now describe the detailed hardware changes required for Triage. We first discuss the indexing scheme and the tag lookup logic for the metadata store, and then describe the minimal changes to the baseline data cache replacement logic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Metadata Store Indexing</head><p>Triage's metadata store is addressed by using the first address of each correlated pair. To index into Triage's metadata store, we could use the middle-order bits in the metadata address 4 to determine the setID because like data addresses, the middle bits of metadata addresses are likely to be uniformly distributed in the address space. However, Triage's metadata store does not see such a uniform distribution in the middle-order bits. Fig. <ref type="figure" target="#fig_5">6</ref> explains why. Each row in Fig. <ref type="figure" target="#fig_5">6</ref> represents a metadata entry's address, and different columns represent different portions of the entry's address. We see that since the stream length field is fixed to 6-bits (maximum stream length of 64), long spatial streams get broken down into small streams, and adjacent spatial streams, which differ by a multiple of the stream length, all have identical middle-order bits that map to set 0. Because the middle-order bits of the metadata addresses are not uniformly distributed, a naive set indexing scheme results in an unacceptable number of cache conflicts.</p><p>One solution to avoid conflict misses is to determine setID by using the higher-order bits of the metadata address. However, this solution is undesirable for workloads with predominantly irregular accesses because they tend to have more diversity in middle-order bits than the higher-order bits of their addresses, resulting in many conflict misses.</p><p>To solve this problem, we take inspiration from the Touche compressed cache design <ref type="bibr" target="#b38">[38]</ref>. In particular, we determine the setID of the metadata store by applying an XOR between different portions of the address. Fig. <ref type="figure" target="#fig_6">7</ref> shows how this indexing scheme works, and we find that this approach distributes metadata entries uniformly across sets for a diverse set of workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Metadata Store Tag Lookup</head><p>Each metadata entry in Triage is 42 bits long, but LLC line sizes are typically 64 to 128 bytes. Therefore, Triage's metadata entries within an LLC line must be organized at a fine granularity because metadata entries for irregular prefetchers do not exhibit spatial locality. We store multiple tagged metadata entries within each LLC cache line. For example, for a 64 byte LLC line, we store 12 metadata entries within a cache line. The metadata entries within a cache line are stored in the following format: tag-entry-tag-entry-Á Á Á -tagentry. On a metadata lookup, we first choose a physical LLC cache line from the metadata store, and we then find the relevant metadata entry by comparing the sub-tags within each cache line.</p><p>To store the metadata within 4 bytes, we use a compressed tag. To understand our compressed tag, realize that each physical address has a cache line offset of 6 bits and set_id of 16 bits, and the remaining bits are tags. We use a compression method similar Touche compression cache <ref type="bibr" target="#b38">[38]</ref> to compress the tag to 7 bits. Thus, each metadata entry records the compressed tag of the trigger address and the compressed tag and set_id of the next address, which require a total of 30 bits. 5 We restrict the maximum stride and stream length to be both 64, and we need another 12 bits to store these information, making it total of 42 bits.</p><p>To identify the finer-grain metadata entries within a cache line, we require additional logic in the form of comparators and multiplexors. The extra logic is similar to that used in the Amoeba-Cache <ref type="bibr" target="#b39">[39]</ref> and may incur additional latency or pipeline stages, but only for metadata accesses. We find that penalizing the LLC access latencies for both data and metadata by up to 6 cycles results in minimal performance impact (around 1 percent lower speedup on average for the irregular SPEC workloads).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Data Cache Replacement Logic</head><p>Since the cache is way-partitioned between data and metadata, we only need minor modifications to the data cache replacement logic to support Triage. Every time the partitions are recomputed by Triage, the outcome is stored in a status register. The status register holds the number of ways currently assigned for metadata. During replacement, the replacement logic simultaneously consults the metadata in the tag array and the status register to identify a candidate way for eviction. For example, for a 4-way LLC with SRRIP <ref type="bibr" target="#b40">[40]</ref> replacement policy (2-3 bits per entry storing RRPV values), if Triage recently assigned 2 ways for metadata, on replacement, the status register is consulted to identify that only ways 0 and 1 store data and only the RRPV values for these ways are considered to identify the way with the largest RRPV for eviction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methodology</head><p>We evaluate Triage on single-core configurations using a cycle-level industrial simulator that models ARMv8 AArch64 CPUs and that has been correlated to be highly accurate against commercial CPU designs. The parameters of the CPU and memory system model used in the simulation are shown in Table <ref type="table" target="#tab_2">2</ref>. This model uses a simple memory model with fixed latency, but it models memory bandwidth constraints accurately.  4. Middle bits the bits that precede the bits representing the cache line offset.</p><p>5. The set_id of the trigger address is implicit in a set-associative cache, so it does not need to be stored.</p><p>For multi-core evaluation of Triage, we use Champ-Sim [41], a trace-based simulator that includes an out-oforder core model and a detailed memory system. ChampSim's cache subsystem includes FIFO read and prefetch queues, with demand requests having higher priority than prefetch requests. The main memory model simulates data bus contention, bank contention, and bus turnaround delays; bus contention increases memory latency. Our modeled processor for ChampSim also uses the configuration shown in Table <ref type="table" target="#tab_2">2</ref>. We confirm that the performance trends on the two simulators are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Benchmarks</head><p>We present single-core results for a subset of SPEC2006 benchmarks that are memory bound and are known to have irregular access patterns <ref type="bibr" target="#b11">[12]</ref>. For SPEC benchmarks we use the reference input set. For all single-core benchmarks, we use SimPoints <ref type="bibr" target="#b41">[42]</ref> to find representative regions. Each Sim-Point is warmed up for 200 million instruction and run for 50 million instructions, and we generate at most 10 Sim-Points for each SPEC benchmark.</p><p>We present multi-core results for CloudSuite <ref type="bibr" target="#b42">[43]</ref> and multi-programmed SPEC benchmarks. For CloudSuite, we use the traces provided with the 2 nd Cache Replacement Championship. The traces were generated by running CloudSuite in a full-system simulator to intercept both application and OS instructions. Each CloudSuite benchmark includes 6 samples, where each sample has 100 million instructions. We warm up for 50 million instructions and measure performance for the next 50 million instructions.</p><p>For multi-programmed SPEC simulations, we simulate 4, 8, and 16 cores, such that each core runs a benchmark chosen uniformly randomly from all memory-bound benchmarks, including both regular and irregular programs. Overall, we simulate 80 4-core mixes, 80 8-core mixes, and 80 16-core mixes. Of the 80 mixes, 30 mixes include random mixes of irregular programs only, and the remaining 50 mixes include both regular and irregular programs. For each mix, we simulate the simultaneous execution of Sim-Points of the constituent benchmarks until each benchmark has executed at least 30 million instructions. To ensure that slow-running applications always observe contention, we restart benchmarks that finish early so that all benchmarks in the mix run simultaneously throughout the execution. We warm the cache for 30 million instructions and measure the behavior of the next 30 million instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Prefetchers</head><p>We evaluate Triage against two state-of-the-art on-chip prefetchers, namely, Spatial Memory Streaming (SMS) <ref type="bibr" target="#b16">[17]</ref> and the Best Offset Prefetcher (BO) <ref type="bibr" target="#b28">[28]</ref>. SMS captures irregular patterns by applying irregular spatial footprints across memory regions. BO is a regular prefetcher that won the Second Data Prefetching Championship.</p><p>We also evaluate Triage against existing off-chip temporal prefetchers, namely, Sampled Temporal Memory Streaming (STMS) <ref type="bibr" target="#b0">[1]</ref>, Domino <ref type="bibr" target="#b43">[44]</ref>, and MISB <ref type="bibr" target="#b5">[6]</ref>. STMS, Domino, and MISB represent the state-of-the-art in temporal prefetching. For simplicity, we model idealized versions of STMS and Domino, such that their off-chip metadata transactions incur no latency or traffic penalty. Thus, our performance results for these prefetchers represent the upper bound performance of these prefetchers. For MISB, we faithfully model the latency and traffic of all metadata requests.</p><p>We evaluate two versions of Triage, denoted as Triage and Triage-ISR. The former reproduces the design from our MICRO paper <ref type="bibr" target="#b2">[3]</ref> and does not include the integrated stream representation. It also uses a coarse-grained partitioning scheme that only permits metadata allocations of 0 ways, 4 ways, or 8 ways. Triage-ISR represents the design discussed in Section 3, including the integrated stream representation and the fine-grained partitioning using the Bloom Filter. For static versions of these designs, we pick a fixed metadata store size that gives the best average performance, and we then use this size to statically partition the LLC; we find that the best static metadata store size for a 2MB LLC is 1MB on both simulators.</p><p>All prefetchers train on the L2 access stream-including prefetch requests from L1-and prefetches are inserted into the L2. The metadata is stored in L3. Unless specified, all prefetchers use a prefetch degree of 1, which means that they issue at most one prefetch on every trigger access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison With Prefetchers That Store</head><p>Metadata On Chip Fig. <ref type="figure" target="#fig_7">8</ref> shows that Triage outperforms state-of-the-art prefetchers that use only on-chip metadata. In particular, Triage-ISR-the best Triage configuration-achieves a speedup of 27.3 percent, whereas BO and SMS see a speedup of 4.3 and 2.2 percent, respectively. The comparison among the different configurations of Triage leads to three observations. First, the integrated stream representation improves Triage's performance by 2.7 percent compared to the baseline version of Triage <ref type="bibr" target="#b2">[3]</ref> that does not distinguish between regular and irregular accesses (27.3 percent speedup for Triage-ISR versus 24.6 percent  <ref type="figure">Static</ref>, which shows that it is beneficial to modulate the metadata store size based on a precise estimation of metadata requirements. Finally, the integrated stream representation accentuates the benefit of the dynamic scheme because the benefits of compression vary across benchmarks and the dynamic scheme is able to adapt to these variations. As we will see later, the benefit of our dynamic scheme is even more pronounced in a shared cache setting where there is significantly more cache contention. Triage's superior performance over BO and SMS can be explained by its higher coverage (33.6 percent for Triage versus 4.9 percent for BO and 3.4 percent for SMS) and higher accuracy (77.4 percent for Triage versus 36.2 percent for BO and 36.5 percent for SMS) as shown in Fig. <ref type="figure" target="#fig_8">9</ref>. Triage-ISR also improves coverage over Triage across all benchmarks except astar. 6 This improvement in coverage can be attributed to Triage-ISR's compressed metadata representation, which enables storing more metadata given a smaller metadata store. Fig. <ref type="figure" target="#fig_9">10</ref> shows that the integrated stream representation reduces the metadata requirements by 3 percent on average and up to 12 percent for benchmarks with more regular accesses (sphinx3 and soplex).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Higher Degree Prefetching</head><p>Fig. <ref type="figure" target="#fig_10">11</ref> shows the performance of our prefetchers at different prefetch degrees. As we increase degree from 1 to 4, Triage-ISR's performance grows from 27.3 to 41.1 percent. Increasing the degree beyond 4 does not improve Triage-ISR's performance. By comparison, BO and SMS achieve their best performance at degree 8 and 16, respectively, where they improve performance by 10.9 and 7.9 percent (over a baseline with no prefetcher), respectively. We also see that the gap between Triage and Triage-ISR increases as we increase degree, and with a degree of 4, Triage-ISR improves over Triage by 4.4 percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Hybrid Prefetchers</head><p>Since Triage targets irregular memory accesses, it makes sense to evaluate it as a hybrid with regular memory prefetchers, such as BO. Fig. <ref type="figure" target="#fig_11">12</ref> shows that a BO+Triage-ISR hybrid outperforms BO (27.8 percent speedup for BO+Triage-ISR versus 4.3 percent for BO), which confirms that Triage successfully prefetches lines that BO cannot. We also see that Triage-ISR continues to outperform Triage even in this hybrid setting, which suggests that Triage-ISR's improvement over Triage can be attributed to its better coverage for irregular accesses.</p><p>For completeness, Fig. <ref type="figure" target="#fig_12">13</ref> compares all prefetchers on the remaining memory-intensive SPEC 2006 benchmarks. 7  Because these benchmarks are regular, BO+Triage-ISR does not significantly improve over BO, but Triage's dynamic partitioning ensures that it causes no harm. As we would expect, Triage-ISR achieves high metadata compression for    6. For astar, Triage-ISR has lower accuracy, which results in lower prefetch coverage.</p><p>7. For astar, gcc, and soplex, we show results for the reference inputs which are more regular. these benchmarks (9.6 percent on average and 86 percent for cactus), but because these benchmarks are highly regular and predictable, Triage does not benefit from either the extra metadata space or the extra cache space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Understanding Triage' s Benefits</head><p>We now take a deeper look into the sources of Triage's performance benefits. Fig. <ref type="figure" target="#fig_13">14</ref> shows that the performance benefit of irregular prefetching significantly outweighs the performance loss of reduced LLC capacity. In particular, we see that an optimistic version of Triage-ISR that is given a 1 MB on-chip metadata store in addition to its usual LLC capacity achieves a 34.0 percent speedup. On the other hand, a system with no Triage and a reduced LLC capacity of 1 MB lowers performance by only 7.4 percent. This loss in performance is easily compensated by Triage's benefits, as Triage-ISR sees an overall speedup of 26.3 percent with a fixed 1 MB metadata store and a 1 MB LLC.</p><p>Fig. <ref type="figure" target="#fig_14">15</ref> compares the performance of Triage at different metadata store sizes and with different replacement policies (assuming no loss in LLC capacity). We make two observations. First, with just 1MB of metadata store, Triage achieves 75 percent of the performance of an idealized PC-localized temporal prefetcher, which is significant because typical temporal prefetchers consume tens of megabytes of off-chip storage. This result confirms the main insight of Triage that most prefetches can be attributed to a small percentage of metadata entries. Our second observation is that a smart replacement policy can improve the effectiveness of Triage at smaller metadata cache sizes, but when the metadata cache is sufficiently large (1 MB), the gap between LRU and Hawkeye shrinks. In particular, with a 256 KB metadata cache, Triage with an LRU policy achieves 11.3 percent speedup whereas Triage with the Hawkeye policy sees a 15.2 percent speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison With Prefetchers That Use Off-Chip Metadata</head><p>We now show that compared to temporal prefetchers that use tens of megabytes of off-chip metadata, Triage and Triage-ISR provide a more desirable tradeoff between performance, energy, and off-chip metadata traffic. Fig. <ref type="figure" target="#fig_16">16</ref> compares Triage and Triage-ISR against overly optimistic idealized versions of STMS and Domino and against a realistic version of MISB <ref type="bibr" target="#b5">[6]</ref>. We see that Triage-ISR outperforms idealized STMS and Domino (27.3 percent for Triage-ISR versus 13.4 percent for Domino and 14.0 percent for STMS). Triage doesn't match MISB's 33.3 percent performance, but we see that it incurs much less traffic overhead (bottom graph in Fig. <ref type="figure" target="#fig_16">16</ref>). In particular, compared to a baseline with a 2 MB cache and no prefetching, Triage and Triage-ISR increase traffic by 53.9 and 56.9 percent, respectively, whereas STMS, Domino and MISB increase traffic by 441.8, 441.1, and 260.8 percent, respectively. While Triage and Triage-ISR incur some additional traffic due to inaccurate prefetches, we find that most of their traffic increase can be attributed to an effectively smaller LLC, which results in more demand misses. However, this increase in traffic is a good tradeoff as it facilitates high prefetcher coverage.</p><p>To put these results in context, Fig. <ref type="figure" target="#fig_17">17</ref> compares all temporal prefetchers and the Best Offset (BO) prefetcher along two axes, namely, performance and traffic overhead. STMS, Domino, and MISB all use off-chip metadata, so they incur high off-chip traffic overheads and are in general more complex due to the complications of storing metadata off chip. Triage outperforms STMS and Domino while eliminating metadata overheads. Triage has lower performance than MISB, but it reduces traffic by more than half, offering an attractive design point for temporal prefetching. In fact, Triage's traffic overhead of 59.3 percent is not far from BO's 21.4 percent traffic overhead. BO's traffic overhead can be    attributed to its large volume of inaccurate prefetches on irregular programs. By contrast, Triage is more accurate, but it incurs more traffic due to an effectively smaller LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Energy Evaluation</head><p>Triage is more energy-efficient than other temporal prefetchers. Fig. <ref type="figure" target="#fig_18">18</ref> shows that Triage's metadata accesses are 4-22Â more energy efficient than MISB's. To estimate the energy consumption of Triage's metadata accesses, we count the number of LLC accesses for metadata, assuming 1 unit of energy for each LLC access. To estimate the energy consumption of MISB's memory accesses, we count the number of off-chip metadata accesses and multiply it by the average energy of a DRAM access. Since a DRAM access can consume anywhere from 10Â to 50Â more energy than an LLC access <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, we assume that each DRAM access consumes 25 units of energy, and we add error bars to account for the lower bound (10 units of energy per DRAM access) and upper bound (50 units of energy DRAM access) of MISB's overall energy consumption.</p><p>At higher degrees, Triage's table-based design requires multiple LLC lookups, which will increase its overall energy requirements. In particular, we find that Triage's energy consumption doubles at degree 8, which is still much more energy efficient than MISB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation on Multi-Programmed SPEC Mixes</head><p>We now evaluate Triage and Triage-ISR on multi-core systems and show that (1) the benefits of the dynamic scheme are greater for shared caches, and that (2) in bandwidth-constrained 8-core systems, Triage outperforms MISB due its lower traffic overhead.</p><p>Fig. <ref type="figure" target="#fig_19">19</ref> shows that for multi-programmed mixes of irregular SPEC programs sharing an 8 MB last-level cache on a 4-core system, Triage-ISR-Dynamic is a significant improvement over Triage-ISR-Static. In fact, in this multi-core setting, a static version of Triage-ISR that allocates 4 MB for metadata performs worse than the Best Offset Prefetcher These results can be explained by noting that the LLC is a more valuable resource in shared systems. Triage-ISR-Dynamic works well in this setting because (1) it can modulate the portion of the LLC dedicated to metadata depending on the expected benefit of irregular prefetching, and (2) it can distribute the available metadata store among individual applications such that the application that benefits the most from irregular prefetching gets a larger portion of the metadata store.    Fig. <ref type="figure" target="#fig_20">20</ref> compares the average speedup of Triage-ISR with MISB on 2-core, 4-core, 8-core, and 16-core systems where the cache is shared among different irregular programs. We see that while MISB outperforms Triage-ISR on a 2-core system (7.1 percent for Triage versus 8.9 percent for MISB), Triage-ISR performs better on 8-core and 16-core systems. On a 16-core system, Triage-ISR outperforms MISB significantly (6.9 percent for Triage versus 4.9 percent for MISB). These trends suggest that MISB's performance does not scale well to bandwidth-constrained environments because of its large metadata traffic overheads. By contrast, Triage's performance scales well with higher core counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Comparison on Mixes With Regular Benchmarks</head><p>For completeness, Fig. <ref type="figure" target="#fig_21">21</ref> shows that Triage composes well with BO when the multi-programmed mixes include both regular and irregular programs. For a 4-core system, BO +Triage-ISR improves performance by 14.7 percent, whereas BO alone improves performance by 11.7 percent. Triage-ISR alone does not work well in this setting (3.5 percent speedup) because it cannot prefetch compulsory misses for regular programs.</p><p>The dynamic version of Triage is essential in these scenarios because the cache is shared among irregular programs-which benefit from Triage-and regular programs-which do not benefit from Triage. For regular programs, a static version of Triage would reduce effective LLC capacity without providing much prefetching benefit. Fig. <ref type="figure" target="#fig_1">22</ref> shows the number of ways allocated to each core on this 4-core system, and we see that (1) the total number of ways allocated to the metadata store varies across mixes, and (2) each application receives varying amounts of metadata space depending on a dynamic estimate of the usefulness of the metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation on Server Workloads</head><p>We now use the CloudSuite benchmark suite running on a 4-core system to show that Triage is effective for server workloads (See Fig. <ref type="figure" target="#fig_3">4</ref>.5). On the highly irregular Cassandra, Classification, and Cloud9 benchmarks, Triage-ISR improves performance by 5.3 percent, whereas BO improves performance by 0.5 percent and SMS sees no performance gains. On the more regular Nutch and Streaming benchmarks, BO does well with 8.9 percent performance, whereas Triage sees no performance improvement because temporal prefetchers cannot prefetch compulsory misses.</p><p>In a hybrid setting, BO and Triage compose well, as Triage works well for the irregular benchmarks and BO works well for the regular ones. In particular, a BO+Triage-ISR hybrid outperforms all other prefetchers as it improves performance by 9.7 percent, whereas BO alone improves performance by only 3.8 percent. Fig. <ref type="figure" target="#fig_3">4</ref>.5 also shows that Triage-ISR-Dynamic provides benefit over a static version of Triage-ISR in this setting, so we conclude that our dynamic scheme makes good decisions in trading off cache space for metadata storage. This benefit is most pronounced for the irregular benchmarks (Cassandra, Classification, and Cloud9) where the dynamic version outperforms the static scheme by 2.4 percent (7.6 percent for Triage-ISR-Dynamic versus 5.3 percent for Triage-ISR-Static).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have introduced and evaluated the Triage data prefetcher, which represents a new design point for temporal prefetchers, one that dramatically reduces memory traffic at the expense of some coverage. Our empirical results show that this is a good tradeoff: When compared with stateof-the-art temporal prefetchers that use off-chip metadata, Triage significantly reduces traffic overhead (56.9 percent traffic overhead for Triage versus 260.8 percent for MISB) while modestly reducing performance in bandwidth-rich environments and improving performance in bandwidthconstrained environments. When compared with other (non-temporal) prefetchers that only use on-chip metadata, Triage provides a significant performance advantage (41.1 percent speedup for Triage versus 10.9 percent for BO).</p><p>We have also shown that by removing the use of off-chip metadata, temporal prefetchers can use vastly simpler metadata organizations. In particular, tables are a simpler and more compact representation in this setting, signifying a return to simplicity for temporal prefetchers. We have also  introduced a new metadata representation that compactly stores regular memory accesses, which is beneficial for irregular workloads, because it allows Triage to store a large amount of metadata in a limited amount of space.</p><p>Finally, this paper has made three larger points. First, we have made temporal prefetching practical for commercial deployment by removing the need for maintaining off-chip metadata, which adds complexity and energy overheads. Second, we have made temporal prefetching profitable in bandwidth-constrained environments by significantly reducing its traffic requirements. Indeed, Triage outperforms MISB in bandwidth-constrained 8-core and 16-core systems despite maintaining a metadata store that is orders of magnitude smaller. Finally, and more broadly, we show that on-chip caches can at times be used more profitably for caching metadata instead of data, which suggests future work that explores other kinds of metadata that could be usefully stored in on-chip data caches. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Triage's metadata organization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Triage-ISR uses an integrated representation for spatio-temporal streams (spatial components are marked in red and temporal components are marked in green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Metadata reuse distribution for the mcf benchmark: For an execution with 60K metadata entries, only 15 percent of metadata entries are reused more than 15 times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Triage's metadata replacement is based on the Hawkeye [4] cache replacement policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overview of Triage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Set conflicts in Triage-ISR using middle-order bits as setID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Triage's set indexing scheme avoids conflict misses in the metadata store.</figDesc><graphic url="image-2.png" coords="7,336.76,45.53,156.04,82.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Triage outperforms BO and SMS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Triage improves coverage and accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Metadata achieved by Triage-ISR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Sensitivity to Prefetch degree.</figDesc><graphic url="image-3.png" coords="9,302.23,212.54,225.03,117.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Triage performs well as part of a hybrid prefetcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Results on regular SPEC 2006 benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Breakdown of Triage's performance improvements.</figDesc><graphic url="image-4.png" coords="10,302.57,175.12,224.38,122.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Sensitivity to metadata store size (assuming no loss in LLC capacity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(3.2 percent for Triage-ISR-Static versus 4.8 percent for BO). By contrast, Triage-ISR-Dynamic improves performance by 7.3 percent. At a prefetch degree of 16, Triage-ISR-Dynamic improves performance by 11.1 percent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Triage reduces traffic compared to off-chip temporal prefetchers while offering good performance improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Design space of temporal prefetchers.</figDesc><graphic url="image-5.png" coords="11,41.39,376.27,220.71,129.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Triage is more energy efficient than MISB.</figDesc><graphic url="image-6.png" coords="11,299.45,173.76,230.66,126.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Triage works well on multi-programmed mixes of irregular programs running on a 4-core system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Triage outperforms MISB when bandwidth is constrained.</figDesc><graphic url="image-8.png" coords="12,40.20,45.52,223.20,110.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Triage-ISR works well on multi-programmed mixes of regular and irregular programs running on a 4-core system.</figDesc><graphic url="image-9.png" coords="12,38.84,184.99,225.82,111.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Hao</head><label></label><figDesc>Wu received the BS degree in science technology from Tsinghua University, in July 2010, and the PhD degree in computer science from the University of Texas at Austin, in July 2020. He is a software engineer at Google Platform. His works focuses on computer architecture, especially modeling and improving server performance through software simulation. Krishnendra Nathella received the BTech in electronics and communication neering from SASTRA University, India, in 2010, and the master's degree in computer engineering from the University of Wisconsin at Madison, in 2012. He is a staff research engineer at Arm Research. His research interests include computer architecture, with a focus on developing new micro-architecture to resolve front-end and memory bottlenecks for high-performance processors. Matthew Pabst is currently working toward the graduate at the University of at Austin in the Turing Scholars Honors Program. He has research interests include architecture, systems, and security. Dam Sunwoo received the BS degree in electriengineering from Seoul University, in 2003, and the MS and PhD degrees in electrical and computer engineering from the University of Texas at Austin, in 2005 and 2010, respectively. He is a senior principal research engineer with Arm Research. He has broad interests include computer architecture with a focus on microarchitecture for high-performance processors. Akanksha Jain received the BTech and MTech in computer science and from the Indian Institute of Technology Madras, in 2009, the PhD degree in computer science from the University of Texas, in August 2016. She is a research engineer with Arm Research. Her research interests include computer architecture, with a particular focus on the memory system and on using machine learning techniques to improve the design of memory system optimizations. Calvin Lin received the BSE degree in computer science from Princeton University, in 1984, and the PhD degree in computer science from the University of Washington, in 1992. He is a University Distinguished Teaching professor with the University of Texas at Austin, where he is also the director of Turing Scholars Honors Program. He has broad interests include systems research, including compilers and computer architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-10.png" coords="13,79.43,45.53,407.78,179.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Triage's Training Algorithm</cell></row><row><cell>Existing entry for PC</cell><cell>New Address</cell><cell>Action</cell></row><row><cell>A; B; stride; len</cell><cell>B (same as last address)</cell><cell>Do nothing</cell></row><row><cell>A; B; 0; 1</cell><cell>B þ stride (new spatial stream)</cell><cell>Update TU entry to A; B; stride; len þ 1</cell></row><row><cell>A; B; stride; len</cell><cell>B þ ðstride Ã kÞ (within a constant stride from</cell><cell>Update TU entry to A; B; stride; len þ 1</cell></row><row><cell></cell><cell>last address)</cell><cell></cell></row><row><cell>A; B; stride; len</cell><cell>C (no fixed stride)</cell><cell>Move TU entry to on-chip metadata</cell></row><row><cell></cell><cell></cell><cell>Re-initialize TU to B þ stride Ã ðlen À 1ÞÞ; C; 0; 1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Machine Configuration</cell></row><row><cell>Core</cell><cell>Out-of-order, 2 GHz,</cell></row><row><cell></cell><cell>4-wide fetch, decode, and dispatch</cell></row><row><cell></cell><cell>128 ROB entries</cell></row><row><cell>TLB</cell><cell>48-entry fully-assoc L1 I/D-TLB</cell></row><row><cell></cell><cell>1024-entry 4-way assoc L2 TLB</cell></row><row><cell>L1I</cell><cell>64 KB, 4-way assoc, 3-cycle latency</cell></row><row><cell>L1D</cell><cell>64 KB, 4-way assoc, 3-cycle latency</cell></row><row><cell></cell><cell>Stride prefetcher</cell></row><row><cell>L2</cell><cell>512 KB, private, 8-way assoc</cell></row><row><cell></cell><cell>11-cycle load to use latency</cell></row><row><cell>L3</cell><cell>2 MB/core, shared, 16-way assoc</cell></row><row><cell></cell><cell>20-cycle load-to-use latency</cell></row><row><cell></cell><cell>Line size 64 bytes</cell></row><row><cell>DRAM</cell><cell>Single-Core:</cell></row><row><cell></cell><cell>85ns latency, 32 GB/s bandwidth</cell></row><row><cell></cell><cell>Multi-Core:</cell></row><row><cell></cell><cell>8B channel width, 800MHz,</cell></row><row><cell></cell><cell>tCAS = 20, tRP = 20, tRCD = 20</cell></row><row><cell></cell><cell>2 channels, 8 ranks, 8 banks, 32 K rows</cell></row><row><cell></cell><cell>32 GB/s bandwidth</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:16:24 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was funded in part by a gift from Arm Research, NSF Grant CCF-1823546, and a gift from Intel Corporation through the NSF/Intel Partnership on Foundational Microarchitecture Research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Practical off-chip meta-data for temporal memory streaming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 15th Int. Symp. High Perform</title>
				<meeting>IEEE 15th Int. Symp. High Perform</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="79" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making address-correlated prefetching practical</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="59" />
			<date type="published" when="2010-02">Jan./Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal prefetching without the off-chip metadata</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 52nd Annu</title>
				<meeting>52nd Annu</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="996" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Back to the future: Leveraging belady&apos;s algorithm for improved cache replacement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Comput</title>
				<meeting>Int. Symp. Comput</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="78" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prefetching using markov predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Annu</title>
				<meeting>24th Annu</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient metadata management for irregular data prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 46th Int. Symp. Comput. Architecture</title>
				<meeting>46th Int. Symp. Comput. Architecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="449" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient representations and abstractions for quantifying and exploiting data reference locality</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chilimbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIG-PLAN Conf. Program. Lang. Des. Implementation</title>
				<meeting>SIG-PLAN Conf. Program. Lang. Des. Implementation</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal streams in commercial server applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Workload Characterization</title>
				<meeting>IEEE Int. Symp. Workload Characterization</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="97" />
			<date type="published" when="2005-02">Jan./Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TCP: Tag correlating prefetchers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Symp. High-Perform</title>
				<meeting>9th Int. Symp. High-Perform</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic locality and context-based prefetching using reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE 42nd Annu</title>
				<meeting>ACM/IEEE 42nd Annu</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="285" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 46th Annu</title>
				<meeting>46th Annu</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-cost epoch-based correlation prefetching for commercial applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th Annu</title>
				<meeting>40th Annu</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="301" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using a user-level memory thread for correlation prefetching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annu</title>
				<meeting>29th Annu</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatiotemporal memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Comput. Architecture</title>
				<meeting>Int. Symp. Comput. Architecture</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predictor virtualization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Burcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int. Conf. Architect. Support Program. Languages Operating Syst</title>
				<meeting>13th Int. Conf. Architect. Support Program. Languages Operating Syst</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="157" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33th Annu</title>
				<meeting>33th Annu</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 41st Annu</title>
				<meeting>41st Annu</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="1978-12">Dec. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Comput. Architecture</title>
				<meeting>Int. Symp. Comput. Architecture</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Memory prefetching using adaptive stream detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th Int. Symp. Microarchitecture</title>
				<meeting>39th Int. Symp. Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="397" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluating stream buffers as a secondary cache replacement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Comput. Architecture</title>
				<meeting>Int. Symp. Comput. Architecture</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective hardware-based data prefetching for high-performance processors</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="609" to="623" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Memory-system design considerations for dynamically-scheduled processors</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Vranesic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Annu</title>
				<meeting>24th Annu</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamic triage allocates different metadata store sizes to different cores</title>
		<idno>Fig. 22</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Access map pattern matching for high performance data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A decoupled predictordirected stream prefetching architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="276" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sandbox prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 20th Int. Symp. High Perform</title>
				<meeting>IEEE 20th Int. Symp. High Perform</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="626" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. High Perform</title>
				<meeting>IEEE Int. Symp. High Perform</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Run-time spatial locality detection and optimization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Annu. ACM/ IEEE Int. Symp. Microarchitecture</title>
				<meeting>30th Annu. ACM/ IEEE Int. Symp. Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting spatial locality in data caches using spatial footprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Architecture News</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="357" to="368" />
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Filtering superfluous prefetches using density vectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Des.: VLSI Comput. Processors</title>
				<meeting>Int. Conf. Comput. Des.: VLSI Comput. essors</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate and complexity-effective spatial pattern prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Symp. High Perform</title>
				<meeting>10th Int. Symp. High Perform</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="276" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointer cache assisted prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th Annu. ACM/IEEE Int. Symp. Microarchitecture</title>
				<meeting>35th Annu. ACM/IEEE Int. Symp. Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Effective jump-pointer prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. Symp. Comput. Architecture</title>
				<meeting>26th Annu. Int. Symp. Comput. Architecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A stateless, contentdirected data prefetching mechanism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cooksey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Architecture News</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 15th Int. Symp. High Perform</title>
				<meeting>IEEE 15th Int. Symp. High Perform</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="7" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Touche: Towards ideal and efficient cache compression by mitigating tag area overhead</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosungolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 52nd Annu</title>
				<meeting>52nd Annu</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="453" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Amoeba-cache: Adaptive blocks for eliminating waste in the memory hierarchy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dwarkadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 45th Annu</title>
				<meeting>45th Annu</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="376" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (RRIP)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Comput. Architecture</title>
				<meeting>Int. Symp. Comput. Architecture</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scaleout workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int. Conf. Architect. Support Program. Languages Operating Syst</title>
				<meeting>17th Int. Conf. Architect. Support Program. Languages Operating Syst</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domino temporal data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 24th Int. Symp. High Perform</title>
				<meeting>IEEE 24th Int. Symp. High Perform</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="131" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The exascale challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Borkar</surname></persName>
		</author>
		<ptr target="https://parasol.tamu.edu/pact11/ShekarBorkar-PACT2011-keynote.pdf" />
		<imprint>
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Memory Systems: Cache, DRAM, Disk</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
