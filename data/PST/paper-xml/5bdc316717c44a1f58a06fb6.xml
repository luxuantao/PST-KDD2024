<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Learning on Graphs with Generative Adversarial Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-09-01">1 Sep 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Learning on Graphs with Generative Adversarial Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-01">1 Sep 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3269206.3271768</idno>
					<idno type="arXiv">arXiv:1809.00130v1[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph learning</term>
					<term>semi-supervised learning</term>
					<term>generative adversarial networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate how generative adversarial nets (GANs) can help semi-supervised learning on graphs. We first provide insights on working principles of adversarial learning over graphs and then present GraphSGAN, a novel approach to semi-supervised learning on graphs. In GraphSGAN, generator and classifier networks play a novel competitive game. At equilibrium, generator generates fake samples in low-density areas between subgraphs. In order to discriminate fake samples from the real, classifier implicitly takes the density property of subgraph into consideration. An efficient adversarial learning algorithm has been developed to improve traditional normalized graph Laplacian regularization with a theoretical guarantee.</p><p>Experimental results on several different genres of datasets show that the proposed GraphSGAN significantly outperforms several state-of-the-art methods. GraphSGAN can be also trained using mini-batch, thus enjoys the scalability advantage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semi-supervised learning on graphs has attracted great attention both in theory and practice. Its basic setting is that we are given a graph comprised of a small set of labeled nodes and a large set of unlabeled nodes, and the goal is to learn a model that can predict label of the unlabeled nodes.</p><p>There is a long line of works about semi-supervised learning over graphs. One important category of the research is mainly based on the graph Laplacian regularization framework. For example, Zhu et al. <ref type="bibr" target="#b40">[41]</ref> proposed a method called Label Propagation for learning from labeled and unlabeled data on graphs, and later the method has been improved by Lu and Getoor <ref type="bibr" target="#b19">[20]</ref> under the bootstrap-iteration framework. Blum and Chawla <ref type="bibr" target="#b3">[4]</ref> also formulated the graph learning problem as that of finding min-cut on graphs. Zhu et al. <ref type="bibr" target="#b41">[42]</ref> proposed an algorithm based on Gaussian random field and formalized graph Laplacian regularization framework. Belkin et al. <ref type="bibr" target="#b1">[2]</ref> presented a regularization method called ManiReg by exploiting geometry of marginal distribution for semi-supervised learning. The second category of the research is to combine semi-supervised learning with graph embedding. Weston et al. <ref type="bibr" target="#b36">[37]</ref> first incorporated deep neural networks into the graph Laplacian regularization framework for semi-supervised learning and embedding. Yang et al. <ref type="bibr" target="#b37">[38]</ref> proposed the Planetoid model for jointly learning graph embedding and predicting node labels. Recently, Defferrard et al. <ref type="bibr" target="#b8">[9]</ref> utilized localized spectral Chebyshev filters to perform convolution on graphs for machine learning tasks. Graph convolution networks (GCN) <ref type="bibr" target="#b16">[17]</ref> and its extension based on attention techniques <ref type="bibr" target="#b33">[34]</ref> demonstrated great power and achieved state-of-art performance on this problem.</p><p>This paper investigates the potential of generative adversarial nets (GANs) for semi-supervised learning over graphs. GANs <ref type="bibr" target="#b12">[13]</ref> are originally designed for generating images, by training two neural networks which play a min-max game: discriminator D tries to discriminate real from fake samples and generator G tries to generate "real" samples to fool the discriminator. To the best of our knowledge, there are few works on semi-supervised learning over graphs with GANs.</p><p>We present a novel method GraphSGAN for semi-supervised learning on graphs with GANs. GraphSGAN maps graph topologies into feature space and jointly trains generator network and classifier network. Previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> tried to explain semi-supervised GANs' working principles, but only found that generating moderate fake samples in complementary areas benefited classification and analyzed under strong assumptions. This paper explains the working principles behind the proposed model from the perspective of game theory. We have an intriguing observation that fake samples in low-density areas between subgraphs can reduce the influence of samples nearby, thus help improve the classification accuracy. A novel GAN-like game is designed under the guidance of this observation. Sophisticated losses guarantee the generator generates samples in these low-density areas at equilibrium. In addition, integrating with the observation, the graph Laplacian regularization framework (Equation ( <ref type="formula" target="#formula_16">9</ref>)) can leverage clustering property to make stable progress. It can be theoretically proved that this adversarial learning technique yields perfect classification for semi-supervised learning on graphs with plentiful but finite generated samples.</p><p>The proposed GraphSGAN is evaluated on several different genres of datasets. Experimental results show that GraphSGAN significantly outperforms several state-of-the-art methods. GraphSGAN can be also trained using mini-batch, thus enjoys the scalability advantage.</p><p>Our contributions are as follows:</p><p>• We introduce GANs as a tool to solve classification tasks on graphs under semi-supervised settings. GraphSGAN generates fake samples in low-density areas in graph and leverages clustering property to help classification. The rest of the paper is arranged as follows. In Section 2, we introduce the necessary definitions and GANs. In Section 3, we present GraphSGAN and discuss why and how the model is designed in detail. A theoretical analysis of the working principles behind GraphSGANis given in Section 4. We outline our experiments in Section 5 and show the superiority of our model. We close with a summary of related work in Section 6, and our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 Problem Definition</head><p>Let G = (V , E) denote a graph, where V is a set of nodes and E ⊆ V ×V is a set of edges. Assume each node v i is associated with a k−dimensional real-valued feature vector w i ∈ R k and a label y i ∈ {0, ..., M − 1}. If the label y i of node v i is unknown, we say node v i is an unlabeled node. We denote the set of labeled nodes as V L and the set of unlabeled nodes as V U = V \V L . Usually, we have |V L | ≪ |V U |. We also call the graph G as partially labeled graph <ref type="bibr" target="#b30">[31]</ref>. Given this, we can formally define the semi-supervised learning problem on graph. Definition 1. Semi-supervised Learning on Graph. Given a partially labeled graph G = (V L ∪ V U , E), the objective here is to learn a function f using features w associated with each node and the graphical structure, in order to predict the labels of unlabeled nodes in the graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Adversarial Nets (GANs)</head><p>GAN <ref type="bibr" target="#b12">[13]</ref> is a new framework for estimating generative models via an adversarial process, in which a generative model G is trained to best fit the original training data and a discriminative model D is trained to distinguish real samples from samples generated by model G. The process can be formalized as a min-max game between G and D, with the following loss (value) function:</p><formula xml:id="formula_0">min G max D V (G, D) = E x∼p d (x) log D(x) + E z∼p z (z) log[1 − D(G(z))] (1)</formula><p>where p d is the data distribution from the training data, p z (z) is a prior on input noise variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL FRAMEWORK 3.1 Motivation</head><p>We now introduce how we leverage the power of GANs for semisupervised learning over graphs. Directly applying GAN to graph learning is infeasible, as it does not consider the graph structure. To show how GANs help semi-supervised learning over graphs, we begin with one example. The left figure in Figure <ref type="figure" target="#fig_0">1</ref> shows a typical example in graph-based semi-supervised learning. The two labeled nodes are in blue and orange respectively. Traditional methods such as Label Propagation <ref type="bibr" target="#b40">[41]</ref> does not consider the graph topology, thus cannot differentiate the propagations from node v + to nodes v 1 , v 2 , and v 3 . Taking a closer look at the graph structure, we can see there are two subgraphs. We call the area between the two subgraphs as density gap.</p><p>Our idea is to use GAN to estimate the density subgraphs and then generate samples in the density gap area. We then request the classifier firstly to discriminate fake samples before classifying them into different classes. In this way, discriminating fake samples from real samples will result in a higher curvature of the learned classification function around density gaps, which weakens the effect of propagation across density gaps (as shown in the right figure of Figure <ref type="figure" target="#fig_0">1</ref>). Meanwhile inside each subgraph, confidence on correct labels will be gradually boosted because of supervised loss decreasing and general smoothing techniques for example stochastic layer. A more detailed analysis will be reported in § 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head><p>GAN-based models cannot be directly applied to graph data. To this end, GraphSGAN first uses network embedding methods (e.g., DeepWalk <ref type="bibr" target="#b22">[23]</ref>, LINE <ref type="bibr" target="#b28">[29]</ref>, or NetMF <ref type="bibr" target="#b23">[24]</ref>) to learn latent distributed representation q i for each node, and then concatenates the latent distribution q i with the original feature vector w i , i.e., x i = (w i , q i ). Finally, x i is taken as input to our method.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the architecture of GraphSGAN. Both classifier D and generator G in GraphSGAN are multiple layer perceptrons.</p><p>More specifically, the generator takes a Gaussian noise z as input and outputs fake samples having the similar shape as x i . In the generator, batch normalization <ref type="bibr" target="#b14">[15]</ref> is used. Generator's output layer is constrained by weight normalization trick <ref type="bibr" target="#b26">[27]</ref> with a trainable weight scale. Discriminator in GANs is substituted by a classifier, where stochastic layers(additive Gaussian noise) are added after input and full-connected layers for smoothing purpose. Noise is removed in prediction mode. Parameters in full-connected layers are constrained by weight normalization for regularization. Outputs of the last hidden layer in classifier h (n) (x) are features extracted by non-linear transformation from input x, which is essential for feature matching <ref type="bibr" target="#b25">[26]</ref> when training generator. The classifier ends with a (M +1)-unit output layer and softmax activation. The outputs of unit 0 to unit M − 1 can be explained as probabilities of different classes and output of unit M represents probability to be fake. In practice, we only consider the first M units and assume the output for fake class P M is always 0 before softmax, because subtracting an identical number from all units before softmax does not change the softmax results. In this case, the only Nash Equilibrium can be reached by minimax strategies <ref type="bibr" target="#b34">[35]</ref>. To find the equilibrium is equivalent to solve the optimization:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Algorithm</head><formula xml:id="formula_1">min G max D V D (G, D)</formula><p>Goodfellow et al. <ref type="bibr" target="#b12">[13]</ref> proved that if V D (G, D) was defined as that in equation 1, G would generate samples subject to data distribution at the equilibrium. The distribution of generate samples p д (x) is an approximation of the distribution of real data p d (x). But we want to generate samples in density gaps instead of barely mimicking real data. So, original GANs cannot solve this task.</p><p>In the proposed GraphSGAN, we modify L D (G, D) and L G (G, D) to design a new game, in which G would generate samples in density gaps at the equilibrium. More precisely, we expect that the real and fake samples are mapped like Figure <ref type="figure" target="#fig_3">3</ref> in its final representative layer h (n) (x). Because the concept of "density gap" is more straightforward in a representative layer than in a graph, we define that a node lies in density gap if and only if it lies in density gap in h (n) (x) layer. How to map nodes into representative layer is explained in section 3.2.</p><p>The intuition behind the design is based on the famous phenomenon known as "curse of dimensionality" <ref type="bibr" target="#b9">[10]</ref>. In high-dimensional space like h (n) (x), the central area is far more narrow than outer areas. Training data in the central area are easy to become hubs <ref type="bibr" target="#b24">[25]</ref>. Hubs frequently occurs in the nearest neighbors of samples from other classes, which might deeply affect semi-supervised learning and become a main difficulty. So, we want the central area become a density gap instead of one of clusters.</p><p>We define L D (G, D) and L G (G, D) as below to guarantee the expected equilibrium.</p><formula xml:id="formula_2">L D =loss sup + λ 0 loss un + λ 1 loss ent + loss pt L G =loss f m + λ 2 loss pt<label>(2)</label></formula><p>Next, we explain these loss terms in L D (G, D) and L G (G, D) and how they take effect in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Discriminative Losses.</head><p>At equilibrium, no player can change their strategy to reduce his loss unilaterally. Supposing that G generates samples in central areas at equilibrium, we put forward four conditions for D to guarantee the expected equilibrium in h (n) (x):</p><p>(1) Nodes from different classes should be mapped into different clusters.</p><p>(2) Both labeled and unlabeled nodes should not be mapped into the central area so as to make it a density gap. (3) Every unlabeled node should be mapped into one cluster representing a particular label. (4) Different clusters should be far away enough. The most natural way to satisfy condition (1) is a supervised loss loss sup . loss sup is defined as the cross entropy between predicted distribution over M classes and one-hot representation for real label.</p><formula xml:id="formula_3">loss sup = −E x i ∈X L log P(y i |x i , y i &lt; M) (3)</formula><p>where X L is the set of inputs for labeled nodes V L . Condition ( <ref type="formula" target="#formula_2">2</ref>) is equivalent to the D's aim in original GAN given that G generates fake samples in central density gaps. Thus we still use the loss in equation 1 and call it loss un . The classifier D incurs loss un when real-or-fake misclassification happens.</p><formula xml:id="formula_4">loss un = − E x i ∈X U log[1 − P(M |x i )] − E x i ∼G(z) log P(M |x i ) (4)</formula><p>where X U is set of pretreated inputs for unlabeled nodes</p><formula xml:id="formula_5">V U ; G(z)</formula><p>is the distribution of generated samples; and P(M |x i ) denotes the predicted fake probability of x i . Condition (3) requests D to assign an unambiguous label to every unlabeled node. We solve the problem by adding an entropy regularization term loss ent , the entropy of distribution over M labels. Entropy is a measurement of uncertainty of probability distributions. It has become a regularization term in semi-supervised learning for a long time <ref type="bibr" target="#b13">[14]</ref> and is firstly combined with GANs in <ref type="bibr" target="#b27">[28]</ref>. Reducing entropy could encourage the classifier to determine a definite label for every node.</p><formula xml:id="formula_6">loss ent = −E x i ∈X U M −1 y=0 P(y|x i , y i &lt; M) log P(y|x i , y i &lt; M) (5)</formula><p>Condition (4) widens density gaps to help classification. We leverage pull-away term loss pt <ref type="bibr" target="#b38">[39]</ref> to satisfy it. loss pt is originally designed for generating diverse samples in ordinary GANs. It is the average cosine distance between vectors in a batch. It keeps representations in h (n) (x) layer as far from the others as possible. Hence, it also encourages clusters to be far from the others.</p><formula xml:id="formula_7">loss pt = 1 m(m − 1) m i=1 j i h (n) (x i ) ⊤ h (n) (x j ) ||h (n) (x i )||||h (n) (x j )|| 2 (6)</formula><p>where x i , x j are in the same batch and m is batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Generative Losses.</head><p>Similarly, supposing that D has satisfied the four conditions above, we also have two conditions for G to guarantee the expected equilibrium in h (n) (x):</p><p>(1) G generates samples which are mapped into the central area.</p><p>(2) Generated samples should not overfit at the only center point.</p><p>For condition (1), we train G using feature matching loss <ref type="bibr" target="#b25">[26]</ref>. It minimizes the distances between generated samples and the center point of real samples</p><formula xml:id="formula_8">E x i ∈X U ∪X L h (n) (x i ).</formula><p>Actually, in training process the center point is replaced by center of samples in a real batch E x i ∈X bat ch h (n) (x i ), which helps satisfy condition <ref type="bibr" target="#b1">(2)</ref>. The distances are originally measured in L2 norm. (But, in practice, we found that L1 norm also works well, with even slightly better performance.)</p><formula xml:id="formula_9">loss f m = ||E x i ∈X bat ch h (n) (x i ) − E x j ∼G(z) h (n) (x j )|| 2 2 (7)</formula><p>Condition (2) requests generated samples to cover as much central areas as possible. We also use a pull-away loss term(Equation</p><formula xml:id="formula_10">Algorithm 1: Minibatch stochastic gradient descent training of GraphSGAN Input: Node features {w i }, Labels y L , Graph G = (V , E), Embedding Algorithm A, batch size m. Calculate {w ′ i } according to Eq. (8) Calculate {q i } via A Concatenate {w ′ i } with {q i } for X L ∪ X U repeat Sample m labeled samples {x L 1 , ..., x L m } from X L Sample m unlabeled samples {x U 1 , ..., x U m } from X U Sample m noise samples {z 1 , ..., z m } from p z (z)</formula><p>Update the classifier by descending gradients of losses:</p><formula xml:id="formula_11">∇ θ D 1 m loss sup + λ 0 loss un + λ 1 loss ent + loss pt for t steps do Sample m unlabeled samples {x U 1 , ..., x U m } from X U Sample m noise samples {z 1 , ..., z m } from p z (z)</formula><p>Update the generator by descending gradients of losses:</p><formula xml:id="formula_12">∇ θ G 1 m loss f m + λ 2 loss pt end until Convergence;</formula><p>6) to guarantee the satisfication of this condition, because it encourage G to generate diverse samples. A trade-off is needed between centrality and diversity, thus we use a hyper-parameter λ 2 to balance loss f m and loss pt . The stochastic layers in D add noise to fake inputs, which not only improves robustness but also prevents fake samples from overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Training.</head><p>GANs train D and G by iteratively minimizing D and G's losses. In game theory, it is called myopic best response <ref type="bibr" target="#b0">[1]</ref>, an effective heuristic method to find equilibriums. GraphSGAN is also trained in this way.</p><p>The first part of training is to turn nodes in the graph to vectors in feature space. We use LINE <ref type="bibr" target="#b28">[29]</ref> for pretreatment of q, which performs fast and stable on our dataset. We also test other network embedding algorithms and find similar performances in classification. To accelerate the convergence, nodes' features w are recalculated using neighbor fusion technique. Let N e(v i ) be the set of neighbors of v i , node v i 's weights are recalculated by</p><formula xml:id="formula_13">w ′ i = αw i + 1 − α |N e(v i )| v j ∈N e(v i ) w j .<label>(8)</label></formula><p>The neighbor fusion idea is similar to the pretreatment tricks using attention mechanisms <ref type="bibr" target="#b33">[34]</ref>.</p><p>In the main training loop, we iteratively trains D and G. To compute L D , we need three batches of labeled, unlabeled and generated samples respectively. loss sup needs labeled data. loss un is computed based on unlabeled and generated data. Theoretically, loss un should also take account of labeled data to make sure that they are classified as real. But loss sup has made labeled data classified correctly as its real label so that it is not necessary to consider labeled data in loss un . loss ent only considers unlabeled data and loss pt should "pull away" both labeled and unlabeled data. Usually three hyperparameters are needed to balance the scales of four losses. We only use two parameters λ 0 and λ 1 in GraphSGAN because loss sup will soon be optimized nearly to 0 due to few labeled samples under the semi-supervised setting.</p><p>Both real and generated batches of data are needed to train G. loss f m compares the batch center of real and generated data in h (n) (x) and loss pt measures the diversity of generated data. We always want G to generate samples in central areas, which is an assumption when discussing L D in section 3.3.2. So, we train G several steps to convergence after every time we train D. Detailed process is illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL BASIS</head><p>We provide theoretical analyses on why GANs can help semisupervised learning on graph. In section 3.1, we claim that the working principle is to reduce the influence of labeled nodes across density gaps. In view of the difficulty to directly analyze the dynamics in training of deep neural networks, we base the analysis on the graph Laplacian regularization framework. Definition 2. Marginal Node and Interior Node. Marginal Nodes M are nodes linked to nodes with different labels while Interior Nodes I not. Formally,</p><formula xml:id="formula_14">M = {v i |v i ∈ V ∧ (∃v j ∈ V , (v i , v j ) ∈ E ∧ y i y j )}, I = V \ M.</formula><p>Assumption 1. Convergence conditions. When G converges, we expect it to generate fake samples linked to nearby marginal nodes. More specifically, let V д and E д be the set of generated fake samples and generated links from generated nodes to nearby original nodes. we have</p><formula xml:id="formula_15">∀v д ∈ V д , (∃v i ∈ M, (v д , v i ) ∈ E д ) ∧ (∀(v д , v i ) ∈ E д , v i ∈ M).</formula><p>The loss function of graph Laplacian regularization framework is as follows:</p><formula xml:id="formula_16">L(y ′ ) = v i ∈V L loss(y i , y ′ i ) + λ i j v i ,v j ∈V α i j • neq(y ′ i , y ′ j )<label>(9)</label></formula><p>where y ′ i denotes predicted label of node v i . The loss(•, •) function measures the supervised loss between real and predicted labels. neq(•, •) is a 0-or-1 function representing not equal.</p><formula xml:id="formula_17">α i j = Ãij = A i j deд(i)deд(j) , (i j)<label>(10)</label></formula><p>where A and Ã are the adjacent matrix and negative normalized graph Laplacian matrix, and deд(i) means the degree of v i . It should be noted that our equation is slightly different from <ref type="bibr" target="#b39">[40]</ref>'s because we only consider explicit predicted label rather than label distribution.</p><p>Normalization is the core of reducing the marginal nodes' influence. Our approach is simple: generating fake nodes, linking them to nearest real nodes and solving graph Laplacian regularization. Fake label is not allowed to be assigned to unlabeled nodes and loss computation only considers edges between real nodes. The only difference between before and after generation is that marginal nodes' degree changes. And then the regularization parameter α i j changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Proof</head><p>We analyze how generated fake samples help acquire correct classification.</p><p>Corollary 1. Under Assumption 1, let L(C дt ) and L(C дt ) ′ be losses of ground truth on graph</p><formula xml:id="formula_18">(V +V д , E + E д ) and (V +V ′ д , E + E ′ д ). We have ∀V д ⫌ V ′ д , L(C дt ) &lt; L(C дt ) ′</formula><p>, where V д and E д are set of generated nodes and edges.</p><p>Corollary 1 can be easily deduced because of α i j decreasing. Loss of ground truth continues to decrease along with new fake samples being generated. That indicates ground truth is more likely to be acquired. However, there might exist other classification solutions whose loss decreases more. Thus, we will further prove that we can make a perfect classification under reasonable assumptions with adequate generated samples. Definition 3. Partial Graph. We define the subgraph induced by all nodes labeled c (aka. V c ) and their other neighbors N e c as partial graph G c .</p><p>Assumption 2. Connectivity. The subgraph induced by all interior nodes in each class is connected. Besides, every marginal node connects to at least one interior node in the same class.</p><p>Most real-world networks are dense and big enough to satisfy Assumption 2. There actually implies another weak assumption that at least one labeled node exists for each class. This is the usually guaranteed by the setting of semi-supervised learning. Let m c be the number of edges between marginal nodes in G c . Besides, we define deд c as the maximum of degrees of nodes in G c and loss i as the supervised loss for misclassified labeled node v i . Theorem 1. Perfect Classification. If enough fake samples are generated such that ∀v ∈ M, deд(v) &gt; d 0 , all nodes will be correctly classified. d 0 is the maximum of max Proof. We firstly consider a simplified problem in partial graphs G c , where nodes from N e c have already been assigned fixed label c ′ . We will prove that the new optimal classification C min are the classification C ′ , which correctly assigns</p><formula xml:id="formula_19">V c label c. Since L(C min ) &lt; L(C ′ ) &lt; λm c • 1 √ d 0 •d 0 &lt; λm c / max c,v i ∈V c λm c loss i ≤ min v i ∈V c loss i , optimal</formula><p>solution C min should classify all labeled nodes correctly. Suppose that C min assigns v i , v j ∈ I with different labels. The inequality Suppose that C min assigns v i ∈ M ∩ V c , v j ∈ I with different labels and (v i , v j ) ∈ E. Let v i be assigned with c ′ . If we change v i 's label to c, then α i j between v i and its interior neighbors will be excluded from the loss function. But some other edges weights between v i and its marginal neighbors might be added to the loss function. Let λ∆ denotes the variation of loss. The following equation will show that the decrease of the loss would lead to a contradiction.</p><formula xml:id="formula_20">L(C min ) ≥ λα i j = λ √ deд(v i )deд(v j ) ≥ λ deд c ≥ λ m c deд c ≥ λm c</formula><formula xml:id="formula_21">∆ ≤ v k ∈M (v i ,v k )∈Ec 1 deд(v i )deд(v k ) − v j ∈I (v i ,v j )∈Ec 1 deд(v i )deд(v j ) ≤ 1 deд(v i ) ( m c max c m 2 c deд c − v j ∈I (v i ,v j )∈Ec 1 deд(v j ) ) &lt; 0</formula><p>Suppose that C min avoids all situations discussed above while v i ∈ M ∩ V c is still assigned with c ′ . Under Assumption 2, there exists an interior node v j connecting with v i . As we discussed, v j must be assigned c in C min , leading to contradiction. Therefore, C ′ is the only choice for optimal binary classification in G c . That means all nodes in class c are classified correctly. But what if in G c not all nodes in N e c are labeled c ′ ? Actually no matter which labels they are assigned, all nodes in V c are classified correctly. If nodes in N e c are assigned labels except c and c ′ , the proof is almost identical and C ′ is still optimal. If any nodes in N e c are mistakenly assigned with label c, the only result is to encourage nodes to be classified as c correctly.</p><p>Finally, the analysis is correct for all classes thus all nodes will be correctly classified. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conduct experiments on two citations networks and one entity extraction dataset. Table <ref type="table" target="#tab_1">1</ref> summaries statistics of the three datasets.</p><p>To avoid over-tuning the network architectures and hyperparameters, in all our experiments we use a default settings for training and test. Specifically, the classifier has 5 hidden layers with (500, 500, 250, 250, 250) units. Stochastic layers are zero-centered Gaussian noise, with 0.05 standard deviation for input and 0.5 for outputs of hidden layers. Generator has two 500-units hidden layers, each followed by a batch normalization layer. Exponential Linear Unit (ELU) <ref type="bibr" target="#b5">[6]</ref> is used for improving the learning accuracy except the output layer of G, which instead uses tanh to generate samples ranging from −1 to 1. The trade-off factors in Algorithm 1 are λ 0 = 2, λ 1 = 1, λ 2 = 0.3. Models are optimized by Adam <ref type="bibr" target="#b15">[16]</ref>, where β 1 = 0.5, β 2 = 0.999. All parameters are initialized with Xavier <ref type="bibr" target="#b11">[12]</ref> initializer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on Citation Networks</head><p>The two citation networks contain papers and citation links between papers. Each paper has features represented as a bag-ofwords and belongs to a specific class based on topic, such as "database" or "machine learning". The goal is to classify all papers into its correct class. For fair comparison, we follow exactly the experimental setting in <ref type="bibr" target="#b37">[38]</ref>, where for each class 20 random instances (papers) are selected as labeled data and 1,000 instances as test data. The reported performance is the average of ten random splits. In both datasets, we compare our proposed methods with three categories of methods: • regularization-based methods including LP <ref type="bibr" target="#b40">[41]</ref>, ICA <ref type="bibr" target="#b19">[20]</ref>, and ManiReg <ref type="bibr" target="#b1">[2]</ref>; • embedding-based methods including DeepWalk <ref type="bibr" target="#b22">[23]</ref>, SemiEmb <ref type="bibr" target="#b36">[37]</ref>, and Planetoid <ref type="bibr" target="#b37">[38]</ref>; • and convolution-based methods including Chebyshev <ref type="bibr" target="#b8">[9]</ref>, GCN <ref type="bibr" target="#b16">[17]</ref> and GAT <ref type="bibr" target="#b33">[34]</ref>.</p><p>We train our models using early stop with 500 nodes for validation (average 20 epochs on Cora and 35 epochs on Citeseer). Every epoch contains 100 batches with batch size 64. Table <ref type="table" target="#tab_2">2</ref> shows the results of all comparison methods. Our method significantly outperforms all the regularization-and embedding-based methods, and also performs much better than Chebyshev and graph convolution networks (GCN), meanwhile slightly better than GCN with attentions (GAT). Compared with convolution-based methods, GraphSGAN is more sensitive to labeled data and thus a larger variance is observed. The large variance might originate from the instability of training of GANs. For example, the mode collapse phenomenon <ref type="bibr" target="#b32">[33]</ref> will hamper GraphSGAN from generating fake nodes evenly in density gaps. These instabilities are currently main problems in research of GANs. More advanced techniques for stabilizing GraphSGAN are left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Verification</head><p>We provide more insights into GraphSGAN with experimental verifications. There are two verification experiments: one is about the expected equilibrium, and the other verifies the working principles of GraphSGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Verification of equilibrium.</head><p>The first experiment is about whether GraphSGAN converges at the equilibrium described in § 3.3.2. In Figure <ref type="figure" target="#fig_8">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Verification of working principles.</head><p>The second experiment is to verify the proposed working principles. We have proved in § 4 theoretically that reducing the influence of marginal nodes can help classification. But we should further verify whether generated samples reduce the influence of marginal nodes. On one hand, nodes are mapped into distinct and far clusters in h (n) (x). On the other hand, the "influence" is related with "smooth degree". For example in graph Laplacian regularization framework, difference between labels of adjacent nodes are minimized explicitly to guarantee the smoothness. Thus we examine classifier function's smooth degree around density gaps. Smooth degrees at x i are measured by the norm of the gradient of maximum in probabilities for each class.</p><formula xml:id="formula_22">||д(x i )|| = ||∇ x i M −1 max y=0 P(y|x i , y i &lt; M)|| Let p f (x i )</formula><p>be predicted fake probability of x i . We draw curves of ||д(x i )|| and p f (x i ) during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on Entity Extraction</head><p>The DIEL dataset <ref type="bibr" target="#b2">[3]</ref> is a dataset for information extraction. It contains pre-extracted features for each entity mentions in text, and a graph connecting entity mentions to corresponding coordinateitem lists. The objective is to extract medical entities from items given feature vectors, the graph topologies and a few known medical entities. Again, we follow the same setting as in the original paper <ref type="bibr" target="#b2">[3]</ref> for the purpose of comparison, including data split and the average of different runs. Because the features are very high dimensional sparse vectors, we reduce its dimensions to 128 by Truncated SVD algorithm. We use neighbor fusion on item string nodes with α = 0 as only entity mention nodes have features. We treat the top-k (k = 240,000) entities given by a model as positive, and compare recall of top-k ranked results by different methods. Note that as many items in ground truth do not appear in text, the upper bound of recall is 0.617 in this dataset.</p><p>We also compare it with the different types of methods. Table <ref type="table" target="#tab_3">3</ref> reports the average recall@k of standard data splits for 10 runs by all the comparison methods. DIEL represents the method in original paper <ref type="bibr" target="#b2">[3]</ref>, which uses outputs of multi-class label propagation to train a classifier. The result of Planetoid is the inductive result which shows the best performance among three versions. As the DIEL dataset has millions of nodes and edges, which makes fullbatch training, for example GCN, infeasible(using sparse storage, memory needs &gt; 200GB), we do not report GCN and GAT here. From Table <ref type="table" target="#tab_3">3</ref>, we see that our method GraphSGAN achieves the best performance, significantly outperforming all the comparison methods (p−value≪0.01, t−test). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Space Efficiency</head><p>Since GCN cannot handle large-scale networks, we examine the memory consumption of GraphSGAN in practice. GPU has become the standard platform for deep learning algorithms. The memory on GPU are usually very limited compared with the main memory. We compare the GPU memory consumption of four representative algorithms from different categories in Figure <ref type="figure" target="#fig_9">5</ref>. Label Propagation does not need GPU, we show its result on CPU for the purpose of comparison.</p><p>For small dataset, GraphSGAN consumes the largest space due to the most complex structure. But for large dataset, GraphSGAN uses the least GPU memories. LP is usually implemented by solving equations, whose space complexity is O(N 2 ). Here we use the "propagation implementation" to save space. GCN needs full-batch training and cannot handle a graph with millions of nodes. Planetoid and GraphSGAN are trained in mini-batch way so that the space consumption is independent of the number of nodes. High dimensional features in DIEL dataset are also challenging. Planetoid uses sparse storage to handle sparse features and GraphSGAN reduces the dimension using Truncated SVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Adversarial Label Propagation</head><p>Will adversarial learning help conventional semi-supervised learning algorithms? Yes, we have proved theoretically that reducing the influence of marginal nodes can help classification in § 4. So, we further propose an adversarial improvement for graph Laplacian regularization with generated samples to verify our proofs (Cf. § 4). We conduct the experiment by incorporating adversarial learning into the Label Propagation framework <ref type="bibr" target="#b39">[40]</ref> to see whether the performance can be improved or not. To incorporate nodes' features, we reconstruct graph by linking nodes to their k nearest neighbors in feature space x. We use the Citeseer network in this experiment, whose settings are described in § 5.1 meanwhile k = 10. Generating enough fake data is time-consumable, therefore we directly determined marginal nodes by p f (x i ) &gt; τ (τ is a threshold), because</p><formula xml:id="formula_23">E p д (x) p д (x) + p dat a (x) = p f (x)</formula><p>where p f (x) is the probability of x to be fake. Then, we increase marginal nodes' degree up to r to reduce their influence.</p><p>Figure <ref type="figure" target="#fig_10">6</ref> shows the results. Smaller p f means more marginal nodes. The curves indicates that τ = 0.1 is a good threshold to discriminate marginal and interior nodes. A bigger r always perform better, which encourages us to generate as much fake nodes in density gaps as possible. The performance of LP has been improved by increasing marginal nodes' degree, but still underperforms GraphSGAN.</p><p>In our opinion, the main reason is that GraphSGAN uses neural networks to capture high-order interrelation between features. Thus, we also try to reconstruct the graph using the first layer's output of a supervised multi-layer perceptron and further observed improvements in performance, highlighting the power of neural networks in this problem.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Related works mainly fall into three categories: Algorithms for semi-supervised learning on graphs, GANs for semi-supervised learning and GAN-based applications on graphs. We discuss them and summarize the main differences between our proposed model and these works as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Semi-supervised Learning on Graphs</head><p>As mentioned in § 1, previous methods for this task can be divided into three categories.</p><p>Label Propagation <ref type="bibr" target="#b40">[41]</ref> is the first work under the graph Laplacian framework. Labeled nodes continues to propagate their labels to adjacent nodes until convergence. After revealing the relationship between LP and graph Laplacian regularization <ref type="bibr" target="#b41">[42]</ref>, the method are improved by sophisticated smoothing regularizations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42]</ref> and bootstrap method <ref type="bibr" target="#b19">[20]</ref>. This kind of methods mainly focus on local smoothness but neglect clustering property of graphs, making situations like Figure <ref type="figure" target="#fig_0">1</ref> hard cases.</p><p>Deepwalk <ref type="bibr" target="#b22">[23]</ref> is the first work for graph embedding. As an unsupervised method to learn latent representations for nodes, DeepWalk can easily be turned to a semi-supervised baseline model if combined with SVM classifier. Since labels help learn embeddings and then help classification, Planetoid <ref type="bibr" target="#b37">[38]</ref> jointly learns graph embeddings and predicts node labels. Graph embedding becomes one step in GraphSGAN and we incorporate GANs for better performance.</p><p>GCN <ref type="bibr" target="#b16">[17]</ref> is the first graph convolution model for semi-supervised learning on graphs. Every filter in GCN learns linear transformations on spectral domain for every feature and combines them. More complex graph convolution methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref> show better performances. An obvious disadvantage of graph convolution is huge consumptions of space, which is overcome by GraphSGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">GANs for Semi-supervised Learning</head><p>Semi-supervised GANs(SGAN) were first put forward in computer vision domain <ref type="bibr" target="#b21">[22]</ref>. SGAN just replaces the discriminator in GANs with a classifier and becomes competitive with state-of-art semisupervised models for image classification. Feature matching loss is first put forward to prevent generator from overtraining <ref type="bibr" target="#b25">[26]</ref>. The technique is found helpful for semi-supervised learning, leaving the working principles unexplored <ref type="bibr" target="#b25">[26]</ref>. Analysis on the trade-off between the classification performance of semi-supervised and the quality of generator was given in <ref type="bibr" target="#b7">[8]</ref>. Kumar et al. <ref type="bibr" target="#b17">[18]</ref> find a smoothing method by estimating the tangent space to the data manifold. In addition, various auxiliary architectures are combined with semi-supervised GANs to classify images more accurately <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>. All these works focus on image data and leverage CNN architectures. GraphSGAN introduces this thought to graph data and first designs a new GAN-like game with clear and convincing working principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">GAN-based Applications on Graphs</head><p>Although we firstly introduce GANs to graph-based semi-supervised learning problem, GANs have made successes in many other machine learning problems on graphs.</p><p>One category is about graph generation. Liu et al. <ref type="bibr" target="#b18">[19]</ref> present a hierarchical architecture composed by multiple GANs to generate graphs. The model preserves topological features of training graphs. Tavakoli et al. <ref type="bibr" target="#b31">[32]</ref> apply GANs for link formation in social networks. Generated network preserves the distribution of links with minimal risk of privacy breaches.</p><p>Another category is about graph embedding. In GraphGAN <ref type="bibr" target="#b35">[36]</ref>, generator learns embeddings for nodes and discriminator solves link prediction task based on embeddings. Classification-oriented embeddings are got at the equilibrium. Dai et al. <ref type="bibr" target="#b6">[7]</ref> leveraged adversarial learning to regularize training of graph representations. Generator transforms embeddings from traditional algorithms into new embeddings, which not only preserve structure information but also mimic a prior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>We propose GraphSGAN, a novel approach for semi-supervised learning over graphs using GANs. We design a new competitive game between generator and classifier, in which generator generates samples in density gaps at equilibrium. Several sophisticated loss terms together guarantee the expected equilibrium. Experiments on three benchmark datasets demonstrate the effectiveness of our approach.</p><p>We also provide a thorough analysis of working principles behind the proposed model GraphSGAN. Generated samples reduce the influence of nearby nodes in density gaps so as to make decision boundaries clear. The principles can be generalized to improve traditional algorithms based on graph Laplacian regularization with theoretical guarantees and experimental validation. GraphSGAN is scalable. Experiments on DIEL dataset suggest that our model shows good performance on large graphs too.</p><p>As future work, one potential direction is to investigate more ideal equilibrium, stabilizing the training further, accelerating training, strengthening theoretical basis of this method and extending the method to other tasks on graph data such as <ref type="bibr" target="#b29">[30]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A qualitative illustration of working principles in GraphSGAN. The two labeled nodes are in solid blue and solid orange respectively and all the other nodes are unlabeled nodes. The left figure is a bad case of vanilla Label Propagation algorithm. In this case, node v 3 is assigned a wrong label due to its direct link to node v + . The right figure illustrates how GraphSGAN works. It generates fake nodes (in black) in the density gap thus reduces the influence of nodes across the density gap.</figDesc><graphic url="image-1.png" coords="2,317.96,83.69,240.25,108.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of our model. Fake inputs are generated by generator and real inputs are acquired by concatenating original feature w i and learned embedding q i . Both real inputs and fake samples generated by generator are fed into the classifier.</figDesc><graphic url="image-2.png" coords="3,116.85,83.68,378.31,229.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3. 3 . 1</head><label>31</label><figDesc>Game and Equilibrium. GANs try to generate samples similar to training data but we want to generate fake samples in density gaps. So, the optimization target must be different from original GANs in the proposed GraphSGAN model. For better explanation, we revisit GANs from a more general perspective in game theory.In a normal two-player game, G and D have their own loss functions and try to minimize them. Their losses are interdependent. We denote the loss functions L G (G, D) and L D (G, D). Utility functions V G (G, D) and V D (G, D) are negative loss functions. GANs define a zero-sum game, where L G (G, D) = −L D (G, D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of the expected equilibrium in h (n) (x). The dotted areas are clusters of training data with different labels(colorful points). Unlabeled samples(white points) are mapped into a particular clusters. Distinct density gaps appear in the center, in which lie the generated samples(black points).</figDesc><graphic url="image-3.png" coords="4,83.83,83.69,180.19,156.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>c m 2 c</head><label>2</label><figDesc>deд c and max c,v i ∈V c λm c loss i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>d 0 &gt;</head><label>0</label><figDesc>loss C ′ would result in contradiction. According to analysis above and Assumption 2, all interior nodes in G c are assigned label a c in C min .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a), we visualize the training process in the Citeseer experiment using t-SNE algorithm (Cf. § 5 for detailed experimental settings). At the beginning, G generates samples very different from real samples and the boundaries of clusters are ambiguous. During training, classifier D gradually learns a non-linear transformation in h (n) (x) to map real and fake samples into distinct clusters, while G tries to generate samples in the central areas of real samples. Mini-batch training, loss pt in L G and Gaussian noises prevent G from overfitting on the only center point. Adversarial training finally reaches the expected equilibrium where fake samples are in the central areas surrounded by real samples clusters after 20 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 4(b)(c) show two representative patterns: (b) is a marginal node, whose ||д(x i )|| and p f (x i ) change synchronously and strictly share the same trend, while (c) is an interior node never predicted fake. The classifier function around (c) remains smooth after determining a definite label. Pearson correlation coefficient r p = cov(||д||, p f ) σ | |д | | σ p f exceeds 0.6, indicating obvious positive correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a)Visualization of outputs of feature layer during training. Color of point indicates its class, where black nodes indicate fake samples. (b)Typical ||д|| and p f curve for marginal nodes. Horizontal axis represents the number of training iterations, vertical axis representing the value. (c) Typical ||д|| and p f curve for interior nodes.</figDesc><graphic url="image-4.png" coords="8,53.80,83.68,504.37,262.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: GPU Memory Consumption of four typical semisupervised learning algorithms on graphs. GCN cannot handle large-scale networks. Planetoid and GraphSGAN are trained in mini-batch way, which makes them able to scale up. We reduce the dimensions of features in DIEL dataset(Cf.§ 5.3) so that the GPU consumption of GraphSGAN even decreases.</figDesc><graphic url="image-5.png" coords="9,71.82,211.00,204.21,153.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of LP with adversarial learning. Larger threshold means less marginal nodes. Two curves below take x i as inputs and curves above use outputs of the first layer of MLP.</figDesc><graphic url="image-6.png" coords="9,65.81,452.05,216.22,127.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• We formulate a novel competitive game between generator and discriminator for GraphSGAN and thoroughly analyze the dynamics, equilibrium and working principles during training. In addition, we generalize the working principles to improve traditional algorithms. Our theoretical proof and experimental verification both outline the effectiveness of this method.• We evaluate our model on several dataset with different scales. GraphSGAN significantly outperforms previous works and demonstrates outstanding scalability.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics</figDesc><table><row><cell>Dataset</cell><cell>nodes</cell><cell>edges</cell><cell cols="3">features classes labeled data</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell><cell>140</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell><cell>120</cell></row><row><cell>DIEL</cell><cell cols="3">4,373,008 4,464,261 1,233,597</cell><cell>4</cell><cell>3413.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of results of classification accuracy (%).</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell></row><row><cell></cell><cell>LP</cell><cell>68.0</cell><cell>45.3</cell></row><row><cell>Regularization</cell><cell>ICA</cell><cell>75.1</cell><cell>69.1</cell></row><row><cell></cell><cell>ManiReg</cell><cell>59.5</cell><cell>60.1</cell></row><row><cell></cell><cell>DeepWalk</cell><cell>67.2</cell><cell>43.2</cell></row><row><cell>Embedding</cell><cell>SemiEmb</cell><cell>59.0</cell><cell>59.6</cell></row><row><cell></cell><cell>Planetoid</cell><cell>75.7</cell><cell>64.7</cell></row><row><cell></cell><cell>Chebyshev</cell><cell>81.2</cell><cell>69.8</cell></row><row><cell>Convolution</cell><cell>GCN</cell><cell cols="2">80.1 ± 0.5 67.9 ± 0.5</cell></row><row><cell></cell><cell>GAT</cell><cell cols="2">83.0 ± 0.7 72.5 ± 0.7</cell></row><row><cell>Our Method</cell><cell cols="3">GraphSGAN 83.0 ± 1.3 73.1 ± 1.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Recall@k on DIEL dataset (%).</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>Recall@K</cell></row><row><cell></cell><cell>LP</cell><cell>16.2</cell></row><row><cell>Regularization</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ManiReg</cell><cell>47.7</cell></row><row><cell></cell><cell>DeepWalk</cell><cell>25.8</cell></row><row><cell>Embedding</cell><cell>SemiEmb</cell><cell>48.6</cell></row><row><cell></cell><cell>Planetoid</cell><cell>50.1</cell></row><row><cell>Original</cell><cell>DIEL</cell><cell>40.5</cell></row><row><cell>Our Method</cell><cell>GraphSGAN</cell><cell>51.8</cell></row><row><cell></cell><cell>Upper bound</cell><cell>61.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Please note that in semi-supervised learning, training and prediction are usually performed simultaneously. In this case, the learning considers both labeled nodes and unlabeled nodes, as well as the structure of the whole graph. In this paper, we mainly consider transductive learning setting, though the proposed model can be also applied to other machine learning settings. Moreover, we only consider undirected graphs, but the extension to directed graphs is straightforward.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The work is supported by the (2015AA124102), National Natural Science Foundation of China (61631013,61561130160), and the Royal Society-Newton Advanced Fellowship Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Subjectivity and correlation in randomized strategies</title>
		<author>
			<persName><surname>Robert J Aumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical Economics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="67" to="96" />
			<date type="published" when="1974">1974. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<pubPlace>Nov</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving distant supervision for information extraction using label propagation through lists</title>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="524" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data using graph mincuts</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchi</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Triple generative adversarial nets</title>
		<author>
			<persName><forename type="first">Taufik</forename><surname>Li Chongxuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4091" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07838</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial Network Embedding. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Good Semi-supervised Learning that Requires a Bad GAN</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09783</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-dimensional data analysis: The curses and blessings of dimensionality</title>
		<author>
			<persName><surname>David L Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMS Math Challenges Lecture</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS&apos;10</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>NIPS&apos;14. 2672-2680</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5540" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Weiyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Hwan</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sailung</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingli</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03545</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Learning Graph Topological Features via GAN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;03</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><surname>Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05473</idno>
		<title level="m">Auxiliary deep generative models</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno>KDD&apos;14. 701-710</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Network Embedding As Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec. In WSDM&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hubs in space: Popular nearest neighbors in high-dimensional data</title>
		<author>
			<persName><forename type="first">Miloš</forename><surname>Radovanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alexandros Nanopoulos, and Mirjana Ivanović</title>
				<imprint>
			<date type="published" when="2010-09">2010. Sep (2010</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2487" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Springenberg</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Arnet-Miner: Extraction and Mining of Academic Social Networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to Infer Social Ties in Large Networks</title>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD&apos;11</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning Social Graph Topologies using Generative Adversarial Neural Networks</title>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Hajibagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gita</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<idno>arXiv:1710.10903</idno>
		<ptr target="http://arxiv.org/abs/1710.10903" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph Attention Networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neumann</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Oskar</forename><surname>Morgenstern</surname></persName>
		</author>
		<title level="m">Theory of games and economic behavior</title>
				<imprint>
			<publisher>Princeton university press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>commemorative edition</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">GraphGAN: Graph Representation Learning with Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08267</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Thomas N Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;04</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;03</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
