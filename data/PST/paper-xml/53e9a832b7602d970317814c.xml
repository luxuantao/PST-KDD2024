<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning How to Inpaint from Global Image Statistics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anat</forename><surname>Levin</surname></persName>
							<email>alevin@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
							<email>zomet@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
							<email>yweiss¡@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning How to Inpaint from Global Image Statistics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F352C4A72029E2C0CA576E7665BE566B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inpainting is the problem of filling-in holes in images. Considerable progress has been made by techniques that use the immediate boundary of the hole and some prior information on images to solve this problem. These algorithms successfully solve the local inpainting problem but they must, by definition, give the same completion to any two holes that have the same boundary, even when the rest of the image is vastly different.</p><p>In this paper we address a different, more global inpainting problem. How can we use the rest of the image in order to learn how to inpaint? We approach this problem from the context of statistical learning. Given a training image we build an exponential family distribution over images that is based on the histograms of local features. We then use this image specific distribution to inpaint the hole by finding the most probable image given the boundary and the distribution. The optimization is done using loopy belief propagation. We show that our method can successfully complete holes while taking into account the specific image statistics. In particular it can give vastly different completions even when the local neighborhoods are identical.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inpainting, dis-occlusion and filling-in are various names for the same task: Given an image with a missing region (a hole), restore the values in the hole in an undetectable way <ref type="bibr" target="#b2">[3]</ref>. Applications include restoration of old images, removal of overlaid text and logos and removal of objects from images.</p><p>The inpainting problem is clearly ill-posed. Any method must therefore use some prior assumptions about the unknown missing values and their relations with the known hole neighborhood. Most existing approaches (see e.g. <ref type="bibr" target="#b16">[17]</ref> for a review) use a generic prior on images (e.g. high smoothness, low total variation or low curvature) and use an optimization to find the most probable completion given The results of the algorithm in <ref type="bibr" target="#b2">[3]</ref> run with a single resolution. As can be expected from a local algorithm, the completion is identical. In this paper we ask: how can we use the global information in the image to cause the completions to be different?</p><p>the prior model and the immediate boundary of the hole. We call these approaches local inpainting algorithms. Despite the impressive successes of local approaches, they must by definition give identical completions when the immediate boundary of the hole is identical. Consider Fig. <ref type="figure" target="#fig_0">1-a,b</ref>: two images of a square and a circle, each with a missing square region on the bottom-right part. While the circle and square are different, the small neighborhoods around the holes are identical. Indeed, up to numerical error, the gradients and gray levels in the immediate boundary of the hole, are identical. Thus any algorithm that is based on a generic model and these boundary conditions will restore the two holes identically.  of the local algorithm in <ref type="bibr" target="#b2">[3]</ref> 1 . While the completions are very reasonable given the local information, they do not appear "perceptually correct". Evidently our visual system is taking more global information into account.</p><p>The global statistics are similarly important in painting restoration, we would want to restore a hole in a Mondrian painting very differently from a locally identical hole in a Bruegel painting (indeed even a Picasso from the blue period should be restored completely differently from a Picasso in the cubist period). Figure <ref type="figure" target="#fig_2">2</ref> shows an example. The immediate vicinity of the two holes are very similar but the global statistics are very different: the cubist painting has a different "look" compared to the Blue period painting and we want inpainting algorithms to preserve this look. We call this problem "learning how to inpaint".</p><p>In this paper we address this problem in the context of statistical learning. We use the input image to learn a probability distribution over images that is in the exponential family. We then use loopy belief propagation to find the most probable completion of the hole under this learned distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous work</head><p>In most existing inpainting approaches the hole is estimated as the most "smooth" continuation of the local structure of the image, where smoothness can be defined in different ways. This main advantage of this approach is that when properly formulated it can be applied to an arbitrary image patch with minimal user interaction. A smooth continuation can be defined in various ways. Bertalmio et. al. <ref type="bibr" target="#b2">[3]</ref>, inspired by professional art restorators , propagate gra- 1 We thank Joan Verdera for providing these results. The results are with a single resolution. Multi resolution results may be different for the two figures dient direction and gray values from the surrounding neighborhood into the hole. They formulate the process elegantly in a PDE framework, and solve it using fast iterative solvers.</p><p>In a later paper <ref type="bibr" target="#b0">[1]</ref>, they use similar ideas, but reformulate the inpainting process in a variational framework. They propose to minimize the following cost function over the image and its normal field ¡ :</p><p>¢ ¤£ ¦¥ ¨ § © ¡ "! # $ &amp;% (' 0) 1 2 ! ' 43 5¥ ¨ § © 6 7 2 &amp;8 ¡ 59 2 ¨! (1)</p><p>The @ ¡ "! term penalizes curvature, the 2 term penalizes large gradients and the second integral is a relaxation of the constraint that the ¡ is indeed the normal field of . As we discuss in the next section, the impressive results in <ref type="bibr" target="#b0">[1]</ref> using only local image operators motivated us to use only local image statistics to define our prior.</p><p>Another way to define a smooth filling-in was presented by Chan and Shen <ref type="bibr" target="#b3">[4]</ref>, who minimized the total variation in the result image. As mentioned in <ref type="bibr" target="#b0">[1]</ref>, such approach handles noise very well, but tends to complete straight lines.</p><p>Filling-in of holes is also performed by texture synthesis algorithms where it is assumed that the missing data is part of a (usually homogeneous) texture. The region can be filled-in by a texture synthesis engine, e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref>. The texture-synthesis approach can process large holes, and fill them with rich structures learned from similar regions in the image. We found two application of texture synthesis to image inpainting. Hirani and Totsuka <ref type="bibr" target="#b12">[13]</ref> fill in a selected texture by combining spectral and spatial information, achieving impressive results. Criminisi et. al. used an exampler-based approach, adapting the synthesis method of Efros and Leung <ref type="bibr" target="#b8">[9]</ref> to image inpainting <ref type="bibr" target="#b4">[5]</ref>. Recent works combine texture synthesis with inpainting of structure <ref type="bibr" target="#b1">[2]</ref>.</p><p>Filling-in of holes in simple images consisting of a single contour has also been extensively studied in the human vision literature <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. These approaches also rely on a notion of smooth continuation, but in general they cannot be applied to an arbitrary image patch in a natural image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Exponential family models of image statistics</head><p>In order to learn how to inpaint, we want a method that will capture the "look" of a training image in a probability distribution over images. The main challenge is to estimate the parameters of such a distribution from a single image. Obviously, if one represented image patch statistics using a huge look up table of all possible A 1B 1C A 1B image patches and their respective probabilities then learning how to inpaint would be trivial but estimating such a table is, of course, impossible.</p><p>Our approach is motivated by the success of exponential family distributions in the modeling of images and natural language <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>. In this approach the probability of an image is defined by means of a small number of sufficient statistics or features, each of which can be evaluated at an arbitrary location in the image. The probability of an image is given by:</p><formula xml:id="formula_0">¥ §¦ © ! £ "! $# %! '&amp; )( 0 21 # 43 65 7# 43 § © ©8 98 (2)</formula><p>Where @ C BA 7C ! is the value of feature at location C BA 7C ! in the image , and is a normalization factor. The parameters of an exponential family distribution are the choice of features and the tables D @ ! : intuitively the larger the aver- age value of @ ! for a particular image, the more probable the image is.</p><p>Learning exponential family distributions from data is a difficult but well studied problem. Typically, one chooses the features based on a notion of informativeness and then chooses the tables based on the maximum likelihood or maximum entropy criterion <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref>. For exponential fam-ily distributions these criteria are identical and simplify to a requirement that the predicted marginals of all features match the empirical marginals in the data:</p><formula xml:id="formula_1">¥ FE £ $G ¦ H I ! £ QP¥ 0 FE £ RG ! (3)</formula><p>The computational difficulty arises from the left hand side of equation 3: calculating the predicted marginals requires summing over all possible images, and while Monte-Carlo methods can be applied they are still too slow for our application where the model is learned anew for every image that we process.</p><p>We simplify the learning in two ways. First, the features in the distribution are fixed for all images. Motivated by previous work on natural image statistics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> and the work of <ref type="bibr" target="#b0">[1]</ref> we chose two features that we thought would be informative: gradient magnitude E TS 1 C UA VC 6! £ 2 C UA VC 6! and pairwise gradient angle. Pairwise gradient angle is defined for every pair of neighboring pixels C S A 7C S ! )A C §W XA 7C XW ! . Denote the gradients S £ 2 C S A 7C S ! , ¢W £ 2 C §W XA 7C ¢W ! .</p><p>We define the angle ¡ S W between the two gradients as ¡ S W £ ¢ ¤£ ¦¥ ¤ § S ©¨ A ¦ W ! and set:</p><formula xml:id="formula_2">E W C S A 7C S A C W A 7C W ! £ ¡ S W<label>(4)</label></formula><p>Note that this feature is measured for all pairs of neighboring pixels: we do not assume that we can decide which pairs of gradients belong to the same curve. Since the angle of the gradient is noisy at gradients of low magnitude, we only measure the angle feature at gradients above a certain threshold. Figure <ref type="figure" target="#fig_3">3</ref> shows the marginal histograms of these two features on a natural image and the two synthetic images described earlier. Note that the different histograms capture the different "looks" of the three images. The fact that the natural image is more textured than the synthetic images is captured in the magnitude histogram: there are much more nonzero gradients in the real image. The difference between the circle and the square is captured in the relative angle histogram. In the square the distribution is bimodal, indicating that adjacent pixels either have identical angle (i.e. along straight lines) or have a very different angle (i.e. at sharp corners). In the circle, adjacent pixels always have similar angle (i.e. there are no sharp corners). (Note that the axis limits in the angle histograms are vastly different for the square and the circle: in the circle all angular differences are between B and B B ¦ ).</p><p>The second simplification we make is the method by which we estimate . To understand the complexity of solv- ing equation 3 note that equation 2 with our choice of features is equivalent to a Markov Random Field distribution on the gradient field, ¨! of an image :</p><formula xml:id="formula_3">¥ ! 7! £ Q ! ! ¤" # ! A $ !<label>(5)</label></formula><p>where % '&amp; )( refers to pairs of neighboring pixels in the image and ¢ ! £ 10 ¤2 43 S ¢ ! 7! and ! ¢ 7A $ 5 ! £ 0 ¤2 43 ( W ¡ ! ! . Thus estimating D 6 is as difficult as estimat- ing the potential functions in a MRF. Since exact ML estimation of the potentials of the MRF is intractable, a number of approximations have been proposed. We used an approximation similar to the one used in <ref type="bibr" target="#b9">[10]</ref> where the potentials are approximated by the empirical marginal and conditional probabilities. Specifically we set:</p><formula xml:id="formula_4">¢ ! £ P¥ 7 ¢ ! (6) ! ¢ VA $ 5 ! £ P¥ ¡ ! !<label>(7)</label></formula><p>It is easy to show that when the MRF is singly connected these settings will give rise to tables that exactly sat- isfy the maximum likelihood equation (eq. 3). Thus if we wanted to model a single scan line of an image, these parameters would be exact. In our case, of course, the graph has many loops so these parameters are not the maximum likelihood parameters, but this approximation has proven to be successful in a number of vision applications <ref type="bibr" target="#b9">[10]</ref>.</p><p>The only tweakable parameter in our model is the definition of the region over which the histograms are calculated. This can either be the entire image, a subregion of the input image or even a different image that has a similar "look" to the one the user wants.</p><p>To summarize: in order to fill in a hole in the image, we first measure histograms of our features over the training image, and then search for an integrable gradient field that agrees with the image gradients on the boundary of the hole and maximizes the probability defined by equations 5,6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimization using loopy Belief Propagation</head><p>In order to find the most probable filling-in we need to optimize the probability, conditioned on the hole boundary</p><formula xml:id="formula_5">7 : ¥ 7 ! £ W 8 ! 6 ¤" 9 6 A @ ! ! BA C" 6 BA A @ A $ A ! (8)</formula><p>where W is a normalization factor and ! BA enforces integrability of the gradient field (differentiating the x derivative with respect to y should give the same answer as differentiating the y derivative with respect to x). The product is taken over all gradients inside the hole and those in the boundary, and gradients on the boundary are fixed to their observed values.</p><p>Naive optimization of equation 8 is of course exponential in the size of the hole. The max-product belief propagation algorithm <ref type="bibr" target="#b19">[20]</ref> is a local, message passing algorithm that can be used to perform the optimization. It is guaranteed to find the global optimum when the graph has no loops. In our case, the graphical model defined by equation 8 has many loops. Nevertheless motivated by the recent results on similar graphs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> we expected good results for our problem. When it converges, the max-product algorithm is guaranteed to find a gradient field © X that is a local maximum of equation 8 with respect to a large neighborhood <ref type="bibr" target="#b19">[20]</ref>. Finally, given the gradient field we integrate it by robustly solving the following linear equation:</p><formula xml:id="formula_6">D C £ (9)</formula><p>where C is a vectorized version of the image, D is the differentiation matrix of size A FE HG IE (where E is the number of pixels) and is a vectorized version of the gradient field. This is an overconstrained set of equations and we find the solution that minimizes the P S norm using linear programming.</p><p>In order to run the max-product belief propagation algorithm one needs to discretize the gradient field. We used A 1B candidate gradients chosen using a clustering algorithm from the gradients of the input image surrounding the hole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the previous section, we saw that the marginal statistics of the square and circle are quite different. Can our algorithm use this difference to correctly learn how to inpaint?</p><p>The results are shown in figure <ref type="figure" target="#fig_4">4</ref>. The training image was the full image. To avoid aliasing artifacts we anti-aliased both figures. Since the number of distinct gradients in the areas surrounding the holes here are quite small, we augmented the discretization with an additional set of gradients chosen to tile the space of orientations. The same discrete set of gradients was used for both images. Note that the algorithm correctly adapts to the particular image and completes a circle in one case and a square in the second case, despite the fact that the local neighborhoods are identical. This is due to the vastly different relative angle histograms in the two images. For the square image, the relative angle is almost always zero, except for a few instances where the relative angle is large. For the circle image, on the other hand, the relative angle is typically small but nonzero. The right column shows what happens when each image is filled-in based on marginal statistics from the other image.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the output of our algorithm on the two Pablo Picasso paintings with different styles. For each image, we used a small patch from that image to estimate the histograms. To avoid aliasing artifacts the images are smoothed. While the holes in the two images are not identical, they are very similar locally. Yet our algorithm completes sharp corners for the cubist painting and smooth curves for the blue period painting.</p><p>Can we learn how to inpaint in real images ? Figure <ref type="figure" target="#fig_6">6</ref> shows a result. We trained the algorithm on an urban scene and a fruit image (top row). We then used the urban scene statistics to fill-in the holes in the ruins image and the fruit scene statistics to fill-in the fruit image. As a result, our algorithm completes sharp corners on the ruins but smooth curves on the fruit. Again, note that the boundary of the hole in the fruit image is very similar (up to rotation) to the boundaries of the top two holes in the ruins image. Thus classical inpainting approaches should give similar results in the two cases.</p><p>In the previous two examples, our algorithm can successfully adapt to the image statistics and give completions that depend not only on the local boundary of the hole but also on the global "look" of the image. This is in contrast to existing inpainting approaches that give identical completions when the boundary is identical. However, given the success of existing approaches in many images, one wonders: will our algorithm cause a decrease in performance in images where the boundary information is sufficient? Figure <ref type="figure" target="#fig_7">7</ref> shows that our algorithm also performs well in the cases where classical algorithms perform well. In fact, the completion is very similar to the one calculated by <ref type="bibr" target="#b2">[3]</ref>. This is because the marginal statistics in this image (figure 3) favor "smooth" completions: since both the magnitude histogram and the angle histogram are peaked at zero, the learned distribution favors completions with short lines and low curvature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Inpainting is obviously an ill-posed problem and hence is impossible without some assumption about the statistics of images. In that sense, all previous approaches to inpainting can also be viewed as having an implicit probabilistic model of images: e.g. that images tend to be smooth <ref type="bibr" target="#b2">[3]</ref> or that images tend to contain homogeneous texture <ref type="bibr" target="#b7">[8]</ref>. In this work, we have asked: how can we learn this probabilistic assumption from the input image? We have shown that a model based on histograms of local features can capture the "look" of an image and shown how to use these histograms to define the filling in of a hole.</p><p>In future work we would like to extend our model to use more image features. In particular the probability model presented in this paper does a poor job of representing texture. It would be interesting to see whether a small number of additional features (e.g. the ones used in <ref type="bibr" target="#b15">[16]</ref>) would enable our algorithm to inpaint textured regions. A promising direction to explore is to choose the features anew for the particular image that needs to be inpainted. This would require efficient approximations to the Minimax approach used in <ref type="bibr" target="#b21">[22]</ref>. An alternative way to use our method in textured images is to use the decomposition into a "structure" image and a "texture image" proposed in <ref type="bibr" target="#b1">[2]</ref> and apply our current method only to the structure image.</p><p>In future work we would also like to explore other optimization methods. While our framework is probabilistic, we are interested only in the most probable completion so calculation of marginal probabilities is not necessary. We are interested in exploring some of the powerful optimization techniques used in local inpainting to find the most probable completion given our model. In particular, our current optimization algorithm is single scale and hence our completions are not as sharp as the state of the art local algorithms. One approach worth exploring is to initialize the local algorithms with our completion and thus obtain a sharper image.</p><p>The challenge of defining simple probability models that capture the "look" of an image is common to a large number of problems in vision and image processing including superresolution, denoising, transparency analysis and more. We believe that progress in learning how to inpaint will directly translate into progress in these additional domains.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. a.-b. Two images with holes. In both cases the boundary of the holes are identical thus local inpainting algorithms would complete them identically. c.-d. The results of the algorithm in<ref type="bibr" target="#b2">[3]</ref> run with a single resolution. As can be expected from a local algorithm, the completion is identical. In this paper we ask: how can we use the global information in the image to cause the completions to be different?</figDesc><graphic coords="1,348.96,333.09,72.01,72.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figs 1-c,dshow the results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Two Pablo Picasso paintings with holes. The vicinity of the hole is nearly identical in the two paintings but the global style is vastly different. We would like the completion to be different.</figDesc><graphic coords="2,150.72,72.09,152.98,144.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The marginal statistics of the features we are using: gradient magnitude (center) and relative gradient angle (right).These simple, local histograms capture the different "look" of the three images. Note that the axis limits in the angle histograms are vastly different for the square and the circle: in the circle all angular differences are between and ¢¡ ¤£ .</figDesc><graphic coords="3,143.91,310.74,106.81,106.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Filling in a circle and a rectangle. The completion depends on the marginal statistics. Middle column: Each figure is completed based on marginal statistics taken from the same image. The circle completion is curved and the square completion has a sharp corner. Right column: Each image is completed based on marginal statistics taken from the other image.</figDesc><graphic coords="6,166.68,147.70,72.00,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The output of our algorithm on the two Pablo Picasso paintings with different styles. The algorithm completes sharp corners for the cubist painting and smooth curves for the blue period painting.</figDesc><graphic coords="6,20.64,291.09,106.01,144.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Filling in two images with different statistics. Top: Two different training images. In the urban scene there are many corners while in the fruit scene most curves are smooth. Middle: Two images to be filled in. We would like holes to be filled in differently even though the boundaries of the top two holes in the ruins image are very similar to the boundary of the hole in the fruit image. Bottom: The results of our algorithm. The algorithm trained on an urban scene completes sharp corners while the one trained on fruit completes smooth curves.</figDesc><graphic coords="7,362.76,511.30,118.21,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Results of our algorithm on the Monty Python image.</figDesc><graphic coords="8,5.40,72.10,286.18,144.00" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simultaneous structure and texture image inpainting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mathematical models for local nontexture inpaintings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1019" to="1043" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object removal by exemplar-based inpainting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiresolution sampling procedure for analysis and synthesis of texture images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Bonet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-GRAPH</title>
		<meeting>SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inducing features of random fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="380" to="393" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to estimate scenes from images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Very loopy belief propagation for unwrapping phase images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petrovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Information Processing Systems 14</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analsysis and synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining frequency and spatial domain information for fast interactive image noise removal</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Totsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="269" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to perceive transparency from the statistics of natural scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="607" to="608" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="71" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inpainting and the fundamental problem of image processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM news</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical models for images:compression restoration and synthesis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Asilomar Conference on Signals, Systems and Computers</title>
		<meeting>Asilomar Conference on Signals, Systems and Computers</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">filling in the gaps: the shape of subjective contours and a model for their generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the optimality of solutions of the max-product belief propagation algorithm in arbitrary graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="723" to="735" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Orientation, scale, and discontinuity as emergent properties of illusory contour shape</title>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thornber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimax entropy principle and its application to texture modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1627" to="1660" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
