<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Manifold Mixup: Better Representations by Interpolating Hidden States</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
							<email>&lt;vikasverma.iitm@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Montréal Institute for Learning Algorithms (MILA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
							<email>&lt;lambalex@iro.umontreal.ca&gt;</email>
							<affiliation key="aff1">
								<orgName type="institution">Montréal Institute for Learning Algorithms (MILA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Montréal Institute for Learning Algorithms (MILA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Sharif Univer-sity of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Montréal Institute for Learning Algorithms (MILA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Facebook Research Correspondence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Montréal Institute for Learning Algorithms (MILA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Vikas Verma</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Manifold Mixup: Better Representations by Interpolating Hidden States</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn flatter class-representations, that is, with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it empirically on practical situations, and connect it to the previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks are the backbone of state-of-the-art systems for computer vision, speech recognition, and language translation <ref type="bibr" target="#b15">(LeCun et al., 2015)</ref>. However, these systems perform well only when evaluated on instances very similar to those from the training set. When evaluated on slightly different distributions, neural networks often provide incorrect predictions with strikingly high confidence. This is a worrying prospect, since deep learning systems are being deployed in settings where data may be subject to distributional shifts. Adversarial examples <ref type="bibr" target="#b22">(Szegedy et al., 2014)</ref> are one such failure case: deep neural networks with nearly perfect performance provide incorrect predictions with very high confidence when evaluated on perturbations imperceptible to the human eye. Adversarial examples are a serious hazard when deploying machine learning systems in security-sensitive applications. More generally, deep learning systems quickly degrade in performance as the distributions of training and testing data differ slightly from each other <ref type="bibr" target="#b4">(Ben-David et al., 2010)</ref>.</p><p>In this paper, we realize several troubling properties concerning the hidden representations and decision boundaries of state-of-the-art neural networks. First, we observe that the decision boundary is often sharp and close to the data. Second, we observe that the vast majority of the hidden representation space corresponds to high confidence predictions, both on and off of the data manifold.</p><p>Motivated by these intuitions we propose Manifold Mixup (Section 2), a simple regularizer that addresses several of these flaws by training neural networks on linear combinations of hidden representations of training examples. Previous work, including the study of analogies through word embeddings (e.g. king − man + woman ≈ queen), has shown that interpolations are an effective way of combining factors <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref>. Since high-level representations are often low-dimensional and useful to linear classifiers, linear interpolations of hidden representations should explore meaningful regions of the feature space effectively. To use combinations of hidden representations of data as novel training signal, we also perform the same linear interpolation in the associated pair of one-hot labels, leading to mixed examples with soft targets.</p><p>To start off with the right intuitions, Figure <ref type="figure" target="#fig_0">1</ref> illustrates the impact of Manifold Mixup on a simple two-dimensional classification task with small data. In this example, vanilla training of a deep neural network leads to an irregular decision boundary (Figure <ref type="figure" target="#fig_0">1a</ref>), and a complex arrangement of hidden representations (Figure <ref type="figure" target="#fig_0">1d</ref>). Moreover, every point in both the raw (Figure <ref type="figure" target="#fig_0">1a</ref>) and hidden (Figure <ref type="figure" target="#fig_0">1d</ref>) data representations is assigned a prediction with very high confidence. ). Second, it improves the arrangement of hidden representations and encourages broader regions of low-confidence predictions (from d. to e.). Black dots are the hidden representation of the inputs sampled uniformly from the range of the input space. Third, it flattens the representations (c. at layer 1, f. at layer 3). Figure <ref type="figure">2</ref> shows that these effects are not accomplished by other well-studied regularizers (input mixup, weight decay, dropout, batch normalization, and adding noise to the hidden representations).</p><p>This includes points (depicted in black) that correspond to inputs from off of the data manifold! In contrast, training the same deep neural network with Manifold Mixup leads to a smoother decision boundary (Figure <ref type="figure" target="#fig_0">1b</ref>) and a simpler (linear) arrangement of hidden representations (Figure <ref type="figure" target="#fig_0">1e</ref>). In sum, the representations obtained by Manifold Mixup have two desirable properties: the class-representations are flattened into a minimal amount of directions of variation, and all points in-between these flat representations, most unobserved during training and off the data manifold, are assigned low-confidence predictions. This example conveys the central message of this paper:</p><p>Manifold mixup improves the hidden representations and decision boundaries of neural networks at multiple layers.</p><p>More specifically, Manifold Mixup improves generalization in deep neural networks because it:</p><p>• Leads to smoother decision boundaries that are further away from the training data, at multiple levels of representation. Smoothness and margin are wellestablished factors of generalization <ref type="bibr">(Bartlett &amp; Shawetaylor, 1998;</ref><ref type="bibr" target="#b16">Lee et al., 1995)</ref>.</p><p>• Leverages interpolations in deeper hidden layers, which capture higher level information <ref type="bibr" target="#b26">(Zeiler &amp; Fergus, 2013)</ref> to provide additional training signal.</p><p>• Flattens the class-representations, reducing their number of directions with significant variance (Section 3). This can be seen as a form of compression, which is linked to generalization by a well-established theory <ref type="bibr" target="#b23">(Tishby &amp; Zaslavsky, 2015;</ref><ref type="bibr" target="#b21">Shwartz-Ziv &amp; Tishby, 2017)</ref>.</p><p>Throughout a wide variety of experiments, we demonstrate four substantial benefits of Manifold Mixup:</p><p>• Better generalization than other competitive regularizers (such as Cutout, Mixup, AdaMix, and Dropout) (Section 5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Space</head><p>Weight Decay Hidden space</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise</head><p>Dropout Batch-Norm Input Mixup</p><p>Figure <ref type="figure">2</ref>: The same experimental setup as Figure <ref type="figure" target="#fig_0">1</ref>, but using a variety of competitive regularizers. This shows that the effect of concentrating the hidden states for each class and providing a broad region of low confidence between the regions is not accomplished by the other regularizers (although input space mixup does produce regions of low confidence, it does not flatten the class-specific state distribution). Noise refers to gaussian noise in the input layer, dropout refers to dropout of 50% in all layers except the bottleneck itself (due to its low dimensionality), and batch normalization refers to batch normalization in all layers.</p><p>• Improved log-likelihood on test samples (Section 5.1).</p><p>• Increased performance at predicting data subject to novel deformations (Section 5.2).</p><p>• Improved robustness to single-step adversarial attacks. This is evidence Manifold Mixup pushes the decision boundary away from the data in some directions (Section 5.3). This is not to be confused with full adversarial robustness, which is defined in terms of moving the decision boundary away from the data in all directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Manifold Mixup</head><p>Consider training a deep neural network f (x) = f k (g k (x)), where g k denotes the part of the neural network mapping the input data to the hidden representation at layer k, and f k denotes the part mapping such hidden representation to the output f (x). Training f using Manifold Mixup is performed in five steps. First, we select a random layer k from a set of eligible layers S in the neural network. This set may include the input layer g 0 (x). Second, we process two random data minibatches (x, y) and (x , y ) as usual, until reaching layer k. This provides us with two intermediate minibatches (g k (x), y) and (g k (x ), y ). Third, we perform Input Mixup <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref> on these intermediate minibatches. This produces the mixed minibatch:</p><formula xml:id="formula_0">(g k , ỹ) := (Mix λ (g k (x), g k (x )), Mix λ (y, y )), where Mix λ (a, b) = λ • a + (1 − λ) • b.</formula><p>Here, (y, y ) are one-hot labels, and the mixing coefficient λ ∼ Beta(α, α) as proposed in mixup <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref>. For instance, α = 1.0 is equivalent to sampling λ ∼ U (0, 1). Fourth, we continue the forward pass in the network from layer k until the output using the mixed minibatch (g k , ỹ). Fifth, this output is used to compute the loss value and gradients that update the parameters of the neural network.</p><p>Mathematically, Manifold Mixup minimizes:</p><formula xml:id="formula_1">L(f ) = E (x,y)∼P E (x ,y )∼P E λ∼Beta(α,α) E k∼S (1) (f k (Mix λ (g k (x), g k (x ))), Mix λ (y, y )).</formula><p>Some implementation considerations. We backpropagate gradients through the entire computational graph, including those layers before the mixup layer k (Section 5.1 and appendix Section B explore this issue in more detail). In the case where S = {0}, Manifold Mixup reduces to the original mixup algorithm of <ref type="bibr" target="#b27">Zhang et al. (2018)</ref>. While one could try to reduce the variance of the gradient updates by sampling a random (k, λ) per example, we opted for the simpler alternative of sampling a single (k, λ) per minibatch, which in practice gives the same performance. As in Input Mixup, we use a single minibatch to compute the mixed minibatch. We do so by mixing the minibatch with copy of itself with shuffled rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Manifold Mixup Flattens Representations</head><p>We turn to the study of how Manifold Mixup impacts the hidden representations of a deep neural network. At a high level, Manifold Mixup flattens the class-specific representations. More specifically, this flattening reduces the number of directions with significant variance (akin to reducing their</p><formula xml:id="formula_2">B1 B2 A1 A2 B1 B2 A1 A2 B1 B2 A1 A2</formula><p>Figure <ref type="figure">3</ref>: We show a simple case with two classes (interpolations depicted as grey lines) illustrating why Manifold Mixup learns flatter representations. The interpolation between A1 and B2 in the left panel soft-labels the black dot as 50% red and 50% blue, regardless of being very close to a blue point. In the middle panel a different interpolation between A2 and B1 soft-labels the same point as 95% blue and 5% red. However, since Manifold Mixup learns the hidden representations, the pressure to predict consistent soft-labels at interpolated points causes the states to become flattened (right panel).</p><p>number of principal components).</p><p>In the sequel, we first prove a theory (Section 3.1) that characterizes this behavior precisely under idealized conditions. Second, we show that this flattening also happens in practice, by performing the SVD of class-specific representations of neural networks trained on real datasets (Section 3.2). Finally, we discuss why the flattening of class-specific representations is a desirable property (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Theory</head><p>We start by characterizing how the representations of a neural network are changed by Manifold Mixup, under a simplifying set of assumptions. More concretely, we will show that if one performs mixup in a sufficiently deep hidden layer in a neural network, then the loss can be driven to zero if the dimensionality of that hidden layer dim (H) is greater than the number of classes d. As a consequence of this, the resulting representations for that class will fall onto a subspace of dimension dim (H) − d + 1.</p><p>A more intuitive and less formal version of this argument is given in Figure <ref type="figure">3</ref> and Appendix F.</p><p>To this end, assume that X and H denote the input and representation spaces, respectively. We denote the label-set by Y and let Z = X × Y. Let G ⊆ H X denote the set of functions realizable by the neural network, from the input to the representation. Similarly, let F ⊆ Y H be the set of all functions realizable by the neural network, from the representation to the output.</p><p>We are interested in the solution of the following problem in some asymptotic regimes:</p><formula xml:id="formula_3">J(P ) = inf g∈G,f ∈F E (x,y),(x ,y ),λ<label>(2)</label></formula><p>(f (Mix λ (g(x), g(x ))), Mix λ (y, y )).</p><p>More specifically, let P D be the empirical distribution defined by a dataset D = {(x i , y i )} n i=1 . Then, let f ∈ F and g ∈ G be the minimizers of (2) for P = P D . Assume the number of layers are asymptotically increased, which results in G → H X , F → Y H , with H being a vector space. These is a direct consequence of the universal approximation theorem <ref type="bibr" target="#b5">(Cybenko, 1989)</ref>, which states that the mappings realizable by large neural networks are dense in the set of all continuous bounded functions. Under this setting, the objective (2) can be rewritten as:</p><formula xml:id="formula_4">J(P D ) = inf h1,...,hn∈H 1 n (n − 1) n i =j inf f ∈F (3) 1 0 (f (Mix λ (h i , h j )), Mix λ (y i , y j )) p(λ)dλ ,</formula><p>where h i = g(x i ). We now propose our main theorem, which establishes sufficient conditions for Manifold Mixup to achieve zero training error. Theorem 1. Let H be a space with dimension dim (H), and let d ∈ N to represent the number of classes in a dataset D. If dim (H) ≥ d−1, then J(P D ) = 0 and the corresponding minimizer f is a linear function from H to R d .</p><p>Proof. First, we observe that the following statement is true</p><formula xml:id="formula_5">if dim (H) ≥ d − 1: ∃ A, H ∈ R dim(H)×d , b ∈ R d : A H + b1 d = I d×d ,</formula><p>where I d×d and 1 d denote the d-dimensional identity matrix and all-one vector, respectively. In fact, b1 d is a rank-one matrix, while the rank of identity matrix is d. Therefore, A H only needs to be rank d − 1.</p><formula xml:id="formula_6">Let f (h) = A h + b for all h ∈ H. Let g (x i ) = H ζi,:</formula><p>be the ζ i -th column of H, where ζ i ∈ {1, . . . , d} stands for the class-index of the example x i . These choices minimize our objective (3) to zero, since:</p><formula xml:id="formula_7">(f (Mix λ (g (x i ), g (x j ))), Mix λ (y i , y j )) = (A Mix λ (H ζi,: , H ζj ,: ) + b, Mix λ (y i,ζi , y j,ζj )) = (u, u) = 0.</formula><p>The result follows from A H ζi,: + b = y i,ζi for all i. This result implies that if the Manifold Mixup loss is minimized, then the representation of each class lies on a subspace of dimension dim (H) − d + 1. In the extreme case where dim (H) = d − 1, each class representation will collapse to a single point, meaning that hidden representations would not change in any direction, for each class-conditional manifold. In the more general case with larger dim (H), the majority of directions in H-space will contain zero variance in the class-conditional manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Empirical Investigation of Flattening</head><p>We now show that the "flattening" theory that we have just developed also holds in practice, for real neural networks networks trained on real data. To this end, we trained a collection of fully-connected neural networks on the MNIST dataset using multiple regularizers, including Manifold Mixup. When using Manifold Mixup, we mixed representations at a single, fixed hidden layer per network.</p><p>After training, we performed the Singular Value Decomposition (SVD) of the hidden representations of each network, and analyzed their spectrum decay.</p><p>More specifically, we computed the largest singular value per class, as well as the sum of the all other singular values. We computed these statistics at the first hidden layer for all networks and regularizers. For the largest singular value, we obtained: 51.73 (baseline), 33.76 (weight decay), 28.83 (dropout), 33.46 (input mixup), and 31.65 (manifold mixup). For the sum of all the other singular values, we obtained: 78.67 (baseline), 73.36 (weight decay), 77.47 (dropout), 66.89 (input mixup), and 40.98 (manifold mixup). Therefore, weight decay, dropout, and input mixup all reduce the largest singular value, but only Manifold Mixup achieves a reduction of the sum of the all other singular values (e.g. flattening). For more details regarding this experiment, consult Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Why is Flattening Representations Desirable?</head><p>We have presented evidence to conclude that Manifold Mixup leads to flatter class-specific representations, and that such flattening is not accomplished by other regularizers.</p><p>But why is this flattening desirable? First, it means that the hidden representations computed from our data occupy a much smaller volume. Thus, a randomly sampled hidden representation within the convex hull spanned by the data in this space is more likely to have a classification score with lower confidence (higher entropy). Second, compression has been linked to generalization in the information theory literature <ref type="bibr" target="#b23">(Tishby &amp; Zaslavsky, 2015;</ref><ref type="bibr" target="#b21">Shwartz-Ziv &amp; Tishby, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Regularization is a major area of research in machine learning. Manifold Mixup is a generalization of Input Mixup, the idea of building random interpolations between training examples and perform the same interpolation for their labels <ref type="bibr" target="#b27">(Zhang et al., 2018;</ref><ref type="bibr" target="#b24">Tokozume et al., 2018)</ref>.</p><p>Intriguingly, our experiments show that Manifold Mixup changes the representations associated to the layers before and after the mixing operation, and that this effect is crucial to achieve good results (Section 5.1, Appendix G). This suggests that Manifold Mixup may work for different reasons than Input Mixup.</p><p>Another line of research closely related to Manifold Mixup involves regularizing deep networks by perturbing their hidden representations. These methods include dropout <ref type="bibr" target="#b13">(Hinton et al., 2012)</ref>, batch normalization <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref>, and the information bottleneck <ref type="bibr" target="#b0">(Alemi et al., 2017)</ref>. Notably, <ref type="bibr" target="#b13">(Hinton et al., 2012)</ref> and <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref> demonstrated that regularizers that work well in the input space can also be applied to the hidden layers of a deep network, often to further improve results. We believe that Manifold Mixup is a complimentary form of regularization.</p><p>Zhao &amp; Cho (2018) explored improving adversarial robustness by classifying points using a function of the nearest neighbors in a fixed feature space. This involves applying mixup between each set of nearest neighbor examples in that feature space. The similarity between <ref type="bibr" target="#b28">(Zhao &amp; Cho, 2018)</ref> and Manifold Mixup is that both consider linear interpolations of hidden representations with the same interpolation applied to their labels. However, an important difference is that Manifold Mixup backpropagates gradients through the earlier parts of the network (the layers before the point where mixup is applied), unlike <ref type="bibr" target="#b28">(Zhao &amp; Cho, 2018)</ref>. In Section 3 we explain how this discrepancy significantly affects the learning process.</p><p>AdaMix <ref type="bibr" target="#b10">(Guo et al., 2018a)</ref> is another related method which attempts to learn better mixing distributions to avoid overlap. AdaMix performs interpolations only on the input space, reporting that their method degrades significantly when applied to hidden layers. Thus, AdaMix may likely work for different reasons than Manifold Mixup, and perhaps the two are complementary. AgrLearn <ref type="bibr" target="#b11">(Guo et al., 2018b)</ref> adds an information bottleneck layer to the output of deep neural networks. AgrLearn leads to substantial improvements, achieving 2.45% test error on CIFAR-10 when combined with Input Mixup <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref>. As AgrLearn is complimentary to Input Mixup, it may be also complimentary to Manifold Mixup. <ref type="bibr" target="#b25">Wang et al. (2018)</ref> proposed an interpolation exclusively in the output space, does not backpropagate through the interpolation procedure, and has a very different framing in terms of the Euler-Lagrange equation (Equation <ref type="formula" target="#formula_3">2</ref>) where the cost is based on unlabeled data (and the pseudolabels at those points) and the labeled data provide constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We now turn to the empirical evaluation of Manifold Mixup. We will study its regularization properties in supervised learning (Section 5.1), as well as how it affects the robustness of neural networks to novel input deformations (Section 5.2), and adversarial examples (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Generalization on Supervised Learning</head><p>We train a variety of residual networks <ref type="bibr" target="#b12">(He et al., 2016)</ref> using different regularizers: no regularization, AdaMix, Input Mixup, and Manifold Mixup. We follow the training procedure of <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref>, which is to use SGD with momentum, a weight decay of 10 −4 , and a step-wise learning rate decay. Please refer to Appendix C for further details (including the values of the hyperparameter α). We show results for the CIFAR-10 (Table <ref type="table">1a</ref>), CIFAR-100 (Table <ref type="table">1b</ref>), SVHN (Table <ref type="table" target="#tab_1">2</ref>), and TinyImageNET (Table <ref type="table" target="#tab_2">3</ref>) datasets. Manifold Mixup outperforms vanilla training, AdaMix, and Input Mixup across datasets and model architectures. Furthermore, Manifold Mixup leads to models with significantly better Negative Log-Likelihood (NLL) on the test data. In the case of CIFAR-10, Manifold Mixup models achieve as high as 50% relative improvement of test NLL.</p><p>As a complimentary experiment to better understand why Manifold Mixup works, we zeroed gradient updates immediately after the layer where mixup is applied. On the dataset CIFAR-10 and using a PreActResNet18, this led to a 4.33% test error, which is worse than our results for Input Mixup and Manifold Mixup, yet better than the baseline. Because Manifold Mixup select the mixing layer at random, each layer is still being trained even when zeroing gradients, although it will receive less updates. This demonstrates that Manifold Mixup improves performance by updating the layers both before and after the mixing operation.</p><p>We also compared Manifold Mixup against other strong regularizers. For each regularizer, we selected the best hyper-parameters using a validation set. Then, training a PreActResNet50 on CIFAR-10 for 600 epochs led to the following test errors (%): no regularization (4.96 ± 0.19), Dropout (5.09 ± 0.09), Cutout <ref type="bibr" target="#b6">(Devries &amp; Taylor, 2017)</ref> (4.77 ± 0.38), Mixup (4.25 ± 0.11), and Manifold Mixup (3.77 ± 0.18). (Note that the results in Table <ref type="table">1</ref> for PreAc-tResNet were run for 1200 epochs, and therefore are not directly comparable to the numbers in this paragraph.)</p><p>To provide further evidence about the quality of representations learned with Manifold Mixup, we applied a k-nearest neighbour classifier on top of the features extracted from a PreActResNet18 trained on CIFAR-10. We achieved test errors of 6.09% (vanilla training), 5.54% (Input Mixup), and 5.16% (Manifold Mixup).</p><p>Finally, we considered a synthetic dataset where the data generating process is a known function of disentangled factors of variation, and mixed in this space factors. As shown in Appendix A, this led to significant improvements in performance. This suggests that mixing in the correct level of representation has a positive impact on the decision boundary. However, our purpose here is not to make any claim about when do deep networks learn representations corresponding to disentangled factors of variation.</p><p>Finally, Table <ref type="table" target="#tab_3">4</ref> and Table <ref type="table" target="#tab_5">6</ref> show the sensitivity of Manifold Mixup to the hyper-parameter α and the set of eligible layers S. (These results are based on training a PreActResNet18 for 2000 epochs, so these numbers are not exactly comparable to the ones in Table <ref type="table">1</ref>.) This shows that Manifold Mixup is robust with respect to choice of hyper-parameters, with improvements for many choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Generalization to Novel Deformations</head><p>To further evaluate the quality of representations learned with Manifold Mixup, we train PreActResNet34 models on the normal CIFAR-100 training split, but test them on novel (not seen during training) deformations of the test split. These deformations include random rotations, random shearings, and different rescalings. Better representations should generalize to a larger variety of deformations. Table 5 shows that networks trained using Manifold Mixup are the most able to classify test instances subject to novel deformations, which suggests the learning of better representations. For more results see Appendix C, Table <ref type="table">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Robustness to Adversarial Examples</head><p>Adversarial robustness is related to the position of the decision boundary relative to the data. Because Manifold Mixup only considers some directions around data points (those Table <ref type="table">1</ref>: Classification errors on (a) CIFAR-10 and (b) CIFAR-100. We include results from <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref> † and <ref type="bibr" target="#b9">(Guo et al., 2016)</ref> ‡. We run experiments five times to report the mean and the standard deviation of errors and neg-log-likelihoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PreActResNet18</head><p>Test   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An experiment on a network trained on the 2D spiral dataset with a 2D bottleneck hidden state in the middle of the network. Manifold mixup has three effects on learning when compared to vanilla training. First, it smoothens decision boundaries (from a. to b.). Second, it improves the arrangement of hidden representations and encourages broader regions of low-confidence predictions (from d. to e.). Black dots are the hidden representation of the inputs sampled uniformly from the range of the input space. Third, it flattens the representations (c. at layer 1, f. at layer 3). Figure2shows that these effects are not accomplished by other well-studied regularizers (input mixup, weight decay, dropout, batch normalization, and adding noise to the hidden representations).</figDesc><graphic url="image-4.png" coords="2,85.35,213.60,129.60,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Furthermore</head><label></label><figDesc>, if dim (H) &gt; d − 1, then data points in the representation space H have some degrees of freedom to move independently.Corollary 1. Consider the setting in Theorem 1 with dim (H) &gt; d − 1. Let g ∈ G minimize (2) under P = P D . Then, the representations of the training points g (x i ) fall on a (dim (H) − d + 1)-dimensional subspace.Proof. From the proof of Theorem 1, A H = I d×d − b1 d . The r.h.s. of this expression is a rank-(d − 1) matrix for a properly chosen b. Thus, A can have a null-space of dimension dim (H) − d + 1. This way, one can assign g (x i ) = H ζi,: + e i , where H ζi,: is defined as in the proof of Theorem 1, and e i are arbitrary vectors in the null-space of A, for all i = 1, . . . , n.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification errors and neg-log-likelihoods on SVHN. We run each experiment five times.</figDesc><table><row><cell>PreActResNet18</cell><cell>Test Error (%)</cell><cell>Test NLL</cell></row><row><cell>No Mixup</cell><cell cols="2">2.89 ± 0.224 0.136 ± 0.001</cell></row><row><cell>Input Mixup (α = 1)</cell><cell cols="2">2.76 ± 0.014 0.212 ± 0.011</cell></row><row><cell>Manifold Mixup (α = 2)</cell><cell cols="2">2.27 ± 0.011 0.122 ± 0.006</cell></row><row><cell>PreActResNet34</cell><cell></cell><cell></cell></row><row><cell>No Mixup</cell><cell cols="2">2.97 ± 0.004 0.165 ± 0.003</cell></row><row><cell>Input Mixup (α = 1)</cell><cell cols="2">2.67 ± 0.020 0.199 ± 0.009</cell></row><row><cell>Manifold Mixup (α = 2)</cell><cell cols="2">2.18 ± 0.004 0.137 ± 0.008</cell></row><row><cell>Wide-Resnet-28-10</cell><cell></cell><cell></cell></row><row><cell>No Mixup</cell><cell cols="2">2.80 ± 0.044 0.143 ± 0.002</cell></row><row><cell>Input Mixup (α = 1)</cell><cell cols="2">2.68 ± 0.103 0.184 ± 0.022</cell></row><row><cell>Manifold Mixup (α = 2)</cell><cell cols="2">2.06 ± 0.068 0.126 ± 0.008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracies on TinyImagenet.</figDesc><table><row><cell>PreActResNet18</cell><cell>top-1 top-5</cell></row><row><cell>No Mixup</cell><cell>55.52 71.04</cell></row><row><cell>Input Mixup (α = 0.2)</cell><cell>56.47 71.74</cell></row><row><cell>Input Mixup (α = 0.5)</cell><cell>55.49 71.62</cell></row><row><cell>Input Mixup (α = 1.0)</cell><cell>52.65 70.70</cell></row><row><cell>Input Mixup (α = 2.0)</cell><cell>44.18 68.26</cell></row><row><cell cols="2">Manifold Mixup (α = 0.2) 58.70 73.59</cell></row><row><cell cols="2">Manifold Mixup (α = 0.5) 57.24 73.48</cell></row><row><cell cols="2">Manifold Mixup (α = 1.0) 56.83 73.75</cell></row><row><cell cols="2">Manifold Mixup (α = 2.0) 48.14 71.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy (%) of Input Mixup and Manifold Mixup for different α on CIFAR-10.</figDesc><table><row><cell cols="3">α Input Mixup Manifold Mixup</cell></row><row><cell>0.5</cell><cell>96.68</cell><cell>96.76</cell></row><row><cell>1.0</cell><cell>96.75</cell><cell>97.00</cell></row><row><cell>1.2</cell><cell>96.72</cell><cell>97.03</cell></row><row><cell>1.5</cell><cell>96.84</cell><cell>97.10</cell></row><row><cell>1.8</cell><cell>96.80</cell><cell>97.15</cell></row><row><cell>2.0</cell><cell>96.73</cell><cell>97.23</cell></row><row><cell cols="3">corresponding to interpolations), we would not expect the</cell></row><row><cell cols="3">model to be robust to adversarial attacks that consider any</cell></row><row><cell cols="3">direction around each example. However, since Manifold</cell></row><row><cell cols="3">Mixup expands the set of examples seen during training, an</cell></row><row><cell cols="3">intriguing hypothesis is that these expansions overlap with</cell></row><row><cell cols="3">the set of possible adversarial examples, providing some</cell></row><row><cell cols="3">degree of defense. If this hypothesis is true, Manifold Mixup</cell></row><row><cell cols="3">would force adversarial attacks to consider a wider set of</cell></row><row><cell cols="3">directions, leading to a larger computational expense for the</cell></row><row><cell cols="3">attacker. To explore this, we consider the Fast Gradient Sign</cell></row><row><cell cols="3">Method (FGSM, Goodfellow et al., 2015), which constructs</cell></row><row><cell cols="3">adversarial examples in one single step, thus considering a</cell></row><row><cell cols="3">relatively small subset of directions around examples. The</cell></row><row><cell cols="3">performance of networks trained using Manifold Mixup</cell></row><row><cell cols="3">against FGSM attacks is given in Table 7. One challenge</cell></row><row><cell cols="3">in evaluating robustness against adversarial examples is the</cell></row></table><note>"gradient masking problem", in which a defense succeeds only by reducing the quality of the gradient signal.(Athalye  et al., 2018)  explored this issue in depth, and proposed running an unbounded search for a large number of iterations to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy on samples subject to novel deformations. All models were trained on normal CIFAR-100.</figDesc><table><row><cell>Deformation</cell><cell cols="4">No Mixup Input Mixup (α = 1) Input Mixup (α = 2) Manifold Mixup (α = 2)</cell></row><row><cell>Rotation U(−20 • ,20 • )</cell><cell>52.96</cell><cell>55.55</cell><cell>56.48</cell><cell>60.08</cell></row><row><cell>Rotation U(−40 • ,40 • )</cell><cell>33.82</cell><cell>37.73</cell><cell>36.78</cell><cell>42.13</cell></row><row><cell>Shearing U(−28.6 • , 28.6 • )</cell><cell>55.92</cell><cell>58.16</cell><cell>60.01</cell><cell>62.85</cell></row><row><cell>Shearing U(−57.3 • , 57.3 • )</cell><cell>35.66</cell><cell>39.34</cell><cell>39.7</cell><cell>44.27</cell></row><row><cell>Zoom In (60% rescale)</cell><cell>12.68</cell><cell>13.75</cell><cell>13.12</cell><cell>11.49</cell></row><row><cell>Zoom In (80% rescale)</cell><cell>47.95</cell><cell>52.18</cell><cell>50.47</cell><cell>52.70</cell></row><row><cell>Zoom Out (120% rescale)</cell><cell>43.18</cell><cell>60.02</cell><cell>61.62</cell><cell>63.59</cell></row><row><cell>Zoom Out (140% rescale)</cell><cell>19.34</cell><cell>41.81</cell><cell>42.02</cell><cell>45.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy (%) of Manifold Mixup for different sets of eligible layers S on CIFAR-10/CIFAR-100.</figDesc><table><row><cell>S</cell><cell cols="2">CIFAR-10 CIFAR-100</cell></row><row><cell>{0, 1, 2}</cell><cell>97.23</cell><cell>79.60</cell></row><row><cell>{0, 1}</cell><cell>96.94</cell><cell>78.93</cell></row><row><cell>{0, 1, 2, 3}</cell><cell>96.92</cell><cell>80.18</cell></row><row><cell>{1, 2}</cell><cell>96.35</cell><cell>78.69</cell></row><row><cell>{0}</cell><cell>96.73</cell><cell>78.15</cell></row><row><cell>{1, 2, 3}</cell><cell>96.51</cell><cell>79.31</cell></row><row><cell>{1}</cell><cell>96.10</cell><cell>78.72</cell></row><row><cell>{2, 3}</cell><cell>95.32</cell><cell>76.46</cell></row><row><cell>{2}</cell><cell>95.19</cell><cell>76.50</cell></row><row><cell>{}</cell><cell>95.27</cell><cell>76.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy on white-box FGSM adversarial examples on CIFAR-10/CIFAR-100 (using a PreActResNet18 model) and SVHN (using a WideResNet20-10 model). We include the results of<ref type="bibr" target="#b17">(Madry et al., 2018)</ref> †. of the gradient signal. Manifold Mixup passes this sanity check (consult Appendix D for further details). While we found that using Manifold Mixup improves the robustness to single-step FGSM attack (especially over Input Mixup), we found that Manifold Mixup did not significantly improve robustness against stronger, multi-step attacks such as PGD (Madry et al., 2018). Deep neural networks often give incorrect, yet extremely confident predictions on examples that differ from those seen during training. This problem is one of the most central challenges in deep learning. We have investigated this issue from the perspective of the representations learned by deep neural networks. We observed that vanilla neural networks spread the training data widely throughout the representation space, and assign high confidence predictions to almost the entire volume of representations. This leads to major drawbacks since the network will provide high-confidence predictions to examples off the data manifold, thus lacking enough incentives to learn discriminative representations about the training data. To address these issues, we introduced Manifold Mixup, a new algorithm to train neural networks on interpolations of hidden representations. Manifold Mixup encourages the neural network to be uncertain across the volume of the representation space unseen during training. This leads to concentrating the representations of the real training examples in a low dimensional subspace, resulting in more discriminative features. Throughout a variety of experiments, we have shown that neural networks trained using Manifold Mixup have better generalization in terms of error and log-likelihood, as well as better robustness to novel deformations of the data and adversarial examples. Being easy to implement and incurring little additional computational cost, we hope that Manifold Mixup will become a useful regularization tool for deep learning practitioners.</figDesc><table><row><cell>CIFAR-10</cell><cell>FGSM</cell></row><row><cell>No Mixup</cell><cell>36.32</cell></row><row><cell>Input Mixup (α = 1)</cell><cell>71.51</cell></row><row><cell cols="2">Manifold Mixup (α = 2) 77.50</cell></row><row><cell>PGD training (7-steps) †</cell><cell>56.10</cell></row><row><cell>CIFAR-100</cell><cell>FGSM</cell></row><row><cell>Input Mixup (α = 1)</cell><cell>40.7</cell></row><row><cell cols="2">Manifold Mixup (α = 2) 44.96</cell></row><row><cell>SVHN</cell><cell>FGSM</cell></row><row><cell>No Mixup</cell><cell>21.49</cell></row><row><cell>Input Mixup (α = 1)</cell><cell>56.98</cell></row><row><cell cols="2">Manifold Mixup (α = 2) 65.91</cell></row><row><cell>PGD training (7-steps) †</cell><cell>72.80</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Vikas Verma was supported by Academy of Finland project 13312683 / Raiko Tapani AT kulut. We would also like to acknowledge Compute Canada for providing computing resources used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/athalye18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date>10-15 Jul PMLR</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generalization performance of support vector machines and other pattern classifiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of control, signals and systems</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>CoRR, abs/1708.04552</idno>
		<ptr target="http://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1809.02499" />
		<title level="m">MixUp as Locally Linear Out-Of-Manifold Regularization. ArXiv e-prints</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">MixUp as Locally Linear Out-Of-Manifold Regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09">September 2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-07">July 2018b</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>CoRR, abs/1207.0580</idno>
		<ptr target="http://arxiv.org/abs/1207.0580" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lower bounds on the vc dimension of smoothly parameterized function classes</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1995.7.5.1040</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1040" to="1053" />
			<date type="published" when="1995-09">Sep. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJzIBfZAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1QRgziT-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<idno>CoRR, abs/1703.00810</idno>
		<ptr target="http://arxiv.org/abs/1703.00810" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
		<idno>CoRR, abs/1503.02406</idno>
		<ptr target="http://arxiv.org/abs/1503.02406" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep learning with data dependent implicit activation function</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
		<idno>CoRR, abs/1802.00168</idno>
		<ptr target="http://arxiv.org/abs/1802.00168" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>CoRR, abs/1311.2901</idno>
		<ptr target="http://arxiv.org/abs/1311.2901" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><surname>Mixup</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Retrieval-augmented convolutional neural networks for improved robustness against adversarial examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR, abs/1802.09502</idno>
		<ptr target="http://arxiv.org/abs/1802.09502" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
