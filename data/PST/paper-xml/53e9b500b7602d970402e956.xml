<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modular Neural Networks architecture optimization with a new nature inspired method using a fuzzy combination of Particle Swarm Optimization and Genetic Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-02-20">20 February 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fevrier</forename><surname>Valdez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tijuana Institute of Technology</orgName>
								<address>
									<settlement>Tijuana</settlement>
									<region>BC</region>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patricia</forename><surname>Melin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tijuana Institute of Technology</orgName>
								<address>
									<settlement>Tijuana</settlement>
									<region>BC</region>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Oscar</forename><surname>Castillo</surname></persName>
							<email>ocastillo@hafsamx.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Tijuana Institute of Technology</orgName>
								<address>
									<settlement>Tijuana</settlement>
									<region>BC</region>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modular Neural Networks architecture optimization with a new nature inspired method using a fuzzy combination of Particle Swarm Optimization and Genetic Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-02-20">20 February 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">D2D68CC8FDE67C75850CB840471638B1</idno>
					<idno type="DOI">10.1016/j.ins.2014.02.091</idno>
					<note type="submission">Received 24 February 2010 Received in revised form 10 January 2013 Accepted 12 February 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Genetic Algorithm PSO Fuzzy Logic Neural Network Hybrid optimization method</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe in this paper a new hybrid approach for optimization combining Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) using Fuzzy Logic to integrate the results. The new optimization method combines the advantages of PSO and GA to provide an improved FPSO + FGA hybrid method. Fuzzy Logic is used to combine the results of the PSO and GA in the best way possible. Also Fuzzy Logic is used to adjust parameters in FPSO and FGA. The proposed optimization method was tested with a set of benchmark mathematical functions and then with a more complex problem of neural network architecture optimization. The results of the proposed hybrid optimization method are shown to outperform other methods for these problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optimization is a term used to refer to a branch of computational science concerned with finding the ''best'' solution to a given problem. Here, ''best'' refers to an acceptable solution, which may be the absolute best over a set of candidate solutions, or any of the candidate solutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The characteristics and requirements of the problem determine whether the overall best solution can be found. For example, it may be required that the solution has to be found within a limited time period. In such cases a good candidate solution may be sufficient <ref type="bibr" target="#b6">[7]</ref>. Optimization problems can be found everywhere in life, including diverse fields such as engineering, manufacturing, finance, medicine, computational art and music, physics, chemistry, and many others. The need to find best solutions is encountered in everyday life. For example, the simple action of making a telephone call involves optimization procedures to find the most cost-effective, least congested route to carry the call <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the field of evolutionary computation, it is common to compare different algorithms using a large test set, especially when the test set involves function optimization. If we compare 3 searching algorithms with all possible functions, the performance of any of the 3 algorithms will be, on average, the same. This is due to the fact that, when an algorithm is evaluated, we must look for the kind of problems where its performance is good, in order to characterize the type of problems for which the algorithm is suitable. In this way, we have made a previous study of the functions to be optimized for constructing a test set with 7 benchmark functions <ref type="bibr" target="#b8">[9]</ref>. This allows us to reach conclusions of the performance of the algorithm depending on the type of function. The mathematical functions considered in this paper are the Rastrigin's function, Rosenbrock's function, Ackley's function, Sphere's function, Griewank's function, Michalewics's function and Zakharov function <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>.</p><p>The main contribution in this paper is a new optimization method combining PSO and GA, to provide an improved FPSO + FGA hybrid method that combines the advantages of both individual methods (PSO and GA). We apply the hybrid method to mathematical function optimization to evaluate the performance of the new approach <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. The results of the proposed hybrid optimization method are shown to outperform the individual PSO and GA methods for these problems. We also apply the hybrid method to optimize the architectures of Modular Neural Networks (MNNs) for pattern recognition applications. The problem of MNN optimization is rather complex as it involves finding the number of modules, layers and nodes of the best MNN architecture for a particular application <ref type="bibr" target="#b19">[20]</ref>. In the literature what is usually found is the optimization of monolithic neural networks, while MNN optimization has not been considered because it involves a set of monolithic NNs used at the same time to solve a complex problem by dividing it into several simplex problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. Usually the design of MNN architectures is performed manually or the individual modules (monolithic neural networks) are designed separately and then they are joined, which results in not optimal architectures. In this paper, we consider the optimal design of the complete MNN architecture, which is a more challenging task because the search space is larger.</p><p>The paper is organized as follows: in Section 2 a description about the Genetic Algorithms for optimization problems is given, in Section 3 the Particle Swarm Optimization is presented, the FPSO + FGA method is presented in Sections 4 and 5, the simulation results for Modular Neural Network optimization are presented in Section 6, finally we can find the conclusions reached after the study of the proposed optimization method in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Genetic Algorithms for optimization</head><p>John Holland, from the University of Michigan initiated his work on Genetic Algorithms at the beginning of the 1960s. His first achievement was the publication of Adaptation in Natural and Artificial System <ref type="bibr" target="#b10">[11]</ref> in 1975.</p><p>He had two goals in mind: to improve the understanding of natural adaptation process, and to design artificial systems having properties similar to natural systems <ref type="bibr" target="#b9">[10]</ref>.</p><p>The basic idea is as follows: the genetic pool of a given population potentially contains the solution, or a better solution, to a given adaptive problem. This solution is not ''active'' because the genetic combination on which it relies is split between several subjects. Only the association of different genomes can lead to the solution.</p><p>Holland's method is especially effective because it not only considers the role of mutation, but it also uses genetic recombination, (crossover) <ref type="bibr" target="#b7">[8]</ref>. The crossover of partial solutions greatly improves the capability of the algorithm to approach, and eventually find, the optimal solution.</p><p>The essence of the GA in both theoretical and practical domains has been well demonstrated <ref type="bibr" target="#b14">[15]</ref>. The concept of applying a GA to solve engineering problems is feasible and sound. However, despite the distinct advantages of a GA for solving complicated, constrained and multi-objective functions where other techniques may have failed, the full power of the GA in application is yet to be exploited <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>.</p><p>In Fig. <ref type="figure" target="#fig_0">1</ref> we show the reproduction cycle of the Genetic Algorithm. The Simple Genetic Algorithm can be expressed in pseudo code with the following cycle:</p><p>1. Generate the initial population of individuals aleatorily P(0).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Particle Swarm Optimization</head><p>Particle Swarm Optimization (PSO) is a population based stochastic optimization technique developed by Eberhart and Kennedy in 1995, inspired by social behavior of bird flocking or fish schooling <ref type="bibr" target="#b13">[14]</ref>.</p><p>PSO shares many similarities with evolutionary computation techniques such as Genetic Algorithms (GAs) <ref type="bibr" target="#b8">[9]</ref>. The system is initialized with a population of random solutions and searches for optima by updating generations. However, unlike the GA, the PSO has no evolution operators such as crossover and mutation. In the PSO, the potential solutions, called particles, fly through the problem space by following the current optimum particles <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Each particle keeps track of its coordinates in the problem space, which are associated with the best solution (fitness) it has achieved so far (the fitness value is also stored). This value is called pbest. Another ''best'' value that is tracked by the particle swarm optimizer is the best value, obtained so far by any particle in the neighbors of the particle. This location is called lbest. When a particle takes all the population as its topological neighbors, the best value is a global best and is called gbest <ref type="bibr" target="#b11">[12]</ref>.</p><p>The Particle Swarm Optimization concept consists of, at each time step, changing the velocity of (accelerating) each particle toward its pbest and lbest locations (local version of PSO). Acceleration is weighted by a random term, with separate random numbers being generated for acceleration toward pbest and lbest locations <ref type="bibr" target="#b11">[12]</ref>.</p><p>In the past several years, PSO has been successfully applied in many research and application areas. It is demonstrated that PSO gets better results in a faster, cheaper way compared with other methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Another reason that PSO is attractive is that there are few parameters to adjust. One version, with slight variations, works well in a wide variety of applications. Particle Swarm Optimization has been used for approaches that can be used across a wide range of applications, as well as for specific applications focused on a specific requirement <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>The pseudo code of the PSO is as follows:</p><p>For each particle Initialize particle End Do</p><p>For each particle Calculate fitness value If the fitness value is better than the best fitness value (pBest) in history set current value as the new pBest End Choose the particle with the best fitness value of all the particles as the gBest For each particle Calculate particle velocity Update particle position End While maximum iterations or minimum error criteria is not attained</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed FPSO + FGA method</head><p>This method combines the characteristics of PSO and GA using several fuzzy systems for integration of results and parameter adaptation. In this section, the proposed FPSO + FGA method is presented.</p><p>The general idea of the proposed FPSO + FGA method can be appreciated in Fig. <ref type="figure" target="#fig_1">2</ref>. The method can be described as follows:</p><p>1. A mathematical function to be optimized is received. 2. The role of both FPSO and FGA is evaluated. 3. A main fuzzy system is responsible for receiving values resulting from step 2. 4. The main fuzzy system decides which method to use (FPSO or FGA). 5. Another fuzzy system receives the values of Error and DError as inputs to evaluate if it is necessary to change the parameters in FPSO or FGA. 6. There are 3 fuzzy systems. One is for decision making (is called 'fuzzymain'), the second one is to change the parameters of the GA (is called 'fuzzyga'), in this case change the value of the crossover and mutation rate and the third fuzzy system is used to change the parameters of the PSO (is called 'fuzzypso') in this case change the values of the cognitive acceleration 'c 1 ', and social acceleration 'c 2 '. The main fuzzy system (called 'fuzzymain') decides in the final step the optimum value for the function introduced in step 1. 7. Repeat the above steps until the termination criterion of the algorithm is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Detailed description of FPSO+FGA</head><p>The basic idea of the FPSO + FGA scheme is to combine the advantages of the individual methods using a fuzzy system for decision making and the others two fuzzy systems to improve the parameters of the FGA and FPSO when it is necessary.</p><p>As can be noted in the proposed hybrid FPSO + FGA method, it is the internal fuzzy system structure, which has the primary function of receiving as inputs the results (Error and DError) of the FGA and FPSO outputs. The fuzzy system is responsible for integrating and decides which are the best results being generated at run time of the FPSO + FGA. It is also responsible for selecting and sending the problem to the ''fuzzypso'', which is a fuzzy system for when the FPSO is activated or to the ''fuzzyga'', which is a fuzzy system for when the FGA is activated. Also activating or temporarily stopping depending on the results being generated. Fig. <ref type="figure" target="#fig_2">3</ref> shows the membership functions of the main fuzzy system that is implemented in this method. The fuzzy system is of Mamdani type because it is more common in this type of fuzzy control and the defuzzification method is the centroid. In this case, we are using this type of defuzzification because in other papers we have achieved good results <ref type="bibr" target="#b3">[4]</ref>. The membership functions are of triangular form in the inputs and outputs as is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Also, the membership functions were chosen of triangular form based on past experience in this type of fuzzy control. The fuzzy system consists of 9 rules. For example, one rule is if Error is P and DError is P then best value is P (this can be found in Fig. <ref type="figure" target="#fig_3">4</ref>). Fig. <ref type="figure" target="#fig_4">5</ref> shows the fuzzy system rule viewer, which shows how the fuzzy rules are activated. Fig. <ref type="figure">6</ref> shows the non-linear surface corresponding to this fuzzy system. The other two fuzzy systems are similar to the main fuzzy system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">FPSO (Fuzzy Particle Swarm Optimization)</head><p>This section presents a detailed description of the FPSO model. In the case of an n x -dimensional search space, each individual consists of n x variables with each variable encoded as a binary string.</p><p>The swarm is typically modeled by particles in multidimensional space that have a position and a velocity. These particles fly through hyperspace (i.e., R n ) and have two essential reasoning capabilities: their memory of their own best position and knowledge of the global or their neighborhood's best. In a minimization optimization problem, ''best'' simply means the position with the smallest objective value. Members of a swarm communicate good positions to each other and adjust their own position and velocity based on these good positions. So a particle has the following information to make a suitable change in its position and velocity: A global best that is known to all and immediately updated when a new best position is found by any particle in the swarm. The neighborhood best that the particle obtains by communicating with a subset of the swarm. The local best, which is the best solution that the particle has seen.</p><p>In this case, the social information is the best position found by the swarm, referred as ŷðtÞ. For gbest FPSO, the velocity of particle i is calculated as</p><formula xml:id="formula_0">v ij ðt þ 1Þ ¼ w Á v ij ðtÞ þ c 1 r 1j ðtÞ½y ij ðtÞ À x ij ðtÞ þ c 2 r 2j ðtÞ½ŷ j ðtÞ À x ij ðtÞ<label>ð1Þ</label></formula><p>where v ij (t) is the velocity of particle i in dimension j = 1,. . ., n x at time step t, x ij (t) is the position of particle i in dimension j at time step t, c 1 and c 2 represents the cognitive and social acceleration. In this case, these values are fuzzy because they are changing dynamically when the FPSO is running, and r 1j (t), r 2j $ U(0, 1) are random values in the range [0, 1]. The inertia weight ''w'' or inertia factor is a value, which regulates the influence of the previous velocity of the particle v (t) in calculating the new speed v ij (t + 1), by way of regulating the flight of the particle making a balance between exploitation and exploration of the search space. The inertia factor under certain conditions promotes the convergence of the swarm, i.e. all the particles approach the leader of the swarm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">FGA (Fuzzy Genetic Algorithm)</head><p>This section presents a brief description of the FGA. Several crossover operators have been developed for GAs, depending on the format in which individuals are represented. For binary representations, uniform crossover, one point crossover and two points cross over are the most popular. In this case we are using two point crossover with fuzzy crossover rate because we are adding a fuzzy system called 'fuzzyga' that is able of change the crossover and mutation rate.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Definition of the fuzzy systems used in FPSO + FGA</head><p>The details of the fuzzy systems are as follows:</p><p>'fuzzypso': In this case we are using a fuzzy system called 'fuzzypso', and the structure of this fuzzy system is as follow: Number of Inputs: 2 Number of Outputs: 2 Number of membership functions: 3 Type of the membership functions: Triangular Number of rules: 9 Defuzzification: Centroid The main function of the fuzzy system called 'fuzzypso' is to adjust the parameters of the PSO. In this case, we are adjusting the following parameters: 'c 1 ' and 'c 2 '; where: 'c 1 ' = Cognitive Acceleration 'c 2 ' = Social Acceleration We are changing these parameters to test the proposed method. In this case, with 'fuzzypso' is possible to adjust in real time the 2 parameters that belong to the PSO. 'fuzzyga': In this case we are using a fuzzy system called 'fuzzyga', the structure of this fuzzy system is as follows: Number of Inputs: 2 Number of Outputs: 2 Number of membership functions: 3 Type of membership functions: Triangular Number of rules: 9 Defuzzification: Centroid The main function of the fuzzy system called 'fuzzyga' is to adjust the parameters of the GA. In this case, we are adjusting the following parameters: 'mu', 'c r '; where: 'mu' = mutation 'c r ' = crossover 'fuzzymain': In this case, we are using a fuzzy system called 'fuzzymain'. The structure of this fuzzy system is as follows: The main function of the fuzzy system, called 'fuzzymain' is to decide on the best way for solving the problem, in other words if it is more reliable to use the FPSO or FGA. This fuzzy system is able to receive two inputs, called error and derror, it is to evaluate the results that are generated by FPSO and FGA in the last step of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Simulation results for Modular Neural Network and function optimization</head><p>Several tests of the FPSO + FGA method for MNN optimization were performed with an implementation of the method in the Matlab programming language. Results of mathematical function optimization are also presented to illustrate the proposed method.</p><p>All the implementations were developed using a computer with a quad core2 processor of 64 bits that works to a frequency of clock of 2.5 GHz, 4 GB of RAM Memory and Windows Vista operating system.</p><p>We describe below simulation results of our approach for face recognition with Modular Neural Networks (MNNs). We used two-layer feed-forward MNNs with the Conjugated-Gradient training algorithm <ref type="bibr" target="#b3">[4]</ref>. The challenge is to find the optimal architecture of this type of neural network, which means finding out the optimal number of layers and nodes of the neural network <ref type="bibr" target="#b5">[6]</ref>. We are using the Yale face database <ref type="bibr" target="#b22">[23]</ref> that contains 165 grayscale images in GIF format of 15 individuals, for this paper only 10 subjects were used for training the MNN. There are 5 images per subject, one per different facial expression: center-light, happy, left-light, normal and right-light.</p><p>In total 50 images were used (see Fig. <ref type="figure" target="#fig_9">9</ref>). Three images per subject were used for training the MNN and the other two for the recognition. Regarding the Genetic Algorithm for NN evolution, we used a hierarchical chromosome for representing the relevant information of the network. First, we have the bits for representing the number of layers of the MNN, in this case, the initial topology was of 3 modules with 2 layers per module with 500 neurons in the first layer, 300 neurons in the second layer in each module. Therefore we used a representation the 2415 bits in total (see Fig. <ref type="figure">7</ref>). Regarding the FPSO the particle representation is shown in Fig. <ref type="figure" target="#fig_8">8</ref>, which indicates a real number representation of the information of the neural networks. In Fig. <ref type="figure" target="#fig_13">11</ref> we can find the architecture of a MNN that we are using with the evolutionary proposed method FPSO + FGA.</p><p>The fitness function used in this case for the MNN combines the information of the error objective and also the information about the number of nodes as a second objective. This is shown in the following equation.</p><formula xml:id="formula_1">f ðzÞ ¼ 1 a Ã RankingðObjV1Þ þ b Ã ObjV2 Ã 10<label>ð2Þ</label></formula><p>where a and b values must satisfy the condition: a &gt; bM, and M is the maximum number of nodes in the neural network <ref type="bibr" target="#b14">[15]</ref>.</p><p>The first objective is basically the average sum of squared of errors as calculated by the predicted outputs of the MNN compared with real values of the function. This is given by the following equation.   </p><formula xml:id="formula_2">f 1 ¼ 1 N X N i¼1 ðZ i À z i Þ 2<label>ð3Þ</label></formula><p>The second objective is the complexity of the neural network, which is measured by the total number of nodes in the architecture.</p><p>The final topology of the neural network for the problem of face recognition is obtained by the hybrid evolutionary method FPSO + FGA. The comparison of the final objective values (errors) will be shown in the following section. In the final architecture, the result of the MNN evolution is a particular architecture with different number of nodes by layers. Several tests were made; we obtained different optimized architectures for this Modular Neural Network; the best architecture obtained was the following:    We can appreciate in Fig. <ref type="figure" target="#fig_12">10</ref> the binary representation for this optimized architecture. With this final topology the neural network was trained and the ten images were recognized. In Table <ref type="table" target="#tab_1">1</ref> the different architectures obtained with this method are presented. The proposed method optimizes the initial architecture proposed for the problem of face recognition.</p><p>In Table <ref type="table" target="#tab_1">1</ref> we summarize the results of MNN architecture optimization with the proposed FPSO + FGA method and the individual PSO and GA methods for the same task. We also show the best results that were manually (trial and error) found for this same problem of face recognition, so that the advantage of using optimization techniques is appreciated. From Table <ref type="table" target="#tab_1">1</ref> it is clear that the proposed method outperforms both PSO and GA and that these differences are statistical significant, this shows the advantage of using the FPSO + FGA method. Also this FPSO + FGA have been applied, for optimization of complex mathematical functions to validate our approach. Table <ref type="table" target="#tab_2">2</ref>, shows the simulation results with 10 mathematical functions. It can be seen in Table <ref type="table" target="#tab_2">2</ref> that this method is good alternative to solve this type of problems. The mathematical functions are evaluated with 2, 4, 8 and 16 variables. The mean was calculated after running the FPSO + FGA 50 times. The parameters in FPSO + FGA as crossover, mutation, social and cognitive acceleration are fuzzy, because are changing dynamically when the method is running, this is main characteristic of this method to find the best results.</p><p>In Table <ref type="table">3</ref> the comparison of the results obtained among the GA, PSO and PSO + GA methods for the optimization of the 7 proposed mathematical functions is shown. In all cases, 50 experiments were executed to show the mean and standard deviation values (in Table <ref type="table">3</ref> the standard deviations are shown in parenthesis). It can be noticed that the proposed PSO + GA method was better than GA and PSO, because with this method all test functions were optimized. In some cases the GA was better but in Table <ref type="table">3</ref> it can be seen that the better mean values were obtained for the FPSO + FGA, only in the Sphere function the PSO was better than the other two methods. Also in the Rosenbrock's function the GA was better than the other two methods. Regarding the number of fitness evaluations, FPSO + FGA required 5% less fitness evaluations than PSO and 10% less fitness evaluations than GA (on average), which is also an indication that less computing effort is required with the hybrid optimization method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>The analysis of the simulation results of the optimization method considered in this paper, FPSO + FGA, lead us to the conclusion that for the optimization of Modular Neural Networks with this method is a good alternative because it is easier to optimize the architecture of Modular Neural Network than to try it with PSO or GA separately. This is, because the combination PSO and GA with fuzzy rules gives a new hybrid FPSO + FGA method. It can be seen in Table <ref type="table" target="#tab_1">1</ref> that the results obtained after applying FPSO + FGA have higher recognition rates, and as a consequence we are demonstrating that it is reliable for this type of applications. Also, we have shown that this method has been tested with 7 benchmark mathematical functions to validate our hybrid optimization approach. In Table <ref type="table" target="#tab_2">2</ref> we can find the simulation results obtained with the method, which also indicate that the hybrid optimization approach performs well when compared with the PSO and GA individual methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The reproduction cycle.</figDesc><graphic coords="2,168.72,562.51,207.26,110.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The FPSO + FGA scheme.</figDesc><graphic coords="4,117.69,54.71,312.25,274.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Fuzzy system membership functions.</figDesc><graphic coords="5,113.39,54.71,311.67,133.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Fuzzy system rules.</figDesc><graphic coords="5,113.39,218.55,310.82,215.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Fuzzy system rules viewer.</figDesc><graphic coords="6,117.69,54.71,312.40,197.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Surface of fuzzy system.</figDesc><graphic coords="6,168.72,300.02,209.97,142.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Number of Inputs: 2 Number of Outputs: 1 Number of membership functions: 3 Type of membership functions: Triangular Number of rules: 9 Defuzzification: Centroid</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>NNL1M1= Number of neurons of the layer 1 in module 1. NNL2M1= Number of neurons of the layer 2 in module 1. NNL1M2= Number of neurons of the layer 1 in module 2. NNL2M2= Number of neurons of the layer 2 in module 2. NNL1M3= Number of neurons of the layer 1 in module 3. NNL2M3= Number of neurons of the layer 2 in module 3. W= Inertia Weight of the particle C 1 = Cognitive acceleration of the particle C 2 = Social acceleration of the particle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Particle representation for the FPSO.</figDesc><graphic coords="8,168.72,269.18,207.12,291.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Images of the Yale face database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Layers</head><label></label><figDesc>of neurons of the layer 1 in module 1. NNL2M1 = Number of neurons of the layer 2 in module 1. NNL1M2 = Number of neurons of the layer 1 in module 2. NNL2M2 = Number of neurons of the layer 2 in module 2. NNL1M3 = Number of neurons of the layer 1 in module 3. NNL2M3 = Number of neurons of the layer 2 in module 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Binary representation optimized with the FGA.</figDesc><graphic coords="9,56.66,54.68,425.33,180.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Architecture of the Modular Neural Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Simulation results for MNN optimization with different methods.</figDesc><table><row><cell>Optimization method</cell><cell>Mean recognition (%)</cell><cell>Standard deviation (%)</cell><cell>Best recognition (%)</cell></row><row><cell>Trial and error</cell><cell>79.55</cell><cell>1.23</cell><cell>85</cell></row><row><cell>GAs</cell><cell>88.72</cell><cell>2.57</cell><cell>92</cell></row><row><cell>PSO</cell><cell>92.33</cell><cell>1.88</cell><cell>95</cell></row><row><cell>FPSO + FGA</cell><cell>98.55</cell><cell>1.29</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Simulation results for mathematical functions with FPSO + FGA.</figDesc><table><row><cell>Function</cell><cell>Variables = 2</cell><cell></cell><cell>Variables = 4</cell><cell></cell><cell>Variables = 8</cell><cell></cell><cell cols="2">Variables = 16</cell></row><row><cell>Math</cell><cell>BEST</cell><cell>MEAN</cell><cell>BEST</cell><cell>MEAN</cell><cell>BEST</cell><cell>MEAN</cell><cell>BEST</cell><cell>MEAN</cell></row><row><cell>Rastrigin</cell><cell>1.45EÀ06</cell><cell>3.05EÀ04</cell><cell>0.00034</cell><cell>0.0755</cell><cell>0.01318</cell><cell>0.9311</cell><cell>0.548397</cell><cell>5.0946</cell></row><row><cell>Rosenbrock</cell><cell>1.17EÀ02</cell><cell>1.17EÀ02</cell><cell>0.0285</cell><cell>0.5991</cell><cell>0.15800</cell><cell>3.8925</cell><cell>0.25555</cell><cell>4.33334</cell></row><row><cell>Ackley</cell><cell>8.42EÀ04</cell><cell>4.98EÀ03</cell><cell>8.42EÀ01</cell><cell>4.98EÀ02</cell><cell>0.7</cell><cell>1.56</cell><cell>2.35</cell><cell>2.63</cell></row><row><cell>Sphere</cell><cell>5.75EÀ11</cell><cell>1.05EÀ10</cell><cell>1.94EÀ05</cell><cell>4.51EÀ004</cell><cell>0.00059</cell><cell>0.0057</cell><cell>0.00248</cell><cell>0.0211</cell></row><row><cell>Griewank</cell><cell>7.88EÀ11</cell><cell>1.07EÀ07</cell><cell>7.18EÀ06</cell><cell>1.11EÀ004</cell><cell>0.00016</cell><cell>9.29EÀ004</cell><cell>0.000408</cell><cell>0.0043</cell></row><row><cell>Michalewics</cell><cell>À1.8010</cell><cell>À1.8201</cell><cell>À1.8012</cell><cell>À1.8002</cell><cell>À1.80130</cell><cell>À1.8005</cell><cell>À1.80130</cell><cell>À1.8004</cell></row><row><cell>Zakharov</cell><cell>6.00EÀ07</cell><cell>0.00168</cell><cell>3.23EÀ07</cell><cell>8.41EÀ005</cell><cell>1.33EÀ07</cell><cell>6.49EÀ005</cell><cell>8.63EÀ07</cell><cell>7.0EÀ005</cell></row><row><cell>Dixon</cell><cell>0.0070</cell><cell>0.007</cell><cell>0.0036</cell><cell>0.1343</cell><cell>0.0581</cell><cell>0.7676</cell><cell>0.94444</cell><cell>4.5968</cell></row><row><cell>Levy</cell><cell>0.000001</cell><cell>0.00026</cell><cell>0.000001</cell><cell>0.0031</cell><cell>0.0058</cell><cell>0.00229</cell><cell>0.0111</cell><cell>0.1093</cell></row><row><cell>Perm</cell><cell>0.0001</cell><cell>0.0068</cell><cell>0.1488</cell><cell>5.6553</cell><cell>0.0018</cell><cell>7.3555</cell><cell>0.0075</cell><cell>8.5670</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Comparative computational results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mathematical function</cell><cell>GA</cell><cell></cell><cell>PSO</cell><cell></cell><cell>FPSO + FGA</cell><cell></cell><cell>Objective value</cell></row><row><cell>Rastrigin</cell><cell></cell><cell cols="2">2.15EÀ03 (1.11EÀ04)</cell><cell>5.47EÀ05 (4.3EÀ06)</cell><cell></cell><cell cols="2">3.05EÀ04 (2.55 EÀ05)</cell><cell>0</cell></row><row><cell>Rosenbrock</cell><cell></cell><cell cols="2">1.02EÀ05 (2.15EÀ06)</cell><cell cols="2">1.97EÀ03 (3.45EÀ04)</cell><cell cols="2">1.17EÀ02 (6.89 EÀ03)</cell><cell>0</cell></row><row><cell>Ackley</cell><cell></cell><cell>2.98 (0.53)</cell><cell></cell><cell>2.98 (0.34)</cell><cell></cell><cell cols="2">4.98EÀ03 (7.21 EÀ05)</cell><cell>0</cell></row><row><cell>Sphere</cell><cell></cell><cell cols="2">1.62EÀ04 (3.56EÀ05)</cell><cell cols="2">8.26EÀ11 (9.33EÀ12)</cell><cell cols="2">1.05EÀ10 (1.27 EÀ11)</cell><cell>0</cell></row><row><cell>Griewank</cell><cell></cell><cell cols="2">2.552EÀ05 (1.23EÀ06)</cell><cell cols="2">2.56EÀ02 (5.33EÀ03)</cell><cell cols="2">1.07EÀ07 (2.99 EÀ08)</cell><cell>0</cell></row><row><cell>Michalewics</cell><cell></cell><cell>À1.7829 (0.3345)</cell><cell></cell><cell cols="2">À7.44EÀ01 (1.11EÀ02)</cell><cell>À1.8201 (0.1567)</cell><cell></cell><cell>À1.8013</cell></row><row><cell>Zakharov</cell><cell></cell><cell cols="2">0.00146674 (0.00056)</cell><cell>8.10EÀ01 (6.7EÀ02)</cell><cell></cell><cell>0.00168 (0.00043)</cell><cell></cell><cell>0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>F. Valdez et al. / Information Sciences 270 (2014) 143-153</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to express our gratitude to CONACYT and Tijuana Institute of Technology for the facilities and resources granted for the development of this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using selection to improve Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angeline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1998 IEEE World Congress on Computational Intelligence</title>
		<meeting>1998 IEEE World Congress on Computational Intelligence<address><addrLine>Anchorage, Alaska</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="84" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evolutionary Optimization versus Particle Swarm Optimization: Philosophy and Performance Differences, Evolutionary Programming VII</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angeline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1447</biblScope>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
		<title level="m">Handbook of Evolutionary Computation</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Fogel</surname></persName>
		</editor>
		<editor>
			<persName><surname>Michalewicz</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huesca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Valdez</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence, IC-AI &apos;04</title>
		<meeting>the International Conference on Artificial Intelligence, IC-AI &apos;04<address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">June 21-24, 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="98" to="104" />
		</imprint>
	</monogr>
	<note>Evolutionary computing for fuzzy system optimization in intelligent control</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical genetic algorithms for topology optimization in fuzzy control systems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Valdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Gen. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="575" to="591" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hybrid intelligent systems for time series prediction using neural networks, fuzzy logic, and fractal theory</title>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1395" to="1408" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new optimizer using particle swarm theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Symposium on Micromachine and Human Science</title>
		<meeting>the Sixth International Symposium on Micromachine and Human Science<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Garden in the Machine. The Emerging Science of Artificial Life</title>
		<author>
			<persName><forename type="first">C</forename><surname>Emmeche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An introduction to simulated evolutionary optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Fogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Genetic Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artificial System</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>The University of Michigan Press</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Population structure and particle swarm performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Evolutionary Computation</title>
		<meeting>IEEE Conference on Evolutionary Computation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1671" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The particle swarm-explosion, stability, and convergence in a multidimensional complex space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="73" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Neural Networks</title>
		<meeting>IEEE International Conference on Neural Networks<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Genetic Algorithms: Concepts and Designs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human evolutionary model: a new approach to optimization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepulveda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2075" to="2098" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evolutionary Computing for the Optimization of Mathematical Functions, Analysis and Design of Intelligent Systems using Soft Computing Techniques</title>
		<author>
			<persName><forename type="first">F</forename><surname>Valdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Soft Computing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="2007-06">June 2007</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel evolutionary computing using a cluster for mathematical function optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Valdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAFIPS</title>
		<meeting>NAFIPS<address><addrLine>San Diego CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="598" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A New Evolutionary Method with a Hybrid Approach Combining Particle Swarm Optimization and Genetic Algorithms using Fuzzy Logic for Decision Making</title>
		<author>
			<persName><forename type="first">F</forename><surname>Valdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="1333" to="1339" />
			<date type="published" when="2008">2008. 2008. 2008</date>
			<pubPlace>Hong Kong</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural Network Optimization with a Hybrid Evolutionary Method that combines Particle Swarm Optimization and Genetic Algorithms with Fuzzy Rules</title>
		<author>
			<persName><forename type="first">F</forename><surname>Valdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-05">2008. 2008. May 2008</date>
			<publisher>Fuzzy Information Processing Society</publisher>
			<biblScope unit="page" from="19" to="22" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fuzzy logic for parameter tuning in evolutionary computation and bio-inspired methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Valdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. MICAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="465" to="474" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving classifier fusion using Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Osadciw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Fusion Conference</title>
		<meeting><address><addrLine>Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wavelet-based Illumination Compensation for Face Recognition using Eigenface Method Intelligent Control and Automation, WCICA</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiatao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhongzxiu</surname></persName>
		</author>
		<author>
			<persName><surname>Zheru</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-06">2006. June 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="10356" to="10360" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
