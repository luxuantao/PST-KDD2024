<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speech Driven Talking Face Generation from a Single Image and an Emotion Condition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Sefik</forename><forename type="middle">Emre</forename><surname>Eskimez</surname></persName>
							<email>emreeskimez@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">You</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speech Driven Talking Face Generation from a Single Image and an Emotion Condition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Talking face generation</term>
					<term>emotion</term>
					<term>audiovisual</term>
					<term>multimodal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual emotion expression plays an important role in audiovisual speech communication. In this work, we propose a novel approach to rendering visual emotion expression in speech-driven talking face generation. Specifically, we design an end-to-end talking face generation system that takes a speech utterance, a single face image, and a categorical emotion label as input to render a talking face video in sync with the speech and expressing the condition emotion. Objective evaluation on image quality, audiovisual synchronization, and visual emotion expression shows that the proposed system outperforms a stateof-the-art baseline system. Subjective evaluation of visual emotion expression and video realness also demonstrates the superiority of the proposed system. Furthermore, we conduct a pilot study on human emotion recognition of generated videos with mismatched emotions between the audio and visual modalities, and results show that humans reply on the visual modality more significantly than the audio modality on this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PEECH communication does not solely depend on the acoustic signal. Visual cues, when present, also play a vital role. The presence of visual cues improves speech comprehension <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> in noisy environments and for the hardof-hearing population. Consequently, researchers developed systems that can automatically generate talking faces from speech in order to provide the visual cues when they are not available <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. These systems can increase the accessibility of abundantly available audioonly resources for the hearing impaired population. They can also find wide applications in entertainment, education, and healthcare.</p><p>During speech communication, emotion has a direct impact on the transmitted message and can change the meaning drastically <ref type="bibr" target="#b12">[13]</ref>. Studies have shown that predicting emotions purely from speech audio is quite difficult for untrained people <ref type="bibr" target="#b13">[14]</ref> and that we heavily rely on visual cues in emotion interpretation <ref type="bibr" target="#b14">[15]</ref>. Therefore, to make the visual rendering more realistic and to improve speech communication, it is important for automatic talking face generation systems to render visual emotion expressions.</p><p>One approach to emotional talking face generation is to first estimate the expressed emotions from the speech utterance and then render them in the generated talking faces. This approach, however, is limited by the speech emotion recognition accuracy and does not allow an independent control of emotion expression in the visual rendering. In this work, we take a different approach: We ignore emotions expressed in the speech audio and condition the talking face generation on an independent emotion variable. This approach provides a direct and more flexible control of visual emotion expression, and can enable more personalized applications in entertainment, education, and personal assistants. It also provides a powerful tool for behavioral psychologists to conduct emotion-relevant experiments that were not possible before. For example, one can investigate how humans respond to and interact with their conversational partners' emotion expressions by manipulating these emotions in audio and visual modalities independently.</p><p>In this work, we propose the first neural network system that generates emotional talking faces from speech conditioned on categorical emotions. The network that takes a speech utterance, a reference face image, and a categorical emotion condition as inputs, then generates a talking face that is in sync with the input speech with emotional expressions. Our main contributions are as follows:</p><p>• We propose a new talking face generation method that can be conditioned on categorical emotions. • We propose an emotion discriminative loss that classifies rendered visual emotions. • We conduct a pilot study on human emotion perception of talking face videos with mismatched emotions between the audio and visual modalities. The rest of the paper is organized as follows: We first present related work on talking faces in Section II. We then describe the proposed method and objective functions in Section III. Then, we present experimentation details, the objective evaluations, and Amazon Mechanical Turk subjective evaluations in Section IV. Finally, we conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Emotional Talking Face Generation</head><p>Automatically generating talking faces from speech is drawing increasing attention from researchers in recent years. One approach is to first convert the speech input to face landmarks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref> and then estimate video frames using the predicted face landmarks. In Suwajanakorn et al.'s two-stage system <ref type="bibr" target="#b15">[16]</ref>, an LSTM network first predicts the principal component analysis (PCA) coefficients of face landmarks from speech features, then retrieves candidate frames from the dataset according to the PCA arXiv:2008.03592v1 [eess.AS] 8 Aug 2020 coefficients and stitches them together. However, this system works only for a single speaker. Another two-stage system is proposed by Chen et al. <ref type="bibr" target="#b4">[5]</ref>. The system first predicts 68 face landmarks from speech using an LSTM-based network <ref type="bibr" target="#b6">[7]</ref>, and then predicts a few talking face images from the condition image and the face landmarks. They employ a discriminator network to improve image quality. In another work, Egor et al. <ref type="bibr" target="#b18">[19]</ref> proposed a style-based landmark-to-image conversion method using generative adversarial networks (GANs) with a few shots of the target face. This method, however, lacks landmark adaptation methods to solve personality mismatch issues.</p><p>Some researchers designed systems that directly map speech features to video frames. Features extracted from the speech often include the mel-frequency cepstral coefficients (MFCCs), energy, and the first-and second-order temporal derivatives of these features. Chung et al. <ref type="bibr" target="#b5">[6]</ref> proposed a CNN that takes a condition face image and speech features and generates a talking face video. The generated video is then sharpened by another CNN which is trained on pairs of artificially blurred images and their clear originals. Chen et al. <ref type="bibr" target="#b19">[20]</ref> proposed another method that predicts video frames of the lip region from speech features and a condition lip image. They introduced a GAN loss in addition to the reconstruction loss sharpen the generated overly smooth video frames. However, this method is limited to only generating the lip region instead of the entire talking face. Zhou et al. <ref type="bibr" target="#b11">[12]</ref> proposed a GAN-based method that models the whole face and introduced a temporal-GAN loss besides the reconstruction loss to improve the temporal dependency across frames. Song et al. <ref type="bibr" target="#b8">[9]</ref> proposed another method that generates talking faces by using a conditional recurrent adversarial network to improve the realness. Yu et al. <ref type="bibr" target="#b10">[11]</ref> adopted optical flow and a self-attention mechanism to capture adjacent and long-range temporal dependencies of video frames.</p><p>In addition to the above-mentioned two-stage or speechfeature-driven approaches, there are also end-to-end systems that generate talking faces directly from a condition image and the speech signal. Vougioukas et al. <ref type="bibr" target="#b17">[18]</ref> proposed a temporal-GAN method to generate more realistic image sequences. They further improved their methods with three discriminators <ref type="bibr" target="#b9">[10]</ref> that focus on improving the realness of video frames, the continuity between generated frames, and the synchronization between audio and visual data. Eskimez et al. <ref type="bibr" target="#b20">[21]</ref> proposed an end-to-end talking face generation system that is robust to noisy speech input. The system contains a frame discriminator to improve image quality and a pair discriminator to improve lip-speech synchronization. They proposed a mouth region mask (MRM) to improve the lip-speech synchronization further and showed that it provides better alignment than the baselines.</p><p>Regarding emotional talking face generation, existing work is somewhat limited. Karras et al. <ref type="bibr" target="#b16">[17]</ref> adopted an end-to-end network to learn a latent representation of emotion states and use the latent code as a control to generate 3D mesh animation. This method effectively discovers emotion variations in the data, but the learned emotion states are difficult to interpret and do not model facial features such as wrinkled eyes and head motion to generate facial expressions. Sadoughi et al. <ref type="bibr" target="#b21">[22]</ref> extended the conditional-GAN-based model to take the target emotion as an input, but this method is limited to generating the lip area instead of the whole face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multimodal Human Emotion Perception</head><p>Emotion perception from auditory and visual stimuli has been examined in recent years. Existing work <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> concludes that different modalities complement each other, and there are also intermodal effects. Cowie <ref type="bibr" target="#b25">[26]</ref> showed that perception is sensitive to stimuli from multiple modalities in acted data and naturalistic data. Jessen et al. <ref type="bibr" target="#b24">[25]</ref> suggested that emotional visual content allows more reliable prediction of auditory information. Schirmer et al. <ref type="bibr" target="#b22">[23]</ref> explored modalities in term of neural responses and showed that each modality provides a distinct insight and multimodal perception converges for holistic emotion recognition.</p><p>Most of the existing work was focused on emotionally congruent stimuli from these two modalities; little work examined incongruent stimuli. Tsiourti et al. <ref type="bibr" target="#b27">[28]</ref> investigated human responses to emotions expressed by the body and voice of humanoid robots, showing that cross-modal incongruency decreased emotion recognition accuracy. Piwek et al. <ref type="bibr" target="#b28">[29]</ref> found that subjects weighted visual cues higher in emotion judgments when presented emotionally incongruent audiovisual clips with happy or angry emotion. However, the visual content was conveyed by point-light displays instead of natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Instead of inferring emotion from the input speech <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, in this work, we propose to use emotions as a condition input to our system. The motivation is to decouple the speech and emotion conditions. This allows us to manipulate emotions during the generation of face videos. Figure <ref type="figure">1</ref> shows the system overview, which employs the generative adversarial network (GAN) framework. Our generator network architecture is built based on our previous work <ref type="bibr" target="#b20">[21]</ref>, with a modification to accept the emotion condition input. For discriminator networks, we use one discriminator to distinguish the emotions expressed in videos, and another discriminator to distinguish the real and generated video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generator</head><p>The generator network contains the following sub-networks: speech, image, noise, and emotion encoders, and a video decoder.</p><p>1) Speech Encoder: The speech encoder processes the input speech waveform and outputs a speech embedding. It follows the original implementation of <ref type="bibr" target="#b20">[21]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WGAN-GP</head><p>Generator Fig. <ref type="figure">1</ref>: Overview of the proposed neural network system. It accepts a reference image, a speech waveform, a random vector from the standard normal distribution, and a categorical emotion as input, concatenates their embeddings, and generates a talking face video that is in sync with the input speech and expresses the input emotion. During training, besides the mouth region mask (MRM) reconstruction loss and perceptual loss, the network employs two discriminative losses: the frame discriminator for image quality and the emotion discriminator for emotion expression.</p><p>1 second of speech. We add a context layer after these five convolutional layers to concatenate the past and future speech features. The context layer reduces the 125 time-steps to 25 time-steps by passing only every fifth frame to the next layer. Therefore, our generated videos are in 25 frame-perseconds (FPS). The output of the context layer is fed to a fully connected layer, followed by two LSTM layers, which output the speech embedding sequence.</p><p>2) Image Encoder: The image encoder computes an image embedding from the input condition face image. The architecture follows the original implementation without any modification <ref type="bibr" target="#b20">[21]</ref>. It contains six layers of 2-D convolutional layers with the following number of filters, kernel sizes, and down-sampling factors: (64, 3, 2), (128, 3, 2), (256, 3, 2), (512, 3, 2), (512, 3, 2), (512, 4, 1), respectively. A LeakyReLU activation with 0.2 slope follows each convolutional layer. Note that nearest-neighbor interpolation is used for downsampling rather than using strides, which eliminates the artifacts in the generated images. The final image embeddings and intermediate representations are all passed to the video decoder using U-Net style skip connections <ref type="bibr" target="#b29">[30]</ref>.</p><p>3) Emotion Encoder: The emotion label is first encoded as a one-hot vector and is fed into the emotion encoder. The emotion encoder uses a two-layer fully-connected (FC) neural network to project the one-hot vector to an emotion embedding. This embedding is replicated for each time-step. Again, we use a LeakyReLU activation with 0.2 slope after every FC layer.</p><p>4) Noise Encoder: For each frame of the video, we generate a noise vector drawn from the standard Gaussian distribution. A single-layer LSTM processes this sequence of noise vectors and outputs the noise embedding. This noise embedding models the head movements that are not correlated with the speech, image, and the emotion condition.</p><p>5) Video Decoder: We modify the video decoder described in <ref type="bibr" target="#b20">[21]</ref> to accept the additional emotion embedding. We concatenate the speech, image, noise and emotion embeddings, and feed them into the decoder. For each time step, the decoder uses convolutional layers to project the embeddings into 4 × 4 images using two FC layers and reshape operations. These 4 × 4 images are concatenated channel-wise with the skip connections coming from the image encoder in the U-Net fashion for the next layers, except for the last layer. The number of filters in each convolutional layer is the same as its corresponding layer in the image encoder. A LeakyReLU activation with 0.2 slope follows each convolutional layer, except for the last layer, where instead, hyperbolic tangent activation is used since the images are normalized between -1 to 1 value range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Frame Discriminator</head><p>The frame discriminator aims to improve the image quality of the generated video and to keep the target identity consistent throughout the video. First, we repeat the target image for the number of frames in the input video and concatenate them together. Then, each frame is processed by five layers of 2-D convolutional layers. The number of filters, kernel sizes and strides of these convolutional layers are as follows: (64, 3, 2), (128, 3, 2), (256, 3, 2), (512, 3, 2), (512, 3, 2), respectively. The output is then flattened and fed into a two-layer FC network, which classifies the frame as fake or real. Each layer is followed by a LeakyReLU activation with 0.2 slope except for the last layer, where we do not use an activation since our system employs Wasserstein GAN with a gradient penalty <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Emotion Discriminator</head><p>The emotion discriminator is essentially a video-based emotion classifier, with the inclusion of an additional class for fake videos. It aims to improve the emotion expression generated by our network. The first part of the network follows the same architecture as the frame discriminator: five layers of 2-D convolutional layer followed by two FC layers. We process each frame of the video and feed the resulting sequence into an LSTM layer. The last time step of the output of the LSTM layer is fed into an FC layer that outputs probabilities of the seven classes: six emotions (anger, disgust, fear, happiness, neutral, and sadness) plus the fake class as in <ref type="bibr" target="#b31">[32]</ref>. When we take a training step for the discriminator, we calculate the sparse categorical cross-entropy loss using the emotion label for the real video and the fake label for the generated video. When updating the generator, we calculate the sparse categorical cross-entropy loss using the emotion label we used for generating the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Objective Functions</head><p>Our system employs multiple objective functions that focus on different aspects of the generated videos: a mouth region mask (MRM) loss proposed in <ref type="bibr" target="#b20">[21]</ref> to improve mouth-audio synchronization, a perceptual loss to improve image quality, a frame GAN loss for image quality, and an emotion GAN loss for emotion expression.</p><p>1) Mouth Region Mask (MRM) Loss: The MRM loss is a weighted L1 reconstruction loss between the generated and ground-truth videos around the mouth region. It uses a 2D Gaussian centered at the mean position of mouth coordinates as the weights. The intuition of MRM is to manually drive the attention of the network to the mouth region to improve the mouth-audio synchronization.</p><p>2) Perceptual Loss: We employ a pre-trained VGG-19 network <ref type="bibr" target="#b32">[33]</ref> and calculate intermediate features of the following layers from both the generated and ground-truth videos: 4, 9, 18, 27, and 36. Then, a mean-squared loss between these intermediate features is calculated as the perceptual loss to improve image quality.</p><p>3) Frame GAN Loss: To further improve the image quality, especially the sharpness, we use a frame GAN loss calculated by the frame discriminator. Instead of the vanilla GAN loss, we use Wasserstein GAN for more stable training.</p><p>4) Emotion GAN Loss: To ensure emotion expression in generated videos, we use an emotion GAN loss calculated by the emotion discriminator, which is a categorical crossentropy loss using six emotion classes plus a "fake" class, similar to <ref type="bibr" target="#b31">[32]</ref>. In vanilla GAN discriminator, the samples are only classified as real or fake instead of the multiple emotion classes; if the generator only generates samples from a single emotion class all the time, the vanilla discriminator would still classify them as real. The proposed discriminator, on the other hand, incorporates multi-class classification losses and mitigates this issue of mode collapse for multi-class generation.</p><p>The full objective function for the generator step is as follows:</p><formula xml:id="formula_0">J GEN = αL M RM 1 + βL P erceptual 2 + γJ F D + δJ ED , (1)</formula><p>where J GEN is the generator loss, L M RM 1 is the mouth region mask loss, L P erceptual 2 is the perceptual loss, J F D is the frame GAN loss, J ED is the emotion GAN loss, and α, β, γ, δ are the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we describe the data used in experiments, the hyper-parameters of the neural networks, and objective and subjective evaluations. We choose the temporal GAN approach described in <ref type="bibr" target="#b9">[10]</ref> as our baseline since it is the closest to our method. We use the pre-trained model and inference code provided by the authors to generate baseline videos. Although it cannot control the emotions through a condition input, it can generate emotional expressions that are inferred from the speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We used the Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D) dataset <ref type="bibr" target="#b33">[34]</ref>. It contains video clips of 91 actors (48 male and 43 female) expressing six categorical emotions: anger, disgust, fear, happiness, neutral, and sadness. The age range of the actors is between 20 to 74. Each video clip shows one actor speaking a sentence from a set of 12 sentences with one of the emotion categories. The image resolution of the provided videos is 480x360, and the sampling rate is 30 frames per second (FPS). The audio is sampled at 44.1 kHz. We downsampled the video to 25 FPS and audio to 8 kHz. We followed the same train (70%), validation (15%), and test (15%) splits as <ref type="bibr" target="#b9">[10]</ref>. We used the same files for these splits to ensure a fair comparison. During testing, the speech utterance, the condition emotion and the condition image input to the generator network for each generation are all from the same ground-truth video, where the condition image is the first frame of the video.</p><p>As the same actor's face in different videos may be at different spatial locations, for easing the training, we need to align them across videos. For alignment, first, we choose a template image for the actor where the face is symmetrical. We extracted face landmarks from this template image as the template landmarks. Then, for each video of the actor, we estimated the similarity transform parameters between the template landmarks and extracted landmarks of the first frame using three points: the temporal mean points of the left eye, the right eye, and the nose. Note that we only took the first frame of each video to estimate the transformation, and used it to align the remaining frames to the template image. In this way, the faces in the resulting videos start from the same spatial location but can wander off to different parts of the scene. This allows us to model the natural head movements in addition to facial expressions. During training, we randomly augmented the data using the Albumentations library <ref type="bibr" target="#b34">[35]</ref> to improve the generalization capability of our network. The data augmentation includes randomly changing brightness, contrast, gamma, hue, saturation, and value. Besides, it includes contrast limited adaptive histogram equalization, adding random Gaussian noise to the image, and shuffling the channels and shifting RGB values for each channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>To initialize our network, we trained it from scratch using only MRM and perceptual losses for 100k iterations. Then, we trained it for another 100k iteration using the full objective function. We used Adam optimizer for all networks with β 1 = 0.5, β 2 = 0.99. The learning rate for the generator was 1e-4 during the initialization and 1e-5 during the GAN training. Both discriminators' learning rates were 1e-4. The constants α, β, γ, andδ mentioned in Section III-D were 100, 1, 0.01, and 0.001, respectively. The weight for the gradient penalty when training the frame discriminator was 10. All images were normalized between -1 to 1 value range. During initialization, the mini-batch size was set to 8, and during GAN training, it was set to 4. The number of frames per sample was set to 32.</p><p>The training took approximately a week using a GTX 1080 TI GPU. For the baseline method, we used pre-trained model (trained with CREMA-D dataset) provided by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Objective Evaluation</head><p>We evaluated the image quality of the generated videos using Peak SNR (PSNR) and Structural Similarity (SSIM) <ref type="bibr" target="#b35">[36]</ref> between the generated video frames and the ground-truth video frames. To measure the audiovisual synchronization, we used the normalized landmarks distance (NLMD) <ref type="bibr" target="#b20">[21]</ref> between landmarks extracted from the generated and ground-truth video frames.</p><p>The baseline method generates 96x128 images, while our method yields 128x128 images. In other words, the foreground/background ratio differs in the generated videos. To ensure a fair comparison, we aligned the ground-truth, baseline, and proposed videos into a template image and cropped them into the same size using similarity transformation. Figure <ref type="figure" target="#fig_0">2</ref> shows the aligned videos, and Figure <ref type="figure">3</ref> shows example videos generated from the same condition image and speech, but different emotion conditions.</p><p>Table <ref type="table" target="#tab_1">I</ref> shows the objective evaluation results of the baseline and our proposed methods. It can be seen that our method outperforms the baseline on all of the three metrics. We believe that the improvement on image quality (PSNR and SSIM) is thanks to the perceptual loss. For audiovisual synchronization  (NLMD), even though our method does not use a discriminator calculating a synchronization loss as in <ref type="bibr" target="#b9">[10]</ref>, the improvement is as high as 8.9%, showing the effectiveness of the MRM loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Video-based Emotion Classification:</head><p>In order to validate the emotion expression in the generated videos, we trained a video-based emotion recognition network using the CREMA-D train set. This network uses the same architecture as the emotion discriminator in Figure <ref type="figure">1</ref>. We then classified emotions of the ground-truth videos and our generated videos of the test set. The results are shown in Table <ref type="table" target="#tab_2">II</ref>. The 6-class emotion classification accuracy on the ground-truth videos is 62.71%, which is comparable with <ref type="bibr" target="#b36">[37]</ref>, suggesting the validity of the video-based emotion classifier. The accuracy and F1-Score on the generated videos are slighted higher, even though the classifier was not trained on generated videos. This suggests that emotions are well expressed, and slightly exaggerated perhaps, in the generated videos.</p><p>We further show the confusion matrices of these two classification results in Figure <ref type="figure" target="#fig_1">4</ref>. We observe similar patterns. First, they both have a strong diagonal. In particular, happiness is the easiest emotion to classify. This may be because happiness Fig. <ref type="figure">3</ref>: Frames of different talking face videos generated (in different rows) using the same face image (the first column) and speech utterance but different emotion conditions (from top to bottom: anger, disgust, fear, happiness, neutral, and sadness). One frame is for every 0.2 seconds. often contains smiling that is distinctive from other facial expressions, allowing the classifier as well as our generation system to capture it clearly. Second, some emotions are commonly confused with each other, such as fear and sadness. On the other hand, there are also differences in these confusion matrices. In particular, in the ground-truth videos, both fear and sadness are often misclassified as disgust, while in the generated videos, no other emotions are misclassified as disgust. Overall, the similarities outweigh the differences, showing that the emotion expressions in the generated videos resemble that in the ground-truth.<ref type="foot" target="#foot_0">1</ref> D. Subjective Evaluation 1) Research Questions: We design our subjective evaluation to investigate the following research questions: 1) Is our model effective in expressing emotions for video rendering? 2) How real are the generated videos of our model? 3) Which modality do people primarily rely on to perceive emotions? We conduct our evaluation on Amazon Mechanical Turk (AMT).</p><p>2) Experimental Setup: Our AMT study consists of two Human Intelligence Tasks (HIT). For the first task, we randomly presented subjects generated and ground-truth videos, and asked them to rate the realness and aspects that could be improved to make videos more real. We also asked subjects to assign an emotion label to each video. This task aimed to answer the first and second research questions. For the second task, we generated videos that contain mismatched emotions in the audio and visual modalities. We asked subjects to assign one or two emotion labels to these videos. By doing so, we aimed to answer the third research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 1 -Emotion Classification and Realness Evaluation.</head><p>In the first task, we pooled videos taken from ground-truth videos of the test set of CREMA-D dataset, and generated videos from the baseline and our models. For the baseline system, each video was generated from the speech recording and the first frame of the ground-truth video, while for the proposed system, each video was generated from the speech recording and the first frame of the ground-truth video, as well as the ground-truth emotion condition. We downsampled the ground-truth and baseline videos to 25 FPS to make them consistent with our generated ones. As described earlier, our method generates talking faces in 128 × 128 image size, while the baseline method generates videos in 96×128 image size. If we were to set the ground-truth videos to any of the two sizes, the subjects might be negatively biased toward the generated videos with the other size. To avoid this potential problem, we aligned the ground-truth videos with template faces of both sizes and obtained two sets of ground truth videos.</p><p>We released six batches of videos in total. For each batch, we randomly selected five videos from the two sets of ground- truth videos, five from the baseline videos, and five from our generated videos. One video from each category was repeated to check the consistency of the subjects' answers. Therefore, there were in total 18 videos in each batch. The videos in each batch were randomly shuffled. Across all of the six batches, the total number of videos for each emotion category was equal. We recruited a total of 60 valid subjects (i.e., 10 for each batch) from Amazon Mechanical Turk (AMT). The subjects were required to be located in the United States and to have a lifetime HIT approval rate higher than 95%. To encourage the subjects to treat the experiments more seriously, we made a bonus payment based on the subject's performance. Subjects were informed about the bonus payment before they started the experiments.</p><p>Subjects were informed that some of the 18 videos were recordings of real people, while other videos were rendered by artificial intelligence (AI) based on a single face image of one person and a speech recording of another person. Before presenting the 18 videos, we also presented two example videos of real recordings for each emotion, each in the two image sizes (128×128 and 96×128), to familiarize the subjects with these emotion expression. These example emotions were ordered in alphabetical order.</p><p>We then asked subjects the following three questions for each video: 1) Which emotion is primarily expressed by the person? This question is a multiple choice question, and the subjects were asked to select one from the six emotions. 2) How realistic is the video? The subjects can choose from Definitely real, Somewhat real, Neutral, Somewhat unreal, and Definitely unreal. 3) Which aspect(s) can be improved to make the video more real? This question is a checkbox question, and the subjects could choose more than one aspect. The choices are None, Image quality, Lip synchronization, Head movement, and Other.</p><p>After receiving a survey, we checked its completeness and the consistency of answers to the nine questions of the three repeated videos. We rejected a total of 11 incomplete surveys and those did not meet the consistency requirement, and recruited other subjects until we collected 60 valid surveys. For our answer consistency requirement, an answer was considered inconsistent from the previous answer, if 1) the emotion classification was different for the first question, 2) the realness rating differed more than one level for the second question, or 3) the aspect selection differed for more than one options. Among the nine repeated questions, if more than five answers were inconsistent, then the entire survey was rejected.</p><p>Task 2 -Emotion Perception of Videos with Mismatched Emotions. As described in Section II-B, little work on human emotion perception used emotionally incongruent stimuli between the audio and visual modalities, and among these work, none used videos of humans as stimuli. Our emotional talking face generation system makes it possible to investigate human emotion perception from emotionally incongruent stimuli of human speaking videos.</p><p>In the second task, we presented generated videos from our proposed system based on a face image, a speech recording, and an emotion condition. Both the face image and the speech recording were taken from a ground-truth video in the test set of the CREMA-D dataset, therefore, the speech recording conveyed a certain emotion. The emotion condition input, however, was not necessarily the same as the speech emotion to generate videos with mismatched emotions between the audio and visual modalities. As there are six emotions in the dataset, there are 36 emotion pairs and 30 of them are mismatched. We generated 2 videos for each of the 36 pairs, shuffled them and split them evenly into six batches. We also repeated two videos in each batch to check the answer consistency. Therefore, there are a total of 14 videos in each batch.</p><p>We recruited a total of 60 subjects (i.e., 10 for each batch) from AMT, with the same requirements as Task 1. We rejected a total of 4 incomplete surveys and those who had more than two inconsistent answers among the four repeated questions, and recruited other subjects until we collected 60 valid surveys. The participants who completed Task 1 could not see this task from the AMT platform. The same bonus mechanism in Task 1 was applied to Task 2. In the survey, subjects were notified that all of the videos were AI rendered. Before presenting the generated videos, the subjects were also presented two example ground-truth videos for each emotion, only in the image size of 128 × 128, to familiarize them with the emotions. They were asked the following two multiple choice questions for each video: 1) Which is the primary emotion expressed by the person? The subjects could select one from the six emotions. 2) Which is the secondary emotion expressed by the person? The subjects could select one from the six emotions and a None option if they only perceived the primary emotion.</p><p>3) Experimental Results: Task 1 -Emotion Classification. The confusion matrices of subjective emotion classification for ground-truth videos, baseline generated videos, and our generated videos are shown in Figure <ref type="figure" target="#fig_2">5</ref>. Our videos engender a more diagonal confusion matrix compared with the baseline videos and result in similar patterns with the ground-truth videos. Specifically, subjects are more likely to classify the emotions in the baseline videos as neutral, while this happens much less frequently for our generated videos. This shows the power of the emotion condition input that our method takes. The overall classification accuracy is 59.2% (groundtruth), 28.9% (baseline), 55.3% (ours), respectively, demonstrating the efficacy in expressing emotions of our proposed emotional talking face generation system. It must be noted that the baseline system infers emotion from the speech input instead of taking the emotion condition as input. As emotion recognition from the speech is itself a challenging task, errors in this stage naturally influence visual emotion expression in the generated videos. Therefore, the poor performance from the baseline system is expected. Interestingly, the 59.2% human emotion classification accuracy on ground-truth videos is slightly lower than that of our emotional classifier in Section IV-C1, showing the challenge of visual speech emotion classification for humans. This observation is similar to a speech emotion classification observation in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Task 1 -Realness Evaluation. For the realness question, the five options are mapped to a scale from 1 to 5, where "definitely real" corresponds to 5 and "definitely unreal" corresponds to 1. The result is shown in Figure <ref type="figure" target="#fig_3">6</ref>. The average rating across all videos and workers is 3.94, 3.71 and 3.81 for ground-truth, baseline, and our videos, respectively. This suggests that our generated videos are slightly more realistic than the baseline videos, yet they are still not as realistic as the ground-truth videos. Interestingly, even the ground-truth videos only received an average rating close to 4 (somewhat real). We think that this might be due to the relatively lower image resolution than what the workers typically see in their daily life. This might also because the generated videos (especially OURS) are quite realistic, lowering the workers' confidence in rating the ground-truth videos. A Wilcoxon signed-rank test <ref type="bibr" target="#b37">[38]</ref> shows that the median difference between our ratings and the baseline ratings is statistically significantly greater than zero, at the significance level of 0.05 (p = 0.048).</p><p>Figure <ref type="figure" target="#fig_4">7</ref> shows the histograms of aspects considered by the subjects to improve the realness of the videos. Consistent with the realness question, ground-truth videos received the most "none" votes, while our generated ones received the second and the baseline received the least . The total count of votes to the four aspects to improve ("image quality", "lip synchronization", "head movement", "other") is 299 (groundtruth), 337 (baseline) and 325 (ours), respectively. Among the detailed aspects, the baseline videos received the most votes on "image quality" and "lip synchronization"; but it also received the least votes on "head movement" . This might be due to the fact that the baseline method is trained with 30 FPS videos and adopted a sequence discriminator to render head movements. On the other hand, our generated videos performed similarly to ground-truth ones on "lip synchronization" and "head movement", suggesting the effectiveness of our proposed MRM loss. Nevertheless, the "image quality" of our generated videos are considered to need more improvement than the groundtruth videos.</p><p>Task 2-Emotion Perception of Videos with Mismatched Emotions. In Task 2, subjects were asked the primary and secondary (if any) emotions they perceive from each video generated by our system, to investigate which modality people primarily rely on for emotion recognition. Overall, 426 of the 840 videos received two emotion labels. We first compared the primary emotion label with the visual emotion (i.e., the condition emotion when generating the video) and the audio emotion, respectively. The confusion matrices are shown  in Figure <ref type="figure" target="#fig_5">8</ref>. Overall, 35.2% of the primary emotion labels match with the visual emotion, while only 25.1% of them match with audio emotion. If we only consider videos with mismatched emotions, these numbers become to 31.4% and 19.6%, respectively. This suggests that the subjects relied on the visual modality much more heavily than the audio modality for emotion perception. Among the six emotions, happiness and disgust seem to be the easiest to perceive from the visual modality, while anger and fear are most difficult.</p><p>We then considered both primary and secondary emotions when comparing with the audio and visual emotions. In this case, 44.9% of labeled emotions can be matched to the visual emotion, while 33.8% can be matched to the audio emotion. Similarly, if we only consider videos with mismatched emotions, these numbers become to 41.1% and 28.2%. Again, this shows that the visual modality affects much more than the audio modality on audiovisual speech emotion recognition by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this work, we proposed a novel emotional talking face generation system that is conditioned on speech, reference image, and categorical emotion inputs. We evaluated our network against the ground-truth videos and a baseline system <ref type="bibr" target="#b9">[10]</ref> and validated that our method can generate emotional expressions effectively. In addition, we conducted a subjective study on Amazon Mechanical Turk (AMT) showing that our method yields close performance to the ground-truth videos in terms of realness and emotion classification. Furthermore, we also conducted a pilot study on human emotion perception from audiovisual speech with mismatched emotions expressed in the audio and visual modalities, showing that visual perception is more dominant than auditory perception. For future work, we plan to improve the image quality of generated videos. We also plan to extend this work to 3D animation and rendering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Four examples comparing spatially aligned and cropped videos of the ground-truth (GT), baseline (BL) and proposed (OURS) for objective evaluation. Every fifth frame is shown for each video.</figDesc><graphic url="image-142.png" coords="5,323.34,269.35,237.23,71.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Confusion matrices of video-based emotion classification on the ground-truth (left) and generated (right) videos using the proposed talking face generation system on the test dataset of CREMA-D. Each row sums to 100%.</figDesc><graphic url="image-217.png" coords="7,119.59,241.06,122.60,122.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Confusion matrices of human emotion classification in Task 1 on ground-truth videos (left), baseline generated videos (middle) and our generated videos (right).</figDesc><graphic url="image-219.png" coords="8,87.41,60.18,103.55,103.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: User ratings on the realness of ground-truth (GT), baseline generated (BL) and our generated (OURS) videos.</figDesc><graphic url="image-225.png" coords="9,99.18,56.07,150.63,149.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Total count of chosen aspects for realness improvement of videos.</figDesc><graphic url="image-226.png" coords="9,48.96,249.00,251.04,121.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Confusion matrices of the primary emotion label that AMT subjects give from each video against the visual emotion (left) and audio emotion (right) in Task 2.</figDesc><graphic url="image-229.png" coords="9,382.60,241.11,122.60,122.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Objective evaluation results for the baseline and our proposed method. For PSNR and SSIM, higher values are better; for NLMD, lower values are better.</figDesc><table><row><cell>Method</cell><cell cols="3">PSNR SSIM NLMD</cell></row><row><cell>Baseline [10]</cell><cell>29.64</cell><cell>0.82</cell><cell>0.124</cell></row><row><cell>Proposed</cell><cell>30.91</cell><cell>0.85</cell><cell>0.113</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Video-based emotion classification results.</figDesc><table><row><cell>Data</cell><cell cols="2">Accuracy F1-Score</cell></row><row><cell>Ground-truth</cell><cell>62.71</cell><cell>62.39</cell></row><row><cell>Generated</cell><cell>65.67</cell><cell>66.65</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For video samples, please visit the project webpage: http://www2.ece. rochester.edu/projects/air/projects/tfaceemo.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work is funded by the National Science Foundation grant No. 1741472.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bi-sensory articulation functions for normal hearing and sensorineural hearing loss patients</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Binnie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Academy of Rehabilitative Audiology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Auditory and auditory-visual intelligibility of speech in fluctuating maskers for normal-hearing and hearingimpaired listeners</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3358" to="3372" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The role of visual speech cues in reducing energetic and informational masking</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Freyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="842" to="849" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Auditory selective attention is enhanced by a task-irrelevant temporally coherent visual stimulus in human listeners</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Atilgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Bizley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical crossmodal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating talking face landmarks from speech</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Eskimez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">You said that?: Synthesising talking faces from audio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">11-12</biblScope>
			<biblScope unit="page" from="1767" to="1779" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/129</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/129" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="919" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Realistic speech-driven facial animation with gans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining audio, text and visual information for talking face generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="787" to="795" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transient voice changes associated with emotional stimuli</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kurtzberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Friedhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of General Psychiatry</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="362" to="365" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotion classification: how does an automated system compare to naive human coders</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Eskimez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Imade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sturge-Apple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heinzelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2274" to="2278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The perceptual and cognitive role of visual and auditory channels in conveying emotional information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="278" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end speech-driven facial animation with temporal gans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Few-shot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9459" to="9468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end generation of talking faces from noisy speech</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Eskimez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1948" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speech-driven expressive talking lips with conditional sequential generative adversarial networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emotion perception from face, voice, and touch: comparisons and convergence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schirmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="216" to="228" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal databases of everyday emotion: Facing up to complexity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savvidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abrilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth European conference on speech communication and technology</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the role of crossmodal prediction in audiovisual emotion perception</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">369</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perceiving emotion: towards a realistic understanding of the task</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">1535</biblScope>
			<biblScope unit="page" from="3515" to="3525" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analysis of emotion recognition using facial expressions, speech and multimodal information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international conference on Multimodal interfaces</title>
				<meeting>the 6th international conference on Multimodal interfaces</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="205" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal integration of emotional signals from voice, body, and context: Effects of (in) congruence on emotion recognition and attitudes towards robots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tsiourti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="573" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Audiovisual integration of emotional signals from others&apos; social interactions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Piwek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pollick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">611</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bagan: Data augmentation with balancing gan</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Istrate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Malossi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09655</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crema-d: Crowd-sourced emotional multimodal actors dataset</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Keutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on affective computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="390" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Albumentations: Fast and flexible image augmentations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/2078-2489/11/2/125" />
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Emotion recognition system from speech and visual information based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">N.-C</forename><surname>Ristea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Dut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Speech Technology and Human-Computer Dialogue (SpeD)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The normal approximation to the signed-rank sampling distribution when zero differences are present</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Cureton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">319</biblScope>
			<biblScope unit="page" from="1068" to="1069" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
