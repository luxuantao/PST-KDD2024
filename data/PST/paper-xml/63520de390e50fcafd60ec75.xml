<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Arxiv preprint. DIFFUSION MODELS ALREADY HAVE A SEMANTIC LATENT SPACE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-20">20 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingi</forename><surname>Kwon</surname></persName>
							<email>kwonmingi@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence Yonsei University Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaeseok</forename><surname>Jeong</surname></persName>
							<email>jetejeong@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence Yonsei University Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence Yonsei University Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Arxiv preprint. DIFFUSION MODELS ALREADY HAVE A SEMANTIC LATENT SPACE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-20">20 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.10960v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boosting by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD-DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUNbedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In image synthesis, diffusion models have advanced to achieve state-of-the-art performance regarding quality and mode coverage since the introduction of denoising diffusion probabilistic models <ref type="bibr" target="#b10">(Ho et al., 2020)</ref>. They disrupt images by adding noise through multiple steps of forward process and generate samples by progressive denoising through multiple steps of reverse (i.e., generative) process. We refer to the intermediate noisy images as latent variables. Since their deterministic version enables nearly perfect reconstruction of input images <ref type="bibr">(Song et al., 2020a)</ref>, they are suitable for image editing, which renders target attributes on the real images. However, simply editing the latent variables causes degraded results (Appendix D5 in <ref type="bibr" target="#b14">Kim &amp; Ye (2021)</ref>). Instead, they require complicated procedures: providing guidance in the reverse process or finetuning models for an attribute.</p><p>Figure <ref type="figure" target="#fig_0">1(a-c</ref>) briefly illustrates the existing approaches. Image guidance mixes the latent variables of the input image with unconditional latent variables <ref type="bibr" target="#b2">(Choi et al. (2021)</ref>, <ref type="bibr" target="#b20">Lugmayr et al. (2022)</ref>, <ref type="bibr" target="#b21">Meng et al. (2021)</ref>). Though it provides some control, it is ambiguous to specify which attribute to reflect among the ones in the input and unconditional result, and lacks intuitive control for the magnitude of change. Classifier guidance manipulates images by imposing gradients of a classifier on the latent variables in the reverse process to match the target class <ref type="bibr">(Dhariwal &amp; Nichol (2021)</ref>, <ref type="bibr" target="#b0">Avrahami et al. (2022)</ref>, <ref type="bibr" target="#b18">Liu et al. (2021))</ref>. It requires training an extra classifier for the latent variables, i.e., noisy images. Furthermore, computing gradients through the classifier during sampling is costly. Finetuning the whole model can steer the resulting images to the target attribute without the above problems <ref type="bibr" target="#b14">(Kim &amp; Ye, 2021)</ref>. Still, it requires multiple models to reflect multiple descriptions.</p><p>On the other hand, generative adversarial networks <ref type="bibr" target="#b8">(Goodfellow et al., 2020)</ref> inherently provide straightforward image editing in their latent space. Given a latent vector for an original image, we can find the direction in the latent space that maximizes the similarity of the resulting image with a target description in CLIP embedding <ref type="bibr" target="#b24">(Patashnik et al., 2021)</ref>. The latent direction found on one image leads to the same manipulation of other images. However, given a real image, finding its exact latent vector is often challenging and produces unexpected appearance changes. It would allow an admirable image editing if the diffusion models with nearly perfect inversion property have such a semantic latent space. <ref type="bibr" target="#b25">Preechakul et al. (2022)</ref> introduces an additional input to the reverse diffusion process: a latent vector from an original image embedded by an extra encoder. This latent vector contains the semantics to condition the process. However, it cannot use pretrained diffusion models because they cannot handle the external latent vectors. Inherently, the model should be trained from scratch.</p><p>In this paper, we propose an asymmetric reverse process (Asyrp) which discovers the semantic latent space of a frozen diffusion model where modification in the space synthesizes various attributes on input images. Our semantic latent space, named h-space, has practical properties for editing applications as follows. The same shift in this space results in the same attribute change in all images. Linear changes in this space lead to linear changes in the attributes. The changes do not degrade the quality of the resulting images. The changes throughout the timesteps are almost identical with the desired attribute changes. Figure <ref type="figure" target="#fig_0">1</ref>(d) illustrates some of these properties and ? 5.3 provides detailed analyses. To the best of our knowledge, it is the first attempt to discover the semantic latent space in the frozen pretrained diffusion models. Spoiler alert: our semantic latent space is different from the intermediate latent variables in the diffusion process. Moreover, we introduce a principled design of the generative process for versatile editing and quality boosting by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Extensive experiments demonstrate that our method is generally applicable to various architectures (DDPM++, iDDPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN-bedroom, and METFACES).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We briefly describe essential backgrounds. The rest of related work is deferred to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DENOISING DIFFUSION PROBABILITY MODEL (DDPM)</head><p>DDPM is a latent variable model that learns data distribution by denoising noisy images <ref type="bibr" target="#b10">(Ho et al., 2020)</ref>. The forward process diffuses the data samples through Gaussian transitions parameterized with a Markov process:</p><formula xml:id="formula_0">q (x t | x t-1 ) = N x t ; 1 -? t x t-1 , ? t I = N ? t ? t-1 x t-1 , 1 - ? t ? t-1 I ,<label>(1)</label></formula><p>where {? t } T t=1 is variance schedule and ? t = t s=1 (1 -? s ). Then the reverse process becomes p ? (x 0:T ) := p (x T ) T t=1 p ? (x t-1 | x t ), starting from x T ? N (0, I) with noise predictor ? t :</p><formula xml:id="formula_1">x t-1 = 1 ? 1 -? t x t - ? t ? 1 -? t ? t (x t ) + ? t z t ,<label>(2)</label></formula><p>where z t ? N (0, I) and ? 2 t is a variance of the reverse process which is set to ? 2 t = ? t by DDPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DENOISING DIFFUSION IMPLICIT MODEL (DDIM)</head><p>DDIM redefines Eq. ( <ref type="formula" target="#formula_0">1</ref>) as</p><formula xml:id="formula_2">q ? (x t-1 |x t , x 0 ) = N ( ? ? t-1 x 0 + 1 -? t-1 -? 2 t ? xt- ? ?tx0 ? 1-?t , ? 2 t I</formula><p>) which is a non-Markovian process <ref type="bibr">(Song et al., 2020a)</ref>. Accordingly, the reverse process becomes</p><formula xml:id="formula_3">x t-1 = ? ? t-1 x t - ? 1 -? t ? t (x t ) ? ? t "predicted x0 " + 1 -? t-1 -? 2 t ? ? t (x t ) "direction pointing to xt " + ? t z t random noise ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">? t = ? (1 -? t-1 ) / (1 -? t ) 1 -? t /? t-1</formula><p>. When ? = 1 for all t, it becomes DDPM. As ? = 0, the process becomes deterministic and guarantees nearly perfect inversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">IMAGE MANIPULATION WITH CLIP</head><p>CLIP learns multi-modal embeddings with an image encoder E I and a text encoder E T whose similarity indicates semantic similarity between images and texts <ref type="bibr" target="#b26">(Radford et al., 2021)</ref>. Compared to directly minimizing the cosine distance between the edited image and the target description <ref type="bibr" target="#b24">(Patashnik et al., 2021)</ref>, directional loss with cosine distance achieves homogeneous editing without mode collapse <ref type="bibr" target="#b7">(Gal et al., 2021)</ref>:</p><formula xml:id="formula_5">L direction x edit , y target ; x source , y source := 1 - ?I ? ?T ?I ?T ,<label>(4)</label></formula><p>where ?T = E T (y target ) -E T (y source ) and ?I = E I x edit -E I (x source ) for edited image x edit , target description y target , original image x source , and source description y source . We use the prompts 'smiling face' and 'face' as the target and source descriptions for facial attribute smiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DISCOVERING SEMANTIC LATENT SPACE IN DIFFUSION MODELS</head><p>This section explains why naive approaches do not work and proposes a new controllable reverse process. Then we describe the techniques to control the generative process. Throughout this paper, we use an abbreviated version of Eq. (3):</p><formula xml:id="formula_6">x t-1 = ? ? t-1 P t ( ? t (x t )) + D t ( ? t (x t )) + ? t z t ,<label>(5)</label></formula><p>where P t ( ? t (x t )) denotes the predicted x 0 and D t ( ? t (x t )) denotes the direction pointing to x t . We omit ? t z t for brevity except when ? = 0. We further abbreviate P t ( ? t (x t )) as P t and D t ( ? t (x t )) as D t when the context clearly specifies the arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PROBLEM</head><p>We aim to allow semantic latent manipulation on the images x 0 generated from x T given a pretrained and frozen diffusion models. The easiest idea for manipulating x 0 is simply updating x T to optimize directional CLIP loss given text prompts with Eq. (4). However, it leads to distorted images or incorrect manipulation <ref type="bibr" target="#b14">(Kim &amp; Ye, 2021)</ref>.</p><p>An alternative approach is to shift the noise ? t predicted by the network at each sampling step. However, it does not achieve manipulating x 0 because the intermediate changes in P t and D t cancel out each other resulting in the same p ? (x 0:T ), similarly to destructive interference.</p><p>Theorem 1. Let ? t be a predicted noise during the original reverse process at t and ? ? t be its shifted counterpart. Then, xt-1 ? x t-1 where xt-1 = ? ? t-1 P t (? ? t (x t )) + D t (? ? t (x t )). I.e., the shifted terms of ? ? t in P t and D t destruct each other in the reverse process.</p><p>Appendix B proves above theorem. Figure <ref type="figure" target="#fig_8">13</ref>(a-b) shows that x0 is almost identical to x 0 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ASYMMETRIC REVERSE PROCESS</head><p>In order to break the interference, we propose a new controllable reverse process with asymmetry:</p><formula xml:id="formula_7">x t-1 = ? ? t-1 P t (? ? t (x t )) + D t ( ? t (x t )),<label>(6)</label></formula><p>i.e., we modify only P t by shifting ? t to ? ? t while preserving D t . Intuitively, it drifts the original reverse process according to ? t = ? ? t -? t while it does not alter the direction toward x t so that x t-1 follows the original drift D t at each sampling step. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the above intuition.</p><p>As in <ref type="bibr" target="#b0">Avrahami et al. (2022)</ref>, we use the modified P edit t and the original P source t as visual inputs for the directional CLIP loss in Eq. ( <ref type="formula" target="#formula_5">4</ref>), and regularize the difference between the modified x edit t and the original x source t . We find ? = arg min ? Et L (t) where</p><formula xml:id="formula_8">L (t) = ? CLIP L direction P edit t , y ref ; P source t , y source + ? recon x edit t -x source t (7)</formula><p>Although ? indeed renders the attribute in the x edit 0 , -space lacks the necessary properties of the semantic latent space in diffusion models that will be described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">h-space</head><p>Note that ? t is implemented as U-Net in all state-of-the-art diffusion models. We choose its bottleneck, the deepest feature maps h t , to control ? t . By design, h t has smaller spatial resolutions and high-level semantics than ? t . Accordingly, the sampling equation becomes</p><formula xml:id="formula_9">x t-1 = ? ? t-1 P t ( ? t (x t |?h t )) + D t ( ? t (x t )) + ? t z t ,<label>(8)</label></formula><p>where ? t (x t |?h t ) adds ?h t to the original feature maps h t . The ?h t minimizing the same loss in Eq. ( <ref type="formula">7</ref>) with P t ( ? t (x t |?h t )) instead of P t (? ? t (x t )) successfully manipulates the attributes. We observe that h-space in Asyrp has the following properties that others do not have.</p><p>? The same ?h leads to the same effect on different samples.</p><p>? Linearly scaling ?h controls the magnitude of attribute change, even with negative scales.</p><p>? Adding multiple ?h manipulates the corresponding multiple attributes simultaneously.</p><p>? ?h preserves the quality of the resulting images without degradation. ? ?h t is roughly consistent across different timesteps t.</p><p>Above properties are thoroughly demonstrated in ? 5.3. Appendix C.3 provides details of h-space and results with suboptimal choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IMPLICIT NEURAL DIRECTIONS</head><p>Although ?h succeeds in manipulating images, directly optimizing ?h t on multiple timesteps requires many iterations of training with a carefully chosen learning rate and its scheduling. Instead, We choose the intervals by quantifying three measures (top left inset). Editing strength of an interval [T, t] measures its perceptual difference from T until t. We set [T, t] to the interval with the smallest editing strength that synthesizes P t close to x, i.e., LPIPS(x, P t ) = 0.33. Editing flexibility of an interval [t, 0] measures the potential amount of changes after t. Quality deficiency at t measures the amount of noise in x t . We set [t, 0] to handle large quality deficiency (i.e., LPIPS(x, x t ) = 1.2) with small editing flexibility.</p><p>we define an implicit function f t (h t ) which produces ?h t for given h t and t. f t is implemented as a small neural network with two 1 ? 1 convolutions concatenating timestep t. See Appendix D for the details. Accordingly, we optimize the same loss in Eq. ( <ref type="formula">7</ref>) with</p><formula xml:id="formula_10">P edit t = P t ( ? t (x t |f t ))</formula><p>. Learning f t is more robust to learning rate settings and converges faster than learning every ?h t . In addition, as f t learns an implicit function for given timesteps and bottleneck features, it generalizes to unseen timesteps and bottleneck features. The generalization allows us to borrow the accelerated training scheme of DDIM defined on a sub-sequence {x ?i } ?i? <ref type="bibr">[1,S]</ref> where {? i } is a sub-sequence of [1, ..., T ] and S &lt; T . Then, we can use the generative process with a custom sub-sequence {? i } with length S &lt; T through normalization: ? h? = f ? (h ? )S/ S. Therefore, we can use f t trained on any sub-sequence for any length of generative process. See Appendix E for details. We use f t to get ?h t for all experiments except Figure <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GENERATIVE PROCESS DESIGN</head><p>This section describes the full editing process which consists of three phases: editing with Asyrp, traditional denoising, and quality boosting. We design formulas to determine the length of each phase with quantifiable measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EDITING PROCESS WITH ASYRP</head><p>Diffusion models generate high-level context in the early stage and imperceptible fine details in the later stage <ref type="bibr" target="#b3">(Choi et al., 2022)</ref>. Likewise, we modify the generative process in the early stage to achieve semantic changes. We refer to the early stage as the editing interval [T, t edit ].</p><p>LPIPS(x, P T ) and LPIPS(x, P t ) compute the perceptual distance between the original image and the predicted original image at timesteps T and t, respectively. Intuitively, the high-level content is already determined by the predicted terms at the respective timesteps and LPIPS measures the remaining component to be edited through the remaining reverse process. Consequently, we define editing strength of an interval [T, t]:</p><formula xml:id="formula_11">? t = LPIPS(x, P T ) -LPIPS(x, P t )</formula><p>indicating the perceptual change from timestep T to t in the original generative process. Figure <ref type="figure" target="#fig_2">3</ref> illustrates LPIPS(x, ?) for P t and x t with examples and the inset depicts editing strength. The shorter editing interval has the lower ? t , and the longer editing interval brings more changes in the resulting images. We seek the shortest editing interval which will bring enough distinguishable changes in the images in general. We empirically find that t edit with LPIPS(x, P t edit ) = 0.33 builds the shortest editing interval with enough editing strength as P t edit has nearly all visual attributes in x.</p><p>However, some attributes require more visual changes than others, e.g., pixar &gt; smile. For such attributes, we increase the editing strength ? t by ? = 0.33d(E T (y source ), E T (y target )) where E T (?) produces CLIP text embedding, y (?) denotes the descriptions, and d(?, ?) computes the cosine distance between the arguments. Choosing t edit with LPIPS(x, P t edit ) = 0.33 -? expands the editing interval to a suitable length. It consistently produces good results in various settings. Supporting experiments are shown in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">QUALITY BOOSTING WITH STOCHASTIC NOISE INJECTION</head><p>Although DDIM achieves nearly perfect inversion by removing stochasticity (? = 0), <ref type="bibr" target="#b13">Karras et al. (2022)</ref> demonstrate that the stochasticity improves the image quality. Likewise, we inject stochastic noise in the boosting interval [t boost , 0].</p><p>Though the longer boosting interval would achieve the higher quality, boosting over excessively long intervals would modify the content. Hence, we want to determine the shortest interval that shows enough quality boosting to guarantee minimal change in the content. We consider the noise in the image as the capacity for the quality boosting and define quality deficiency at t: ? t = LPIPS(x, x t ) indicating the amount of noise on x t compared to the original image. We use x t instead of P t because we consider the actual image rather than the semantics. Figure <ref type="figure" target="#fig_2">3</ref> inset depicts editing flexibility and quality deficiency. We empirically find that t boost with ? t boost = 1.2 achieves quality boosting with minimal content change. We confirmed that the editing strength of the intervals [t boost , 0] is guaranteed to be less than 0.25. In Figure <ref type="figure" target="#fig_2">3</ref>, after t boost , LPIPS(x, x t ) sharply drops in the original generative process while LPIPS(x, P t ) changes little. Note that most of the quality degradation of resulting images is caused by DDIM reverse process, not by Asyrp. We use this quality boosting for all experiments except the ablation in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OVERALL PROCESS OF IMAGE EDITING</head><p>Using t edit and t boost determined by the above formulas, we modify the generative process of DDIM with</p><formula xml:id="formula_12">p (t) ? (x t-1 | x t ) = ? ? ? N ? ? t-1 P t ( ? t (x t |f t )) + D t , ? 2 t I , ? = 0 if T ? t ? t edit N ? ? t-1 P t ( ? t (x t )) + D t , ? 2 t I , ? = 0 if t edit &gt; t ? t boost N ? ? t-1 P t ( ? t (x t )) + D t , ? 2 t I , ? = 1 if t boost &gt; t<label>(9)</label></formula><p>The visual overview and comprehensive algorithms of the entire process are in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we show the effectiveness of semantic latent editing in h-space with Asyrp on various attributes, datasets and architectures in ? 5.1. Moreover, we provide quantitative results including user study in ? 5.2. Lastly, we provide detailed analyses for the properties of the semantic latent space on h-space and alternatives in ? 5.3.</p><p>Implementation details. We implement our method on various settings: CelebA-HQ <ref type="bibr" target="#b11">(Karras et al., 2018)</ref> and LSUN-bedroom/-church <ref type="bibr" target="#b38">(Yu et al., 2015)</ref> on DDPM++ <ref type="bibr">(Song et al., 2020b</ref>) <ref type="bibr" target="#b21">(Meng et al., 2021)</ref>; AFHQ-dog <ref type="bibr" target="#b4">(Choi et al., 2020)</ref> on iDDPM <ref type="bibr">(Nichol &amp; Dhariwal, 2021)</ref>; and METFACES <ref type="bibr" target="#b12">(Karras et al., 2020)</ref> on ADM with P2-weighting <ref type="bibr">(Dhariwal &amp; Nichol, 2021</ref>) <ref type="bibr" target="#b3">(Choi et al., 2022)</ref>.</p><p>Please note that all models are official pretrained checkpoints and are kept frozen. Detailed settings including the coefficients for ? CLIP and ? recon , and source/target descriptions can be found in Appendix I.1. We train f t with S = 40 for 1 epoch using 1000 samples. The real samples are randomly chosen from each dataset for in-domain-like attributes. For out-of-domain-like attributes, we randomly draw 1,000 latent variables x T ? N (0, I).  training. We set S = 1, 000 for inference. The code is attached to the supplementary materials and will be available online with publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">VERSATILITY OF h-space WITH ASYRP</head><p>Figure <ref type="figure">4</ref> shows the effectiveness of our method on various datasets and any existing U-Net based architectures. Our method can synthesize the attributes that are not even included in the training  . Note that we did not train f t for the negative direction.</p><p>dataset, such as church -? {department, factory, and temple}. Even for dogs, our method synthesizes smiling Poodle and Yorkshire, the species that barely smile in the dataset. Figure <ref type="figure">5</ref> provides results for changing human faces to different identities, painting styles, and ancient primates. More result can be found in Appendix M. Versatility of our method is surprising because we do not alter the models but only shift the bottleneck feature maps in h-space with Asyrp during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CelebA-HQ in-domain</head><p>CelebA Considering that our method can be combined with various diffusion models without finetuning, we do not find such a versatile competitor. Nonetheless, we compare Asyrp against DiffusionCLIP using the official code which edits real images by finetuning the whole model. We asked 80 participants to choose the images with better quality, natural attribute change, and overall preference for given total of 40 sets of original images, ours, and DiffusionCLIP. Table <ref type="table" target="#tab_1">1</ref> shows that Asyrp outperforms Dif-fusionCLIP in the all perspectives including the attributes unseen in the training dataset. We list the settings for fair comparison including the questions and example images in Appendix J.1. See ? J.2 for more evaluation metrics: segmentation-consistency(SC) and directional CLIP similarity(S dir ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ANALYSIS ON h-space</head><p>We provide detailed analyses to validate the properties of semantic latent space for diffusion models: homogeneity, linearity, robustness, and consistency across timesteps.</p><p>Homogeneity. Figure <ref type="figure">6</ref> illustrates homogeneity of h-space compared to -space. One ?h t optimized for an image results in the same attribute change to other input images. On the other hand, Linearity. In Figure <ref type="figure">7</ref>, we observe that linearly scaling a ?h reflects the amount of change in the visual attributes. Surprisingly, it generalizes to negative scales which are not seen during training. Moreover, Figure <ref type="figure">8</ref> shows that combinations of different ?h's yield their combined semantic changes in the resulting images. Appendix M.1 provides mixed interpolation between multiple attributes.</p><p>Robustness. Figure <ref type="figure">9</ref> compares effect of adding random noise in h-space and -space. The random noises are chosen to be the vectors with random directions and magnitude of the example ?h t and ? t in Figure <ref type="figure">6</ref> on each space. Perturbation in h-space leads to realistic images with a minimal difference or some semantic changes. On the contrary, perturbation in -space severely distorts the resulting images. See Appendix C.2 for more analyses.</p><p>Rough consistency across timesteps. Recall that ?h t for all samples are homogeneous and replacing them by their mean ?h mean t yields similar results. Interestingly, in Figure <ref type="figure" target="#fig_0">10</ref>, we observe that adding a time-invariant ?h global = 1 Te t ?h mean t instead of ?h t also yields the similar results where T e denotes the length of the editing interval [T, t edit ]. Though we choose to use ?h t to deliver the best quality and manipulation, using ?h mean t or even ?h global with some compromise would be worth trying for simplicity. We report more detail about mean and global direction in Appendix K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed a new generative process, Asyrp, which facilitates image editing in a semantic latent space h-space for pretrained diffusion models. h-space has nice properties as in the latent space of GANs: homogeneity, linearity, robustness, and consistency across timesteps. The full editing process is designed to achieve versatile editing and high quality according to a quantitative measure of editing strength and quality deficiency at timesteps. We hope our approach and detailed analyses help cultivate a new paradigm of image editing on semantic latent space of diffusion models. Combining previous finetuning or guidance techniques would be an interesting research direction.</p><p>Limitations. Editing with Asyrp seldom yields changes in overall style or peripheral objects but edits attributes of the main object. Style transfer using frozen diffusion models is our future work.</p><p>Societal impact. Techniques for high-quality image manipulation such as Asyrp should be accompanied by social and/or technical solutions to prevent abuse. <ref type="bibr" target="#b39">Y?ksel et al. (2021)</ref>, <ref type="bibr" target="#b24">Patashnik et al. (2021)</ref>, <ref type="bibr" target="#b7">Gal et al. (2021)</ref>, <ref type="bibr" target="#b5">Dai et al. (2019)</ref>, <ref type="bibr" target="#b36">Xu et al. (2022)</ref>). However they have to conduct 'inversion' to their latent space for real image editing and 'GAN inversion' is often challenging and produces unexpected appearance changes.</p><p>On the contrary, Asyrp enables to use latent space of real images by nearly perfect easy inversion of DDIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF THEOREM 1</head><p>Proof of Theorem 1. Let ? t be a predicted noise during the original reverse process at t and ? ? t be its shifted counterpart. Then, xt-1</p><formula xml:id="formula_13">? x t-1 where xt-1 = ? ? t-1 P t (? ? t (x t )) + D t (? ? t (x t )). Define ? ? t (x t ) = ? t (x t ) + ? t . Then, xt-1 = ? ? t-1 P t (? ? t (x t )) + D t (? ? t (x t )) (10) = ? ? t-1 x t - ? 1 -? t ? t (x t ) + ? t ? ? t + 1 -? t-1 -? 2 t ? (?) t (x t ) + ? t (11) = ? ? t-1 P t ( ? t (x t )) + D t ( ? t (x t )) - ? ? t-1 ? 1 -? t ? ? t ? ? t + 1 -? t-1 -? 2 t ? ? t (12) = x t-1 + - ? 1 -? t ? 1 -? t + 1 -? t-1 ? ? t ? ? = 0 (13) = x t-1 + ? ? - ? 1 -? t ? 1 -? t + 1 - t-1 s=1 (1 -? s ) ? 1 -? t ? 1 -? t ? ? ? ? t (14) = x t-1 + ? 1 -? t -? t - ? 1 -? t ? 1 -? t ? ? t ? ? t = t s=1 (1 -? s ) (15) ? x t-1 ? ? t ? 0 (16) C ADDITIONAL SUPPORTS FOR h-space WITH ASYRP C.1 RANDOM PERTURBATION ON -space WITHOUT ASYRP</formula><p>In ? 3.1, we argue that if both P t and D t are shifted, we can not manipulate x 0 . In Figure <ref type="figure" target="#fig_8">13</ref>, we do not observe the noticeable difference between (a) and (b) which are the result of the original reverse process of DDIM and the one with shifting both terms, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 ROBUSTNESS AND SEMANTICS IN h-space AND -space WITH ASYRP</head><p>In ? 3.2, we also argue that h-space is more robust than -space with Asyrp. In Figure <ref type="figure" target="#fig_8">13</ref>, we observe that small random noise z ? N (0, I) in -space degrades the resulting image without semantic changes (c) and much larger random noise in h-space yields random semantic changes without severe artifacts (d).</p><p>Note that diffusion models are designed as latent variable models with learned Gaussian transitions and the reverse process should also be close to Gaussian. Based on the assumption, we manage ? Upper blue line describes applying noise ? t = t + ? t to produce P t (? t ) = the shifted predicted x 0 . However, the shift due to ? t is canceled out by the shift in D t (? t ) due to ? t . As a results, applying ? t both on P t and D t brings identical outputs to the original.</p><p>to follow the original Gaussian distribution as follows. Adding z ? N (0, ?I) to ? t expands the distribution of the predicted noise and may produce distorted images. To preserve the distribution, we scale</p><formula xml:id="formula_14">? ? t = ( ? t + z)/ ? 1 2 + ? 2 .</formula><p>Still, the resulting images are almost identical compared to the original images where ? ? t = ? t + z as shown in Figure <ref type="figure" target="#fig_8">13</ref>. It is no wonder that the scaling does not improve the distorted results since the additive random noise disturbs the denoising operation of the predicted random noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 CHOICE OF h-space IN U-NET</head><p>As shown in Figure <ref type="figure" target="#fig_0">14</ref>, there are many other candidates for h-space in the architecture. Among the layers, we choose the 8th layer, the bridge of the U-Net based architecture. The layer is not influenced by any skip connection, has the smallest spatial dimension with compressed information, and is located just before the upsampling blocks. Thus, we assume that it could possibly be considered as the most suitable latent embedding. To confirm the assumption, we train f t on the other layers. The results are shown in Figure <ref type="figure" target="#fig_0">15</ref>. We carefully tuned the training hyperparameters (? CLIP and ? recon ) for fair comparison. The 1st to the 6th layers hardly bring visible changes. The 7th and 9th layers bring not only the desired changes but also difficulty in finding optimal hyperparameters. After the 9th layer, the results bear severe artifacts. Figure <ref type="figure" target="#fig_0">18</ref> shows the results according to t edit . If t edit is too high, the length of the editing process becomes too short resulting in insufficient changes. On the contrary, too low t edit causes excessively unnecessary manipulation from the long editing process.</p><p>We observe that t edit is one of the important hyperparameters. We argue that the formula for choosing t edit using editing strength is reasonable because it applies to all five different datasets despite its sensitivity, even though the choice of LPIPS = 0.33 is empirical.</p><p>Additionally, these results imply why we need to use sufficiently low t edit in the unseen domains.</p><p>Editing interval with insufficient editing strength struggles to escape from the training domain.</p><p>Figure <ref type="figure" target="#fig_0">18</ref>: Importance of choosing proper t edit . We explore various t edit with smiling. Too short editing interval struggles to manipulate attributes. Excessive editing strength results in degraded images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G QUALITY BOOSTING</head><p>We validate the effectiveness of our quality boosting ( ? 4.2) in the original DDIM process and in Asyrp.</p><p>Figure <ref type="figure" target="#fig_13">20</ref> shows the effect of quality boosting with the original DDIM reverse process. Although the reverse process of DDIM has a nearly-perfect inversion property, we observe some noise by zooming in. Our quality boosting improves the quality of a sample and concurrently keeps nearly perfect inversion property. We observe that t boost is not sensitive, but the larger interval brings the less preservation.</p><p>Figure <ref type="figure" target="#fig_14">21</ref> shows quality improvements by our quality boosting in Asyrp.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ALGORITHM</head><formula xml:id="formula_15">T ? N (0, I) instead of x T = q(x 0 ) ? T t=1 q (x t | x t-1</formula><p>) where x 0 ? p data (x). It allows us to train Asyrp only with the pretrained network and without extra dataset. Using random samples has tradeoff between preservation of contents and possible amount in editing. It take advantages when a target attribute requires large amount changes. We assume that the inversion of real-image is in the long tail of a Gaussian distribution because of realistic background or detailed clothes. On the other hand, random noise is considered to be closer to the mean of the normal, so it is easier to find directions. It can easily bring larger changes but also easily alter the contents. On the contrary, training with inversion shows the opposite property.   Algorithm 1: Editing(Inference)</p><p>Input: x 0 (Input image), {f i t } M i=1 (M Neural implicit functions for M attributes), {c i } M i=1 (user defined M scaling coefficients for M attributes), ? (frozen pretrained model), S f or (# of inversion steps), S gen (# of inference steps), t edit (computed from ? 4.1), t boost (computed from ? 4.2) </p><formula xml:id="formula_16">1 Function Editor(x 0 , ? , {c i } M i=1 , S f or , S gen , * ): // step 1: Semantic encoding 2 Define {? s } S f or s=1 s.t ? 1 = 0, ? S f or = T 3 for s = 1, 2, ..., S f or -1 do 4 ? -? (x ?s , ? s ) 5 x ?s+1 = ? ? ?s x ?s + ? 1 -? ?s //</formula><formula xml:id="formula_17">( M i=1 c i f i ? s (h ? s )) ? = ? (x ? s |?h ? s ) = ? (x ? s ) ? ? s = 0 // phase 2: denoising else if s ? S noise then ? = = ? (x ? s ) ? ? s = 0 // phase 3: quality boosting else ? = = ? (x ? s ) ? ? s = (1 -? ? s -1 ) / (1 -? ? s ) 1 -? ? s /? ? s-1 z ? N (0, 1) x?s-1 = ? ? ? s-1 ( x?s - ? 1-?? s ? ? ?? s ) + 1 -? ? s-1 -? 2 ? s + ? ? s z return x0 (manipulated image)</formula><p>For a fair comparison, we use official checkpoints of DiffusionCLIP and provide scores of the attributes (tanned, red brick) following <ref type="bibr" target="#b14">Kim &amp; Ye (2021)</ref>. Regarding attributes without the official checkpoints (smiling,sad), we train DiffusionCLIP by ourselves with the official code. We use 100 samples per attribute. Asyrp outperforms DiffusionCLIP on S dir for all attributes. On SC, DiffusionCLIP achieves better or competitive scores. Because DiffusionCLIP manipulates images mostly by focusing on texture or color while preserving structure and shape, it takes advantage of getting higher SC scores. However, in Figure <ref type="figure" target="#fig_1">26</ref>, results of smiling show that it is not proper to edit attributes which require structural manipulation. Note that better SC scores do not guarantee better qualitative performance. Results of Pixar also show the similar tendency of each method. We allow more structural changes than DiffusionCLIP while editing. Lower SC of our method comes from desirable structural changes as shown in Figure <ref type="figure" target="#fig_1">26</ref>. ; P src =</p><formula xml:id="formula_18">S</formula><formula xml:id="formula_19">x (i) ?s - ? 1-?? s ? (x (i) ?s ) ? ?? s x(i) ?s-1 = ? ? ?s-1 P + 1 -? ?s-1 ? (x<label>(i) ?s ) x (i)</label></formula><p>?s-1 = ? ? ?s-1 P src + 1 -? ?s-1 ? (x</p><formula xml:id="formula_20">(i) ?s )</formula><p>L total ? -? CLIP L direction (P, y tar , P src , y src ) + ? recon |P -P src | 18 Take a gradient step on L total and update f t K GLOBAL DIRECTION Figure <ref type="figure" target="#fig_1">27</ref> and Figure <ref type="figure" target="#fig_1">28</ref> show that the effects of mean direction and global direction are quite similar with ?h t by f t in various attributes. We compute mean direction and global direction from 20 different images.</p><p>We argue that h-space is roughly homogeneous across samples and timesteps. However, we observe that h-space is not completely independent to the conditions especially on unseen-domain (See Figure <ref type="figure" target="#fig_1">27</ref>). Note that unseen-domains require a longer editing interval with small t edit . Therefore, we conjecture that the consistency of h-space decreases at the end of the generative process. It is supported by additional experiments that L2 distance between ?h t and global direction gradually increases along with timesteps. We leave a more detailed analysis on h-space at different timesteps as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L RANDOM SAMPLING</head><p>We conduct extra experiments: generating images with target attributes using Asyrp not from inversion but from random Gaussian noises. As a consequence, the generative process can be used for conditional random sampling. We provide the results in Figure <ref type="figure" target="#fig_1">29</ref>. However, it is beyond the scope of this paper.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Manipulation approaches for diffusion models. (a) Image guidance suffers ambiguity while controlling the generative process. (b) Classifier guidance requires an extra classifier, is hardly editable, degrades quality, or alters the content. (c) DiffusionCLIP requires fine-tuning the whole model. (d) Our method discovers a semantic latent space of a frozen diffusion model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Generative process of Asyrp. The green box on the left illustrates Asyrp which only alters P t while preserving D t shared by DDIM. The right describes that Asyrp drifts the original reverse process toward the target attribute reflecting the change in h-space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Intuition for choosing the intervals for editing and quality boosting. We choose the intervals by quantifying three measures (top left inset). Editing strength of an interval [T, t] measures its perceptual difference from T until t. We set [T, t] to the interval with the smallest editing strength that synthesizes P t close to x, i.e., LPIPS(x, P t ) = 0.33. Editing flexibility of an interval [t, 0] measures the potential amount of changes after t. Quality deficiency at t measures the amount of noise in x t . We set [t, 0] to handle large quality deficiency (i.e., LPIPS(x, x t ) = 1.2) with small editing flexibility.</figDesc><graphic url="image-50.png" coords="5,90.31,71.26,442.47,143.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Editing results of Asyrp on various datasets. We conduct experiments on CelebA-HQ, LSUN-church, METFACES, AFHQ-dog, and LSUN-bedroom.Real imageNicolas Cage Neanderthal Modigliani Pixar Frida</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>of optimization (b) Result of adaptation to other images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Optimization on h-space and -space. (a) Optimizing ?h t for a sample with smiling results in natural editing while the change due to optimizing ? t is relatively small. (b) Applying ?h t from (a) to other samples yields the same attribute change while ? t distorts the images. The result of ? t at the end is the best sample we could find.</figDesc><graphic url="image-137.png" coords="8,115.76,207.53,380.33,54.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :Figure 10 :</head><label>8910</label><figDesc>Figure 8: Linear combination. Combining multiple ?hs leads to combined attribute changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Illustration of Theorem 1. Upper blue line describes applying noise ? t = t + ? t to produce P t (? t ) = the shifted predicted x 0 . However, the shift due to ? t is canceled out by the shift in D t (? t ) due to ? t . As a results, applying ? t both on P t and D t brings identical outputs to the original.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: (a) The reconstructed image by the original DDIM inversion process which is almost indistinguishable to the real image. (b) The result from adding random noise z both on P t and D t . (a) looks identical to (b). The insets of (b, c) depict the full SSIM image from (a). It shows that simply shifting ? t without Asyrp does not affect the result. (c) Adding z ? N to -space with Asyrp easily degrades image with little semantic change. (d) Adding z ? N to h-space with Asyrp yields random semantic change without image degradation. (e) Correlation between image degradation and noise strength in the two spaces.</figDesc><graphic url="image-178.png" coords="16,332.70,420.68,183.29,64.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :Figure 15 :Figure 16 :Figure 17 :</head><label>14151617</label><figDesc>Figure14: Location of h-space. The U-Net architecture of diffusion models outputs 256 ? 256 images. Each layer is indexed with a number along the operating sequence of the model. The 8th layer is our h-space which is not directly influenced by a skip connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Average LPIPS(x, P t ) and LPIPS(x, x t ) of 100 samples on all datasets.</figDesc><graphic url="image-248.png" coords="21,71.98,57.91,476.60,222.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 22</head><label>22</label><figDesc>Figure 22 illustrates generative process. Algorithm 1 and 2 describe training algorithm and inference algorithm of Asyrp, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Ablation study of quality boosting on the original DDIM process without Asyrp. Our quality boosting enhances fine details and prevents images from being noisy in the original DDIM process. Please zoom in for detailed comparison.</figDesc><graphic url="image-268.png" coords="22,247.47,398.04,118.56,237.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Ablation study of quality boosting with Asyrp. Our quality boosting enhances fine details and prevents images from being noisy. Note that the source of degradation is DDIM process, not Asyrp, confirmed in Figure 20. Please zoom in for detailed comparison.</figDesc><graphic url="image-269.png" coords="22,128.40,398.04,118.56,237.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 23 :Figure 24 :</head><label>2324</label><figDesc>Figure 23: Asyrp vs. DiffusionCLIP on CelebA-HQ in-domain attributes. We observe that Dif-fusionCLIP struggles to change semantic facial attributes. Their official checkpoints do not exist and we ran careful hyperparameter tuning for training. Real image Asyrp (ours) DiffusionCLIP Real image Asyrp (ours) DiffusionCLIP</figDesc><graphic url="image-307.png" coords="27,324.01,343.15,177.61,59.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Combination of multiple attributes. The result shows that Asyrp works for combined ?h of smiling and young.</figDesc><graphic url="image-362.png" coords="31,189.84,397.98,254.99,254.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Details are described in Appendix I.2. Training takes about 20 minutes with three RTX 3090 GPUs. All images in the figures are not used for the</figDesc><table><row><cell>Real image</cell><cell>Recon.</cell><cell cols="2">Smiling</cell><cell>Sad</cell><cell></cell><cell>Angry</cell><cell>Man?Woman</cell><cell>Young</cell><cell>Curly hair</cell></row><row><cell>Real image</cell><cell>Recon.</cell><cell cols="2">Department</cell><cell cols="2">Factory</cell><cell>Gothic</cell><cell>Red bricks</cell><cell>Temple</cell><cell>Wooden</cell></row><row><cell>Real image</cell><cell>Smiling</cell><cell>Disgusted</cell><cell cols="2">Real image</cell><cell>Sleepy</cell><cell>Happy</cell><cell>Real image</cell><cell>Hotel</cell><cell>Princess</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>User study. with 80 participants. Setting of user study is described in ? J.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-HQ unseen-domain</cell><cell>LSUN-church</cell></row><row><cell></cell><cell cols="3">quality attribute overall</cell><cell>quality attribute overall</cell><cell>quality attribute diversity overall</cell></row><row><cell>Asyrp (ours)</cell><cell cols="4">98.36% 88.13% 94.92% 71.56% 59.84% 63.13% 73.19% 71.81%</cell><cell>87.50% 76.81%</cell></row><row><cell cols="2">DiffusionCLIP 1.64%</cell><cell>11.88%</cell><cell cols="2">5.08% 28.44% 40.16% 36.88% 26.81% 28.19%</cell><cell>12.50% 23.19%</cell></row><row><cell cols="4">5.2 QUANTITATIVE COMPARISON</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Prompts, CLIP similarity, t edit , and t boost for all attributes in the experiments.F.2 EDITING FLEXIBILITY AND t boostTable2shows the prompts, t edit , and t boost . Intuitively, the attributes with larger visual changes have smaller cosine similarity and require longer editing interval. Figure19shows the average LPIPS(x, P t ) and LPIPS(x, x t ) of 100 samples on all datasets. Note that t edit and t boost differ across datasets and attributes, and they are chosen by the formulas in ? 4.</figDesc><table><row><cell>Dataset</cell><cell>src txt</cell><cell>trg txt</cell><cell cols="2">CLIP similarity t edit t boost</cell></row><row><cell></cell><cell>Person</cell><cell>Young person</cell><cell>0.905</cell><cell>515</cell></row><row><cell></cell><cell>Face</cell><cell>Smiling face</cell><cell>0.899</cell><cell>513</cell></row><row><cell></cell><cell>Face</cell><cell>Sad face</cell><cell>0.894</cell><cell>513</cell></row><row><cell></cell><cell>Face</cell><cell>Angry face</cell><cell>0.892</cell><cell>512</cell></row><row><cell></cell><cell>Face</cell><cell>Tanned face</cell><cell>0.886</cell><cell>512</cell></row><row><cell></cell><cell>Face</cell><cell>Disgusted face</cell><cell>0.880</cell><cell>511</cell></row><row><cell></cell><cell>Person</cell><cell>Person with makeup</cell><cell>0.875</cell><cell>509</cell></row><row><cell></cell><cell>Human</cell><cell>Zombie</cell><cell>0.868</cell><cell>506</cell></row><row><cell></cell><cell>Person</cell><cell>Person with bald head</cell><cell>0.861</cell><cell>505</cell></row><row><cell>CelebA-HQ</cell><cell>Person Human</cell><cell>Person with curly hair Neanderthal</cell><cell>0.835 0.802</cell><cell>499 490</cell></row><row><cell></cell><cell>Person</cell><cell>Mark Zuckerberg</cell><cell>0.797</cell><cell>489</cell></row><row><cell></cell><cell>Person</cell><cell>Nicolas Cage</cell><cell>0.710</cell><cell>461</cell></row><row><cell></cell><cell>Human</cell><cell>Painting in the style of Pixar</cell><cell>0.667</cell><cell>446</cell></row><row><cell></cell><cell>Photo</cell><cell>Painting in Modigliani style</cell><cell>0.565</cell><cell>403</cell></row><row><cell></cell><cell>Photo</cell><cell>Self-portrait by Frida Kahlo</cell><cell>0.443</cell><cell>321</cell></row><row><cell></cell><cell>Church</cell><cell>Gothic Church</cell><cell>0.912</cell><cell>371</cell></row><row><cell></cell><cell>Church</cell><cell>Temple</cell><cell>0.898</cell><cell>367</cell></row><row><cell></cell><cell>Church</cell><cell>Department store</cell><cell>0.841</cell><cell>349</cell></row><row><cell>LSUN-church</cell><cell>Church Church</cell><cell>Wooden House Ancient traditional Asian tower</cell><cell>0.793 0.784</cell><cell>333 330</cell></row><row><cell></cell><cell>Church</cell><cell>Red brick wall Church</cell><cell>0.774</cell><cell>326</cell></row><row><cell></cell><cell>Church</cell><cell>Factory</cell><cell>0.702</cell><cell>301</cell></row><row><cell>LSUN-bedroom</cell><cell>Bedroom Bedroom</cell><cell>Princess Bedroom Hotel Bedroom</cell><cell>0.912 0.917</cell><cell>370 371</cell></row><row><cell></cell><cell>Dog</cell><cell>Happy Dog</cell><cell>0.883</cell><cell>430</cell></row><row><cell></cell><cell>Dog</cell><cell>Sleepy Dog</cell><cell>0.866</cell><cell>422</cell></row><row><cell>AFHQ</cell><cell>Dog Dog</cell><cell>Angry Dog Wolf</cell><cell>0.860 0.850</cell><cell>419 412</cell></row><row><cell></cell><cell>Dog</cell><cell>Yorkshire Terrier</cell><cell>0.690</cell><cell>302</cell></row><row><cell></cell><cell>Painting of a person</cell><cell>Painting of a Sad person</cell><cell>0.921</cell><cell>330</cell></row><row><cell>METFACES</cell><cell cols="2">Painting of a person Painting of a person Painting of a Disgusted person Painting of a Smiling person</cell><cell>0.908 0.879</cell><cell>320 291</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>reports loss coefficients for each attribute. ? CLIP s near 0.8 are suitable for most in-domain attributes. For unseen domains, higher coefficient leads to more noticeable changes.</figDesc><table><row><cell>I.2 TRAINING WITH RANDOM SAMPLING INSTEAD OF THE TRAINING DATASETS</cell></row><row><cell>Apparently, for training, inverting real-images can be replaced by random sampling. It refers to using</cell></row><row><cell>x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Sgens=1 s.t ? 1 = 0, ? S edit = t edit , ? Snoise = t boost and ? Sgen = T</figDesc><table><row><cell></cell><cell>step 2: Manipulation</cell></row><row><cell>6</cell><cell>Define {? s }</cell></row></table><note><p>7 x? Sgen = x ?S f or 8 for s = S gen , S gen -1, ..., 2 do // phase 1: editing 9 if s ? S edit then Extract feature map h ? s from ? (x ? s ) ?h ? s = S f or Sgen</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Extract feature map h ? s from ? (x</figDesc><table><row><cell>(i) ? s )</cell><cell></cell><cell></cell><cell></cell></row><row><cell>=</cell><cell>?I, ?T ?I ?T</cell><cell>,</cell><cell>(17)</cell></row></table><note><p><p>dir (x gen , y tar ; x ref , y ref ) :</p>13 ?h ?s = f ?s (h ?s ) 14 P = x(i) ?s -? 1-?? s ? (x (i) ?s |?h? s ) ? ?? s</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quantitative evaluation on LSUN-church.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RELATED WORK</head><p>After <ref type="bibr" target="#b31">Sohl-Dickstein et al. (2015)</ref>, denoising diffusion probabilistic models (DDPMs) provide a universal approach for generative modeling <ref type="bibr" target="#b10">(Ho et al., 2020)</ref>. On the other hands, <ref type="bibr">Song et al. (2020b)</ref> suggests score-based model and unifies SDEs incorporating diffusion models with scorebased model. Subsequent works renovate diffusion models by focusing on architectures, scheduling, weighting, and fast sampling <ref type="bibr">(Nichol &amp; Dhariwal (2021)</ref>, <ref type="bibr" target="#b13">Karras et al. (2022)</ref>, <ref type="bibr" target="#b3">Choi et al. (2022)</ref>, <ref type="bibr">Song et al. (2020a)</ref>, <ref type="bibr" target="#b35">Watson et al. (2022)</ref>). They mainly consider random generation rather than controlled generation.</p><p>In the mean time, <ref type="bibr">Dhariwal &amp; Nichol (2021)</ref> introduces classifier guidance not only improving quality of images but also retrieving specific class of images. Since it can apply any guidance, its variants have emerged <ref type="bibr" target="#b29">(Sehwag et al. (2022)</ref>, <ref type="bibr" target="#b0">Avrahami et al. (2022)</ref>, <ref type="bibr" target="#b18">Liu et al. (2021)</ref>, <ref type="bibr" target="#b22">Nichol et al. (2021)</ref>). However it requires noise-dependent classifier (or any off-the-shelf models) and additional cost to compute gradients for the guidance during its sampling process. The other works try to control the generative process using image-space guidance <ref type="bibr" target="#b2">(Choi et al. (2021)</ref>, <ref type="bibr" target="#b21">Meng et al. (2021)</ref>, <ref type="bibr" target="#b20">Lugmayr et al. (2022)</ref>, <ref type="bibr" target="#b0">Avrahami et al. (2022)</ref>). They manipulate resulting images by matching noisy images with target images during the reverse process. Still, it is hard to expect delicate control of the reverse process from the image guidance. Furthermore, <ref type="bibr" target="#b25">Preechakul et al. (2022)</ref> introduces an extra encoder which encodes the semantic features of a real image in order to condition the generative process. Although the semantics allow one to control diffusion models, it requires additional training from scratch with the encoder and inherently can not use the other pretrained diffusion models.</p><p>For controllability, <ref type="bibr" target="#b27">Rombach et al. (2022)</ref> and <ref type="bibr" target="#b34">Vahdat et al. (2021)</ref> apply another approach which adapts VAE (Kingma &amp; Welling, 2013) and autoencoder <ref type="bibr" target="#b28">(Rumelhart et al., 1985)</ref> to diffusion models. In spite of their great success in editing, their diffusion models learn the distribution of the learned embeddings in VAE or autoencoder, not the images. <ref type="bibr" target="#b14">Kim &amp; Ye (2021)</ref> proposes another strategy: fine-tuning a whole diffusion model for image editing. It shows valid performance but it requires each fine-tuned model corresponding each attribute.</p><p>In comparison, Asyrp enables outstanding manipulation without high computation, specifically designed architectures, or fine-tuning whole models. We train f t with random sampling for attributes whose identity preservation is not important to take advantage of these properties. The rightmost column in Table <ref type="table">3</ref> shows the choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J EVALUATION J.1 USER STUDY</head><p>We conduct user study to compare the performance of Asyrp and DiffusionCLIP <ref type="bibr" target="#b14">(Kim &amp; Ye, 2021)</ref> on Celeba-HQ <ref type="bibr" target="#b19">(Liu et al., 2015)</ref> and LSUN-church <ref type="bibr" target="#b38">(Yu et al., 2015)</ref>. We use official checkpoints provided by DiffusionCLIP except for some facial attributes whose checkpoint do not exist. We tried our best to tune their hyperparameters following the manual for fair comparison. Example images are shown in Figure <ref type="figure">23</ref>-25.</p><p>We use smiling and sad for in-domain CelebA-HQ attributes, Pixar and Neanderthal for unseen-domain CelebA-HQ attributes, and department store, ancient, and wooden for LSUN-church.</p><p>In unseen domain and Lsun-church, we use official checkpoints provided by DiffusionCLIP. We also randomly select 8 images for each CelebA-HQ attribute and 12 images for each LSUN-church attribute.</p><p>We observe that DiffusionCLIP works better in changing the holistic style of images. At the same time, it is short of the ability to bring semantic changes and suffers noisy results and a lack of diversity. The problems would be caused by fine-tuning the whole diffusion model.</p><p>We use the following questions for the survey. 1) Quality: Which image quality do you think is better? (clear and less noisy) 2) Attribute: Which image do you think is "Attribute(e.g., Smiling) naturally"? 3) Overall: Which image do you think is better considering the above evaluation criteria?</p><p>As for LSUN-church, we provide a set of four images at once and add a question: 3) Diversity: Which group do you think has a more diverse style? 4) Overall: Which image do you think is better considering the above evaluation criteria?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 SEMANTIC CONSISTENCY AND DIRECTIONAL CLIP SIMILARITY</head><p>We compare Asyrp and DiffusionCLIP using directional CLIP similarity (S dir ) and segmentationconsistency (SC) following the protocols in DiffusionCLIP. A pretrained CLIP <ref type="bibr" target="#b26">(Radford et al., 2021)</ref> and segmentation models <ref type="bibr" target="#b37">(Yu et al. (2018)</ref>; <ref type="bibr" target="#b41">Zhou et al. (2019;</ref><ref type="bibr" target="#b40">2017)</ref>; Lee et al. ( <ref type="formula">2020</ref>)) are used to compute S dir and SC, respectively. We choose three attributes (smiling, sad, tanned) for CelebA-HQ-in-domain, two attributes (Pixar,Neaderthal) for CelebA-HQ-unseen-domain and three attributes (department store, ancient, red brick) for LSUN-church.   Table <ref type="table">3</ref>: The coefficients range from 0.5 to 0.8 for in-domain attributes. Unseen domains need slightly stronger ? CLIP s. We also report which attributes we train with random noise sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METFACES</head><p>The criterion is which attributes require relatively less maintenance of identity.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M MORE SAMPLES M.1 MULTI-INTERPOLATION</head><p>Figure <ref type="figure">30</ref> provides mixed interpolation between multiple attributes. We observe that any interpolation with any attribute is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.2 MORE RESULTS ON ALL DATASETS</head><p>We provide more results on CelebA-HQ (Figure <ref type="figure">31</ref>), LSUN-church (Figure <ref type="figure">32</ref>), AFHQ, LSUNbedroom, METFACES (Figure <ref type="figure">33</ref>).  Although we do not focus on these results, Asyrp can be used for conditional sampling.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Blended diffusion for text-driven editing of natural images</title>
		<author>
			<persName><forename type="first">Omri</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18208" to="18218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Image-based clip-guided essence transfer</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roni</forename><surname>Paiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12427</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghyun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjune</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><surname>Ilvr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02938</idno>
		<title level="m">Conditioning method for denoising diffusion probabilistic models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perception prioritized training of diffusion models</title>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaehun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11472" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianze</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05621</idno>
		<title level="m">Style transformer: Unpaired text style transfer without disentangled latent representation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Rinon</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00946</idno>
		<title level="m">Stylegan-nada: Clipguided domain adaptation of image generators</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ganspace: Discovering interpretable gan controls</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9841" to="9850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Elucidating the design space of diffusionbased generative models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00364</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Diffusionclip: Text-guided image manipulation using diffusion models</title>
		<author>
			<persName><forename type="first">Gwanghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5549" to="5558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Editgan: High-precision semantic image editing</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16331" to="16345" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">More control for free! image synthesis with semantic diffusion guidance</title>
		<author>
			<persName><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Chopikyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05744</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Repaint: Inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11461" to="11471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">Sdedit: Image synthesis and editing with stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Styleclip: Textdriven manipulation of stylegan imagery</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion autoencoders: Toward a meaningful and decodable representation</title>
		<author>
			<persName><forename type="first">Konpat</forename><surname>Preechakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nattanat</forename><surname>Chatthee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suttisak</forename><surname>Wizadwongsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10619" to="10629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating high fidelity data from low-density regions using diffusion models</title>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firat</forename><surname>Ozgenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Canton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11492" to="11501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpreting the disentangled face representation learned by gans</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Interfacegan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11287" to="11302" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning fast samplers for diffusion models by differentiating through sample quality</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=VFBjuF8HEp" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Luc Van Gool, and Errui Ding. Predict, prevent, and evaluate: Disentangled text-driven image manipulation empowered by pre-trained vision-language model</title>
		<author>
			<persName><forename type="first">Zipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18229" to="18238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Latentclr: A contrastive learning approach for unsupervised discovery of interpretable directions</title>
		<author>
			<persName><forename type="first">Enis</forename><surname>Oguz Kaan Y?ksel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ezgi</forename><surname>Simsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinar</forename><surname>G?lperi Er</surname></persName>
		</author>
		<author>
			<persName><surname>Yanardag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14263" to="14272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
